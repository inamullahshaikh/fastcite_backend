{
  "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
  "total_chunks": 294,
  "chunks": [
    {
      "chunk_id": "2e62f813-08cb-4138-998f-ebdf07289f64",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Title Page",
      "original_titles": [
        "Title Page",
        "Copyright Page"
      ],
      "path": "Title Page",
      "start_page": 2,
      "end_page": 17,
      "token_count": 5883,
      "text": "James F. Kurose\nUniversity of Massachusetts, Amherst\nKeith W. Ross\nPolytechnic Institute of NYU\nCOMPUTER\nNETWORKING\nA Top-Down Approach\nSIXTH EDITION\nBoston  Columbus  Indianapolis  New York  San Francisco  Upper Saddle River\nAmsterdam\nCape Town  Dubai  London  Madrid  Milan  Munich  Paris  Montréal  Toronto\nDelhi  Mexico City  São Paulo  Sydney  Hong Kong  Seoul  Singapore  Taipei  Tokyo\nVice President and Editorial Director, ECS:\nMarcia Horton\nEditor in Chief: Michael Hirsch\nEditorial Assistant: Emma Snider\nVice President Marketing: Patrice Jones\nMarketing Manager: Yez Alayan\nMarketing Coordinator: Kathryn Ferranti\nVice President and Director of Production: \nVince O’Brien\nManaging Editor: Jeff Holcomb\nSenior Production Project Manager: \nMarilyn Lloyd\nManufacturing Manager: Nick Sklitsis\nOperations Specialist: Lisa McDowell\nArt Director, Cover: Anthony Gemmellaro\nArt Coordinator: Janet Theurer/\nTheurer Briggs Design\nArt Studio: Patrice Rossi Calkin/\nRossi Illustration and Design\nCover Designer: Liz Harasymcuk \nText Designer: Joyce Cosentino Wells\nCover Image: ©Fancy/Alamy\nMedia Editor: Dan Sandin\nFull-Service Vendor: PreMediaGlobal\nSenior Project Manager: Andrea Stefanowicz\nPrinter/Binder: Edwards Brothers\nCover Printer: Lehigh-Phoenix Color\nCopyright © 2013, 2010, 2008, 2005, 2003 by Pearson Education, Inc., publishing as\nAddison-Wesley. All rights reserved. Manufactured in the United States of America. This\npublication is protected by Copyright, and permission should be obtained from the pub-\nlisher prior to any prohibited reproduction, storage in a retrieval system, or transmission\nin any form or by any means, electronic, mechanical, photocopying, recording, or like-\nwise. To obtain permission(s) to use material from this work, please submit a written\nrequest to Pearson Education, Inc., Permissions Department, One Lake Street, Upper\nSaddle River, New Jersey 07458, or you may fax your request to 201-236-3290.\nMany of the designations by manufacturers and sellers to distinguish their products are\nclaimed as trademarks. Where those designations appear in this book, and the publisher was\naware of a trademark claim, the designations have been printed in initial caps or all caps.\nLibrary of Congress Cataloging-in-Publication Data\nKurose, James F.\nComputer networking : a top-down approach / James F. Kurose, Keith W. Ross.—6th ed.\np. cm.\nIncludes bibliographical references and index.\nISBN-13: 978-0-13-285620-1\nISBN-10: 0-13-285620-4\n1. Internet. 2. Computer networks. I. Ross, Keith W., 1956- II. Title. \nTK5105.875.I57K88 2012\n004.6—dc23\n2011048215\n10  9  8  7  6  5  4  3  2  1\nISBN-13: 978-0-13-285620-1\nISBN-10:\n0-13-285620-4\nThis book was composed in Quark. Basal font is Times. Display font is Berkeley.\n\niii\nAbout the Authors\nJim Kurose\nJim Kurose is a Distinguished University Professor of Computer Science at the\nUniversity of Massachusetts, Amherst.\nDr. Kurose has received a number of recognitions for his educational\nactivities including Outstanding Teacher Awards from the National\nTechnological University (eight times), the University of Massachusetts, and\nthe Northeast Association of Graduate Schools. He received the IEEE Taylor\nBooth Education Medal and was recognized for his leadership of\nMassachusetts’ Commonwealth Information Technology Initiative. He has\nbeen the recipient of a GE Fellowship, an IBM Faculty Development Award,\nand a Lilly Teaching Fellowship.\nDr. Kurose is a former Editor-in-Chief of IEEE Transactions on\nCommunications and of IEEE/ACM Transactions on Networking. He has\nbeen active in the program committees for IEEE Infocom, ACM SIGCOMM,\nACM Internet Measurement Conference, and ACM SIGMETRICS for a\nnumber of years and has served as Technical Program Co-Chair for those\nconferences. He is a Fellow of the IEEE and the ACM. His research interests\ninclude network protocols and architecture, network measurement, sensor\nnetworks, multimedia communication, and modeling and performance\nevaluation. He holds a PhD in Computer Science from Columbia University.\nKeith Ross\nKeith Ross is the Leonard J. Shustek Chair Professor and Head of the Computer\nScience Department at Polytechnic Institute of NYU. Before joining NYU-Poly in\n2003, he was a professor at the University of Pennsylvania (13 years) and a \nprofessor at Eurecom Institute (5 years). He received a B.S.E.E from Tufts\nUniversity, a M.S.E.E. from Columbia University, and a Ph.D. in Computer and\nControl Engineering from The University of Michigan. Keith Ross is also the\nfounder and original CEO of Wimba, which develops online multimedia \napplications for e-learning and was acquired by Blackboard in 2010.\nProfessor Ross’s research interests are in security and privacy, social networks,\npeer-to-peer networking, Internet measurement, video streaming, content distribution\nnetworks, and stochastic modeling. He is an IEEE Fellow, recipient of the Infocom\n2009 Best Paper Award, and recipient of 2011 and 2008 Best Paper Awards\nfor Multimedia Communications (awarded by IEEE Communications Society). He\nhas served on numerous journal editorial boards and conference program commit-\ntees, including IEEE/ACM Transactions on Networking, ACM SIGCOMM, ACM\nCoNext, and ACM Internet Measurement Conference. He also has served as an\nadvisor to the Federal Trade Commission on P2P file sharing.\n\nThis page intentionally left blank \n\nTo Julie and our three precious\nones—Chris, Charlie, and Nina\nJFK\nA big THANKS to my professors, colleagues, \nand students all over the world.\nKWR\n\nThis page intentionally left blank \n\nPreface\nWelcome to the sixth edition of Computer Networking: A Top-Down Approach. Since\nthe publication of the first edition 12 years ago, our book has been adopted for use at\nmany hundreds of colleges and universities, translated into 14 languages, and used\nby over one hundred thousand students and practitioners worldwide. We’ve heard\nfrom many of these readers and have been overwhelmed by the positive response.\nWhat’s New in the Sixth Edition?\nWe think one important reason for this success has been that our book continues to offer\na fresh and timely approach to computer networking instruction. We’ve made changes\nin this sixth edition, but we’ve also kept unchanged what we believe (and the instruc-\ntors and students who have used our book have confirmed) to be the most important\naspects of this book: its top-down approach, its focus on the Internet and a modern\ntreatment of computer networking, its attention to both principles and practice, and its\naccessible style and approach toward learning about computer networking. Neverthe-\nless, the sixth edition has been revised and updated substantially:\n•\nThe Companion Web site has been significantly expanded and enriched to\ninclude VideoNotes and interactive exercises, as discussed later in this Preface.\n•\nIn Chapter 1, the treatment of access networks has been modernized, and the\ndescription of the Internet ISP ecosystem has been substantially revised, account-\ning for the recent emergence of content provider networks, such as Google’s. The\npresentation of packet switching and circuit switching has also been reorganized,\nproviding a more topical rather than historical orientation. \n•\nIn Chapter 2, Python has replaced Java for the presentation of socket program-\nming. While still explicitly exposing the key ideas behind the socket API, Python\ncode is easier to understand for the novice programmer. Moreover, unlike Java,\nPython provides access to raw sockets, enabling students to build a larger variety\nof network applications. Java-based socket programming labs have been\nreplaced with corresponding Python labs, and a new Python-based ICMP Ping\nlab has been added. As always, when material is retired from the book, such as\nJava-based socket programming material, it remains available on the book’s\nCompanion Web site (see following text).\n•\nIn Chapter 3, the presentation of one of the reliable data transfer protocols has\nbeen simplified and a new sidebar on TCP splitting, commonly used to optimize\nthe performance of cloud services, has been added.\n•\nIn Chapter 4, the section on router architectures has been significantly updated,\nreflecting recent developments and practices in the field. Several new integrative\nsidebars involving DNS, BGP, and OSPF are included. \n\n•\nChapter 5 has been reorganized and streamlined, accounting for the ubiquity of\nswitched Ethernet in local area networks and the consequent increased use of\nEthernet in point-to-point scenarios. Also, a new section on data center network-\ning has been added.\n•\nChapter 6 has been updated to reflect recent advances in wireless networks, par-\nticularly cellular data networks and 4G services and architecture. \n•\nChapter 7, which focuses on multimedia networking, has gone through a major\nrevision. The chapter now includes an in-depth discussion of streaming video,\nincluding adaptive streaming, and an entirely new and modernized discussion of\nCDNs. A newly added section describes the Netflix, YouTube, and Kankan video\nstreaming systems. The material that has been removed to make way for these\nnew topics is still available on the Companion Web site. \n•\nChapter 8 now contains an expanded discussion on endpoint authentication.\n•\nSignificant new material involving end-of-chapter problems has been added. As\nwith all previous editions, homework problems have been revised, added, and\nremoved.\nAudience\nThis textbook is for a first course on computer networking. It can be used in both\ncomputer science and electrical engineering departments. In terms of programming\nlanguages, the book assumes only that the student has experience with C, C++, Java,\nor Python (and even then only in a few places). Although this book is more precise\nand analytical than many other introductory computer networking texts, it rarely\nuses any mathematical concepts that are not taught in high school. We have made a\ndeliberate effort to avoid using any advanced calculus, probability, or stochastic\nprocess concepts (although we’ve included some homework problems for students\nwith this advanced background). The book is therefore appropriate for undergradu-\nate courses and for first-year graduate courses. It should also be useful to practition-\ners in the telecommunications industry.\nWhat Is Unique about This Textbook?\nThe subject of computer networking is enormously complex, involving many\nconcepts, protocols, and technologies that are woven together in an intricate\nmanner. To cope with this scope and complexity, many computer networking texts\nare often organized around the “layers” of a network architecture. With a layered\norganization, students can see through the complexity of computer networking—\nthey learn about the distinct concepts and protocols in one part of the architecture\nwhile seeing the big picture of how all parts fit together. From a pedagogical\nperspective, our personal experience has been that such a layered approach\nviii\nPreface\n\nPreface\nix\nindeed works well. Nevertheless, we have found that the traditional approach of\nteaching—bottom up; that is, from the physical layer towards the application\nlayer—is not the best approach for a modern course on computer networking.\nA Top-Down Approach\nOur book broke new ground 12 years ago by treating networking in a top-down\nmanner—that is, by beginning at the application layer and working its way down\ntoward the physical layer. The feedback we received from teachers and students\nalike have confirmed that this top-down approach has many advantages and does\nindeed work well pedagogically. First, it places emphasis on the application layer\n(a “high growth area” in networking). Indeed, many of the recent revolutions in\ncomputer networking—including the Web, peer-to-peer file sharing, and media\nstreaming—have taken place at the application layer. An early emphasis on application-\nlayer issues differs from the approaches taken in most other texts, which have only a\nsmall amount of material on network applications, their requirements, application-layer\nparadigms (e.g., client-server and peer-to-peer), and application programming inter-\nfaces. Second, our experience as instructors (and that of many instructors who have\nused this text) has been that teaching networking applications near the beginning of\nthe course is a powerful motivational tool. Students are thrilled to learn about how\nnetworking applications work—applications such as e-mail and the Web, which most\nstudents use on a daily basis. Once a student understands the applications, the student\ncan then understand the network services needed to support these applications. The\nstudent can then, in turn, examine the various ways in which such services might be\nprovided and implemented in the lower layers. Covering applications early thus pro-\nvides motivation for the remainder of the text.\nThird, a top-down approach enables instructors to introduce network appli-\ncation development at an early stage. Students not only see how popular applica-\ntions and protocols work, but also learn how easy it is to create their own\nnetwork applications and application-level protocols. With the top-down\napproach, students get early exposure to the notions of socket programming, serv-\nice models, and protocols—important concepts that resurface in all subsequent\nlayers. By providing socket programming examples in Python, we highlight the\ncentral ideas without confusing students with complex code. Undergraduates in\nelectrical engineering and computer science should not have difficulty following\nthe Python code.\nAn Internet Focus\nAlthough we dropped the phrase “Featuring the Internet” from the title of this book\nwith the fourth edition, this doesn’t mean that we dropped our focus on the Internet!\nIndeed, nothing could be further from the case! Instead, since the Internet has\nbecome so pervasive, we felt that any networking textbook must have a significant\n\nfocus on the Internet, and thus this phrase was somewhat unnecessary. We continue\nto use the Internet’s architecture and protocols as primary vehicles for studying fun-\ndamental computer networking concepts. Of course, we also include concepts and\nprotocols from other network architectures. But the spotlight is clearly on the Inter-\nnet, a fact reflected in our organizing the book around the Internet’s five-layer archi-\ntecture: the application, transport, network, link, and physical layers.\nAnother benefit of spotlighting the Internet is that most computer science and\nelectrical engineering students are eager to learn about the Internet and its protocols.\nThey know that the Internet has been a revolutionary and disruptive technology and\ncan see that it is profoundly changing our world. Given the enormous relevance of\nthe Internet, students are naturally curious about what is “under the hood.” Thus, it\nis easy for an instructor to get students excited about basic principles when using the\nInternet as the guiding focus.\nTeaching Networking Principles\nTwo of the unique features of the book—its top-down approach and its focus on the\nInternet—have appeared in the titles of our book. If we could have squeezed a third\nphrase into the subtitle, it would have contained the word principles. The field of\nnetworking is now mature enough that a number of fundamentally important issues\ncan be identified. For example, in the transport layer, the fundamental issues include\nreliable communication over an unreliable network layer, connection establishment/\nteardown and handshaking, congestion and flow control, and multiplexing. Two fun-\ndamentally important network-layer issues are determining “good” paths between\ntwo routers and interconnecting a large number of heterogeneous networks. In the\nlink layer, a fundamental problem is sharing a multiple access channel. In network\nsecurity, techniques for providing confidentiality, authentication, and message\nintegrity are all based on cryptographic fundamentals. This text identifies fundamen-\ntal networking issues and studies approaches towards addressing these issues. The\nstudent learning these principles will gain knowledge with a long “shelf life”—long\nafter today’s network standards and protocols have become obsolete, the principles\nthey embody will remain important and relevant. We believe that the combination of\nusing the Internet to get the student’s foot in the door and then emphasizing funda-\nmental issues and solution approaches will allow the student to quickly understand\njust about any networking technology.\nThe Web Site\nEach new copy of this textbook includes six months of access to a Companion Web site\nfor all book readers at http://www.pearsonhighered.com/kurose-ross, which includes:\n•\nInteractive learning material. An important new component of the sixth edition\nis the significantly expanded online and interactive learning material. The\nbook’s Companion Web site now contains VideoNotes—video presentations of\nx\nPreface\n\nPreface\nxi\nimportant topics thoughout the book done by the authors, as well as walk-\nthroughs of solutions to problems similar to those at the end of the chapter.\nWe’ve also added Interactive Exercises that can create (and present solutions\nfor) problems similar to selected end-of-chapter problems. Since students can\ngenerate (and view solutions for) an unlimited number of similar problem\ninstances, they can work until the material is truly mastered. We’ve seeded the\nWeb site with VideoNotes and online problems for chapters 1 through 5 and will\ncontinue to actively add and update this material over time. As in earlier edi-\ntions, the Web site contains the interactive Java applets that animate many key\nnetworking concepts. The site also has interactive quizzes that permit students\nto check their basic understanding of the subject matter. Professors can integrate\nthese interactive features into their lectures or use them as mini labs.\n•\nAdditional technical material. As we have added new material in each edition of\nour book, we’ve had to remove coverage of some existing topics to keep the\nbook at manageable length. For example, to make room for the new material in\nthis edition, we’ve removed material on ATM networks and the RTSP protocol\nfor multimedia. Material that appeared in earlier editions of the text is still of\ninterest, and can be found on the book’s Web site.\n•\nProgramming assignments. The Web site also provides a number of detailed\nprogramming assignments, which include building a multithreaded Web\nserver, building an e-mail client with a GUI interface, programming the sender\nand receiver sides of a reliable data transport protocol, programming a distrib-\nuted routing algorithm, and more.\n•\nWireshark labs. One’s understanding of network protocols can be greatly deep-\nened by seeing them in action. The Web site provides numerous Wireshark\nassignments that enable students to actually observe the sequence of messages\nexchanged between two protocol entities. The Web site includes separate Wire-\nshark labs on HTTP, DNS, TCP, UDP, IP, ICMP, Ethernet, ARP, WiFi, SSL, and\non tracing all protocols involved in satisfying a request to fetch a web page.\nWe’ll continue to add new labs over time.\nPedagogical Features\nWe have each been teaching computer networking for more than 20 years.\nTogether, we bring more than 50 years of teaching experience to this text, during\nwhich time we have taught many thousands of students. We have also been active\nresearchers in computer networking during this time. (In fact, Jim and Keith first\nmet each other as master’s students in a computer networking course taught by\nMischa Schwartz in 1979 at Columbia University.) We think all this gives us a\ngood perspective on where networking has been and where it is likely to go in the\nfuture. Nevertheless, we have resisted temptations to bias the material in this book\n\ntowards our own pet research projects. We figure you can visit our personal Web\nsites if you are interested in our research. Thus, this book is about modern com-\nputer networking—it is about contemporary protocols and technologies as well as\nthe underlying principles behind these protocols and technologies. We also believe\nthat learning (and teaching!) about networking can be fun. A sense of humor, use\nof analogies, and real-world examples in this book will hopefully make this mate-\nrial more fun.\nSupplements for Instructors\nWe provide a complete supplements package to aid instructors in teaching this course.\nThis material can be accessed from Pearson’s Instructor Resource Center\n(http://www.pearsonhighered.com/irc). Visit the Instructor Resource Center or send \ne-mail to computing@aw.com for information about accessing these instructor’s \nsupplements.\n•\nPowerPoint® slides. We provide PowerPoint slides for all nine chapters. The\nslides have been completely updated with this sixth edition. The slides cover\neach chapter in detail. They use graphics and animations (rather than relying\nonly on monotonous text bullets) to make the slides interesting and visually\nappealing. We provide the original PowerPoint slides so you can customize them\nto best suit your own teaching needs. Some of these slides have been contributed\nby other instructors who have taught from our book.\n•\nHomework solutions. We provide a solutions manual for the homework problems\nin the text, programming assignments, and Wireshark labs. As noted earlier, we’ve\nintroduced many new homework problems in the first five chapters of the book.\nChapter Dependencies\nThe first chapter of this text presents a self-contained overview of computer net-\nworking. Introducing many key concepts and terminology, this chapter sets the stage\nfor the rest of the book. All of the other chapters directly depend on this first chap-\nter. After completing Chapter 1, we recommend instructors cover Chapters 2\nthrough 5 in sequence, following our top-down philosophy. Each of these five chap-\nters leverages material from the preceding chapters. After completing the first five\nchapters, the instructor has quite a bit of flexibility. There are no interdependencies\namong the last four chapters, so they can be taught in any order. However, each of\nthe last four chapters depends on the material in the first five chapters. Many\ninstructors first teach the first five chapters and then teach one of the last four chap-\nters for “dessert.”\nxii\nPreface\n\nPreface\nxiii\nOne Final Note: We’d Love to Hear from You\nWe encourage students and instructors to e-mail us with any comments they might\nhave about our book. It’s been wonderful for us to hear from so many instructors\nand students from around the world about our first four editions. We’ve incorporated\nmany of these suggestions into later editions of the book. We also encourage instructors\nto send us new homework problems (and solutions) that would complement the\ncurrent homework problems. We’ll post these on the instructor-only portion of the\nWeb site. We also encourage instructors and students to create new Java applets that\nillustrate the concepts and protocols in this book. If you have an applet that you\nthink would be appropriate for this text, please submit it to us. If the applet (including\nnotation and terminology) is appropriate, we’ll be happy to include it on the text’s\nWeb site, with an appropriate reference to the applet’s authors.\nSo, as the saying goes, “Keep those cards and letters coming!” Seriously,\nplease do continue to send us interesting URLs, point out typos, disagree with\nany of our claims, and tell us what works and what doesn’t work.  Tell us what\nyou think should or shouldn’t be included in the next edition. Send your e-mail\nto kurose@cs.umass.edu and ross@poly.edu.\nAcknowledgments\nSince we began writing this book in 1996, many people have given us invaluable\nhelp and have been influential in shaping our thoughts on how to best organize and\nteach a networking course. We want to say A BIG THANKS to everyone who has\nhelped us from the earliest first drafts of this book, up to this fifth edition. We are also\nvery thankful to the many hundreds of readers from around the world—students, fac-\nulty, practitioners—who have sent us thoughts and comments on earlier editions of\nthe book and suggestions for future editions of the book. Special thanks go out to:\nAl Aho (Columbia University)\nHisham Al-Mubaid (University of Houston-Clear Lake)\nPratima Akkunoor (Arizona State University)\nPaul Amer (University of Delaware)\nShamiul Azom (Arizona State University)\nLichun Bao (University of California at Irvine) \nPaul Barford (University of Wisconsin)\nBobby Bhattacharjee (University of Maryland)\nSteven Bellovin (Columbia University)\nPravin Bhagwat (Wibhu)\nSupratik Bhattacharyya (previously at Sprint)\nErnst Biersack (Eurécom Institute)\n\nShahid Bokhari (University of Engineering & Technology, Lahore)\nJean Bolot (Technicolor Research)\nDaniel Brushteyn (former University of Pennsylvania student)\nKen Calvert (University of Kentucky)\nEvandro Cantu (Federal University of Santa Catarina)\nJeff Case (SNMP Research International)\nJeff Chaltas (Sprint)\nVinton Cerf (Google)\nByung Kyu Choi (Michigan Technological University)\nBram Cohen (BitTorrent, Inc.)\nConstantine Coutras (Pace University)\nJohn Daigle (University of Mississippi)\nEdmundo A. de Souza e Silva (Federal University of Rio de Janeiro)\nPhilippe Decuetos (Eurécom Institute)\nChristophe Diot (Technicolor Research)\nPrithula Dhunghel (Akamai)\nDeborah Estrin (University of California, Los Angeles)\nMichalis Faloutsos (University of California at Riverside)\nWu-chi Feng (Oregon Graduate Institute)\nSally Floyd (ICIR, University of California at Berkeley)\nPaul Francis (Max Planck Institute)\nLixin Gao (University of Massachusetts)\nJJ Garcia-Luna-Aceves (University of California at Santa Cruz)\nMario Gerla (University of California at Los Angeles)\nDavid Goodman (NYU-Poly)\nYang Guo (Alcatel/Lucent Bell Labs)\nTim Griffin (Cambridge University)\nMax Hailperin (Gustavus Adolphus College)\nBruce Harvey (Florida A&M University, Florida State University)\nCarl Hauser (Washington State University)\nRachelle Heller (George Washington University)\nPhillipp Hoschka (INRIA/W3C)\nWen Hsin (Park University)\nAlbert Huang (former University of Pennsylvania student)\nCheng Huang (Microsoft Research)\nEsther A. Hughes (Virginia Commonwealth University)\nVan Jacobson (Xerox PARC)\nPinak Jain (former NYU-Poly student)\nJobin James (University of California at Riverside)\nSugih Jamin (University of Michigan)\nShivkumar Kalyanaraman (IBM Research, India)\nJussi Kangasharju (University of Helsinki)\nSneha Kasera (University of Utah)\nParviz Kermani (formerly of IBM Research)\nxiv\nPreface\n\nPreface\nxv\nHyojin Kim (former University of Pennsylvania student)\nLeonard Kleinrock (University of California at Los Angeles)\nDavid Kotz (Dartmouth College)\nBeshan Kulapala (Arizona State University)\nRakesh Kumar (Bloomberg)\nMiguel A. Labrador (University of South Florida)\nSimon Lam (University of Texas)\nSteve Lai (Ohio State University)\nTom LaPorta (Penn State University)\nTim-Berners Lee (World Wide Web Consortium)\nArnaud Legout (INRIA)\nLee Leitner (Drexel University)\nBrian Levine (University of Massachusetts)\nChunchun Li (former NYU-Poly student)\nYong Liu (NYU-Poly)\nWilliam Liang (former University of Pennsylvania student)\nWillis Marti (Texas A&M University)\nNick McKeown (Stanford University)\nJosh McKinzie (Park University)\nDeep Medhi (University of Missouri, Kansas City)\nBob Metcalfe (International Data Group)\nSue Moon (KAIST)\nJenni Moyer (Comcast)\nErich Nahum (IBM Research)\nChristos Papadopoulos (Colorado Sate University)\nCraig Partridge (BBN Technologies)\nRadia Perlman (Intel)\nJitendra Padhye (Microsoft Research)\nVern Paxson (University of California at Berkeley)\nKevin Phillips (Sprint)\nGeorge Polyzos (Athens University of Economics and Business)\nSriram Rajagopalan (Arizona State University)\nRamachandran Ramjee (Microsoft Research)\nKen Reek (Rochester Institute of Technology)\nMartin Reisslein (Arizona State University)\nJennifer Rexford (Princeton University)\nLeon Reznik (Rochester Institute of Technology)\nPablo Rodrigez (Telefonica)\nSumit Roy (University of Washington)\nAvi Rubin (Johns Hopkins University)\nDan Rubenstein (Columbia University)\nDouglas Salane (John Jay College)\nDespina Saparilla (Cisco Systems)\nJohn Schanz (Comcast)\n\nHenning Schulzrinne (Columbia University)\nMischa Schwartz (Columbia University)\nArdash Sethi (University of Delaware)\nHarish Sethu (Drexel University)\nK. Sam Shanmugan (University of Kansas)\nPrashant Shenoy (University of Massachusetts)\nClay Shields (Georgetown University)\nSubin Shrestra (University of Pennsylvania)\nBojie Shu (former NYU-Poly student)\nMihail L. Sichitiu (NC State University)\nPeter Steenkiste (Carnegie Mellon University)\nTatsuya Suda (University of California at Irvine)\nKin Sun Tam (State University of New York at Albany)\nDon Towsley (University of Massachusetts)\nDavid Turner (California State University, San Bernardino)\nNitin Vaidya (University of Illinois)\nMichele Weigle (Clemson University)\nDavid Wetherall (University of Washington)\nIra Winston (University of Pennsylvania)\nDi Wu (Sun Yat-sen University)\nShirley Wynn (NYU-Poly)\nRaj Yavatkar (Intel)\nYechiam Yemini (Columbia University)\nMing Yu (State University of New York at Binghamton)\nEllen Zegura (Georgia Institute of Technology)\nHonggang Zhang (Suffolk University)\nHui Zhang (Carnegie Mellon University)\nLixia Zhang (University of California at Los Angeles)\nMeng Zhang (former NYU-Poly student)\nShuchun Zhang (former University of Pennsylvania student)\nXiaodong Zhang (Ohio State University)\nZhiLi Zhang (University of Minnesota)\nPhil Zimmermann (independent consultant)\nCliff C. Zou (University of Central Florida)\nWe also want to thank the entire Addison-Wesley team—in particular, Michael Hirsch,\nMarilyn Lloyd, and Emma Snider—who have done an absolutely outstanding job on\nthis sixth edition (and who have put up with two very finicky authors who seem con-\ngenitally unable to meet deadlines!). Thanks also to our artists, Janet Theurer and\nPatrice Rossi Calkin, for their work on the beautiful figures in this book, and to Andrea\nStefanowicz and her team at PreMediaGlobal for their wonderful production work on\nthis edition. Finally, a most special thanks go to Michael Hirsch, our editor at Addison-\nWesley, and Susan Hartman, our former editor at Addison-Wesley. This book would\nnot be what it is (and may well not have been at all) without their graceful manage-\nment, constant encouragement, nearly infinite patience, good humor, and perseverance.\nxvi\nPreface"
    },
    {
      "chunk_id": "7f2aad9e-193a-4cb5-a0d1-7a5b45dd2a10",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Table of Contents",
      "original_titles": [
        "Table of Contents"
      ],
      "path": "Table of Contents",
      "start_page": 18,
      "end_page": 27,
      "token_count": 2035,
      "text": "xvii\nTable of Contents\nChapter 1 Computer Networks and the Internet\n1\n1.1\nWhat Is the Internet?\n2\n1.1.1\nA Nuts-and-Bolts Description\n2\n1.1.2\nA Services Description\n5\n1.1.3\nWhat Is a Protocol?\n7\n1.2\nThe Network Edge\n9\n1.2.1\nAccess Networks\n12\n1.2.2\nPhysical Media\n18\n1.3\nThe Network Core\n22\n1.3.1\nPacket Switching\n22\n1.3.2\nCircuit Switching\n27\n1.3.3\nA Network of Networks\n32\n1.4\nDelay, Loss, and Throughput in Packet-Switched Networks\n35\n1.4.1\nOverview of Delay in Packet-Switched Networks\n35\n1.4.2\nQueuing Delay and Packet Loss\n39\n1.4.3\nEnd-to-End Delay\n42\n1.4.4\nThroughput in Computer Networks\n44\n1.5\nProtocol Layers and Their Service Models\n47\n1.5.1\nLayered Architecture\n47\n1.5.2\nEncapsulation\n53\n1.6\nNetworks Under Attack\n55\n1.7\nHistory of Computer Networking and the Internet\n60\n1.7.1\nThe Development of Packet Switching: 1961–1972 \n60\n1.7.2\nProprietary Networks and Internetworking: 1972–1980\n62\n1.7.3\nA Proliferation of Networks: 1980–1990\n63\n1.7.4\nThe Internet Explosion: The 1990s\n64\n1.7.5\nThe New Millennium\n65\n1.8\nSummary\n66\nHomework Problems and Questions\n68\nWireshark Lab\n78\nInterview: Leonard Kleinrock\n80\n\nChapter 2 Application Layer\n83\n2.1\nPrinciples of Network Applications\n84\n2.1.1\nNetwork Application Architectures\n86\n2.1.2\nProcesses Communicating\n88\n2.1.3\nTransport Services Available to Applications\n91\n2.1.4\nTransport Services Provided by the Internet\n93\n2.1.5\nApplication-Layer Protocols\n96\n2.1.6\nNetwork Applications Covered in This Book\n97\n2.2\nThe Web and HTTP\n98\n2.2.1\nOverview of HTTP\n98\n2.2.2\nNon-Persistent and Persistent Connections\n100\n2.2.3\nHTTP Message Format\n103\n2.2.4\nUser-Server Interaction: Cookies\n108\n2.2.5\nWeb Caching\n110\n2.2.6\nThe Conditional GET\n114\n2.3\nFile Transfer: FTP\n116\n2.3.1\nFTP Commands and Replies\n118\n2.4\nElectronic Mail in the Internet\n118\n2.4.1\nSMTP\n121\n2.4.2\nComparison with HTTP\n124\n2.4.3\nMail Message Format\n125\n2.4.4\nMail Access Protocols\n125\n2.5\nDNS—The Internet’s Directory Service\n130\n2.5.1\nServices Provided by DNS\n131\n2.5.2\nOverview of How DNS Works\n133\n2.5.3\nDNS Records and Messages\n139\n2.6\nPeer-to-Peer Applications\n144\n2.6.1\nP2P File Distribution\n145\n2.6.2\nDistributed Hash Tables (DHTs)\n151\n2.7\nSocket Programming: Creating Network Applications\n156\n2.7.1\nSocket Programming with UDP\n157\n2.7.2\nSocket Programming with TCP\n163\n2.8\nSummary\n168\nHomework Problems and Questions\n169\nSocket Programming Assignments\n179\nWireshark Labs: HTTP, DNS\n181\nInterview: Marc Andreessen\n182\nxviii\nTable of Contents\n\nTable of Contents\nxix\nChapter 3 Transport Layer\n185\n3.1\nIntroduction and Transport-Layer Services\n186\n3.1.1\nRelationship Between Transport and Network Layers\n186\n3.1.2\nOverview of the Transport Layer in the Internet\n189\n3.2\nMultiplexing and Demultiplexing\n191\n3.3\nConnectionless Transport: UDP\n198\n3.3.1\nUDP Segment Structure\n202\n3.3.2\nUDP Checksum\n202\n3.4\nPrinciples of Reliable Data Transfer\n204\n3.4.1\nBuilding a Reliable Data Transfer Protocol\n206\n3.4.2\nPipelined Reliable Data Transfer Protocols\n215\n3.4.3\nGo-Back-N (GBN)\n218\n3.4.4\nSelective Repeat (SR)\n223\n3.5\nConnection-Oriented Transport: TCP\n230\n3.5.1\nThe TCP Connection\n231\n3.5.2\nTCP Segment Structure\n233\n3.5.3\nRound-Trip Time Estimation and Timeout\n238\n3.5.4\nReliable Data Transfer\n242\n3.5.5\nFlow Control\n250\n3.5.6\nTCP Connection Management\n252\n3.6\nPrinciples of Congestion Control\n259\n3.6.1\nThe Causes and the Costs of Congestion\n259\n3.6.2\nApproaches to Congestion Control\n265\n3.6.3\nNetwork-Assisted Congestion-Control Example: \nATM ABR Congestion Control\n266\n3.7\nTCP Congestion Control\n269\n3.7.1\nFairness\n279\n3.8\nSummary\n283\nHomework Problems and Questions\n285\nProgramming Assignments\n300\nWireshark Labs: TCP, UDP\n301\nInterview: Van Jacobson\n302\nChapter 4 The Network Layer\n305\n4.1\nIntroduction\n306\n4.1.1\nForwarding and Routing\n308\n4.1.2\nNetwork Service Models\n310\n4.2\nVirtual Circuit and Datagram Networks\n313\n4.2.1\nVirtual-Circuit Networks\n314\n4.2.2\nDatagram Networks\n317\n4.2.3\nOrigins of VC and Datagram Networks\n319\n\n4.3\nWhat’s Inside a Router?\n320\n4.3.1\nInput Processing\n322\n4.3.2\nSwitching\n324\n4.3.3\nOutput Processing\n326\n4.3.4\nWhere Does Queuing Occur?\n327\n4.3.5\nThe Routing Control Plane\n331\n4.4\nThe Internet Protocol (IP): Forwarding and Addressing in the Internet\n331\n4.4.1\nDatagram Format\n332\n4.4.2\nIPv4 Addressing\n338\n4.4.3\nInternet Control Message Protocol (ICMP)\n353\n4.4.4\nIPv6\n356\n4.4.5\nA Brief Foray into IP Security\n362\n4.5\nRouting Algorithms\n363\n4.5.1\nThe Link-State (LS) Routing Algorithm\n366\n4.5.2\nThe Distance-Vector (DV) Routing Algorithm\n371\n4.5.3\nHierarchical Routing\n379\n4.6\nRouting in the Internet\n383\n4.6.1\nIntra-AS Routing in the Internet: RIP\n384\n4.6.2\nIntra-AS Routing in the Internet: OSPF\n388\n4.6.3\nInter-AS Routing: BGP\n390\n4.7\nBroadcast and Multicast Routing\n399\n4.7.1\nBroadcast Routing Algorithms\n400\n4.7.2\nMulticast\n405\n4.8\nSummary\n412\nHomework Problems and Questions\n413\nProgramming Assignments\n429\nWireshark Labs: IP, ICMP\n430\nInterview: Vinton G. Cerf\n431\nChapter 5 The Link Layer: Links, Access Networks, and LANs\n433\n5.1\nIntroduction to the Link Layer\n434\n5.1.1\nThe Services Provided by the Link Layer\n436\n5.1.2\nWhere Is the Link Layer Implemented?\n437\n5.2\nError-Detection and -Correction Techniques\n438\n5.2.1\nParity Checks\n440\n5.2.2\nChecksumming Methods\n442\n5.2.3\nCyclic Redundancy Check (CRC)\n443\n5.3\nMultiple Access Links and Protocols\n445\n5.3.1\nChannel Partitioning Protocols\n448\n5.3.2\nRandom Access Protocols\n449\n5.3.3\nTaking-Turns Protocols\n459\n5.3.4\nDOCSIS: The Link-Layer Protocol for Cable Internet Access\n460\nxx\nTable of Contents\n\nTable of Contents\nxxi\n5.4\nSwitched Local Area Networks\n461\n5.4.1\nLink-Layer Addressing and ARP\n462\n5.4.2\nEthernet\n469\n5.4.3\nLink-Layer Switches\n476\n5.4.4\nVirtual Local Area Networks (VLANs)\n482\n5.5\nLink Virtualization: A Network as a Link Layer\n486\n5.5.1\nMultiprotocol Label Switching (MPLS)\n487\n5.6\nData Center Networking\n490\n5.7\nRetrospective: A Day in the Life of a Web Page Request\n495\n5.7.1\nGetting Started: DHCP, UDP, IP, and Ethernet\n495\n5.7.2\nStill Getting Started: DNS and ARP\n497\n5.7.3\nStill Getting Started: Intra-Domain Routing to the DNS Server\n498\n5.7.4\nWeb Client-Server Interaction: TCP and HTTP\n499\n5.8\nSummary\n500\nHomework Problems and Questions \n502\nWireshark Labs: Ethernet and ARP, DHCP\n510\nInterview: Simon S. Lam\n511\nChapter 6 Wireless and Mobile Networks\n513\n6.1\nIntroduction\n514\n6.2\nWireless Links and Network Characteristics\n519\n6.2.1\nCDMA\n522\n6.3\nWiFi: 802.11 Wireless LANs\n526\n6.3.1\nThe 802.11 Architecture\n527\n6.3.2\nThe 802.11 MAC Protocol\n531\n6.3.3\nThe IEEE 802.11 Frame\n537\n6.3.4\nMobility in the Same IP Subnet\n541\n6.3.5\nAdvanced Features in 802.11\n542\n6.3.6\nPersonal Area Networks: Bluetooth and Zigbee\n544\n6.4\nCellular Internet Access\n546\n6.4.1\nAn Overview of Cellular Network Architecture\n547\n6.4.2\n3G Cellular Data Networks: Extending the Internet to Cellular \nSubscribers\n550\n6.4.3\nOn to 4G: LTE\n553\n6.5\nMobility Management: Principles\n555\n6.5.1\nAddressing\n557\n6.5.2\nRouting to a Mobile Node\n559\n6.6\nMobile IP\n564\n6.7\nManaging Mobility in Cellular Networks\n570\n6.7.1\nRouting Calls to a Mobile User\n571\n6.7.2\nHandoffs in GSM\n572\n\n6.8\nWireless and Mobility: Impact on Higher-Layer Protocols\n575\n6.9\nSummary\n578\nHomework Problems and Questions\n578\nWireshark Lab: IEEE 802.11 (WiFi)\n583\nInterview: Deborah Estrin\n584\nChapter 7 Multimedia Networking\n587\n7.1\nMultimedia Networking Applications\n588\n7.1.1\nProperties of Video\n588\n7.1.2\nProperties of Audio\n590\n7.1.3\nTypes of Multimedia Network Applications\n591\n7.2\nStreaming Stored Video\n593\n7.2.1\nUDP Streaming\n595\n7.2.2\nHTTP Streaming\n596\n7.2.3\nAdaptive Streaming and DASH\n600\n7.2.4\nContent Distribution Networks\n602\n7.2.5\nCase Studies: Netflix, YouTube, and Kankan\n608\n7.3\nVoice-over-IP\n612\n7.3.1\nLimitations of the Best-Effort IP Service\n612\n7.3.2\nRemoving Jitter at the Receiver for Audio\n614\n7.3.3\nRecovering from Packet Loss\n617\n7.3.4\nCase Study: VoIP with Skype\n620\n7.4\nProtocols for Real-Time Conversational Applications\n623\n7.4.1\nRTP\n624\n7.4.2\nSIP\n627\n7.5\nNetwork Support for Multimedia\n632\n7.5.1\nDimensioning Best-Effort Networks\n634\n7.5.2\nProviding Multiple Classes of Service\n636\n7.5.3\nDiffserv\n648\n7.5.4\nPer-Connection Quality-of-Service (QoS) Guarantees:\nResource Reservation and Call Admission\n652\n7.6\nSummary\n655\nHomework Problems and Questions\n656\nProgramming Assignment\n666\nInterview: Henning Schulzrinne\n668\nChapter 8 Security in Computer Networks\n671\n8.1\nWhat Is Network Security?\n672\n8.2\nPrinciples of Cryptography\n675\n8.2.1\nSymmetric Key Cryptography\n676\n8.2.2\nPublic Key Encryption\n683\nxxii\nTable of Contents\n\nTable of Contents\nxxiii\n8.3\nMessage Integrity and Digital Signatures\n688\n8.3.1\nCryptographic Hash Functions\n689\n8.3.2\nMessage Authentication Code\n691\n8.3.3\nDigital Signatures\n693\n8.4\nEnd-Point Authentication\n700\n8.4.1\nAuthentication Protocol ap1.0\n700\n8.4.2\nAuthentication Protocol ap2.0\n701\n8.4.3\nAuthentication Protocol ap3.0\n702\n8.4.4\nAuthentication Protocol ap3.1\n703\n8.4.5\nAuthentication Protocol ap4.0\n703\n8.5\nSecuring E-Mail\n705\n8.5.1\nSecure E-Mail\n706\n8.5.2\nPGP\n710\n8.6\nSecuring TCP Connections: SSL\n711\n8.6.1\nThe Big Picture\n713\n8.6.2\nA More Complete Picture\n716\n8.7\nNetwork-Layer Security: IPsec and Virtual Private Networks\n718\n8.7.1\nIPsec and Virtual Private Networks (VPNs)\n718\n8.7.2\nThe AH and ESP Protocols\n720\n8.7.3\nSecurity Associations\n720\n8.7.4\nThe IPsec Datagram\n721\n8.7.5\nIKE: Key Management in IPsec\n725\n8.8\nSecuring Wireless LANs\n726\n8.8.1\nWired Equivalent Privacy (WEP)\n726\n8.8.2\nIEEE 802.11i\n728\n8.9\nOperational Security: Firewalls and Intrusion Detection Systems\n731\n8.9.1\nFirewalls\n731\n8.9.2\nIntrusion Detection Systems\n739\n8.10\nSummary\n742\nHomework Problems and Questions\n744\nWireshark Lab: SSL\n752\nIPsec Lab\n752\nInterview: Steven M. Bellovin\n753\nChapter 9 Network Management\n755\n9.1\nWhat Is Network Management?\n756\n9.2\nThe Infrastructure for Network Management\n760\n9.3\nThe Internet-Standard Management Framework\n764\n9.3.1\nStructure of Management Information: SMI\n766\n9.3.2\nManagement Information Base: MIB\n770\n\n9.3.3\nSNMP Protocol Operations and Transport Mappings\n772\n9.3.4\nSecurity and Administration\n775\n9.4\nASN.1\n778\n9.5\nConclusion\n783\nHomework Problems and Questions\n783\nInterview: Jennifer Rexford\n786\nReferences\n789\nIndex\n823\nxxiv\nTable of Contents\n\nCOMPUTER\nNETWORKING\nA Top-Down Approach\nSIXTH EDITION\n\nThis page intentionally left blank"
    },
    {
      "chunk_id": "b40941e0-02a8-4c11-bd9c-51ec8d059f6a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Chapter 1 Computer Networks and the Internet",
      "original_titles": [
        "Chapter 1 Computer Networks and the Internet"
      ],
      "path": "Chapter 1 Computer Networks and the Internet",
      "start_page": 28,
      "end_page": 28,
      "token_count": 322,
      "text": "CHAPTER1\nComputer\nNetworks and\nthe Internet\n1\nToday’s Internet is arguably the largest engineered system ever created by mankind,\nwith hundreds of millions of connected computers, communication links, and\nswitches; with billions of users who connect via laptops, tablets, and smartphones;\nand with an array of new Internet-connected devices such as sensors, Web cams,\ngame consoles, picture frames, and even washing machines. Given that the Internet\nis so large and has so many diverse components and uses, is there any hope of\nunderstanding how it works? Are there guiding principles and structure that can pro-\nvide a foundation for understanding such an amazingly large and complex system?\nAnd if so, is it possible that it actually could be both interesting and fun to learn\nabout computer networks? Fortunately, the answers to all of these questions is a\nresounding YES! Indeed, it’s our aim in this book to provide you with a modern\nintroduction to the dynamic field of computer networking, giving you the principles\nand practical insights you’ll need to understand not only today’s networks, but\ntomorrow’s as well.\nThis first chapter presents a broad overview of computer networking and the\nInternet. Our goal here is to paint a broad picture and set the context for the rest of\nthis book, to see the forest through the trees. We’ll cover a lot of ground in this intro-\nductory chapter and discuss a lot of the pieces of a computer network, without los-\ning sight of the big picture."
    },
    {
      "chunk_id": "f398cd90-0d13-4c3b-9528-04a4d6830cbf",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.1 What Is the Internet?",
      "original_titles": [
        "1.1 What Is the Internet?"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.1 What Is the Internet?",
      "start_page": 29,
      "end_page": 31,
      "token_count": 1375,
      "text": "We’ll structure our overview of computer networks in this chapter as follows.\nAfter introducing some basic terminology and concepts, we’ll first examine the\nbasic hardware and software components that make up a network. We’ll begin at\nthe network’s edge and look at the end systems and network applications running\nin the network. We’ll then explore the core of a computer network, examining the\nlinks and the switches that transport data, as well as the access networks and phys-\nical media that connect end systems to the network core. We’ll learn that the Inter-\nnet is a network of networks, and we’ll learn how these networks connect with\neach other.\nAfter having completed this overview of the edge and core of a computer net-\nwork, we’ll take the broader and more abstract view in the second half of this chap-\nter. We’ll examine delay, loss, and throughput of data in a computer network and\nprovide simple quantitative models for end-to-end throughput and delay: models\nthat take into account transmission, propagation, and queuing delays. We’ll then\nintroduce some of the key architectural principles in computer networking, namely,\nprotocol layering and service models. We’ll also learn that computer networks are\nvulnerable to many different types of attacks; we’ll survey some of these attacks and\nconsider how computer networks can be made more secure. Finally, we’ll close this\nchapter with a brief history of computer networking.\n1.1 What Is the Internet?\nIn this book, we’ll use the public Internet, a specific computer network, as our prin-\ncipal vehicle for discussing computer networks and their protocols. But what is the\nInternet? There are a couple of ways to answer this question. First, we can describe\nthe nuts and bolts of the Internet, that is, the basic hardware and software components\nthat make up the Internet. Second, we can describe the Internet in terms of a net-\nworking infrastructure that provides services to distributed applications. Let’s begin\nwith the nuts-and-bolts description, using Figure 1.1 to illustrate our discussion.\n1.1.1 A Nuts-and-Bolts Description\nThe Internet is a computer network that interconnects hundreds of millions of com-\nputing devices throughout the world. Not too long ago, these computing devices were\nprimarily traditional desktop PCs, Linux workstations, and so-called servers that store\nand transmit information such as Web pages and e-mail messages. Increasingly, \nhowever, nontraditional Internet end systems such as laptops, smartphones, tablets,\nTVs, gaming consoles, Web cams, automobiles, environmental sensing devices, \npicture frames, and home electrical and security systems are being connected to the\nInternet. Indeed, the term computer network is beginning to sound a bit dated, given \nthe many nontraditional devices that are being hooked up to the Internet. In Internet jar-\ngon, all of these devices are called hosts or end systems. As of July 2011, there were \n2\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\n\n1.1\n•\nWHAT IS THE INTERNET?\n3\nFigure 1.1 \u0002 Some pieces of the Internet\nKey:\nHost\n(= end system)\nServer\nMobile\nRouter\nLink-Layer\nswitch\nModem\nBase\nstation\nSmartphone\nCell phone\ntower\nNational or\nGlobal ISP\nMobile Network\nLocal or\nRegional ISP\nEnterprise Network\nHome Network\n\nnearly 850 million end systems attached to the Internet [ISC 2012], not counting\nsmartphones, laptops, and other devices that are only intermittently connected to the\nInternet. Overall, more there are an estimated 2 billion Internet users [ITU 2011].\nEnd systems are connected together by a network of communication links and\npacket switches. We’ll see in Section 1.2 that there are many types of communica-\ntion links, which are made up of different types of physical media, including coaxial\ncable, copper wire, optical fiber, and radio spectrum. Different links can transmit\ndata at different rates, with the transmission rate of a link measured in bits/second.\nWhen one end system has data to send to another end system, the sending end sys-\ntem segments the data and adds header bytes to each segment. The resulting pack-\nages of information, known as packets in the jargon of computer networks, are then\nsent through the network to the destination end system, where they are reassembled\ninto the original data.\nA packet switch takes a packet arriving on one of its incoming communication\nlinks and forwards that packet on one of its outgoing communication links. Packet\nswitches come in many shapes and flavors, but the two most prominent types in\ntoday’s Internet are routers and link-layer switches. Both types of switches for-\nward packets toward their ultimate destinations. Link-layer switches are typically\nused in access networks, while routers are typically used in the network core. The\nsequence of communication links and packet switches traversed by a packet from\nthe sending end system to the receiving end system is known as a route or path\nthrough the network. The exact amount of traffic being carried in the Internet is\ndifficult to estimate but Cisco [Cisco VNI 2011] estimates global Internet traffic will\nbe nearly 40 exabytes per month in 2012.\nPacket-switched networks (which transport packets) are in many ways simi-\nlar to transportation networks of highways, roads, and intersections (which trans-\nport vehicles). Consider, for example, a factory that needs to move a large\namount of cargo to some destination warehouse located thousands of kilometers\naway. At the factory, the cargo is segmented and loaded into a fleet of trucks.\nEach of the trucks then independently travels through the network of highways,\nroads, and intersections to the destination warehouse. At the destination ware-\nhouse, the cargo is unloaded and grouped with the rest of the cargo arriving from\nthe same shipment. Thus, in many ways, packets are analogous to trucks, com-\nmunication links are analogous to highways and roads, packet switches are anal-\nogous to intersections, and end systems are analogous to buildings. Just as a truck\ntakes a path through the transportation network, a packet takes a path through a\ncomputer network.\nEnd systems access the Internet through Internet Service Providers (ISPs),\nincluding residential ISPs such as local cable or telephone companies; corporate\nISPs; university ISPs; and ISPs that provide WiFi access in airports, hotels, coffee\nshops, and other public places. Each ISP is in itself a network of packet switches\nand communication links. ISPs provide a variety of types of network access to the\nend systems, including residential broadband access such as cable modem or DSL,\n4\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET"
    },
    {
      "chunk_id": "7ed244d1-c465-4dc5-b8e4-4e45deeedd4b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.1.1 A Nuts-and-Bolts Description",
      "original_titles": [
        "1.1.1 A Nuts-and-Bolts Description"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.1 What Is the Internet? > 1.1.1 A Nuts-and-Bolts Description",
      "start_page": 29,
      "end_page": 31,
      "token_count": 1375,
      "text": "We’ll structure our overview of computer networks in this chapter as follows.\nAfter introducing some basic terminology and concepts, we’ll first examine the\nbasic hardware and software components that make up a network. We’ll begin at\nthe network’s edge and look at the end systems and network applications running\nin the network. We’ll then explore the core of a computer network, examining the\nlinks and the switches that transport data, as well as the access networks and phys-\nical media that connect end systems to the network core. We’ll learn that the Inter-\nnet is a network of networks, and we’ll learn how these networks connect with\neach other.\nAfter having completed this overview of the edge and core of a computer net-\nwork, we’ll take the broader and more abstract view in the second half of this chap-\nter. We’ll examine delay, loss, and throughput of data in a computer network and\nprovide simple quantitative models for end-to-end throughput and delay: models\nthat take into account transmission, propagation, and queuing delays. We’ll then\nintroduce some of the key architectural principles in computer networking, namely,\nprotocol layering and service models. We’ll also learn that computer networks are\nvulnerable to many different types of attacks; we’ll survey some of these attacks and\nconsider how computer networks can be made more secure. Finally, we’ll close this\nchapter with a brief history of computer networking.\n1.1 What Is the Internet?\nIn this book, we’ll use the public Internet, a specific computer network, as our prin-\ncipal vehicle for discussing computer networks and their protocols. But what is the\nInternet? There are a couple of ways to answer this question. First, we can describe\nthe nuts and bolts of the Internet, that is, the basic hardware and software components\nthat make up the Internet. Second, we can describe the Internet in terms of a net-\nworking infrastructure that provides services to distributed applications. Let’s begin\nwith the nuts-and-bolts description, using Figure 1.1 to illustrate our discussion.\n1.1.1 A Nuts-and-Bolts Description\nThe Internet is a computer network that interconnects hundreds of millions of com-\nputing devices throughout the world. Not too long ago, these computing devices were\nprimarily traditional desktop PCs, Linux workstations, and so-called servers that store\nand transmit information such as Web pages and e-mail messages. Increasingly, \nhowever, nontraditional Internet end systems such as laptops, smartphones, tablets,\nTVs, gaming consoles, Web cams, automobiles, environmental sensing devices, \npicture frames, and home electrical and security systems are being connected to the\nInternet. Indeed, the term computer network is beginning to sound a bit dated, given \nthe many nontraditional devices that are being hooked up to the Internet. In Internet jar-\ngon, all of these devices are called hosts or end systems. As of July 2011, there were \n2\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\n\n1.1\n•\nWHAT IS THE INTERNET?\n3\nFigure 1.1 \u0002 Some pieces of the Internet\nKey:\nHost\n(= end system)\nServer\nMobile\nRouter\nLink-Layer\nswitch\nModem\nBase\nstation\nSmartphone\nCell phone\ntower\nNational or\nGlobal ISP\nMobile Network\nLocal or\nRegional ISP\nEnterprise Network\nHome Network\n\nnearly 850 million end systems attached to the Internet [ISC 2012], not counting\nsmartphones, laptops, and other devices that are only intermittently connected to the\nInternet. Overall, more there are an estimated 2 billion Internet users [ITU 2011].\nEnd systems are connected together by a network of communication links and\npacket switches. We’ll see in Section 1.2 that there are many types of communica-\ntion links, which are made up of different types of physical media, including coaxial\ncable, copper wire, optical fiber, and radio spectrum. Different links can transmit\ndata at different rates, with the transmission rate of a link measured in bits/second.\nWhen one end system has data to send to another end system, the sending end sys-\ntem segments the data and adds header bytes to each segment. The resulting pack-\nages of information, known as packets in the jargon of computer networks, are then\nsent through the network to the destination end system, where they are reassembled\ninto the original data.\nA packet switch takes a packet arriving on one of its incoming communication\nlinks and forwards that packet on one of its outgoing communication links. Packet\nswitches come in many shapes and flavors, but the two most prominent types in\ntoday’s Internet are routers and link-layer switches. Both types of switches for-\nward packets toward their ultimate destinations. Link-layer switches are typically\nused in access networks, while routers are typically used in the network core. The\nsequence of communication links and packet switches traversed by a packet from\nthe sending end system to the receiving end system is known as a route or path\nthrough the network. The exact amount of traffic being carried in the Internet is\ndifficult to estimate but Cisco [Cisco VNI 2011] estimates global Internet traffic will\nbe nearly 40 exabytes per month in 2012.\nPacket-switched networks (which transport packets) are in many ways simi-\nlar to transportation networks of highways, roads, and intersections (which trans-\nport vehicles). Consider, for example, a factory that needs to move a large\namount of cargo to some destination warehouse located thousands of kilometers\naway. At the factory, the cargo is segmented and loaded into a fleet of trucks.\nEach of the trucks then independently travels through the network of highways,\nroads, and intersections to the destination warehouse. At the destination ware-\nhouse, the cargo is unloaded and grouped with the rest of the cargo arriving from\nthe same shipment. Thus, in many ways, packets are analogous to trucks, com-\nmunication links are analogous to highways and roads, packet switches are anal-\nogous to intersections, and end systems are analogous to buildings. Just as a truck\ntakes a path through the transportation network, a packet takes a path through a\ncomputer network.\nEnd systems access the Internet through Internet Service Providers (ISPs),\nincluding residential ISPs such as local cable or telephone companies; corporate\nISPs; university ISPs; and ISPs that provide WiFi access in airports, hotels, coffee\nshops, and other public places. Each ISP is in itself a network of packet switches\nand communication links. ISPs provide a variety of types of network access to the\nend systems, including residential broadband access such as cable modem or DSL,\n4\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET"
    },
    {
      "chunk_id": "3437fbad-d1a2-48e3-8270-a688aba7dc2e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.1.2 A Services Description",
      "original_titles": [
        "1.1.2 A Services Description"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.1 What Is the Internet? > 1.1.2 A Services Description",
      "start_page": 32,
      "end_page": 33,
      "token_count": 1324,
      "text": "high-speed local area network access, wireless access, and 56 kbps dial-up modem\naccess. ISPs also provide Internet access to content providers, connecting Web\nsites directly to the Internet. The Internet is all about connecting end systems to\neach other, so the ISPs that provide access to end systems must also be intercon-\nnected. These lower-tier ISPs are interconnected through national and interna-\ntional upper-tier ISPs such as Level 3 Communications, AT&T, Sprint, and NTT.\nAn upper-tier ISP consists of high-speed routers interconnected with high-speed\nfiber-optic links. Each ISP network, whether upper-tier or lower-tier, is managed\nindependently, runs the IP protocol (see below), and conforms to certain naming\nand address conventions. We’ll examine ISPs and their interconnection more\nclosely in Section 1.3.\nEnd systems, packet switches, and other pieces of the Internet run protocols\nthat control the sending and receiving of information within the Internet. The\nTransmission Control Protocol (TCP) and the Internet Protocol (IP) are two of\nthe most important protocols in the Internet. The IP protocol specifies the format of\nthe packets that are sent and received among routers and end systems. The Internet’s\nprincipal protocols are collectively known as TCP/IP. We’ll begin looking into pro-\ntocols in this introductory chapter. But that’s just a start—much of this book is con-\ncerned with computer network protocols!\nGiven the importance of protocols to the Internet, it’s important that everyone\nagree on what each and every protocol does, so that people can create systems and\nproducts that interoperate. This is where standards come into play. Internet stan-\ndards are developed by the Internet Engineering Task Force (IETF)[IETF 2012].\nThe IETF standards documents are called requests for comments (RFCs). RFCs\nstarted out as general requests for comments (hence the name) to resolve network\nand protocol design problems that faced the precursor to the Internet [Allman 2011].\nRFCs tend to be quite technical and detailed. They define protocols such as TCP, IP,\nHTTP (for the Web), and SMTP (for e-mail). There are currently more than 6,000\nRFCs. Other bodies also specify standards for network components, most notably\nfor network links. The IEEE 802 LAN/MAN Standards Committee [IEEE 802\n2012], for example, specifies the Ethernet and wireless WiFi standards.\n1.1.2 A Services Description\nOur discussion above has identified many of the pieces that make up the Internet.\nBut we can also describe the Internet from an entirely different angle—namely, as\nan infrastructure that provides services to applications. These applications\ninclude electronic mail, Web surfing, social networks, instant messaging, Voice-\nover-IP (VoIP), video streaming, distributed games, peer-to-peer (P2P) file shar-\ning, television over the Internet, remote login, and much, much more. The\napplications are said to be distributed applications, since they involve multiple\nend systems that exchange data with each other. Importantly, Internet applications\n1.1\n•\nWHAT IS THE INTERNET?\n5\n\nrun on end systems—they do not run in the packet switches in the network core.\nAlthough packet switches facilitate the exchange of data among end systems, they\nare not concerned with the application that is the source or sink of data.\nLet’s explore a little more what we mean by an infrastructure that provides\nservices to applications. To this end, suppose you have an exciting new idea for a\ndistributed Internet application, one that may greatly benefit humanity or one that\nmay simply make you rich and famous. How might you go about transforming\nthis idea into an actual Internet application? Because applications run on end sys-\ntems, you are going to need to write programs that run on the end systems. You\nmight, for example, write your programs in Java, C, or Python. Now, because you\nare developing a distributed Internet application, the programs running on the\ndifferent end systems will need to send data to each other. And here we get to a\ncentral issue—one that leads to the alternative way of describing the Internet as a\nplatform for applications. How does one program running on one end system\ninstruct the Internet to deliver data to another program running on another end\nsystem?\nEnd systems attached to the Internet provide an Application Programming\nInterface (API) that specifies how a program running on one end system asks\nthe Internet infrastructure to deliver data to a specific destination program run-\nning on another end system. This Internet API is a set of rules that the sending\nprogram must follow so that the Internet can deliver the data to the destination\nprogram. We’ll discuss the Internet API in detail in Chapter 2. For now, let’s\ndraw upon a simple analogy, one that we will frequently use in this book. Sup-\npose Alice wants to send a letter to Bob using the postal service. Alice, of course,\ncan’t just write the letter (the data) and drop the letter out her window. Instead,\nthe postal service requires that Alice put the letter in an envelope; write Bob’s\nfull name, address, and zip code in the center of the envelope; seal the envelope;\nput a stamp in the upper-right-hand corner of the envelope; and finally, drop the\nenvelope into an official postal service mailbox. Thus, the postal service has its\nown “postal service API,” or set of rules, that Alice must follow to have the\npostal service deliver her letter to Bob. In a similar manner, the Internet has an\nAPI that the program sending data must follow to have the Internet deliver the\ndata to the program that will receive the data.\nThe postal service, of course, provides more than one service to its customers.\nIt provides express delivery, reception confirmation, ordinary use, and many more\nservices. In a similar manner, the Internet provides multiple services to its applica-\ntions. When you develop an Internet application, you too must choose one of the\nInternet’s services for your application. We’ll describe the Internet’s services in\nChapter 2.\nWe have just given two descriptions of the Internet; one in terms of its hardware\nand software components, the other in terms of an infrastructure for providing\nservices to distributed applications. But perhaps you are still confused as to what the\n6\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET"
    },
    {
      "chunk_id": "379acf66-29a1-498a-a541-cdf84710c33e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.1.3 What Is a Protocol?",
      "original_titles": [
        "1.1.3 What Is a Protocol?"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.1 What Is the Internet? > 1.1.3 What Is a Protocol?",
      "start_page": 34,
      "end_page": 35,
      "token_count": 885,
      "text": "Internet is. What are packet switching and TCP/IP? What are routers? What kinds of\ncommunication links are present in the Internet? What is a distributed application?\nHow can a toaster or a weather sensor be attached to the Internet? If you feel a bit\noverwhelmed by all of this now, don’t worry—the purpose of this book is to intro-\nduce you to both the nuts and bolts of the Internet and the principles that govern how\nand why it works. We’ll explain these important terms and questions in the follow-\ning sections and chapters.\n1.1.3 What Is a Protocol?\nNow that we’ve got a bit of a feel for what the Internet is, let’s consider another\nimportant buzzword in computer networking: protocol. What is a protocol? What\ndoes a protocol do?\nA Human Analogy\nIt is probably easiest to understand the notion of a computer network protocol by\nfirst considering some human analogies, since we humans execute protocols all of\nthe time. Consider what you do when you want to ask someone for the time of day.\nA typical exchange is shown in Figure 1.2. Human protocol (or good manners, at\nleast) dictates that one first offer a greeting (the first “Hi” in Figure 1.2) to initiate\ncommunication with someone else. The typical response to a “Hi” is a returned\n“Hi” message. Implicitly, one then takes a cordial “Hi” response as an indication\nthat one can proceed and ask for the time of day. A different response to the initial\n“Hi” (such as “Don’t bother me!” or “I don’t speak English,” or some unprintable\nreply) might indicate an unwillingness or inability to communicate. In this case,\nthe human protocol would be not to ask for the time of day. Sometimes one gets no\nresponse at all to a question, in which case one typically gives up asking that per-\nson for the time. Note that in our human protocol, there are specific messages we\nsend, and specific actions we take in response to the received reply messages or\nother events (such as no reply within some given amount of time). Clearly, trans-\nmitted and received messages, and actions taken when these messages are sent or\nreceived or other events occur, play a central role in a human protocol. If people\nrun different protocols (for example, if one person has manners but the other does\nnot, or if one understands the concept of time and the other does not) the protocols\ndo not interoperate and no useful work can be accomplished. The same is true in\nnetworking—it takes two (or more) communicating entities running the same pro-\ntocol in order to accomplish a task.\nLet’s consider a second human analogy. Suppose you’re in a college class (a\ncomputer networking class, for example!). The teacher is droning on about proto-\ncols and you’re confused. The teacher stops to ask, “Are there any questions?” (a\n1.1\n•\nWHAT IS THE INTERNET?\n7\n\nmessage that is transmitted to, and received by, all students who are not sleeping).\nYou raise your hand (transmitting an implicit message to the teacher). Your teacher\nacknowledges you with a smile, saying “Yes . . .” (a transmitted message encourag-\ning you to ask your question—teachers love to be asked questions), and you then ask\nyour question (that is, transmit your message to your teacher). Your teacher hears\nyour question (receives your question message) and answers (transmits a reply to\nyou). Once again, we see that the transmission and receipt of messages, and a set of\nconventional actions taken when these messages are sent and received, are at the\nheart of this question-and-answer protocol.\nNetwork Protocols\nA network protocol is similar to a human protocol, except that the entities exchang-\ning messages and taking actions are hardware or software components of some\ndevice (for example, computer, smartphone, tablet, router, or other network-capable\n8\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nGET http://www.awl.com/kurose-ross\nTCP connection request\nTime\nTime\nTCP connection reply\n<file>\nHi\nGot the time?\nTime\nTime\nHi\n2:00\nFigure 1.2 \u0002 A human protocol and a computer network protocol"
    },
    {
      "chunk_id": "8d7bd260-0b78-4674-8983-6ffbd888741e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.2 The Network Edge",
      "original_titles": [
        "1.2 The Network Edge"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.2 The Network Edge",
      "start_page": 36,
      "end_page": 38,
      "token_count": 1228,
      "text": "device). All activity in the Internet that involves two or more communicating remote\nentities is governed by a protocol. For example, hardware-implemented protocols in\ntwo physically connected computers control the flow of bits on the “wire” between\nthe two network interface cards; congestion-control protocols in end systems con-\ntrol the rate at which packets are transmitted between sender and receiver; protocols\nin routers determine a packet’s path from source to destination. Protocols are run-\nning everywhere in the Internet, and consequently much of this book is about com-\nputer network protocols.\nAs an example of a computer network protocol with which you are probably\nfamiliar, consider what happens when you make a request to a Web server, that is,\nwhen you type the URL of a Web page into your Web browser. The scenario is illus-\ntrated in the right half of Figure 1.2. First, your computer will send a connection\nrequest message to the Web server and wait for a reply. The Web server will eventu-\nally receive your connection request message and return a connection reply mes-\nsage. Knowing that it is now OK to request the Web document, your computer then\nsends the name of the Web page it wants to fetch from that Web server in a GET\nmessage. Finally, the Web server returns the Web page (file) to your computer.\nGiven the human and networking examples above, the exchange of messages\nand the actions taken when these messages are sent and received are the key defin-\ning elements of a protocol:\nA protocol defines the format and the order of messages exchanged between\ntwo or more communicating entities, as well as the actions taken on the trans-\nmission and/or receipt of a message or other event.\nThe Internet, and computer networks in general, make extensive use of proto-\ncols. Different protocols are used to accomplish different communication tasks. As\nyou read through this book, you will learn that some protocols are simple and\nstraightforward, while others are complex and intellectually deep. Mastering the\nfield of computer networking is equivalent to understanding the what, why, and how\nof networking protocols.\n1.2 The Network Edge\nIn the previous section we presented a high-level overview of the Internet and net-\nworking protocols. We are now going to delve a bit more deeply into the compo-\nnents of a computer network (and the Internet, in particular). We begin in this\nsection at the edge of a network and look at the components with which we are most\nfamiliar—namely, the computers, smartphones and other devices that we use on a\ndaily basis. In the next section we’ll move from the network edge to the network\ncore and examine switching and routing in computer networks.\n1.2\n•\nTHE NETWORK EDGE\n9\n\nRecall from the previous section that in computer networking jargon, the com-\nputers and other devices connected to the Internet are often referred to as end sys-\ntems. They are referred to as end systems because they sit at the edge of the Internet,\nas shown in Figure 1.3. The Internet’s end systems include desktop computers (e.g.,\ndesktop PCs, Macs, and Linux boxes), servers (e.g., Web and e-mail servers), and\nmobile computers (e.g., laptops, smartphones, and tablets). Furthermore, an increas-\ning number of non-traditional devices are being attached to the Internet as end sys-\ntems (see sidebar).\nEnd systems are also referred to as hosts because they host (that is, run) appli-\ncation programs such as a Web browser program, a Web server program, an e-mail\nclient program, or an e-mail server program. Throughout this book we will use the\nterms hosts and end systems interchangeably; that is, host = end system. Hosts are\nsometimes further divided into two categories: clients and servers. Informally,\nclients tend to be desktop and mobile PCs, smartphones, and so on, whereas servers\ntend to be more powerful machines that store and distribute Web pages, stream\nvideo, relay e-mail, and so on. Today, most of the servers from which we receive\n10\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nA DIZZYING ARRAY OF INTERNET END SYSTEMS\nNot too long ago, the end-system devices connected to the Internet were primarily\ntraditional computers such as desktop machines and powerful servers. Beginning in\nthe late 1990s and continuing today, a wide range of interesting devices are being\nconnected to the Internet, leveraging their ability to send and receive digital data.\nGiven the Internet’s ubiquity, its well-defined (standardized) protocols, and the\navailability of Internet-ready commodity hardware, it’s natural to use Internet tech-\nnology to network these devices together and to Internet-connected servers.\nMany of these devices are based in the home—video game consoles (e.g.,\nMicrosoft’s Xbox), Internet-ready televisions, digital picture frames that download\nand display digital pictures, washing machines, refrigerators, and even a toaster\nthat downloads meteorological information and burns an image of the day’s fore-\ncast (e.g., mixed clouds and sun) on your morning toast [BBC 2001]. IP-enabled\nphones with GPS capabilities put location-dependent services (maps, information\nabout nearby services or people) at your fingertips. Networked sensors embedded\ninto the physical environment allow monitoring of buildings, bridges, seismic activi-\nty, wildlife habitats, river estuaries, and the weather. Biomedical devices can be\nembedded and networked in a body-area network. With so many diverse devices\nbeing networked together, the Internet is indeed becoming an “Internet of things”\n[ITU 2005b].\nCASE HISTORY\n\nsearch results, e-mail, Web pages, and videos reside in large data centers. For\nexample, Google has 30–50 data centers, with many having more than one hundred\nthousand servers.\nMobile Network\nNational or\nGlobal ISP\nLocal or\nRegional ISP\nEnterprise Network\nHome Network\nFigure 1.3 \u0002 End-system interaction\n1.2\n•\nTHE NETWORK EDGE\n11"
    },
    {
      "chunk_id": "733e6c01-f36d-4f9c-a530-5e833a59c96d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.2.1 Access Networks",
      "original_titles": [
        "1.2.1 Access Networks"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.2 The Network Edge > 1.2.1 Access Networks",
      "start_page": 39,
      "end_page": 44,
      "token_count": 2736,
      "text": "12\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nNational or\nGlobal ISP\nMobile Network\nLocal or\nRegional ISP\nEnterprise Network\nHome Network\nFigure 1.4 \u0002 Access networks\n1.2.1 Access Networks\nHaving considered the applications and end systems at the “edge of the network,”\nlet’s next consider the access network—the network that physically connects an end\nsystem to the first router (also known as the “edge router”) on a path from the end\nsystem to any other distant end system. Figure 1.4 shows several types of access\n\nnetworks with thick, shaded lines, and the settings (home, enterprise, and wide-area\nmobile wireless) in which they are used.\nHome Access: DSL, Cable, FTTH, Dial-Up, and Satellite\nIn developed countries today, more than 65 percent of the households have Internet\naccess, with Korea, Netherlands, Finland, and Sweden leading the way with more than\n80 percent of households having Internet access, almost all via a high-speed broadband\nconnection [ITU 2011]. Finland and Spain have recently declared high-speed Internet\naccess to be a “legal right.” Given this intense interest in home access, let’s begin our\noverview of access networks by considering how homes connect to the Internet.\nToday, the two most prevalent types of broadband residential access are digital\nsubscriber line (DSL) and cable. A residence typically obtains DSL Internet access\nfrom the same local telephone company (telco) that provides its wired local phone\naccess. Thus, when DSL is used, a customer’s telco is also its ISP. As shown in\nFigure 1.5, each customer’s DSL modem uses the existing telephone line (twisted-\npair copper wire, which we’ll discuss in Section 1.2.2) to exchange data with a digi-\ntal subscriber line access multiplexer (DSLAM) located in the telco’s local central\noffice (CO). The home’s DSL modem takes digital data and translates it to high-\nfrequency tones for transmission over telephone wires to the CO; the analog signals\nfrom many such houses are translated back into digital format at the DSLAM.\nThe residential telephone line carries both data and traditional telephone sig-\nnals simultaneously, which are encoded at different frequencies:\n•\nA high-speed downstream channel, in the 50 kHz to 1 MHz band\n•\nA medium-speed upstream channel, in the 4 kHz to 50 kHz band\n•\nAn ordinary two-way telephone channel, in the 0 to 4 kHz band\nThis approach makes the single DSL link appear as if there were three separate\nlinks, so that a telephone call and an Internet connection can share the DSL link at\nthe same time. (We’ll describe this technique of frequency-division multiplexing in\nHome PC\nHome\nphone\nDSL\nmodem\nInternet\nTelephone\nnetwork\nSplitter\nExisting phone line:\n0-4KHz phone; 4-50KHz\nupstream data; 50KHz–\n1MHz downstream data\nCentral\noffice\nDSLAM\nFigure 1.5 \u0002 DSL Internet access\n1.2\n•\nTHE NETWORK EDGE\n13\n\n14\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nSection 1.3.1). On the customer side, a splitter separates the data and telephone sig-\nnals arriving to the home and forwards the data signal to the DSL modem. On the\ntelco side, in the CO, the DSLAM separates the data and phone signals and sends\nthe data into the Internet. Hundreds or even thousands of households connect to a\nsingle DSLAM [Dischinger 2007].\nThe DSL standards define transmission rates of 12 Mbps downstream and\n1.8 Mbps upstream [ITU 1999], and 24 Mbps downstream and 2.5 Mbps upstream\n[ITU 2003]. Because the downstream and upstream rates are different, the access is\nsaid to be asymmetric. The actual downstream and upstream transmission rates\nachieved may be less than the rates noted above, as the DSL provider may purpose-\nfully limit a residential rate when tiered service (different rates, available at differ-\nent prices) are offered, or because the maximum rate can be limited by the distance\nbetween the home and the CO, the gauge of the twisted-pair line and the degree of\nelectrical interference. Engineers have expressly designed DSL for short distances\nbetween the home and the CO; generally, if the residence is not located within 5 to 10\nmiles of the CO, the residence must resort to an alternative form of Internet access.\nWhile DSL makes use of the telco’s existing local telephone infrastructure,\ncable Internet access makes use of the cable television company’s existing cable\ntelevision infrastructure. A residence obtains cable Internet access from the same\ncompany that provides its cable television. As illustrated in Figure 1.6, fiber optics\nconnect the cable head end to neighborhood-level junctions, from which tradi-\ntional coaxial cable is then used to reach individual houses and apartments. Each\nneighborhood junction typically supports 500 to 5,000 homes. Because both fiber\nand coaxial cable are employed in this system, it is often referred to as hybrid\nfiber coax (HFC).\nFiber\ncable\nCoaxial cable\nHundreds\nof homes\nCable head end\nHundreds\nof homes\nFiber\nnode\nFiber\nnode\nInternet\nCMTS\nFigure 1.6 \u0002 A hybrid fiber-coaxial access network\n\nCable internet access requires special modems, called cable modems. As with a\nDSL modem, the cable modem is typically an external device and connects to the\nhome PC through an Ethernet port. (We will discuss Ethernet in great detail in\nChapter 5.) At the cable head end, the cable modem termination system (CMTS)\nserves a similar function as the DSL network’s DSLAM—turning the analog signal\nsent from the cable modems in many downstream homes back into digital format.\nCable modems divide the HFC network into two channels, a downstream and an\nupstream channel. As with DSL, access is typically asymmetric, with the down-\nstream channel typically allocated a higher transmission rate than the upstream\nchannel. The DOCSIS 2.0 standard defines downstream rates up to 42.8 Mbps and\nupstream rates of up to 30.7 Mbps. As in the case of DSL networks, the maximum\nachievable rate may not be realized due to lower contracted data rates or media\nimpairments.\nOne important characteristic of cable Internet access is that it is a shared\nbroadcast medium. In particular, every packet sent by the head end travels down-\nstream on every link to every home and every packet sent by a home travels on the\nupstream channel to the head end. For this reason, if several users are simultane-\nously downloading a video file on the downstream channel, the actual rate at which\neach user receives its video file will be significantly lower than the aggregate cable\ndownstream rate. On the other hand, if there are only a few active users and they\nare all Web surfing, then each of the users may actually receive Web pages at the\nfull cable downstream rate, because the users will rarely request a Web page at\nexactly the same time. Because the upstream channel is also shared, a distributed\nmultiple access protocol is needed to coordinate transmissions and avoid collisions.\n(We’ll discuss this collision issue in some detail in Chapter 5.)\nAlthough DSL and cable networks currently represent more than 90 percent of\nresidential broadband access in the United States, an up-and-coming technology that\npromises even higher speeds is the deployment of fiber to the home (FTTH)\n[FTTH Council 2011a]. As the name suggests, the FTTH concept is simple—\nprovide an optical fiber path from the CO directly to the home. In the United States,\nVerizon has been particularly aggressive with FTTH with its FIOS service [Verizon\nFIOS 2012].\nThere are several competing technologies for optical distribution from the\nCO to the homes. The simplest optical distribution network is called direct fiber,\nwith one fiber leaving the CO for each home. More commonly, each fiber leav-\ning the central office is actually shared by many homes; it is not until the fiber\ngets relatively close to the homes that it is split into individual customer-specific\nfibers. There are two competing optical-distribution network architectures that\nperform this splitting: active optical networks (AONs) and passive optical net-\nworks (PONs). AON is essentially switched Ethernet, which is discussed in\nChapter 5.\nHere, we briefly discuss PON, which is used in Verizon’s FIOS service.\nFigure 1.7 shows FTTH using the PON distribution architecture. Each home has\n1.2\n•\nTHE NETWORK EDGE\n15\n\nan optical network terminator (ONT), which is connected by dedicated optical\nfiber to a neighborhood splitter. The splitter combines a number of homes (typically\nless than 100) onto a single, shared optical fiber, which connects to an optical line\nterminator (OLT) in the telco’s CO. The OLT, providing conversion between optical\nand electrical signals, connects to the Internet via a telco router. In the home, users\nconnect a home router (typically a wireless router) to the ONT and access the Inter-\nnet via this home router. In the PON architecture, all packets sent from OLT to the\nsplitter are replicated at the splitter (similar to a cable head end).\nFTTH can potentially provide Internet access rates in the gigabits per second\nrange. However, most FTTH ISPs provide different rate offerings, with the higher\nrates naturally costing more money. The average downstream speed of US FTTH\ncustomers was approximately 20 Mbps in 2011 (compared with 13 Mbps for cable\naccess networks and less than 5 Mbps for DSL) [FTTH Council 2011b].\nTwo other access network technologies are also used to provide Internet access\nto the home. In locations where DSL, cable, and FTTH are not available (e.g., in\nsome rural settings), a satellite link can be used to connect a residence to the Inter-\nnet at speeds of more than 1 Mbps; StarBand and HughesNet are two such satellite\naccess providers. Dial-up access over traditional phone lines is based on the same\nmodel as DSL—a home modem connects over a phone line to a modem in the ISP.\nCompared with DSL and other broadband access networks, dial-up access is excru-\nciatingly slow at 56 kbps.\nAccess in the Enterprise (and the Home): Ethernet and WiFi\nOn corporate and university campuses, and increasingly in home settings, a local area\nnetwork (LAN) is used to connect an end system to the edge router. Although there\nare many types of LAN technologies, Ethernet is by far the most prevalent access\ntechnology in corporate, university, and home networks. As shown in Figure 1.8, \nEthernet users use twisted-pair copper wire to connect to an Ethernet switch, a\n16\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nInternet\nCentral office\nOptical\nsplitter\nONT\nONT\nONT\nOLT\nOptical\nfibers\nFigure 1.7 \u0002 FTTH Internet access\n\ntechnology discussed in detail in Chapter 5. The Ethernet switch, or a network of\nsuch interconnected switches, is then in turn connected into the larger Internet. With\nEthernet access, users typically have 100 Mbps access to the Ethernet switch,\nwhereas servers may have 1 Gbps or even 10 Gbps access.\nIncreasingly, however, people are accessing the Internet wirelessly from lap-\ntops, smartphones, tablets, and other devices (see earlier sidebar on “A Dizzying\nArray of Devices”). In a wireless LAN setting, wireless users transmit/receive pack-\nets to/from an access point that is connected into the enterprise’s network (most\nlikely including wired Ethernet), which in turn is connected to the wired Internet. A\nwireless LAN user must typically be within a few tens of meters of the access point.\nWireless LAN access based on IEEE 802.11 technology, more colloquially known\nas WiFi, is now just about everywhere—universities, business offices, cafes, air-\nports, homes, and even in airplanes. In many cities, one can stand on a street corner\nand be within range of ten or twenty base stations (for a browseable global map of\n802.11 base stations that have been discovered and logged on a Web site by people\nwho take great enjoyment in doing such things, see [wigle.net 2012]). As discussed\nin detail in Chapter 6, 802.11 today provides a shared transmission rate of up to \n54 Mbps.\nEven though Ethernet and WiFi access networks were initially deployed in enter-\nprise (corporate, university) settings, they have recently become relatively common\ncomponents of home networks. Many homes combine broadband residential access\n(that is, cable modems or DSL) with these inexpensive wireless LAN technologies to\ncreate powerful home networks [Edwards 2011]. Figure 1.9 shows a typical home\nnetwork. This home network consists of a roaming laptop as well as a wired PC; a\nbase station (the wireless access point), which communicates with the wireless PC; a\ncable modem, providing broadband access to the Internet; and a router, which inter-\nconnects the base station and the stationary PC with the cable modem. This network\nallows household members to have broadband access to the Internet with one mem-\nber roaming from the kitchen to the backyard to the bedrooms.\nEthernet\nswitch\nInstitutional\nrouter\n100 Mbps\n100 Mbps\n100 Mbps\nServer\nTo Institution’s\nISP\nFigure 1.8 \u0002 Ethernet Internet access\n1.2\n•\nTHE NETWORK EDGE\n17"
    },
    {
      "chunk_id": "6a418a23-2d2b-4928-9225-20f83f1cb657",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.2.2 Physical Media",
      "original_titles": [
        "1.2.2 Physical Media"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.2 The Network Edge > 1.2.2 Physical Media",
      "start_page": 45,
      "end_page": 48,
      "token_count": 2246,
      "text": "Wide-Area Wireless Access: 3G and LTE\nIncreasingly, devices such as iPhones, BlackBerrys, and Android devices are being\nused to send email, surf the Web, Tweet, and download music while on the run.\nThese devices employ the same wireless infrastructure used for cellular telephony\nto send/receive packets through a base station that is operated by the cellular net-\nwork provider. Unlike WiFi, a user need only be within a few tens of kilometers (as\nopposed to a few tens of meters) of the base station.\nTelecommunications companies have made enormous investments in so-called\nthird-generation (3G) wireless, which provides packet-switched wide-area wireless\nInternet access at speeds in excess of 1 Mbps. But even higher-speed wide-area\naccess technologies—a fourth-generation (4G) of wide-area wireless networks—are\nalready being deployed. LTE ( for “Long-Term Evolution”—a candidate for Bad\nAcronym of the Year Award) has its roots in 3G technology, and can potentially\nachieve rates in excess of 10 Mbps. LTE downstream rates of many tens of Mbps\nhave been reported in commercial deployments. We’ll cover the basic principles of\nwireless networks and mobility, as well as WiFi, 3G, and LTE technologies (and\nmore!) in Chapter 6.\n1.2.2 Physical Media\nIn the previous subsection, we gave an overview of some of the most important\nnetwork access technologies in the Internet. As we described these technologies,\nwe also indicated the physical media used. For example, we said that HFC uses a\ncombination of fiber cable and coaxial cable. We said that DSL and Ethernet use\ncopper wire. And we said that mobile access networks use the radio spectrum. \n18\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nCable\nhead end\nHouse\nInternet\nFigure 1.9 \u0002 A typical home network\n\nIn this subsection we provide a brief overview of these and other transmission\nmedia that are commonly used in the Internet.\nIn order to define what is meant by a physical medium, let us reflect on the\nbrief life of a bit. Consider a bit traveling from one end system, through a series of\nlinks and routers, to another end system. This poor bit gets kicked around and\ntransmitted many, many times! The source end system first transmits the bit, and\nshortly thereafter the first router in the series receives the bit; the first router then\ntransmits the bit, and shortly thereafter the second router receives the bit; and so\non. Thus our bit, when traveling from source to destination, passes through a series\nof transmitter-receiver pairs. For each transmitter-receiver pair, the bit is sent by\npropagating electromagnetic waves or optical pulses across a physical medium.\nThe physical medium can take many shapes and forms and does not have to be of\nthe same type for each transmitter-receiver pair along the path. Examples of physi-\ncal media include twisted-pair copper wire, coaxial cable, multimode fiber-optic\ncable, terrestrial radio spectrum, and satellite radio spectrum. Physical media fall\ninto two categories: guided media and unguided media. With guided media, the\nwaves are guided along a solid medium, such as a fiber-optic cable, a twisted-pair\ncopper wire, or a coaxial cable. With unguided media, the waves propagate in the\natmosphere and in outer space, such as in a wireless LAN or a digital satellite\nchannel.\nBut before we get into the characteristics of the various media types, let us say\na few words about their costs. The actual cost of the physical link (copper wire,\nfiber-optic cable, and so on) is often relatively minor compared with other network-\ning costs. In particular, the labor cost associated with the installation of the physical\nlink can be orders of magnitude higher than the cost of the material. For this reason,\nmany builders install twisted pair, optical fiber, and coaxial cable in every room in a\nbuilding. Even if only one medium is initially used, there is a good chance that\nanother medium could be used in the near future, and so money is saved by not hav-\ning to lay additional wires in the future.\nTwisted-Pair Copper Wire\nThe least expensive and most commonly used guided transmission medium is\ntwisted-pair copper wire. For over a hundred years it has been used by telephone\nnetworks. In fact, more than 99 percent of the wired connections from the tele-\nphone handset to the local telephone switch use twisted-pair copper wire. Most of\nus have seen twisted pair in our homes and work environments. Twisted pair con-\nsists of two insulated copper wires, each about 1 mm thick, arranged in a regular\nspiral pattern. The wires are twisted together to reduce the electrical interference\nfrom similar pairs close by. Typically, a number of pairs are bundled together in a\ncable by wrapping the pairs in a protective shield. A wire pair constitutes a single\ncommunication link. Unshielded twisted pair (UTP) is commonly used for\n1.2\n•\nTHE NETWORK EDGE\n19\n\ncomputer networks within a building, that is, for LANs. Data rates for LANs\nusing twisted pair today range from 10 Mbps to 10 Gbps. The data rates that can\nbe achieved depend on the thickness of the wire and the distance between trans-\nmitter and receiver.\nWhen fiber-optic technology emerged in the 1980s, many people disparaged\ntwisted pair because of its relatively low bit rates. Some people even felt that fiber-\noptic technology would completely replace twisted pair. But twisted pair did not\ngive up so easily. Modern twisted-pair technology, such as category 6a cable, can\nachieve data rates of 10 Gbps for distances up to a hundred meters. In the end,\ntwisted pair has emerged as the dominant solution for high-speed LAN networking.\nAs discussed earlier, twisted pair is also commonly used for residential Internet\naccess. We saw that dial-up modem technology enables access at rates of up to 56\nkbps over twisted pair. We also saw that DSL (digital subscriber line) technology\nhas enabled residential users to access the Internet at tens of Mbps over twisted pair\n(when users live close to the ISP’s modem).\nCoaxial Cable\nLike twisted pair, coaxial cable consists of two copper conductors, but the two con-\nductors are concentric rather than parallel. With this construction and special insula-\ntion and shielding, coaxial cable can achieve high data transmission rates. Coaxial\ncable is quite common in cable television systems. As we saw earlier, cable televi-\nsion systems have recently been coupled with cable modems to provide residential\nusers with Internet access at rates of tens of Mbps. In cable television and cable\nInternet access, the transmitter shifts the digital signal to a specific frequency band,\nand the resulting analog signal is sent from the transmitter to one or more receivers.\nCoaxial cable can be used as a guided shared medium. Specifically, a number of\nend systems can be connected directly to the cable, with each of the end systems\nreceiving whatever is sent by the other end systems.\nFiber Optics\nAn optical fiber is a thin, flexible medium that conducts pulses of light, with each\npulse representing a bit. A single optical fiber can support tremendous bit rates, up\nto tens or even hundreds of gigabits per second. They are immune to electromag-\nnetic interference, have very low signal attenuation up to 100 kilometers, and are\nvery hard to tap. These characteristics have made fiber optics the preferred long-\nhaul guided transmission media, particularly for overseas links. Many of the long-\ndistance telephone networks in the United States and elsewhere now use fiber optics\nexclusively. Fiber optics is also prevalent in the backbone of the Internet. However,\nthe high cost of optical devices—such as transmitters, receivers, and switches—has\nhindered their deployment for short-haul transport, such as in a LAN or into the\n20\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\n\nhome in a residential access network. The Optical Carrier (OC) standard link speeds\nrange from 51.8 Mbps to 39.8 Gbps; these specifications are often referred to as OC-\nn, where the link speed equals n × 51.8 Mbps. Standards in use today include OC-1,\nOC-3, OC-12, OC-24, OC-48, OC-96, OC-192, OC-768. [Mukherjee 2006,\nRamaswamy 2010] provide coverage of various aspects of optical networking.\nTerrestrial Radio Channels\nRadio channels carry signals in the electromagnetic spectrum. They are an attractive\nmedium because they require no physical wire to be installed, can penetrate walls,\nprovide connectivity to a mobile user, and can potentially carry a signal for long dis-\ntances. The characteristics of a radio channel depend significantly on the propagation\nenvironment and the distance over which a signal is to be carried. Environmental con-\nsiderations determine path loss and shadow fading (which decrease the signal strength\nas the signal travels over a distance and around/through obstructing objects), multi-\npath fading (due to signal reflection off of interfering objects), and interference (due\nto other transmissions and electromagnetic signals).\nTerrestrial radio channels can be broadly classified into three groups: those that\noperate over very short distance (e.g., with one or two meters); those that operate in\nlocal areas, typically spanning from ten to a few hundred meters; and those that\noperate in the wide area, spanning tens of kilometers. Personal devices such as wire-\nless headsets, keyboards, and medical devices operate over short distances; the \nwireless LAN technologies described in Section 1.2.1 use local-area radio channels;\nthe cellular access technologies use wide-area radio channels. We’ll discuss radio\nchannels in detail in Chapter 6.\nSatellite Radio Channels\nA communication satellite links two or more Earth-based microwave transmitter/\nreceivers, known as ground stations. The satellite receives transmissions on one fre-\nquency band, regenerates the signal using a repeater (discussed below), and transmits\nthe signal on another frequency. Two types of satellites are used in communications:\ngeostationary satellites and low-earth orbiting (LEO) satellites.\nGeostationary satellites permanently remain above the same spot on Earth. This\nstationary presence is achieved by placing the satellite in orbit at 36,000 kilometers\nabove Earth’s surface. This huge distance from ground station through satellite back\nto ground station introduces a substantial signal propagation delay of 280 millisec-\nonds. Nevertheless, satellite links, which can operate at speeds of hundreds of Mbps,\nare often used in areas without access to DSL or cable-based Internet access.\nLEO satellites are placed much closer to Earth and do not remain permanently\nabove one spot on Earth. They rotate around Earth (just as the Moon does) and may\ncommunicate with each other, as well as with ground stations. To provide continuous\n1.2\n•\nTHE NETWORK EDGE\n21"
    },
    {
      "chunk_id": "2a5c36ef-f1c4-4afb-9540-deea3bb7145a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.3 The Network Core",
      "original_titles": [
        "1.3 The Network Core"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.3 The Network Core",
      "start_page": 49,
      "end_page": 53,
      "token_count": 2427,
      "text": "22\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\ncoverage to an area, many satellites need to be placed in orbit. There are currently\nmany low-altitude communication systems in development. Lloyd’s satellite con-\nstellations Web page [Wood 2012] provides and collects information on satellite\nconstellation systems for communications. LEO satellite technology may be used\nfor Internet access sometime in the future.\n1.3 The Network Core\nHaving examined the Internet’s edge, let us now delve more deeply inside the net-\nwork core—the mesh of packet switches and links that interconnects the Internet’s\nend systems. Figure 1.10 highlights the network core with thick, shaded lines.\n1.3.1 Packet Switching\nIn a network application, end systems exchange messages with each other. Mes-\nsages can contain anything the application designer wants. Messages may perform a\ncontrol function (for example, the “Hi” messages in our handshaking example in\nFigure 1.2) or can contain data, such as an email message, a JPEG image, or an MP3\naudio file. To send a message from a source end system to a destination end system,\nthe source breaks long messages into smaller chunks of data known as packets.\nBetween source and destination, each packet travels through communication links\nand packet switches (for which there are two predominant types, routers and link-\nlayer switches). Packets are transmitted over each communication link at a rate\nequal to the full transmission rate of the link. So, if a source end system or a packet\nswitch is sending a packet of L bits over a link with transmission rate R bits/sec, then\nthe time to transmit the packet is L/R seconds.\nStore-and-Forward Transmission\nMost packet switches use store-and-forward transmission at the inputs to the\nlinks. Store-and-forward transmission means that the packet switch must receive\nthe entire packet before it can begin to transmit the first bit of the packet onto the\noutbound link. To explore store-and-forward transmission in more detail, consider\na simple network consisting of two end systems connected by a single router, as\nshown in Figure 1.11. A router will typically have many incident links, since its job\nis to switch an incoming packet onto an outgoing link; in this simple example, the\nrouter has the rather simple task of transferring a packet from one (input) link to\nthe only other attached link. In this example, the source has three packets, each\nconsisting of L bits, to send to the destination. At the snapshot of time shown in\nFigure 1.11, the source has transmitted some of packet 1, and the front of packet 1\nhas already arrived at the router. Because the router employs store-and-forwarding,\nat this instant of time, the router cannot transmit the bits it has received; instead it\n\nNational or\nGlobal ISP\nLocal or\nRegional ISP\nEnterprise Network\nHome Network\nMobile Network\nFigure 1.10 \u0002 The network core\n1.3\n•\nTHE NETWORK CORE\n23\n\n24\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nmust first buffer (i.e., “store”) the packet’s bits. Only after the router has received\nall of the packet’s bits can it begin to transmit (i.e., “forward”) the packet onto the\noutbound link. To gain some insight into store-and-forward transmission, let’s now\ncalculate the amount of time that elapses from when the source begins to send the\npacket until the destination has received the entire packet. (Here we will ignore\npropagation delay—the time it takes for the bits to travel across the wire at near the\nspeed of light—which will be discussed in Section 1.4.) The source begins to trans-\nmit at time 0; at time L/R seconds, the source has transmitted the entire packet, and\nthe entire packet has been received and stored at the router (since there is no propa-\ngation delay). At time L/R seconds, since the router has just received the entire\npacket, it can begin to transmit the packet onto the outbound link towards the desti-\nnation; at time 2L/R, the router has transmitted the entire packet, and the entire\npacket has been received by the destination. Thus, the total delay is 2L/R. If\nthe switch instead forwarded bits as soon as they arrive (without first receiving the\nentire packet), then the total delay would be L/R since bits are not held up at\nthe router. But, as we will discuss in Section 1.4, routers need to receive, store, and\nprocess the entire packet before forwarding.\nNow let’s calculate the amount of time that elapses from when the source\nbegins to send the first packet until the destination has received all three packets.\nAs before, at time L/R, the router begins to forward the first packet. But also at time\nL/R the source will begin to send the second packet, since it has just finished send-\ning the entire first packet. Thus, at time 2L/R, the destination has received the first\npacket and the router has received the second packet. Similarly, at time 3L/R, the\ndestination has received the first two packets and the router has received the third\npacket. Finally, at time 4L/R the destination has received all three packets!\nLet’s now consider the general case of sending one packet from source to desti-\nnation over a path consisting of N links each of rate R (thus, there are N-1 routers\nbetween source and destination). Applying the same logic as above, we see that the\nend-to-end delay is:\n(1.1)\nYou may now want to try to determine what the delay would be for P packets sent\nover a series of N links.\ndend@to@end = N L\nR\nSource\nR bps\n1\n2\nDestination\nFront of packet 1\nstored in router,\nawaiting remaining\nbits before forwarding\n3\nFigure 1.11 \u0002 Store-and-forward packet switching\n\nQueuing Delays and Packet Loss\nEach packet switch has multiple links attached to it. For each attached link, the\npacket switch has an output buffer (also called an output queue), which stores\npackets that the router is about to send into that link. The output buffers play a key\nrole in packet switching. If an arriving packet needs to be transmitted onto a link but\nfinds the link busy with the transmission of another packet, the arriving packet must\nwait in the output buffer. Thus, in addition to the store-and-forward delays, packets\nsuffer output buffer queuing delays. These delays are variable and depend on the\nlevel of congestion in the network. Since the amount of buffer space is finite, an\narriving packet may find that the buffer is completely full with other packets wait-\ning for transmission. In this case, packet loss will occur—either the arriving packet\nor one of the already-queued packets will be dropped.\nFigure 1.12 illustrates a simple packet-switched network. As in Figure 1.11,\npackets are represented by three-dimensional slabs. The width of a slab represents\nthe number of bits in the packet. In this figure, all packets have the same width and\nhence the same length. Suppose Hosts A and B are sending packets to Host E. Hosts\nA and B first send their packets along 10 Mbps Ethernet links to the first router. The\nrouter then directs these packets to the 1.5 Mbps link. If, during a short interval of\ntime, the arrival rate of packets to the router (when converted to bits per second)\nexceeds 1.5 Mbps, congestion will occur at the router as packets queue in the link’s\noutput buffer before being transmitted onto the link. For example, if Host A and B\neach send a burst of five packets back-to-back at the same time, then most of these\npackets will spend some time waiting in the queue. The situation is, in fact, entirely\nanalogous to many common-day situations—for example, when we wait in line for\na bank teller or wait in front of a tollbooth. We’ll examine this queuing delay in\nmore detail in Section 1.4.\n1.3\n•\nTHE NETWORK CORE\n25\n10 Mbps Ethernet\nKey:\nPackets\nA\nB\nC\nD\nE\n1.5 Mbps\nQueue of\npackets waiting\nfor output link\nFigure 1.12 \u0002 Packet switching\n\n26\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nForwarding Tables and Routing Protocols\nEarlier, we said that a router takes a packet arriving on one of its attached \ncommunication links and forwards that packet onto another one of its attached com-\nmunication links. But how does the router determine which link it should forward\nthe packet onto? Packet forwarding is actually done in different ways in different\ntypes of computer networks. Here, we briefly describe how it is done in the \nInternet.\nIn the Internet, every end system has an address called an IP address. When a\nsource end system wants to send a packet to a destination end system, the source\nincludes the destination’s IP address in the packet’s header. As with postal addresses,\nthis address has a hierarchical structure. When a packet arrives at a router in the \nnetwork, the router examines a portion of the packet’s destination address and for-\nwards the packet to an adjacent router. More specifically, each router has a\nforwarding table that maps destination addresses (or portions of the destination\naddresses) to that router’s outbound links. When a packet arrives at a router, the\nrouter examines the address and searches its forwarding table, using this destination\naddress, to find the appropriate outbound link. The router then directs the packet to\nthis outbound link.\nThe end-to-end routing process is analogous to a car driver who does not use\nmaps but instead prefers to ask for directions. For example, suppose Joe is driving\nfrom Philadelphia to 156 Lakeside Drive in Orlando, Florida. Joe first drives to his\nneighborhood gas station and asks how to get to 156 Lakeside Drive in Orlando,\nFlorida. The gas station attendant extracts the Florida portion of the address and\ntells Joe that he needs to get onto the interstate highway I-95 South, which has an\nentrance just next to the gas station. He also tells Joe that once he enters Florida,\nhe should ask someone else there. Joe then takes I-95 South until he gets to Jack-\nsonville, Florida, at which point he asks another gas station attendant for directions.\nThe attendant extracts the Orlando portion of the address and tells Joe that he\nshould continue on I-95 to Daytona Beach and then ask someone else. In Daytona\nBeach, another gas station attendant also extracts the Orlando portion of the\naddress and tells Joe that he should take I-4 directly to Orlando. Joe takes I-4 and\ngets off at the Orlando exit. Joe goes to another gas station attendant, and this time\nthe attendant extracts the Lakeside Drive portion of the address and tells Joe the\nroad he must follow to get to Lakeside Drive. Once Joe reaches Lakeside Drive, he\nasks a kid on a bicycle how to get to his destination. The kid extracts the 156 por-\ntion of the address and points to the house. Joe finally reaches his ultimate destina-\ntion. In the above analogy, the gas station attendants and kids on bicycles are\nanalogous to routers.\nWe just learned that a router uses a packet’s destination address to index a for-\nwarding table and determine the appropriate outbound link. But this statement begs\nyet another question: How do forwarding tables get set? Are they configured by"
    },
    {
      "chunk_id": "7df94b9c-903c-405f-8fe5-d443263a852e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.3.1 Packet Switching",
      "original_titles": [
        "1.3.1 Packet Switching"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.3 The Network Core > 1.3.1 Packet Switching",
      "start_page": 49,
      "end_page": 53,
      "token_count": 2427,
      "text": "22\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\ncoverage to an area, many satellites need to be placed in orbit. There are currently\nmany low-altitude communication systems in development. Lloyd’s satellite con-\nstellations Web page [Wood 2012] provides and collects information on satellite\nconstellation systems for communications. LEO satellite technology may be used\nfor Internet access sometime in the future.\n1.3 The Network Core\nHaving examined the Internet’s edge, let us now delve more deeply inside the net-\nwork core—the mesh of packet switches and links that interconnects the Internet’s\nend systems. Figure 1.10 highlights the network core with thick, shaded lines.\n1.3.1 Packet Switching\nIn a network application, end systems exchange messages with each other. Mes-\nsages can contain anything the application designer wants. Messages may perform a\ncontrol function (for example, the “Hi” messages in our handshaking example in\nFigure 1.2) or can contain data, such as an email message, a JPEG image, or an MP3\naudio file. To send a message from a source end system to a destination end system,\nthe source breaks long messages into smaller chunks of data known as packets.\nBetween source and destination, each packet travels through communication links\nand packet switches (for which there are two predominant types, routers and link-\nlayer switches). Packets are transmitted over each communication link at a rate\nequal to the full transmission rate of the link. So, if a source end system or a packet\nswitch is sending a packet of L bits over a link with transmission rate R bits/sec, then\nthe time to transmit the packet is L/R seconds.\nStore-and-Forward Transmission\nMost packet switches use store-and-forward transmission at the inputs to the\nlinks. Store-and-forward transmission means that the packet switch must receive\nthe entire packet before it can begin to transmit the first bit of the packet onto the\noutbound link. To explore store-and-forward transmission in more detail, consider\na simple network consisting of two end systems connected by a single router, as\nshown in Figure 1.11. A router will typically have many incident links, since its job\nis to switch an incoming packet onto an outgoing link; in this simple example, the\nrouter has the rather simple task of transferring a packet from one (input) link to\nthe only other attached link. In this example, the source has three packets, each\nconsisting of L bits, to send to the destination. At the snapshot of time shown in\nFigure 1.11, the source has transmitted some of packet 1, and the front of packet 1\nhas already arrived at the router. Because the router employs store-and-forwarding,\nat this instant of time, the router cannot transmit the bits it has received; instead it\n\nNational or\nGlobal ISP\nLocal or\nRegional ISP\nEnterprise Network\nHome Network\nMobile Network\nFigure 1.10 \u0002 The network core\n1.3\n•\nTHE NETWORK CORE\n23\n\n24\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nmust first buffer (i.e., “store”) the packet’s bits. Only after the router has received\nall of the packet’s bits can it begin to transmit (i.e., “forward”) the packet onto the\noutbound link. To gain some insight into store-and-forward transmission, let’s now\ncalculate the amount of time that elapses from when the source begins to send the\npacket until the destination has received the entire packet. (Here we will ignore\npropagation delay—the time it takes for the bits to travel across the wire at near the\nspeed of light—which will be discussed in Section 1.4.) The source begins to trans-\nmit at time 0; at time L/R seconds, the source has transmitted the entire packet, and\nthe entire packet has been received and stored at the router (since there is no propa-\ngation delay). At time L/R seconds, since the router has just received the entire\npacket, it can begin to transmit the packet onto the outbound link towards the desti-\nnation; at time 2L/R, the router has transmitted the entire packet, and the entire\npacket has been received by the destination. Thus, the total delay is 2L/R. If\nthe switch instead forwarded bits as soon as they arrive (without first receiving the\nentire packet), then the total delay would be L/R since bits are not held up at\nthe router. But, as we will discuss in Section 1.4, routers need to receive, store, and\nprocess the entire packet before forwarding.\nNow let’s calculate the amount of time that elapses from when the source\nbegins to send the first packet until the destination has received all three packets.\nAs before, at time L/R, the router begins to forward the first packet. But also at time\nL/R the source will begin to send the second packet, since it has just finished send-\ning the entire first packet. Thus, at time 2L/R, the destination has received the first\npacket and the router has received the second packet. Similarly, at time 3L/R, the\ndestination has received the first two packets and the router has received the third\npacket. Finally, at time 4L/R the destination has received all three packets!\nLet’s now consider the general case of sending one packet from source to desti-\nnation over a path consisting of N links each of rate R (thus, there are N-1 routers\nbetween source and destination). Applying the same logic as above, we see that the\nend-to-end delay is:\n(1.1)\nYou may now want to try to determine what the delay would be for P packets sent\nover a series of N links.\ndend@to@end = N L\nR\nSource\nR bps\n1\n2\nDestination\nFront of packet 1\nstored in router,\nawaiting remaining\nbits before forwarding\n3\nFigure 1.11 \u0002 Store-and-forward packet switching\n\nQueuing Delays and Packet Loss\nEach packet switch has multiple links attached to it. For each attached link, the\npacket switch has an output buffer (also called an output queue), which stores\npackets that the router is about to send into that link. The output buffers play a key\nrole in packet switching. If an arriving packet needs to be transmitted onto a link but\nfinds the link busy with the transmission of another packet, the arriving packet must\nwait in the output buffer. Thus, in addition to the store-and-forward delays, packets\nsuffer output buffer queuing delays. These delays are variable and depend on the\nlevel of congestion in the network. Since the amount of buffer space is finite, an\narriving packet may find that the buffer is completely full with other packets wait-\ning for transmission. In this case, packet loss will occur—either the arriving packet\nor one of the already-queued packets will be dropped.\nFigure 1.12 illustrates a simple packet-switched network. As in Figure 1.11,\npackets are represented by three-dimensional slabs. The width of a slab represents\nthe number of bits in the packet. In this figure, all packets have the same width and\nhence the same length. Suppose Hosts A and B are sending packets to Host E. Hosts\nA and B first send their packets along 10 Mbps Ethernet links to the first router. The\nrouter then directs these packets to the 1.5 Mbps link. If, during a short interval of\ntime, the arrival rate of packets to the router (when converted to bits per second)\nexceeds 1.5 Mbps, congestion will occur at the router as packets queue in the link’s\noutput buffer before being transmitted onto the link. For example, if Host A and B\neach send a burst of five packets back-to-back at the same time, then most of these\npackets will spend some time waiting in the queue. The situation is, in fact, entirely\nanalogous to many common-day situations—for example, when we wait in line for\na bank teller or wait in front of a tollbooth. We’ll examine this queuing delay in\nmore detail in Section 1.4.\n1.3\n•\nTHE NETWORK CORE\n25\n10 Mbps Ethernet\nKey:\nPackets\nA\nB\nC\nD\nE\n1.5 Mbps\nQueue of\npackets waiting\nfor output link\nFigure 1.12 \u0002 Packet switching\n\n26\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nForwarding Tables and Routing Protocols\nEarlier, we said that a router takes a packet arriving on one of its attached \ncommunication links and forwards that packet onto another one of its attached com-\nmunication links. But how does the router determine which link it should forward\nthe packet onto? Packet forwarding is actually done in different ways in different\ntypes of computer networks. Here, we briefly describe how it is done in the \nInternet.\nIn the Internet, every end system has an address called an IP address. When a\nsource end system wants to send a packet to a destination end system, the source\nincludes the destination’s IP address in the packet’s header. As with postal addresses,\nthis address has a hierarchical structure. When a packet arrives at a router in the \nnetwork, the router examines a portion of the packet’s destination address and for-\nwards the packet to an adjacent router. More specifically, each router has a\nforwarding table that maps destination addresses (or portions of the destination\naddresses) to that router’s outbound links. When a packet arrives at a router, the\nrouter examines the address and searches its forwarding table, using this destination\naddress, to find the appropriate outbound link. The router then directs the packet to\nthis outbound link.\nThe end-to-end routing process is analogous to a car driver who does not use\nmaps but instead prefers to ask for directions. For example, suppose Joe is driving\nfrom Philadelphia to 156 Lakeside Drive in Orlando, Florida. Joe first drives to his\nneighborhood gas station and asks how to get to 156 Lakeside Drive in Orlando,\nFlorida. The gas station attendant extracts the Florida portion of the address and\ntells Joe that he needs to get onto the interstate highway I-95 South, which has an\nentrance just next to the gas station. He also tells Joe that once he enters Florida,\nhe should ask someone else there. Joe then takes I-95 South until he gets to Jack-\nsonville, Florida, at which point he asks another gas station attendant for directions.\nThe attendant extracts the Orlando portion of the address and tells Joe that he\nshould continue on I-95 to Daytona Beach and then ask someone else. In Daytona\nBeach, another gas station attendant also extracts the Orlando portion of the\naddress and tells Joe that he should take I-4 directly to Orlando. Joe takes I-4 and\ngets off at the Orlando exit. Joe goes to another gas station attendant, and this time\nthe attendant extracts the Lakeside Drive portion of the address and tells Joe the\nroad he must follow to get to Lakeside Drive. Once Joe reaches Lakeside Drive, he\nasks a kid on a bicycle how to get to his destination. The kid extracts the 156 por-\ntion of the address and points to the house. Joe finally reaches his ultimate destina-\ntion. In the above analogy, the gas station attendants and kids on bicycles are\nanalogous to routers.\nWe just learned that a router uses a packet’s destination address to index a for-\nwarding table and determine the appropriate outbound link. But this statement begs\nyet another question: How do forwarding tables get set? Are they configured by"
    },
    {
      "chunk_id": "dfc1f93f-a9ee-431c-9799-84014e56206a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.3.2 Circuit Switching",
      "original_titles": [
        "1.3.2 Circuit Switching"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.3 The Network Core > 1.3.2 Circuit Switching",
      "start_page": 54,
      "end_page": 58,
      "token_count": 2999,
      "text": "hand in each and every router, or does the Internet use a more automated procedure?\nThis issue will be studied in depth in Chapter 4. But to whet your appetite here,\nwe’ll note now that the Internet has a number of special routing protocols that are\nused to automatically set the forwarding tables. A routing protocol may, for exam-\nple, determine the shortest path from each router to each destination and use the\nshortest path results to configure the forwarding tables in the routers.\nHow would you actually like to see the end-to-end route that packets take in the\nInternet? We now invite you to get your hands dirty by interacting with the Trace-\nroute program. Simply visit the site www.traceroute.org, choose a source in a particu-\nlar country, and trace the route from that source to your computer. (For a discussion of\nTraceroute, see Section 1.4.)\n1.3.2 Circuit Switching\nThere are two fundamental approaches to moving data through a network of links\nand switches: circuit switching and packet switching. Having covered packet-\nswitched networks in the previous subsection, we now turn our attention to circuit-\nswitched networks.\nIn circuit-switched networks, the resources needed along a path (buffers, link\ntransmission rate) to provide for communication between the end systems are\nreserved for the duration of the communication session between the end systems. In\npacket-switched networks, these resources are not reserved; a session’s messages\nuse the resources on demand, and as a consequence, may have to wait (that is,\nqueue) for access to a communication link. As a simple analogy, consider two\nrestaurants, one that requires reservations and another that neither requires reserva-\ntions nor accepts them. For the restaurant that requires reservations, we have to go\nthrough the hassle of calling before we leave home. But when we arrive at the\nrestaurant we can, in principle, immediately be seated and order our meal. For the\nrestaurant that does not require reservations, we don’t need to bother to reserve a\ntable. But when we arrive at the restaurant, we may have to wait for a table before\nwe can be seated.\nTraditional telephone networks are examples of circuit-switched networks.\nConsider what happens when one person wants to send information (voice or fac-\nsimile) to another over a telephone network. Before the sender can send the infor-\nmation, the network must establish a connection between the sender and the\nreceiver. This is a bona fide connection for which the switches on the path\nbetween the sender and receiver maintain connection state for that connection. In\nthe jargon of telephony, this connection is called a circuit. When the network\nestablishes the circuit, it also reserves a constant transmission rate in the net-\nwork’s links (representing a fraction of each link’s transmission capacity) for the\nduration of the connection. Since a given transmission rate has been reserved for\nthis sender-to-receiver connection, the sender can transfer the data to the receiver\nat the guaranteed constant rate.\n1.3\n•\nTHE NETWORK CORE\n27\n\n28\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nFigure 1.13 \u0002 A simple circuit-switched network consisting of\nfour switches and four links\nFigure 1.13 illustrates a circuit-switched network. In this network, the four cir-\ncuit switches are interconnected by four links. Each of these links has four circuits,\nso that each link can support four simultaneous connections. The hosts (for exam-\nple, PCs and workstations) are each directly connected to one of the switches. When\ntwo hosts want to communicate, the network establishes a dedicated end-to-end\nconnection between the two hosts. Thus, in order for Host A to communicate with\nHost B, the network must first reserve one circuit on each of two links. In this exam-\nple, the dedicated end-to-end connection uses the second circuit in the first link and\nthe fourth circuit in the second link. Because each link has four circuits, for each\nlink used by the end-to-end connection, the connection gets one fourth of the link’s\ntotal transmission capacity for the duration of the connection. Thus, for example, if\neach link between adjacent switches has a transmission rate of 1 Mbps, then each\nend-to-end circuit-switch connection gets 250 kbps of dedicated transmission rate.\nIn contrast, consider what happens when one host wants to send a packet to\nanother host over a packet-switched network, such as the Internet. As with circuit\nswitching, the packet is transmitted over a series of communication links. But different\nfrom circuit switching, the packet is sent into the network without reserving any link\nresources whatsoever. If one of the links is congested because other packets need to be\ntransmitted over the link at the same time, then the packet will have to wait in a buffer\nat the sending side of the transmission link and suffer a delay. The Internet makes its\nbest effort to deliver packets in a timely manner, but it does not make any guarantees.\nMultiplexing in Circuit-Switched Networks\nA circuit in a link is implemented with either frequency-division multiplexing\n(FDM) or time-division multiplexing (TDM). With FDM, the frequency spec-\ntrum of a link is divided up among the connections established across the link.\n\nSpecifically, the link dedicates a frequency band to each connection for the\nduration of the connection. In telephone networks, this frequency band typically\nhas a width of 4 kHz (that is, 4,000 hertz or 4,000 cycles per second). The width\nof the band is called, not surprisingly, the bandwidth. FM radio stations also use\nFDM to share the frequency spectrum between 88 MHz and 108 MHz, with each\nstation being allocated a specific frequency band.\nFor a TDM link, time is divided into frames of fixed duration, and each frame\nis divided into a fixed number of time slots. When the network establishes a connec-\ntion across a link, the network dedicates one time slot in every frame to this connec-\ntion. These slots are dedicated for the sole use of that connection, with one time slot\navailable for use (in every frame) to transmit the connection’s data.\nFigure 1.14 illustrates FDM and TDM for a specific network link supporting up\nto four circuits. For FDM, the frequency domain is segmented into four bands, each\nof bandwidth 4 kHz. For TDM, the time domain is segmented into frames, with four\ntime slots in each frame; each circuit is assigned the same dedicated slot in the\nrevolving TDM frames. For TDM, the transmission rate of a circuit is equal to the\nframe rate multiplied by the number of bits in a slot. For example, if the link trans-\nmits 8,000 frames per second and each slot consists of 8 bits, then the transmission\nrate of a circuit is 64 kbps.\nProponents of packet switching have always argued that circuit switching is\nwasteful because the dedicated circuits are idle during silent periods. For example,\n1.3\n•\nTHE NETWORK CORE\n29\n4KHz\nTDM\nFDM\nLink\nFrequency\n4KHz\nSlot\nKey:\nAll slots labeled “2” are dedicated\nto a specific sender-receiver pair.\nFrame\n1\n2\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nTime\nFigure 1.14 \u0002 With FDM, each circuit continuously gets a fraction of the\nbandwidth. With TDM, each circuit gets all of the bandwidth\nperiodically during brief intervals of time (that is, during slots)\n\n30\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nwhen one person in a telephone call stops talking, the idle network resources (fre-\nquency bands or time slots in the links along the connection’s route) cannot be used\nby other ongoing connections. As another example of how these resources can be\nunderutilized, consider a radiologist who uses a circuit-switched network to\nremotely access a series of x-rays. The radiologist sets up a connection, requests an\nimage, contemplates the image, and then requests a new image. Network resources\nare allocated to the connection but are not used (i.e., are wasted) during the radiolo-\ngist’s contemplation periods. Proponents of packet switching also enjoy pointing out\nthat establishing end-to-end circuits and reserving end-to-end transmission capacity\nis complicated and requires complex signaling software to coordinate the operation\nof the switches along the end-to-end path.\nBefore we finish our discussion of circuit switching, let’s work through a numeri-\ncal example that should shed further insight on the topic. Let us consider how long it\ntakes to send a file of 640,000 bits from Host A to Host B over a circuit-switched net-\nwork. Suppose that all links in the network use TDM with 24 slots and have a bit rate\nof 1.536 Mbps. Also suppose that it takes 500 msec to establish an end-to-end circuit\nbefore Host A can begin to transmit the file. How long does it take to send the file?\nEach circuit has a transmission rate of (1.536 Mbps)/24 = 64 kbps, so it takes (640,000\nbits)/(64 kbps) = 10 seconds to transmit the file. To this 10 seconds we add the circuit\nestablishment time, giving 10.5 seconds to send the file. Note that the transmission\ntime is independent of the number of links: The transmission time would be 10 sec-\nonds if the end-to-end circuit passed through one link or a hundred links. (The actual\nend-to-end delay also includes a propagation delay; see Section 1.4.)\nPacket Switching Versus Circuit Switching\nHaving described circuit switching and packet switching, let us compare the two.\nCritics of packet switching have often argued that packet switching is not suitable\nfor real-time services (for example, telephone calls and video conference calls)\nbecause of its variable and unpredictable end-to-end delays (due primarily to vari-\nable and unpredictable queuing delays). Proponents of packet switching argue that\n(1) it offers better sharing of transmission capacity than circuit switching and (2) it\nis simpler, more efficient, and less costly to implement than circuit switching. An\ninteresting discussion of packet switching versus circuit switching is [Molinero-\nFernandez 2002]. Generally speaking, people who do not like to hassle with restau-\nrant reservations prefer packet switching to circuit switching.\nWhy is packet switching more efficient? Let’s look at a simple example. Sup-\npose users share a 1 Mbps link. Also suppose that each user alternates between peri-\nods of activity, when a user generates data at a constant rate of 100 kbps, and periods\nof inactivity, when a user generates no data. Suppose further that a user is active\nonly 10 percent of the time (and is idly drinking coffee during the remaining 90 per-\ncent of the time). With circuit switching, 100 kbps must be reserved for each user at\n\nall times. For example, with circuit-switched TDM, if a one-second frame is divided\ninto 10 time slots of 100 ms each, then each user would be allocated one time slot\nper frame.\nThus, the circuit-switched link can support only 10 (= 1 Mbps/100 kbps) simul-\ntaneous users. With packet switching, the probability that a specific user is active is\n0.1 (that is, 10 percent). If there are 35 users, the probability that there are 11 or\nmore simultaneously active users is approximately 0.0004. (Homework Problem P8\noutlines how this probability is obtained.) When there are 10 or fewer simultane-\nously active users (which happens with probability 0.9996), the aggregate arrival\nrate of data is less than or equal to 1 Mbps, the output rate of the link. Thus, when\nthere are 10 or fewer active users, users’ packets flow through the link essentially\nwithout delay, as is the case with circuit switching. When there are more than 10\nsimultaneously active users, then the aggregate arrival rate of packets exceeds the\noutput capacity of the link, and the output queue will begin to grow. (It continues to\ngrow until the aggregate input rate falls back below 1 Mbps, at which point the\nqueue will begin to diminish in length.) Because the probability of having more than\n10 simultaneously active users is minuscule in this example, packet switching pro-\nvides essentially the same performance as circuit switching, but does so while\nallowing for more than three times the number of users.\nLet’s now consider a second simple example. Suppose there are 10 users and that\none user suddenly generates one thousand 1,000-bit packets, while other users\nremain quiescent and do not generate packets. Under TDM circuit switching with 10\nslots per frame and each slot consisting of 1,000 bits, the active user can only use its\none time slot per frame to transmit data, while the remaining nine time slots in each\nframe remain idle. It will be 10 seconds before all of the active user’s one million bits\nof data has been transmitted. In the case of packet switching, the active user can con-\ntinuously send its packets at the full link rate of 1 Mbps, since there are no other users\ngenerating packets that need to be multiplexed with the active user’s packets. In this\ncase, all of the active user’s data will be transmitted within 1 second.\nThe above examples illustrate two ways in which the performance of packet\nswitching can be superior to that of circuit switching. They also highlight the crucial\ndifference between the two forms of sharing a link’s transmission rate among multi-\nple data streams. Circuit switching pre-allocates use of the transmission link regard-\nless of demand, with allocated but unneeded link time going unused. Packet\nswitching on the other hand allocates link use on demand. Link transmission capac-\nity will be shared on a packet-by-packet basis only among those users who have\npackets that need to be transmitted over the link.\nAlthough packet switching and circuit switching are both prevalent in today’s\ntelecommunication networks, the trend has certainly been in the direction of packet\nswitching. Even many of today’s circuit-switched telephone networks are slowly\nmigrating toward packet switching. In particular, telephone networks often use\npacket switching for the expensive overseas portion of a telephone call.\n1.3\n•\nTHE NETWORK CORE\n31"
    },
    {
      "chunk_id": "f7caae22-9d21-4749-8885-16b2e6e1c16d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.3.3 A Network of Networks",
      "original_titles": [
        "1.3.3 A Network of Networks"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.3 The Network Core > 1.3.3 A Network of Networks",
      "start_page": 59,
      "end_page": 61,
      "token_count": 1950,
      "text": "1.3.3 A Network of Networks\nWe saw earlier that end systems (PCs, smartphones, Web servers, mail servers,\nand so on) connect into the Internet via an access ISP. The access ISP can pro-\nvide either wired or wireless connectivity, using an array of access technologies\nincluding DSL, cable, FTTH, Wi-Fi, and cellular. Note that the access ISP does\nnot have to be a telco or a cable company; instead it can be, for example, a uni-\nversity (providing Internet access to students, staff, and faculty), or a company\n(providing access for its employees). But connecting end users and content\nproviders into an access ISP is only a small piece of solving the puzzle of con-\nnecting the billions of end systems that make up the Internet. To complete this\npuzzle, the access ISPs themselves must be interconnected. This is done by cre-\nating a network of networks—understanding this phrase is the key to understand-\ning the Internet.\nOver the years, the network of networks that forms the Internet has evolved into\na very complex structure. Much of this evolution is driven by economics and\nnational policy, rather than by performance considerations. In order to understand\ntoday’s Internet network structure, let’s incrementally build a series of network\nstructures, with each new structure being a better approximation of the complex\nInternet that we have today. Recall that the overarching goal is to interconnect the\naccess ISPs so that all end systems can send packets to each other. One naive\napproach would be to have each access ISP directly connect with every other access\nISP. Such a mesh design is, of course, much too costly for the access ISPs, as it\nwould require each access ISP to have a separate communication link to each of the\nhundreds of thousands of other access ISPs all over the world.\nOur first network structure, Network Structure 1, interconnects all of the\naccess ISPs with a single global transit ISP. Our (imaginary) global transit ISP is\na network of routers and communication links that not only spans the globe, but\nalso has at least one router near each of the hundreds of thousands of access ISPs.\nOf course, it would be very costly for the global ISP to build such an extensive\nnetwork. To be profitable, it would naturally charge each of the access ISPs for\nconnectivity, with the pricing reflecting (but not necessarily directly proportional\nto) the amount of traffic an access ISP exchanges with the global ISP. Since the\naccess ISP pays the global transit ISP, the access ISP is said to be a customer and\nthe global transit ISP is said to be a provider.\nNow if some company builds and operates a global transit ISP that is profitable,\nthen it is natural for other companies to build their own global transit ISPs and com-\npete with the original global transit ISP. This leads to Network Structure 2, which\nconsists of the hundreds of thousands of access ISPs and multiple global transit\nISPs. The access ISPs certainly prefer Network Structure 2 over Network Structure\n1 since they can now choose among the competing global transit providers as a\nfunction of their pricing and services. Note, however, that the global transit ISPs\n32\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\n\nthemselves must interconnect: Otherwise access ISPs connected to one of the global\ntransit providers would not be able to communicate with access ISPs connected to\nthe other global transit providers.\nNetwork Structure 2, just described, is a two-tier hierarchy with global transit\nproviders residing at the top tier and access ISPs at the bottom tier. This assumes\nthat global transit ISPs are not only capable of getting close to each and every access\nISP, but also find it economically desirable to do so. In reality, although some ISPs\ndo have impressive global coverage and do directly connect with many access ISPs,\nno ISP has presence in each and every city in the world. Instead, in any given region,\nthere may be a regional ISP to which the access ISPs in the region connect. Each\nregional ISP then connects to tier-1 ISPs. Tier-1 ISPs are similar to our (imaginary)\nglobal transit ISP; but tier-1 ISPs, which actually do exist, do not have a presence in\nevery city in the world. There are approximately a dozen tier-1 ISPs, including\nLevel 3 Communications, AT&T, Sprint, and NTT. Interestingly, no group officially\nsanctions tier-1 status; as the saying goes—if you have to ask if you’re a member of\na group, you’re probably not.\nReturning to this network of networks, not only are there multiple competing tier-\n1 ISPs, there may be multiple competing regional ISPs in a region. In such a hierar-\nchy, each access ISP pays the regional ISP to which it connects, and each regional ISP\npays the tier-1 ISP to which it connects. (An access ISP can also connect directly to a\ntier-1 ISP, in which case it pays the tier-1 ISP). Thus, there is customer-provider\nrelationship at each level of the hierarchy. Note that the tier-1 ISPs do not pay anyone\nas they are at the top of the hierarchy. To further complicate matters, in some regions,\nthere may be a larger regional ISP (possibly spanning an entire country) to which the\nsmaller regional ISPs in that region connect; the larger regional ISP then connects to a\ntier-1 ISP. For example, in China, there are access ISPs in each city, which connect to\nprovincial ISPs, which in turn connect to national ISPs, which finally connect to tier-1\nISPs [Tian 2012]. We refer to this multi-tier hierarchy, which is still only a crude\napproximation of today’s Internet, as Network Structure 3.\nTo build a network that more closely resembles today’s Internet, we must add\npoints of presence (PoPs), multi-homing, peering, and Internet exchange points\n(IXPs) to the hierarchical Network Structure 3. PoPs exist in all levels of the hier-\narchy, except for the bottom (access ISP) level. A PoP is simply a group of one or\nmore routers (at the same location) in the provider’s network where customer ISPs\ncan connect into the provider ISP. For a customer network to connect to a\nprovider’s PoP, it can lease a high-speed link from a third-party telecommunica-\ntions provider to directly connect one of its routers to a router at the PoP. Any ISP\n(except for tier-1 ISPs) may choose to multi-home, that is, to connect to two or\nmore provider ISPs. So, for example, an access ISP may multi-home with two\nregional ISPs, or it may multi-home with two regional ISPs and also with a tier-1\nISP. Similarly, a regional ISP may multi-home with multiple tier-1 ISPs. When an\n1.3\n•\nTHE NETWORK CORE\n33\n\nISP multi-homes, it can continue to send and receive packets into the Internet even\nif one of its providers has a failure.\nAs we just learned, customer ISPs pay their provider ISPs to obtain global\nInternet interconnectivity. The amount that a customer ISP pays a provider ISP\nreflects the amount of traffic it exchanges with the provider. To reduce these costs, a\npair of nearby ISPs at the same level of the hierarchy can peer, that is, they can\ndirectly connect their networks together so that all the traffic between them passes\nover the direct connection rather than through upstream intermediaries. When two\nISPs peer, it is typically settlement-free, that is, neither ISP pays the other. As noted\nearlier, tier-1 ISPs also peer with one another, settlement-free. For a readable dis-\ncussion of peering and customer-provider relationships, see [Van der Berg 2008].\nAlong these same lines, a third-party company can create an Internet Exchange\nPoint (IXP) (typically in a stand-alone building with its own switches), which is a\nmeeting point where multiple ISPs can peer together. There are roughly 300 IXPs in\nthe Internet today [Augustin 2009]. We refer to this ecosystem—consisting of\naccess ISPs, regional ISPs, tier-1 ISPs, PoPs, multi-homing, peering, and IXPs—as\nNetwork Structure 4.\nWe now finally arrive at Network Structure 5, which describes the Internet of\n2012. Network Structure 5, illustrated in Figure 1.15, builds on top of Network\nStructure 4 by adding content provider networks. Google is currently one of the\nleading examples of such a content provider network. As of this writing, it is esti-\nmated that Google has 30 to 50 data centers distributed across North America,\nEurope, Asia, South America, and Australia. Some of these data centers house\nover one hundred thousand servers, while other data centers are smaller, housing\nonly hundreds of servers. The Google data centers are all interconnected via\nGoogle’s private TCP/IP network, which spans the entire globe but is nevertheless\nseparate from the public Internet. Importantly, the Google private network only\n34\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\naccess\nISP\naccess\nISP\naccess\nISP\naccess\nISP\naccess\nISP\naccess\nISP\naccess\nISP\naccess\nISP\nRegional\nISP\nTier 1\nISP\nContent provider\n(e.g., Google)\nTier 1\nISP\nIXP\nRegional\nISP\nIXP\nIXP\nFigure 1.15 \u0002 Interconnection of ISPs"
    },
    {
      "chunk_id": "9845c63f-8a1e-4120-bfb0-4624ef8aa227",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.4 Delay, Loss, and Throughput in Packet-Switched Networks",
      "original_titles": [
        "1.4 Delay, Loss, and Throughput in Packet-Switched Networks"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.4 Delay, Loss, and Throughput in Packet-Switched Networks",
      "start_page": 62,
      "end_page": 65,
      "token_count": 2303,
      "text": "carries traffic to/from Google servers. As shown in Figure 1.15, the Google\nprivate network attempts to “bypass” the upper tiers of the Internet by peering\n(settlement free) with lower-tier ISPs, either by directly connecting with them or\nby connecting with them at IXPs [Labovitz 2010]. However, because many access\nISPs can still only be reached by transiting through tier-1 networks, the Google\nnetwork also connects to tier-1 ISPs, and pays those ISPs for the traffic it\nexchanges with them. By creating its own network, a content provider not only\nreduces its payments to upper-tier ISPs, but also has greater control of how its\nservices are ultimately delivered to end users. Google’s network infrastructure is\ndescribed in greater detail in Section 7.2.4.\nIn summary, today’s Internet—a network of networks—is complex, consisting of\na dozen or so tier-1 ISPs and hundreds of thousands of lower-tier ISPs. The ISPs are\ndiverse in their coverage, with some spanning multiple continents and oceans, and\nothers limited to narrow geographic regions. The lower-tier ISPs connect to the\nhigher-tier ISPs, and the higher-tier ISPs interconnect with one another. Users and\ncontent providers are customers of lower-tier ISPs, and lower-tier ISPs are customers\nof higher-tier ISPs. In recent years, major content providers have also created their\nown networks and connect directly into lower-tier ISPs where possible.\n1.4 Delay, Loss, and Throughput \nin Packet-Switched Networks\nBack in Section 1.1 we said that the Internet can be viewed as an infrastructure\nthat provides services to distributed applications running on end systems. Ideally,\nwe would like Internet services to be able to move as much data as we want\nbetween any two end systems, instantaneously, without any loss of data. Alas, this\nis a lofty goal, one that is unachievable in reality. Instead, computer networks nec-\nessarily constrain throughput (the amount of data per second that can be trans-\nferred) between end systems, introduce delays between end systems, and can\nactually lose packets. On one hand, it is unfortunate that the physical laws of real-\nity introduce delay and loss as well as constrain throughput. On the other hand,\nbecause computer networks have these problems, there are many fascinating\nissues surrounding how to deal with the problems—more than enough issues to\nfill a course on computer networking and to motivate thousands of PhD theses! In\nthis section, we’ll begin to examine and quantify delay, loss, and throughput in\ncomputer networks.\n1.4.1 Overview of Delay in Packet-Switched Networks\nRecall that a packet starts in a host (the source), passes through a series of routers,\nand ends its journey in another host (the destination). As a packet travels from one\nnode (host or router) to the subsequent node (host or router) along this path, the\n1.4\n•\nDELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS\n35\n\npacket suffers from several types of delays at each node along the path. The most\nimportant of these delays are the nodal processing delay, queuing delay, transmis-\nsion delay, and propagation delay; together, these delays accumulate to give a total\nnodal delay. The performance of many Internet applications—such as search, Web\nbrowsing, email, maps, instant messaging, and voice-over-IP—are greatly affected\nby network delays. In order to acquire a deep understanding of packet switching and\ncomputer networks, we must understand the nature and importance of these delays.\nTypes of Delay\nLet’s explore these delays in the context of Figure 1.16. As part of its end-to-end\nroute between source and destination, a packet is sent from the upstream node\nthrough router A to router B. Our goal is to characterize the nodal delay at router A.\nNote that router A has an outbound link leading to router B. This link is preceded by\na queue (also known as a buffer). When the packet arrives at router A from the\nupstream node, router A examines the packet’s header to determine the appropriate\noutbound link for the packet and then directs the packet to this link. In this example,\nthe outbound link for the packet is the one that leads to router B. A packet can be\ntransmitted on a link only if there is no other packet currently being transmitted on\nthe link and if there are no other packets preceding it in the queue; if the link is \ncurrently busy or if there are other packets already queued for the link, the newly\narriving packet will then join the queue.\nProcessing Delay\nThe time required to examine the packet’s header and determine where to direct the\npacket is part of the processing delay. The processing delay can also include other\nfactors, such as the time needed to check for bit-level errors in the packet that occurred\nin transmitting the packet’s bits from the upstream node to router A. Processing delays\n36\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nA\nB\nNodal\nprocessing\nQueueing\n(waiting for\ntransmission)\nTransmission\nPropagation\nFigure 1.16 \u0002 The nodal delay at router A\n\n1.4\n•\nDELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS\n37\nin high-speed routers are typically on the order of microseconds or less. After this\nnodal processing, the router directs the packet to the queue that precedes the link to\nrouter B. (In Chapter 4 we’ll study the details of how a router operates.)\nQueuing Delay\nAt the queue, the packet experiences a queuing delay as it waits to be transmitted onto\nthe link. The length of the queuing delay of a specific packet will depend on the num-\nber of earlier-arriving packets that are queued and waiting for transmission onto the\nlink. If the queue is empty and no other packet is currently being transmitted, then our\npacket’s queuing delay will be zero. On the other hand, if the traffic is heavy and many\nother packets are also waiting to be transmitted, the queuing delay will be long. We\nwill see shortly that the number of packets that an arriving packet might expect to find\nis a function of the intensity and nature of the traffic arriving at the queue. Queuing\ndelays can be on the order of microseconds to milliseconds in practice.\nTransmission Delay\nAssuming that packets are transmitted in a first-come-first-served manner, as is com-\nmon in packet-switched networks, our packet can be transmitted only after all the\npackets that have arrived before it have been transmitted. Denote the length of the\npacket by L bits, and denote the transmission rate of the link from router A to router\nB by R bits/sec. For example, for a 10 Mbps Ethernet link, the rate is R = 10 Mbps;\nfor a 100 Mbps Ethernet link, the rate is R = 100 Mbps. The transmission delay is\nL/R. This is the amount of time required to push (that is, transmit) all of the packet’s\nbits into the link. Transmission delays are typically on the order of microseconds to\nmilliseconds in practice.\nPropagation Delay\nOnce a bit is pushed into the link, it needs to propagate to router B. The time\nrequired to propagate from the beginning of the link to router B is the propagation\ndelay. The bit propagates at the propagation speed of the link. The propagation\nspeed depends on the physical medium of the link (that is, fiber optics, twisted-pair\ncopper wire, and so on) and is in the range of\n2 \u0002108 meters/sec to 3\u0002108 meters/sec\nwhich is equal to, or a little less than, the speed of light. The propagation delay is\nthe distance between two routers divided by the propagation speed. That is, the\npropagation delay is d/s, where d is the distance between router A and router B and s\nis the propagation speed of the link. Once the last bit of the packet propagates to\nnode B, it and all the preceding bits of the packet are stored in router B. The whole\n\nprocess then continues with router B now performing the forwarding. In wide-area\nnetworks, propagation delays are on the order of milliseconds.\nComparing Transmission and Propagation Delay\nNewcomers to the field of computer networking sometimes have difficulty under-\nstanding the difference between transmission delay and propagation delay. The differ-\nence is subtle but important. The transmission delay is the amount of time required for\nthe router to push out the packet; it is a function of the packet’s length and the trans-\nmission rate of the link, but has nothing to do with the distance between the two\nrouters. The propagation delay, on the other hand, is the time it takes a bit to propagate\nfrom one router to the next; it is a function of the distance between the two routers, but\nhas nothing to do with the packet’s length or the transmission rate of the link.\nAn analogy might clarify the notions of transmission and propagation delay. Con-\nsider a highway that has a tollbooth every 100 kilometers, as shown in Figure 1.17.\nYou can think of the highway segments between tollbooths as links and the toll-\nbooths as routers. Suppose that cars travel (that is, propagate) on the highway at a\nrate of 100 km/hour (that is, when a car leaves a tollbooth, it instantaneously accel-\nerates to 100 km/hour and maintains that speed between tollbooths). Suppose next\nthat 10 cars, traveling together as a caravan, follow each other in a fixed order. You\ncan think of each car as a bit and the caravan as a packet. Also suppose that each\ntollbooth services (that is, transmits) a car at a rate of one car per 12 seconds, and\nthat it is late at night so that the caravan’s cars are the only cars on the highway.\nFinally, suppose that whenever the first car of the caravan arrives at a tollbooth, it\nwaits at the entrance until the other nine cars have arrived and lined up behind it.\n(Thus the entire caravan must be stored at the tollbooth before it can begin to be for-\nwarded.) The time required for the tollbooth to push the entire caravan onto the\nhighway is (10 cars)/(5 cars/minute) = 2 minutes. This time is analogous to the\ntransmission delay in a router. The time required for a car to travel from the exit of\none tollbooth to the next tollbooth is 100 km/(100 km/hour) = 1 hour. This time is\nanalogous to propagation delay. Therefore, the time from when the caravan is stored\nin front of a tollbooth until the caravan is stored in front of the next tollbooth is the\nsum of transmission delay and propagation delay—in this example, 62 minutes.\n38\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nTen-car\ncaravan\nToll\nbooth\nToll\nbooth\n100 km\n100 km\nFigure 1.17 \u0002 Caravan analogy"
    },
    {
      "chunk_id": "d6130d42-8b80-4337-a639-fb354a0ae114",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.4.1 Overview of Delay in Packet-Switched Networks",
      "original_titles": [
        "1.4.1 Overview of Delay in Packet-Switched Networks"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.4 Delay, Loss, and Throughput in Packet-Switched Networks > 1.4.1 Overview of Delay in Packet-Switched Networks",
      "start_page": 62,
      "end_page": 65,
      "token_count": 2303,
      "text": "carries traffic to/from Google servers. As shown in Figure 1.15, the Google\nprivate network attempts to “bypass” the upper tiers of the Internet by peering\n(settlement free) with lower-tier ISPs, either by directly connecting with them or\nby connecting with them at IXPs [Labovitz 2010]. However, because many access\nISPs can still only be reached by transiting through tier-1 networks, the Google\nnetwork also connects to tier-1 ISPs, and pays those ISPs for the traffic it\nexchanges with them. By creating its own network, a content provider not only\nreduces its payments to upper-tier ISPs, but also has greater control of how its\nservices are ultimately delivered to end users. Google’s network infrastructure is\ndescribed in greater detail in Section 7.2.4.\nIn summary, today’s Internet—a network of networks—is complex, consisting of\na dozen or so tier-1 ISPs and hundreds of thousands of lower-tier ISPs. The ISPs are\ndiverse in their coverage, with some spanning multiple continents and oceans, and\nothers limited to narrow geographic regions. The lower-tier ISPs connect to the\nhigher-tier ISPs, and the higher-tier ISPs interconnect with one another. Users and\ncontent providers are customers of lower-tier ISPs, and lower-tier ISPs are customers\nof higher-tier ISPs. In recent years, major content providers have also created their\nown networks and connect directly into lower-tier ISPs where possible.\n1.4 Delay, Loss, and Throughput \nin Packet-Switched Networks\nBack in Section 1.1 we said that the Internet can be viewed as an infrastructure\nthat provides services to distributed applications running on end systems. Ideally,\nwe would like Internet services to be able to move as much data as we want\nbetween any two end systems, instantaneously, without any loss of data. Alas, this\nis a lofty goal, one that is unachievable in reality. Instead, computer networks nec-\nessarily constrain throughput (the amount of data per second that can be trans-\nferred) between end systems, introduce delays between end systems, and can\nactually lose packets. On one hand, it is unfortunate that the physical laws of real-\nity introduce delay and loss as well as constrain throughput. On the other hand,\nbecause computer networks have these problems, there are many fascinating\nissues surrounding how to deal with the problems—more than enough issues to\nfill a course on computer networking and to motivate thousands of PhD theses! In\nthis section, we’ll begin to examine and quantify delay, loss, and throughput in\ncomputer networks.\n1.4.1 Overview of Delay in Packet-Switched Networks\nRecall that a packet starts in a host (the source), passes through a series of routers,\nand ends its journey in another host (the destination). As a packet travels from one\nnode (host or router) to the subsequent node (host or router) along this path, the\n1.4\n•\nDELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS\n35\n\npacket suffers from several types of delays at each node along the path. The most\nimportant of these delays are the nodal processing delay, queuing delay, transmis-\nsion delay, and propagation delay; together, these delays accumulate to give a total\nnodal delay. The performance of many Internet applications—such as search, Web\nbrowsing, email, maps, instant messaging, and voice-over-IP—are greatly affected\nby network delays. In order to acquire a deep understanding of packet switching and\ncomputer networks, we must understand the nature and importance of these delays.\nTypes of Delay\nLet’s explore these delays in the context of Figure 1.16. As part of its end-to-end\nroute between source and destination, a packet is sent from the upstream node\nthrough router A to router B. Our goal is to characterize the nodal delay at router A.\nNote that router A has an outbound link leading to router B. This link is preceded by\na queue (also known as a buffer). When the packet arrives at router A from the\nupstream node, router A examines the packet’s header to determine the appropriate\noutbound link for the packet and then directs the packet to this link. In this example,\nthe outbound link for the packet is the one that leads to router B. A packet can be\ntransmitted on a link only if there is no other packet currently being transmitted on\nthe link and if there are no other packets preceding it in the queue; if the link is \ncurrently busy or if there are other packets already queued for the link, the newly\narriving packet will then join the queue.\nProcessing Delay\nThe time required to examine the packet’s header and determine where to direct the\npacket is part of the processing delay. The processing delay can also include other\nfactors, such as the time needed to check for bit-level errors in the packet that occurred\nin transmitting the packet’s bits from the upstream node to router A. Processing delays\n36\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nA\nB\nNodal\nprocessing\nQueueing\n(waiting for\ntransmission)\nTransmission\nPropagation\nFigure 1.16 \u0002 The nodal delay at router A\n\n1.4\n•\nDELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS\n37\nin high-speed routers are typically on the order of microseconds or less. After this\nnodal processing, the router directs the packet to the queue that precedes the link to\nrouter B. (In Chapter 4 we’ll study the details of how a router operates.)\nQueuing Delay\nAt the queue, the packet experiences a queuing delay as it waits to be transmitted onto\nthe link. The length of the queuing delay of a specific packet will depend on the num-\nber of earlier-arriving packets that are queued and waiting for transmission onto the\nlink. If the queue is empty and no other packet is currently being transmitted, then our\npacket’s queuing delay will be zero. On the other hand, if the traffic is heavy and many\nother packets are also waiting to be transmitted, the queuing delay will be long. We\nwill see shortly that the number of packets that an arriving packet might expect to find\nis a function of the intensity and nature of the traffic arriving at the queue. Queuing\ndelays can be on the order of microseconds to milliseconds in practice.\nTransmission Delay\nAssuming that packets are transmitted in a first-come-first-served manner, as is com-\nmon in packet-switched networks, our packet can be transmitted only after all the\npackets that have arrived before it have been transmitted. Denote the length of the\npacket by L bits, and denote the transmission rate of the link from router A to router\nB by R bits/sec. For example, for a 10 Mbps Ethernet link, the rate is R = 10 Mbps;\nfor a 100 Mbps Ethernet link, the rate is R = 100 Mbps. The transmission delay is\nL/R. This is the amount of time required to push (that is, transmit) all of the packet’s\nbits into the link. Transmission delays are typically on the order of microseconds to\nmilliseconds in practice.\nPropagation Delay\nOnce a bit is pushed into the link, it needs to propagate to router B. The time\nrequired to propagate from the beginning of the link to router B is the propagation\ndelay. The bit propagates at the propagation speed of the link. The propagation\nspeed depends on the physical medium of the link (that is, fiber optics, twisted-pair\ncopper wire, and so on) and is in the range of\n2 \u0002108 meters/sec to 3\u0002108 meters/sec\nwhich is equal to, or a little less than, the speed of light. The propagation delay is\nthe distance between two routers divided by the propagation speed. That is, the\npropagation delay is d/s, where d is the distance between router A and router B and s\nis the propagation speed of the link. Once the last bit of the packet propagates to\nnode B, it and all the preceding bits of the packet are stored in router B. The whole\n\nprocess then continues with router B now performing the forwarding. In wide-area\nnetworks, propagation delays are on the order of milliseconds.\nComparing Transmission and Propagation Delay\nNewcomers to the field of computer networking sometimes have difficulty under-\nstanding the difference between transmission delay and propagation delay. The differ-\nence is subtle but important. The transmission delay is the amount of time required for\nthe router to push out the packet; it is a function of the packet’s length and the trans-\nmission rate of the link, but has nothing to do with the distance between the two\nrouters. The propagation delay, on the other hand, is the time it takes a bit to propagate\nfrom one router to the next; it is a function of the distance between the two routers, but\nhas nothing to do with the packet’s length or the transmission rate of the link.\nAn analogy might clarify the notions of transmission and propagation delay. Con-\nsider a highway that has a tollbooth every 100 kilometers, as shown in Figure 1.17.\nYou can think of the highway segments between tollbooths as links and the toll-\nbooths as routers. Suppose that cars travel (that is, propagate) on the highway at a\nrate of 100 km/hour (that is, when a car leaves a tollbooth, it instantaneously accel-\nerates to 100 km/hour and maintains that speed between tollbooths). Suppose next\nthat 10 cars, traveling together as a caravan, follow each other in a fixed order. You\ncan think of each car as a bit and the caravan as a packet. Also suppose that each\ntollbooth services (that is, transmits) a car at a rate of one car per 12 seconds, and\nthat it is late at night so that the caravan’s cars are the only cars on the highway.\nFinally, suppose that whenever the first car of the caravan arrives at a tollbooth, it\nwaits at the entrance until the other nine cars have arrived and lined up behind it.\n(Thus the entire caravan must be stored at the tollbooth before it can begin to be for-\nwarded.) The time required for the tollbooth to push the entire caravan onto the\nhighway is (10 cars)/(5 cars/minute) = 2 minutes. This time is analogous to the\ntransmission delay in a router. The time required for a car to travel from the exit of\none tollbooth to the next tollbooth is 100 km/(100 km/hour) = 1 hour. This time is\nanalogous to propagation delay. Therefore, the time from when the caravan is stored\nin front of a tollbooth until the caravan is stored in front of the next tollbooth is the\nsum of transmission delay and propagation delay—in this example, 62 minutes.\n38\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nTen-car\ncaravan\nToll\nbooth\nToll\nbooth\n100 km\n100 km\nFigure 1.17 \u0002 Caravan analogy"
    },
    {
      "chunk_id": "d05f8bb0-3b2e-4255-b18b-78ca8bbaba0d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.4.2 Queuing Delay and Packet Loss",
      "original_titles": [
        "1.4.2 Queuing Delay and Packet Loss"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.4 Delay, Loss, and Throughput in Packet-Switched Networks > 1.4.2 Queuing Delay and Packet Loss",
      "start_page": 66,
      "end_page": 68,
      "token_count": 1955,
      "text": "1.4\n•\nDELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS\n39\nLet’s explore this analogy a bit more. What would happen if the tollbooth service\ntime for a caravan were greater than the time for a car to travel between tollbooths?\nFor example, suppose now that the cars travel at the rate of 1,000 km/hour and the toll-\nbooth services cars at the rate of one car per minute. Then the traveling delay between\ntwo tollbooths is 6 minutes and the time to serve a caravan is 10 minutes. In this case,\nthe first few cars in the caravan will arrive at the second tollbooth before the last cars\nin the caravan leave the first tollbooth. This situation also arises in packet-switched\nnetworks—the first bits in a packet can arrive at a router while many of the remaining\nbits in the packet are still waiting to be transmitted by the preceding router.\nIf a picture speaks a thousand words, then an animation must speak a million\nwords. The companion Web site for this textbook provides an interactive Java applet\nthat nicely illustrates and contrasts transmission delay and propagation delay. The\nreader is highly encouraged to visit that applet. [Smith 2009] also provides a very\nreadable discussion of propagation, queueing, and transmission delays.\nIf we let dproc, dqueue, dtrans, and dprop denote the processing, queuing, transmis-\nsion, and propagation delays, then the total nodal delay is given by\ndnodal = dproc + dqueue + dtrans + dprop\nThe contribution of these delay components can vary significantly. For example,\ndprop can be negligible (for example, a couple of microseconds) for a link connect-\ning two routers on the same university campus; however, dprop is hundreds of mil-\nliseconds for two routers interconnected by a geostationary satellite link, and can be\nthe dominant term in dnodal. Similarly, dtrans can range from negligible to significant.\nIts contribution is typically negligible for transmission rates of 10 Mbps and higher\n(for example, for LANs); however, it can be hundreds of milliseconds for large\nInternet packets sent over low-speed dial-up modem links. The processing delay,\ndproc, is often negligible; however, it strongly influences a router’s maximum\nthroughput, which is the maximum rate at which a router can forward packets.\n1.4.2 Queuing Delay and Packet Loss\nThe most complicated and interesting component of nodal delay is the queuing\ndelay, dqueue. In fact, queuing delay is so important and interesting in computer net-\nworking that thousands of papers and numerous books have been written about it\n[Bertsekas 1991; Daigle 1991; Kleinrock 1975, 1976; Ross 1995]. We give only a\nhigh-level, intuitive discussion of queuing delay here; the more curious reader may\nwant to browse through some of the books (or even eventually write a PhD thesis\non the subject!). Unlike the other three delays (namely, dproc, dtrans, and dprop), the\nqueuing delay can vary from packet to packet. For example, if 10 packets arrive\nat an empty queue at the same time, the first packet transmitted will suffer no queu-\ning delay, while the last packet transmitted will suffer a relatively large queuing\ndelay (while it waits for the other nine packets to be transmitted). Therefore, when\n\n40\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\ncharacterizing queuing delay, one typically uses statistical measures, such as aver-\nage queuing delay, variance of queuing delay, and the probability that the queuing\ndelay exceeds some specified value.\nWhen is the queuing delay large and when is it insignificant? The answer to this\nquestion depends on the rate at which traffic arrives at the queue, the transmission\nrate of the link, and the nature of the arriving traffic, that is, whether the traffic arrives\nperiodically or arrives in bursts. To gain some insight here, let a denote the average\nrate at which packets arrive at the queue (a is in units of packets/sec). Recall that R is\nthe transmission rate; that is, it is the rate (in bits/sec) at which bits are pushed out of\nthe queue. Also suppose, for simplicity, that all packets consist of L bits. Then the\naverage rate at which bits arrive at the queue is La bits/sec. Finally, assume that the\nqueue is very big, so that it can hold essentially an infinite number of bits. The ratio\nLa/R, called the traffic intensity, often plays an important role in estimating the\nextent of the queuing delay. If La/R > 1, then the average rate at which bits arrive at\nthe queue exceeds the rate at which the bits can be transmitted from the queue. In this\nunfortunate situation, the queue will tend to increase without bound and the queuing\ndelay will approach infinity! Therefore, one of the golden rules in traffic engineering\nis: Design your system so that the traffic intensity is no greater than 1.\nNow consider the case La/R ≤1. Here, the nature of the arriving traffic impacts\nthe queuing delay. For example, if packets arrive periodically—that is, one packet\narrives every L/R seconds—then every packet will arrive at an empty queue and\nthere will be no queuing delay. On the other hand, if packets arrive in bursts but\nperiodically, there can be a significant average queuing delay. For example, suppose\nN packets arrive simultaneously every (L/R)N seconds. Then the first packet trans-\nmitted has no queuing delay; the second packet transmitted has a queuing delay of\nL/R seconds; and more generally, the nth packet transmitted has a queuing delay of\n(n \u0003 1)L/R seconds. We leave it as an exercise for you to calculate the average queu-\ning delay in this example.\nThe two examples of periodic arrivals described above are a bit academic. Typ-\nically, the arrival process to a queue is random; that is, the arrivals do not follow any\npattern and the packets are spaced apart by random amounts of time. In this more\nrealistic case, the quantity La/R is not usually sufficient to fully characterize the\nqueueing delay statistics. Nonetheless, it is useful in gaining an intuitive understand-\ning of the extent of the queuing delay. In particular, if the traffic intensity is close to\nzero, then packet arrivals are few and far between and it is unlikely that an arriving\npacket will find another packet in the queue. Hence, the average queuing delay will\nbe close to zero. On the other hand, when the traffic intensity is close to 1, there will\nbe intervals of time when the arrival rate exceeds the transmission capacity (due to\nvariations in packet arrival rate), and a queue will form during these periods of time;\nwhen the arrival rate is less than the transmission capacity, the length of the queue\nwill shrink. Nonetheless, as the traffic intensity approaches 1, the average queue\nlength gets larger and larger. The qualitative dependence of average queuing delay\non the traffic intensity is shown in Figure 1.18.\n\n1.4\n•\nDELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS\n41\nOne important aspect of Figure 1.18 is the fact that as the traffic intensity\napproaches 1, the average queuing delay increases rapidly. A small percentage\nincrease in the intensity will result in a much larger percentage-wise increase in\ndelay. Perhaps you have experienced this phenomenon on the highway. If you regu-\nlarly drive on a road that is typically congested, the fact that the road is typically\ncongested means that its traffic intensity is close to 1. If some event causes an even\nslightly larger-than-usual amount of traffic, the delays you experience can be huge.\nTo really get a good feel for what queuing delays are about, you are encouraged\nonce again to visit the companion Web site, which provides an interactive Java\napplet for a queue. If you set the packet arrival rate high enough so that the traffic\nintensity exceeds 1, you will see the queue slowly build up over time.\nPacket Loss\nIn our discussions above, we have assumed that the queue is capable of holding an\ninfinite number of packets. In reality a queue preceding a link has finite capacity,\nalthough the queuing capacity greatly depends on the router design and cost.\nBecause the queue capacity is finite, packet delays do not really approach infinity as\nthe traffic intensity approaches 1. Instead, a packet can arrive to find a full queue.\nWith no place to store such a packet, a router will drop that packet; that is, the\npacket will be lost. This overflow at a queue can again be seen in the Java applet for\na queue when the traffic intensity is greater than 1.\nFrom an end-system viewpoint, a packet loss will look like a packet having\nbeen transmitted into the network core but never emerging from the network at the\ndestination. The fraction of lost packets increases as the traffic intensity increases.\nTherefore, performance at a node is often measured not only in terms of delay, but\nalso in terms of the probability of packet loss. As we’ll discuss in the subsequent\nAverage queuing delay\nLa/R\n1\nFigure 1.18 \u0002 Dependence of average queuing delay on traffic intensity"
    },
    {
      "chunk_id": "583bfc53-50b2-4559-9133-e69374bde170",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.4.3 End-to-End Delay",
      "original_titles": [
        "1.4.3 End-to-End Delay"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.4 Delay, Loss, and Throughput in Packet-Switched Networks > 1.4.3 End-to-End Delay",
      "start_page": 69,
      "end_page": 70,
      "token_count": 1342,
      "text": "chapters, a lost packet may be retransmitted on an end-to-end basis in order to\nensure that all data are eventually transferred from source to destination\n1.4.3 End-to-End Delay\nOur discussion up to this point has focused on the nodal delay, that is, the delay at a\nsingle router. Let’s now consider the total delay from source to destination. To get a\nhandle on this concept, suppose there are N \u0003 1 routers between the source host and\nthe destination host. Let’s also suppose for the moment that the network is uncon-\ngested (so that queuing delays are negligible), the processing delay at each router\nand at the source host is dproc, the transmission rate out of each router and out of the\nsource host is R bits/sec, and the propagation on each link is dprop. The nodal delays\naccumulate and give an end-to-end delay,\ndend-end = N (dproc + dtrans + dprop)\n(1.2)\nwhere, once again, dtrans = L/R, where L is the packet size. Note that Equation 1.2 is a\ngeneralization of Equation 1.1, which did not take into account processing and propa-\ngation delays. We leave it to you to generalize Equation 1.2 to the case of heteroge-\nneous delays at the nodes and to the presence of an average queuing delay at each node.\nTraceroute\nTo get a hands-on feel for end-to-end delay in a computer network, we can make use\nof the Traceroute program. Traceroute is a simple program that can run in any Inter-\nnet host. When the user specifies a destination hostname, the program in the source\nhost sends multiple, special packets toward that destination. As these packets work\ntheir way toward the destination, they pass through a series of routers. When a\nrouter receives one of these special packets, it sends back to the source a short mes-\nsage that contains the name and address of the router.\nMore specifically, suppose there are N \u0003 1 routers between the source and the\ndestination. Then the source will send N special packets into the network, with each\npacket addressed to the ultimate destination. These N special packets are marked 1\nthrough N, with the first packet marked 1 and the last packet marked N. When the nth\nrouter receives the nth packet marked n, the router does not forward the packet\ntoward its destination, but instead sends a message back to the source. When the des-\ntination host receives the Nth packet, it too returns a message back to the source. The\nsource records the time that elapses between when it sends a packet and when it\nreceives the corresponding return message; it also records the name and address of\nthe router (or the destination host) that returns the message. In this manner, the source\ncan reconstruct the route taken by packets flowing from source to destination, and\nthe source can determine the round-trip delays to all the intervening routers. Trace-\nroute actually repeats the experiment just described three times, so the source actually\nsends 3 • N packets to the destination. RFC 1393 describes Traceroute in detail.\n42\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nVideoNote\nUsing Traceroute to\ndiscover network\npaths and measure\nnetwork delay\n\n1.4\n•\nDELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS\n43\nHere is an example of the output of the Traceroute program, where the route\nwas being traced from the source host gaia.cs.umass.edu (at the University of Mass-\nachusetts) to the host cis.poly.edu (at Polytechnic University in Brooklyn). The out-\nput has six columns: the first column is the n value described above, that is, the\nnumber of the router along the route; the second column is the name of the router;\nthe third column is the address of the router (of the form xxx.xxx.xxx.xxx); the last\nthree columns are the round-trip delays for three experiments. If the source receives\nfewer than three messages from any given router (due to packet loss in the network),\nTraceroute places an asterisk just after the router number and reports fewer than\nthree round-trip times for that router.\n1 cs-gw (128.119.240.254) 1.009 ms 0.899 ms 0.993 ms\n2 128.119.3.154 (128.119.3.154) 0.931 ms 0.441 ms 0.651 ms\n3 border4-rt-gi-1-3.gw.umass.edu (128.119.2.194) 1.032 ms 0.484 ms 0.451 ms\n4 acr1-ge-2-1-0.Boston.cw.net (208.172.51.129) 10.006 ms 8.150 ms 8.460 ms\n5 agr4-loopback.NewYork.cw.net (206.24.194.104) 12.272 ms 14.344 ms 13.267 ms\n6 acr2-loopback.NewYork.cw.net (206.24.194.62) 13.225 ms 12.292 ms 12.148 ms\n7 pos10-2.core2.NewYork1.Level3.net (209.244.160.133) 12.218 ms 11.823 ms 11.793 ms\n8 gige9-1-52.hsipaccess1.NewYork1.Level3.net (64.159.17.39) 13.081 ms 11.556 ms 13.297 ms\n9 p0-0.polyu.bbnplanet.net (4.25.109.122) 12.716 ms 13.052 ms 12.786 ms\n10 cis.poly.edu (128.238.32.126) 14.080 ms 13.035 ms 12.802 ms\nIn the trace above there are nine routers between the source and the destination.\nMost of these routers have a name, and all of them have addresses. For example, the\nname of Router 3 is border4-rt-gi-1-3.gw.umass.edu and its address is\n128.119.2.194. Looking at the data provided for this same router, we see that\nin the first of the three trials the round-trip delay between the source and the router\nwas 1.03 msec. The round-trip delays for the subsequent two trials were 0.48 and\n0.45 msec. These round-trip delays include all of the delays just discussed, includ-\ning transmission delays, propagation delays, router processing delays, and queuing\ndelays. Because the queuing delay is varying with time, the round-trip delay of\npacket n sent to a router n can sometimes be longer than the round-trip delay of\npacket n+1 sent to router n+1. Indeed, we observe this phenomenon in the above\nexample: the delays to Router 6 are larger than the delays to Router 7!\nWant to try out Traceroute for yourself? We highly recommended that you visit\nhttp://www.traceroute.org, which provides a Web interface to an extensive list of\nsources for route tracing. You choose a source and supply the hostname for any des-\ntination. The Traceroute program then does all the work. There are a number of free\nsoftware programs that provide a graphical interface to Traceroute; one of our\nfavorites is PingPlotter [PingPlotter 2012].\nEnd System, Application, and Other Delays\nIn addition to processing, transmission, and propagation delays, there can be addi-\ntional significant delays in the end systems. For example, an end system wanting to"
    },
    {
      "chunk_id": "f2789385-a16f-4eb1-96b7-16d166a1f162",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.4.4 Throughput in Computer Networks",
      "original_titles": [
        "1.4.4 Throughput in Computer Networks"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.4 Delay, Loss, and Throughput in Packet-Switched Networks > 1.4.4 Throughput in Computer Networks",
      "start_page": 71,
      "end_page": 73,
      "token_count": 1638,
      "text": "transmit a packet into a shared medium (e.g., as in a WiFi or cable modem scenario)\nmay purposefully delay its transmission as part of its protocol for sharing the\nmedium with other end systems; we’ll consider such protocols in detail in Chapter\n5. Another important delay is media packetization delay, which is present in Voice-\nover-IP (VoIP) applications. In VoIP, the sending side must first fill a packet with\nencoded digitized speech before passing the packet to the Internet. This time to fill a\npacket—called the packetization delay—can be significant and can impact the user-\nperceived quality of a VoIP call. This issue will be further explored in a homework\nproblem at the end of this chapter.\n1.4.4 Throughput in Computer Networks\nIn addition to delay and packet loss, another critical performance measure in com-\nputer networks is end-to-end throughput. To define throughput, consider transfer-\nring a large file from Host A to Host B across a computer network. This transfer\nmight be, for example, a large video clip from one peer to another in a P2P file\nsharing system. The instantaneous throughput at any instant of time is the rate\n(in bits/sec) at which Host B is receiving the file. (Many applications, including\nmany P2P file sharing systems, display the instantaneous throughput during\ndownloads in the user interface—perhaps you have observed this before!) If the\nfile consists of F bits and the transfer takes T seconds for Host B to receive all F\nbits, then the average throughput of the file transfer is F/T bits/sec. For some\napplications, such as Internet telephony, it is desirable to have a low delay and an\ninstantaneous throughput consistently above some threshold (for example, over\n24 kbps for some Internet telephony applications and over 256 kbps for some real-\ntime video applications). For other applications, including those involving file\ntransfers, delay is not critical, but it is desirable to have the highest possible\nthroughput.\nTo gain further insight into the important concept of throughput, let’s consider a\nfew examples. Figure 1.19(a) shows two end systems, a server and a client, con-\nnected by two communication links and a router. Consider the throughput for a file\ntransfer from the server to the client. Let Rs denote the rate of the link between\nthe server and the router; and Rc denote the rate of the link between the router and\nthe client. Suppose that the only bits being sent in the entire network are those\nfrom the server to the client. We now ask, in this ideal scenario, what is the server-\nto-client throughput? To answer this question, we may think of bits as fluid and\ncommunication links as pipes. Clearly, the server cannot pump bits through its link\nat a rate faster than Rs bps; and the router cannot forward bits at a rate faster than Rc\nbps. If Rs < Rc, then the bits pumped by the server will “flow” right through the\nrouter and arrive at the client at a rate of Rs bps, giving a throughput of Rs bps. If, on\nthe other hand, Rc < Rs, then the router will not be able to forward bits as quickly as\nit receives them. In this case, bits will only leave the router at rate Rc, giving an \n44\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\n\n1.4\n•\nDELAY, LOSS, AND THROUGHPUT IN PACKET-SWITCHED NETWORKS\n45\nend-to-end throughput of Rc. (Note also that if bits continue to arrive at the router at\nrate Rs, and continue to leave the router at Rc, the backlog of bits at the router wait-\ning for transmission to the client will grow and grow—a most undesirable situation!)\nThus, for this simple two-link network, the throughput is min{Rc, Rs}, that is, it is\nthe transmission rate of the bottleneck link. Having determined the throughput, we\ncan now approximate the time it takes to transfer a large file of F bits from server to\nclient as F/min{Rs, Rc}. For a specific example, suppose you are downloading an\nMP3 file of F = 32 million bits, the server has a transmission rate of Rs = 2 Mbps,\nand you have an access link of Rc = 1 Mbps. The time needed to transfer the file is\nthen 32 seconds. Of course, these expressions for throughput and transfer time are\nonly approximations, as they do not account for store-and-forward and processing\ndelays as well as protocol issues. \nFigure 1.19(b) now shows a network with N links between the server and the\nclient, with the transmission rates of the N links being R1, R2,..., RN. Applying the\nsame analysis as for the two-link network, we find that the throughput for a file\ntransfer from server to client is min{R1, R2,..., RN}, which is once again the trans-\nmission rate of the bottleneck link along the path between server and client.\nNow consider another example motivated by today’s Internet. Figure 1.20(a)\nshows two end systems, a server and a client, connected to a computer network.\nConsider the throughput for a file transfer from the server to the client. The server is\nconnected to the network with an access link of rate Rs and the client is connected to\nthe network with an access link of rate Rc. Now suppose that all the links in the core\nof the communication network have very high transmission rates, much higher than\nRs and Rc. Indeed, today, the core of the Internet is over-provisioned with high speed\nlinks that experience little congestion. Also suppose that the only bits being sent in\nthe entire network are those from the server to the client. Because the core of the\ncomputer network is like a wide pipe in this example, the rate at which bits can flow\nServer\nRs\nR1\nR2\nRN\nRc\nClient\nServer\na.\nb.\nClient\nFigure 1.19 \u0002 Throughput for a file transfer from server to client\n\nfrom source to destination is again the minimum of Rs and Rc, that is, throughput =\nmin{Rs, Rc}. Therefore, the constraining factor for throughput in today’s Internet is\ntypically the access network.\nFor a final example, consider Figure 1.20(b) in which there are 10 servers and\n10 clients connected to the core of the computer network. In this example, there are\n10 simultaneous downloads taking place, involving 10 client-server pairs. Suppose\nthat these 10 downloads are the only traffic in the network at the current time. As\nshown in the figure, there is a link in the core that is traversed by all 10 downloads.\nDenote R for the transmission rate of this link R. Let’s suppose that all server access\nlinks have the same rate Rs, all client access links have the same rate Rc, and the\ntransmission rates of all the links in the core—except the one common link of rate\nR—are much larger than Rs, Rc, and R. Now we ask, what are the throughputs of the\ndownloads? Clearly, if the rate of the common link, R, is large—say a hundred times\nlarger than both Rs and Rc—then the throughput for each download will once again\nbe min{Rs, Rc}. But what if the rate of the common link is of the same order as Rs\nand Rc? What will the throughput be in this case? Let’s take a look at a specific\n46\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nServer\nRs\nRc\na.\nb.\nClient\n10 Clients\n10 Servers\nBottleneck\nlink of\ncapacity R\nFigure 1.20 \u0002 End-to-end throughput: (a) Client downloads a file from\nserver; (b) 10 clients downloading with 10 servers"
    },
    {
      "chunk_id": "a2623766-3584-400c-8179-a16df34725f6",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.5 Protocol Layers and Their Service Models",
      "original_titles": [
        "1.5 Protocol Layers and Their Service Models"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.5 Protocol Layers and Their Service Models",
      "start_page": 74,
      "end_page": 79,
      "token_count": 3265,
      "text": "example. Suppose Rs = 2 Mbps, Rc = 1 Mbps, R = 5 Mbps, and the common link\ndivides its transmission rate equally among the 10 downloads. Then the bottleneck\nfor each download is no longer in the access network, but is now instead the shared\nlink in the core, which only provides each download with 500 kbps of throughput.\nThus the end-to-end throughput for each download is now reduced to 500 kbps.\nThe examples in Figure 1.19 and Figure 1.20(a) show that throughput depends\non the transmission rates of the links over which the data flows. We saw that when\nthere is no other intervening traffic, the throughput can simply be approximated as\nthe minimum transmission rate along the path between source and destination. The\nexample in Figure 1.20(b) shows that more generally the throughput depends not\nonly on the transmission rates of the links along the path, but also on the intervening\ntraffic. In particular, a link with a high transmission rate may nonetheless be the bot-\ntleneck link for a file transfer if many other data flows are also passing through that\nlink. We will examine throughput in computer networks more closely in the home-\nwork problems and in the subsequent chapters.\n1.5 Protocol Layers and Their Service Models\nFrom our discussion thus far, it is apparent that the Internet is an extremely compli-\ncated system. We have seen that there are many pieces to the Internet: numerous\napplications and protocols, various types of end systems, packet switches, and vari-\nous types of link-level media. Given this enormous complexity, is there any hope of\norganizing a network architecture, or at least our discussion of network architecture?\nFortunately, the answer to both questions is yes.\n1.5.1 Layered Architecture\nBefore attempting to organize our thoughts on Internet architecture, let’s look for a\nhuman analogy. Actually, we deal with complex systems all the time in our every-\nday life. Imagine if someone asked you to describe, for example, the airline system.\nHow would you find the structure to describe this complex system that has ticketing\nagents, baggage checkers, gate personnel, pilots, airplanes, air traffic control, and a\nworldwide system for routing airplanes? One way to describe this system might be\nto describe the series of actions you take (or others take for you) when you fly on an\nairline. You purchase your ticket, check your bags, go to the gate, and eventually get\nloaded onto the plane. The plane takes off and is routed to its destination. After your\nplane lands, you deplane at the gate and claim your bags. If the trip was bad, you\ncomplain about the flight to the ticket agent (getting nothing for your effort). This\nscenario is shown in Figure 1.21.\nAlready, we can see some analogies here with computer networking: You are\nbeing shipped from source to destination by the airline; a packet is shipped from\n1.5\n•\nPROTOCOL LAYERS AND THEIR SERVICE MODELS\n47\n\nsource host to destination host in the Internet. But this is not quite the analogy we\nare after. We are looking for some structure in Figure 1.21. Looking at Figure 1.21,\nwe note that there is a ticketing function at each end; there is also a baggage func-\ntion for already-ticketed passengers, and a gate function for already-ticketed and\nalready-baggage-checked passengers. For passengers who have made it through the\ngate (that is, passengers who are already ticketed, baggage-checked, and through the\ngate), there is a takeoff and landing function, and while in flight, there is an airplane-\nrouting function. This suggests that we can look at the functionality in Figure 1.21\nin a horizontal manner, as shown in Figure 1.22.\nFigure 1.22 has divided the airline functionality into layers, providing a frame-\nwork in which we can discuss airline travel. Note that each layer, combined with the\n48\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nTicket (purchase)\nBaggage (check)\nGates (load)\nRunway takeoff\nAirplane routing\nAirplane routing\nAirplane routing\nTicket (complain)\nBaggage (claim)\nGates (unload)\nRunway landing\nAirplane routing\nTicket\nBaggage\nGate\nTakeoff/Landing\nDeparture airport\nIntermediate air-traffic\ncontrol centers\nFigure 1.22 \u0002 Horizontal layering of airline functionality\nTicket (purchase)\nBaggage (check)\nGates (load)\nRunway takeoff\nAirplane routing\nTicket (complain)\nBaggage (claim)\nGates (unload)\nRunway landing\nAirplane routing\nAirplane routing\nFigure 1.21 \u0002 Taking an airplane trip: actions\n\n1.5\n•\nPROTOCOL LAYERS AND THEIR SERVICE MODELS\n49\nlayers below it, implements some functionality, some service. At the ticketing layer\nand below, airline-counter-to-airline-counter transfer of a person is accomplished.\nAt the baggage layer and below, baggage-check-to-baggage-claim transfer of a per-\nson and bags is accomplished. Note that the baggage layer provides this service only\nto an already-ticketed person. At the gate layer, departure-gate-to-arrival-gate trans-\nfer of a person and bags is accomplished. At the takeoff/landing layer, runway-to-\nrunway transfer of people and their bags is accomplished. Each layer provides its\nservice by (1) performing certain actions within that layer (for example, at the gate\nlayer, loading and unloading people from an airplane) and by (2) using the services\nof the layer directly below it (for example, in the gate layer, using the runway-to-\nrunway passenger transfer service of the takeoff/landing layer).\nA layered architecture allows us to discuss a well-defined, specific part of a\nlarge and complex system. This simplification itself is of considerable value by pro-\nviding modularity, making it much easier to change the implementation of the serv-\nice provided by the layer. As long as the layer provides the same service to the layer\nabove it, and uses the same services from the layer below it, the remainder of the\nsystem remains unchanged when a layer’s implementation is changed. (Note that\nchanging the implementation of a service is very different from changing the serv-\nice itself!) For example, if the gate functions were changed (for instance, to have\npeople board and disembark by height), the remainder of the airline system would\nremain unchanged since the gate layer still provides the same function (loading and\nunloading people); it simply implements that function in a different manner after the\nchange. For large and complex systems that are constantly being updated, the ability\nto change the implementation of a service without affecting other components of the\nsystem is another important advantage of layering.\nProtocol Layering\nBut enough about airlines. Let’s now turn our attention to network protocols. To\nprovide structure to the design of network protocols, network designers organize\nprotocols—and the network hardware and software that implement the protocols—\nin layers. Each protocol belongs to one of the layers, just as each function in the\nairline architecture in Figure 1.22 belonged to a layer. We are again interested in\nthe services that a layer offers to the layer above—the so-called service model of\na layer. Just as in the case of our airline example, each layer provides its service\nby (1) performing certain actions within that layer and by (2) using the services of\nthe layer directly below it. For example, the services provided by layer n may\ninclude reliable delivery of messages from one edge of the network to the other.\nThis might be implemented by using an unreliable edge-to-edge message delivery\nservice of layer n \u0003 1, and adding layer n functionality to detect and retransmit\nlost messages.\nA protocol layer can be implemented in software, in hardware, or in a combina-\ntion of the two. Application-layer protocols—such as HTTP and SMTP—are almost\n\nalways implemented in software in the end systems; so are transport-layer protocols.\nBecause the physical layer and data link layers are responsible for handling commu-\nnication over a specific link, they are typically implemented in a network interface\ncard (for example, Ethernet or WiFi interface cards) associated with a given link.\nThe network layer is often a mixed implementation of hardware and software. Also\nnote that just as the functions in the layered airline architecture were distributed\namong the various airports and flight control centers that make up the system, so too\nis a layer n protocol distributed among the end systems, packet switches, and other\ncomponents that make up the network. That is, there’s often a piece of a layer n pro-\ntocol in each of these network components.\nProtocol layering has conceptual and structural advantages [RFC 3439]. As we\nhave seen, layering provides a structured way to discuss system components. Mod-\nularity makes it easier to update system components. We mention, however, that\nsome researchers and networking engineers are vehemently opposed to layering\n[Wakeman 1992]. One potential drawback of layering is that one layer may dupli-\ncate lower-layer functionality. For example, many protocol stacks provide error\nrecovery on both a per-link basis and an end-to-end basis. A second potential draw-\nback is that functionality at one layer may need information (for example, a time-\nstamp value) that is present only in another layer; this violates the goal of\nseparation of layers.\nWhen taken together, the protocols of the various layers are called the protocol\nstack. The Internet protocol stack consists of five layers: the physical, link, network,\ntransport, and application layers, as shown in Figure 1.23(a). If you examine the\nTable of Contents, you will see that we have roughly organized this book using the\nlayers of the Internet protocol stack. We take a top-down approach, first covering\nthe application layer and then proceeding downward.\n50\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nTransport\nApplication\nNetwork\nLink\nPhysical\na.  Five-layer\n \nInternet\n \nprotocol stack\nTransport\nSession\nApplication\nPresentation\nNetwork\nLink\nPhysical\nb.  Seven-layer\n \nISO OSI\n \nreference model\nFigure 1.23 \u0002 The Internet protocol stack (a) and OSI reference model (b)\n\n1.5\n•\nPROTOCOL LAYERS AND THEIR SERVICE MODELS\n51\nApplication Layer\nThe application layer is where network applications and their application-layer proto-\ncols reside. The Internet’s application layer includes many protocols, such as the HTTP\nprotocol (which provides for Web document request and transfer), SMTP (which pro-\nvides for the transfer of e-mail messages), and FTP (which provides for the transfer of\nfiles between two end systems). We’ll see that certain network functions, such as the\ntranslation of human-friendly names for Internet end systems like www.ietf.org to a\n32-bit network address, are also done with the help of a specific application-layer pro-\ntocol, namely, the domain name system (DNS). We’ll see in Chapter 2 that it is very\neasy to create and deploy our own new application-layer protocols.\nAn application-layer protocol is distributed over multiple end systems, with the\napplication in one end system using the protocol to exchange packets of information\nwith the application in another end system. We’ll refer to this packet of information\nat the application layer as a message.\nTransport Layer\nThe Internet’s transport layer transports application-layer messages between\napplication endpoints. In the Internet there are two transport protocols, TCP and\nUDP, either of which can transport application-layer messages. TCP provides a\nconnection-oriented service to its applications. This service includes guaranteed\ndelivery of application-layer messages to the destination and flow control (that is,\nsender/receiver speed matching). TCP also breaks long messages into shorter seg-\nments and provides a congestion-control mechanism, so that a source throttles its\ntransmission rate when the network is congested. The UDP protocol provides a con-\nnectionless service to its applications. This is a no-frills service that provides no\nreliability, no flow control, and no congestion control. In this book, we’ll refer to a\ntransport-layer packet as a segment.\nNetwork Layer\nThe Internet’s network layer is responsible for moving network-layer packets\nknown as datagrams from one host to another. The Internet transport-layer proto-\ncol (TCP or UDP) in a source host passes a transport-layer segment and a destina-\ntion address to the network layer, just as you would give the postal service a letter\nwith a destination address. The network layer then provides the service of deliver-\ning the segment to the transport layer in the destination host.\nThe Internet’s network layer includes the celebrated IP Protocol, which defines\nthe fields in the datagram as well as how the end systems and routers act on these\nfields. There is only one IP protocol, and all Internet components that have a net-\nwork layer must run the IP protocol. The Internet’s network layer also contains rout-\ning protocols that determine the routes that datagrams take between sources and\n\ndestinations. The Internet has many routing protocols. As we saw in Section 1.3, the\nInternet is a network of networks, and within a network, the network administrator\ncan run any routing protocol desired. Although the network layer contains both the\nIP protocol and numerous routing protocols, it is often simply referred to as the IP\nlayer, reflecting the fact that IP is the glue that binds the Internet together.\nLink Layer\nThe Internet’s network layer routes a datagram through a series of routers between\nthe source and destination. To move a packet from one node (host or router) to the\nnext node in the route, the network layer relies on the services of the link layer. In\nparticular, at each node, the network layer passes the datagram down to the link\nlayer, which delivers the datagram to the next node along the route. At this next\nnode, the link layer passes the datagram up to the network layer.\nThe services provided by the link layer depend on the specific link-layer proto-\ncol that is employed over the link. For example, some link-layer protocols provide\nreliable delivery, from transmitting node, over one link, to receiving node. Note that\nthis reliable delivery service is different from the reliable delivery service of TCP,\nwhich provides reliable delivery from one end system to another. Examples of link-\nlayer protocols include Ethernet, WiFi, and the cable access network’s DOCSIS pro-\ntocol. As datagrams typically need to traverse several links to travel from source to\ndestination, a datagram may be handled by different link-layer protocols at different\nlinks along its route. For example, a datagram may be handled by Ethernet on one\nlink and by PPP on the next link. The network layer will receive a different service\nfrom each of the different link-layer protocols. In this book, we’ll refer to the link-\nlayer packets as frames.\nPhysical Layer\nWhile the job of the link layer is to move entire frames from one network element\nto an adjacent network element, the job of the physical layer is to move the individ-\nual bits within the frame from one node to the next. The protocols in this layer are\nagain link dependent and further depend on the actual transmission medium of the\nlink (for example, twisted-pair copper wire, single-mode fiber optics). For example,\nEthernet has many physical-layer protocols: one for twisted-pair copper wire,\nanother for coaxial cable, another for fiber, and so on. In each case, a bit is moved\nacross the link in a different way.\nThe OSI Model\nHaving discussed the Internet protocol stack in detail, we should mention that it is\nnot the only protocol stack around. In particular, back in the late 1970s, the Interna-\ntional Organization for Standardization (ISO) proposed that computer networks be\n52\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET"
    },
    {
      "chunk_id": "4df05a9c-6f7c-4585-afa9-267dbef06c3a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.5.1 Layered Architecture",
      "original_titles": [
        "1.5.1 Layered Architecture"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.5 Protocol Layers and Their Service Models > 1.5.1 Layered Architecture",
      "start_page": 74,
      "end_page": 79,
      "token_count": 3265,
      "text": "example. Suppose Rs = 2 Mbps, Rc = 1 Mbps, R = 5 Mbps, and the common link\ndivides its transmission rate equally among the 10 downloads. Then the bottleneck\nfor each download is no longer in the access network, but is now instead the shared\nlink in the core, which only provides each download with 500 kbps of throughput.\nThus the end-to-end throughput for each download is now reduced to 500 kbps.\nThe examples in Figure 1.19 and Figure 1.20(a) show that throughput depends\non the transmission rates of the links over which the data flows. We saw that when\nthere is no other intervening traffic, the throughput can simply be approximated as\nthe minimum transmission rate along the path between source and destination. The\nexample in Figure 1.20(b) shows that more generally the throughput depends not\nonly on the transmission rates of the links along the path, but also on the intervening\ntraffic. In particular, a link with a high transmission rate may nonetheless be the bot-\ntleneck link for a file transfer if many other data flows are also passing through that\nlink. We will examine throughput in computer networks more closely in the home-\nwork problems and in the subsequent chapters.\n1.5 Protocol Layers and Their Service Models\nFrom our discussion thus far, it is apparent that the Internet is an extremely compli-\ncated system. We have seen that there are many pieces to the Internet: numerous\napplications and protocols, various types of end systems, packet switches, and vari-\nous types of link-level media. Given this enormous complexity, is there any hope of\norganizing a network architecture, or at least our discussion of network architecture?\nFortunately, the answer to both questions is yes.\n1.5.1 Layered Architecture\nBefore attempting to organize our thoughts on Internet architecture, let’s look for a\nhuman analogy. Actually, we deal with complex systems all the time in our every-\nday life. Imagine if someone asked you to describe, for example, the airline system.\nHow would you find the structure to describe this complex system that has ticketing\nagents, baggage checkers, gate personnel, pilots, airplanes, air traffic control, and a\nworldwide system for routing airplanes? One way to describe this system might be\nto describe the series of actions you take (or others take for you) when you fly on an\nairline. You purchase your ticket, check your bags, go to the gate, and eventually get\nloaded onto the plane. The plane takes off and is routed to its destination. After your\nplane lands, you deplane at the gate and claim your bags. If the trip was bad, you\ncomplain about the flight to the ticket agent (getting nothing for your effort). This\nscenario is shown in Figure 1.21.\nAlready, we can see some analogies here with computer networking: You are\nbeing shipped from source to destination by the airline; a packet is shipped from\n1.5\n•\nPROTOCOL LAYERS AND THEIR SERVICE MODELS\n47\n\nsource host to destination host in the Internet. But this is not quite the analogy we\nare after. We are looking for some structure in Figure 1.21. Looking at Figure 1.21,\nwe note that there is a ticketing function at each end; there is also a baggage func-\ntion for already-ticketed passengers, and a gate function for already-ticketed and\nalready-baggage-checked passengers. For passengers who have made it through the\ngate (that is, passengers who are already ticketed, baggage-checked, and through the\ngate), there is a takeoff and landing function, and while in flight, there is an airplane-\nrouting function. This suggests that we can look at the functionality in Figure 1.21\nin a horizontal manner, as shown in Figure 1.22.\nFigure 1.22 has divided the airline functionality into layers, providing a frame-\nwork in which we can discuss airline travel. Note that each layer, combined with the\n48\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nTicket (purchase)\nBaggage (check)\nGates (load)\nRunway takeoff\nAirplane routing\nAirplane routing\nAirplane routing\nTicket (complain)\nBaggage (claim)\nGates (unload)\nRunway landing\nAirplane routing\nTicket\nBaggage\nGate\nTakeoff/Landing\nDeparture airport\nIntermediate air-traffic\ncontrol centers\nFigure 1.22 \u0002 Horizontal layering of airline functionality\nTicket (purchase)\nBaggage (check)\nGates (load)\nRunway takeoff\nAirplane routing\nTicket (complain)\nBaggage (claim)\nGates (unload)\nRunway landing\nAirplane routing\nAirplane routing\nFigure 1.21 \u0002 Taking an airplane trip: actions\n\n1.5\n•\nPROTOCOL LAYERS AND THEIR SERVICE MODELS\n49\nlayers below it, implements some functionality, some service. At the ticketing layer\nand below, airline-counter-to-airline-counter transfer of a person is accomplished.\nAt the baggage layer and below, baggage-check-to-baggage-claim transfer of a per-\nson and bags is accomplished. Note that the baggage layer provides this service only\nto an already-ticketed person. At the gate layer, departure-gate-to-arrival-gate trans-\nfer of a person and bags is accomplished. At the takeoff/landing layer, runway-to-\nrunway transfer of people and their bags is accomplished. Each layer provides its\nservice by (1) performing certain actions within that layer (for example, at the gate\nlayer, loading and unloading people from an airplane) and by (2) using the services\nof the layer directly below it (for example, in the gate layer, using the runway-to-\nrunway passenger transfer service of the takeoff/landing layer).\nA layered architecture allows us to discuss a well-defined, specific part of a\nlarge and complex system. This simplification itself is of considerable value by pro-\nviding modularity, making it much easier to change the implementation of the serv-\nice provided by the layer. As long as the layer provides the same service to the layer\nabove it, and uses the same services from the layer below it, the remainder of the\nsystem remains unchanged when a layer’s implementation is changed. (Note that\nchanging the implementation of a service is very different from changing the serv-\nice itself!) For example, if the gate functions were changed (for instance, to have\npeople board and disembark by height), the remainder of the airline system would\nremain unchanged since the gate layer still provides the same function (loading and\nunloading people); it simply implements that function in a different manner after the\nchange. For large and complex systems that are constantly being updated, the ability\nto change the implementation of a service without affecting other components of the\nsystem is another important advantage of layering.\nProtocol Layering\nBut enough about airlines. Let’s now turn our attention to network protocols. To\nprovide structure to the design of network protocols, network designers organize\nprotocols—and the network hardware and software that implement the protocols—\nin layers. Each protocol belongs to one of the layers, just as each function in the\nairline architecture in Figure 1.22 belonged to a layer. We are again interested in\nthe services that a layer offers to the layer above—the so-called service model of\na layer. Just as in the case of our airline example, each layer provides its service\nby (1) performing certain actions within that layer and by (2) using the services of\nthe layer directly below it. For example, the services provided by layer n may\ninclude reliable delivery of messages from one edge of the network to the other.\nThis might be implemented by using an unreliable edge-to-edge message delivery\nservice of layer n \u0003 1, and adding layer n functionality to detect and retransmit\nlost messages.\nA protocol layer can be implemented in software, in hardware, or in a combina-\ntion of the two. Application-layer protocols—such as HTTP and SMTP—are almost\n\nalways implemented in software in the end systems; so are transport-layer protocols.\nBecause the physical layer and data link layers are responsible for handling commu-\nnication over a specific link, they are typically implemented in a network interface\ncard (for example, Ethernet or WiFi interface cards) associated with a given link.\nThe network layer is often a mixed implementation of hardware and software. Also\nnote that just as the functions in the layered airline architecture were distributed\namong the various airports and flight control centers that make up the system, so too\nis a layer n protocol distributed among the end systems, packet switches, and other\ncomponents that make up the network. That is, there’s often a piece of a layer n pro-\ntocol in each of these network components.\nProtocol layering has conceptual and structural advantages [RFC 3439]. As we\nhave seen, layering provides a structured way to discuss system components. Mod-\nularity makes it easier to update system components. We mention, however, that\nsome researchers and networking engineers are vehemently opposed to layering\n[Wakeman 1992]. One potential drawback of layering is that one layer may dupli-\ncate lower-layer functionality. For example, many protocol stacks provide error\nrecovery on both a per-link basis and an end-to-end basis. A second potential draw-\nback is that functionality at one layer may need information (for example, a time-\nstamp value) that is present only in another layer; this violates the goal of\nseparation of layers.\nWhen taken together, the protocols of the various layers are called the protocol\nstack. The Internet protocol stack consists of five layers: the physical, link, network,\ntransport, and application layers, as shown in Figure 1.23(a). If you examine the\nTable of Contents, you will see that we have roughly organized this book using the\nlayers of the Internet protocol stack. We take a top-down approach, first covering\nthe application layer and then proceeding downward.\n50\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nTransport\nApplication\nNetwork\nLink\nPhysical\na.  Five-layer\n \nInternet\n \nprotocol stack\nTransport\nSession\nApplication\nPresentation\nNetwork\nLink\nPhysical\nb.  Seven-layer\n \nISO OSI\n \nreference model\nFigure 1.23 \u0002 The Internet protocol stack (a) and OSI reference model (b)\n\n1.5\n•\nPROTOCOL LAYERS AND THEIR SERVICE MODELS\n51\nApplication Layer\nThe application layer is where network applications and their application-layer proto-\ncols reside. The Internet’s application layer includes many protocols, such as the HTTP\nprotocol (which provides for Web document request and transfer), SMTP (which pro-\nvides for the transfer of e-mail messages), and FTP (which provides for the transfer of\nfiles between two end systems). We’ll see that certain network functions, such as the\ntranslation of human-friendly names for Internet end systems like www.ietf.org to a\n32-bit network address, are also done with the help of a specific application-layer pro-\ntocol, namely, the domain name system (DNS). We’ll see in Chapter 2 that it is very\neasy to create and deploy our own new application-layer protocols.\nAn application-layer protocol is distributed over multiple end systems, with the\napplication in one end system using the protocol to exchange packets of information\nwith the application in another end system. We’ll refer to this packet of information\nat the application layer as a message.\nTransport Layer\nThe Internet’s transport layer transports application-layer messages between\napplication endpoints. In the Internet there are two transport protocols, TCP and\nUDP, either of which can transport application-layer messages. TCP provides a\nconnection-oriented service to its applications. This service includes guaranteed\ndelivery of application-layer messages to the destination and flow control (that is,\nsender/receiver speed matching). TCP also breaks long messages into shorter seg-\nments and provides a congestion-control mechanism, so that a source throttles its\ntransmission rate when the network is congested. The UDP protocol provides a con-\nnectionless service to its applications. This is a no-frills service that provides no\nreliability, no flow control, and no congestion control. In this book, we’ll refer to a\ntransport-layer packet as a segment.\nNetwork Layer\nThe Internet’s network layer is responsible for moving network-layer packets\nknown as datagrams from one host to another. The Internet transport-layer proto-\ncol (TCP or UDP) in a source host passes a transport-layer segment and a destina-\ntion address to the network layer, just as you would give the postal service a letter\nwith a destination address. The network layer then provides the service of deliver-\ning the segment to the transport layer in the destination host.\nThe Internet’s network layer includes the celebrated IP Protocol, which defines\nthe fields in the datagram as well as how the end systems and routers act on these\nfields. There is only one IP protocol, and all Internet components that have a net-\nwork layer must run the IP protocol. The Internet’s network layer also contains rout-\ning protocols that determine the routes that datagrams take between sources and\n\ndestinations. The Internet has many routing protocols. As we saw in Section 1.3, the\nInternet is a network of networks, and within a network, the network administrator\ncan run any routing protocol desired. Although the network layer contains both the\nIP protocol and numerous routing protocols, it is often simply referred to as the IP\nlayer, reflecting the fact that IP is the glue that binds the Internet together.\nLink Layer\nThe Internet’s network layer routes a datagram through a series of routers between\nthe source and destination. To move a packet from one node (host or router) to the\nnext node in the route, the network layer relies on the services of the link layer. In\nparticular, at each node, the network layer passes the datagram down to the link\nlayer, which delivers the datagram to the next node along the route. At this next\nnode, the link layer passes the datagram up to the network layer.\nThe services provided by the link layer depend on the specific link-layer proto-\ncol that is employed over the link. For example, some link-layer protocols provide\nreliable delivery, from transmitting node, over one link, to receiving node. Note that\nthis reliable delivery service is different from the reliable delivery service of TCP,\nwhich provides reliable delivery from one end system to another. Examples of link-\nlayer protocols include Ethernet, WiFi, and the cable access network’s DOCSIS pro-\ntocol. As datagrams typically need to traverse several links to travel from source to\ndestination, a datagram may be handled by different link-layer protocols at different\nlinks along its route. For example, a datagram may be handled by Ethernet on one\nlink and by PPP on the next link. The network layer will receive a different service\nfrom each of the different link-layer protocols. In this book, we’ll refer to the link-\nlayer packets as frames.\nPhysical Layer\nWhile the job of the link layer is to move entire frames from one network element\nto an adjacent network element, the job of the physical layer is to move the individ-\nual bits within the frame from one node to the next. The protocols in this layer are\nagain link dependent and further depend on the actual transmission medium of the\nlink (for example, twisted-pair copper wire, single-mode fiber optics). For example,\nEthernet has many physical-layer protocols: one for twisted-pair copper wire,\nanother for coaxial cable, another for fiber, and so on. In each case, a bit is moved\nacross the link in a different way.\nThe OSI Model\nHaving discussed the Internet protocol stack in detail, we should mention that it is\nnot the only protocol stack around. In particular, back in the late 1970s, the Interna-\ntional Organization for Standardization (ISO) proposed that computer networks be\n52\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET"
    },
    {
      "chunk_id": "4e5e5e48-7e1b-4988-9c5e-9bcb60561928",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.5.2 Encapsulation",
      "original_titles": [
        "1.5.2 Encapsulation"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.5 Protocol Layers and Their Service Models > 1.5.2 Encapsulation",
      "start_page": 80,
      "end_page": 81,
      "token_count": 1032,
      "text": "organized around seven layers, called the Open Systems Interconnection (OSI)\nmodel [ISO 2012]. The OSI model took shape when the protocols that were to\nbecome the Internet protocols were in their infancy, and were but one of many dif-\nferent protocol suites under development; in fact, the inventors of the original OSI\nmodel probably did not have the Internet in mind when creating it. Nevertheless,\nbeginning in the late 1970s, many training and university courses picked up on the\nISO mandate and organized courses around the seven-layer model. Because of its\nearly impact on networking education, the seven-layer model continues to linger on\nin some networking textbooks and training courses.\nThe seven layers of the OSI reference model, shown in Figure 1.23(b), are:\napplication layer, presentation layer, session layer, transport layer, network layer,\ndata link layer, and physical layer. The functionality of five of these layers is\nroughly the same as their similarly named Internet counterparts. Thus, let’s consider\nthe two additional layers present in the OSI reference model—the presentation layer\nand the session layer. The role of the presentation layer is to provide services that\nallow communicating applications to interpret the meaning of data exchanged.\nThese services include data compression and data encryption (which are self-\nexplanatory) as well as data description (which, as we will see in Chapter 9, frees\nthe applications from having to worry about the internal format in which data are\nrepresented/stored—formats that may differ from one computer to another). The\nsession layer provides for delimiting and synchronization of data exchange, includ-\ning the means to build a checkpointing and recovery scheme.\nThe fact that the Internet lacks two layers found in the OSI reference model\nposes a couple of interesting questions: Are the services provided by these layers\nunimportant? What if an application needs one of these services? The Internet’s\nanswer to both of these questions is the same—it’s up to the application developer.\nIt’s up to the application developer to decide if a service is important, and if the\nservice is important, it’s up to the application developer to build that functionality\ninto the application.\n1.5.2 Encapsulation\nFigure 1.24 shows the physical path that data takes down a sending end system’s\nprotocol stack, up and down the protocol stacks of an intervening link-layer switch\nand router, and then up the protocol stack at the receiving end system. As we discuss\nlater in this book, routers and link-layer switches are both packet switches. Similar\nto end systems, routers and link-layer switches organize their networking hardware\nand software into layers. But routers and link-layer switches do not implement all of\nthe layers in the protocol stack; they typically implement only the bottom layers. As\nshown in Figure 1.24, link-layer switches implement layers 1 and 2; routers imple-\nment layers 1 through 3. This means, for example, that Internet routers are capable\nof implementing the IP protocol (a layer 3 protocol), while link-layer switches are\nnot. We’ll see later that while link-layer switches do not recognize IP addresses, they\n1.5\n•\nPROTOCOL LAYERS AND THEIR SERVICE MODELS\n53\n\n54\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nare capable of recognizing layer 2 addresses, such as Ethernet addresses. Note that\nhosts implement all five layers; this is consistent with the view that the Internet\narchitecture puts much of its complexity at the edges of the network.\nFigure 1.24 also illustrates the important concept of encapsulation. At the\nsending host, an application-layer message (M in Figure 1.24) is passed to the\ntransport layer. In the simplest case, the transport layer takes the message and\nappends additional information (so-called transport-layer header information, Ht\nin Figure 1.24) that will be used by the receiver-side transport layer. The applica-\ntion-layer message and the transport-layer header information together constitute\nthe transport-layer segment. The transport-layer segment thus encapsulates the\napplication-layer message. The added information might include information\nallowing the receiver-side transport layer to deliver the message up to the appro-\npriate application, and error-detection bits that allow the receiver to determine\nwhether bits in the message have been changed in route. The transport layer then\npasses the segment to the network layer, which adds network-layer header infor-\nmation (Hn in Figure 1.24) such as source and destination end system addresses,\nM\nM\nM\nM\nHt\nHt\nHt\nHn\nHn\nHl\nHt\nHn\nHl\nLink-layer switch\nRouter\nApplication\nTransport\nNetwork\nLink\nPhysical\nMessage\nSegment\nDatagram\nFrame\nM\nM\nM\nM\nHt\nHt\nHt\nHn\nHn\nHl\nLink\nPhysical\nSource\nNetwork\nLink\nPhysical\nDestination\nApplication\nTransport\nNetwork\nLink\nPhysical\nM\nHt\nHn\nHl\nM\nHt\nHn\nM\nHt\nHn\nM\nHt\nHn\nHl\nM\nHt\nHn\nHl\nM\nFigure 1.24 \u0002 Hosts, routers, and link-layer switches; each contains a\ndifferent set of layers, reflecting their differences in\nfunctionality"
    },
    {
      "chunk_id": "84e9fa9f-a59a-4bb1-880c-47eaf014a838",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.6 Networks Under Attack",
      "original_titles": [
        "1.6 Networks Under Attack"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.6 Networks Under Attack",
      "start_page": 82,
      "end_page": 86,
      "token_count": 2849,
      "text": "creating a network-layer datagram. The datagram is then passed to the link\nlayer, which (of course!) will add its own link-layer header information and create\na link-layer frame. Thus, we see that at each layer, a packet has two types of\nfields: header fields and a payload field. The payload is typically a packet from\nthe layer above.\nA useful analogy here is the sending of an interoffice memo from one corpo-\nrate branch office to another via the public postal service. Suppose Alice, who is\nin one branch office, wants to send a memo to Bob, who is in another branch\noffice. The memo is analogous to the application-layer message. Alice puts the\nmemo in an interoffice envelope with Bob’s name and department written on the\nfront of the envelope. The interoffice envelope is analogous to a transport-layer\nsegment—it contains header information (Bob’s name and department number)\nand it encapsulates the application-layer message (the memo). When the sending\nbranch-office mailroom receives the interoffice envelope, it puts the interoffice\nenvelope inside yet another envelope, which is suitable for sending through the\npublic postal service. The sending mailroom also writes the postal address of the\nsending and receiving branch offices on the postal envelope. Here, the postal\nenvelope is analogous to the datagram—it encapsulates the transport-layer seg-\nment (the interoffice envelope), which encapsulates the original message (the\nmemo). The postal service delivers the postal envelope to the receiving branch-\noffice mailroom. There, the process of de-encapsulation is begun. The mailroom\nextracts the interoffice memo and forwards it to Bob. Finally, Bob opens the enve-\nlope and removes the memo.\nThe process of encapsulation can be more complex than that described above.\nFor example, a large message may be divided into multiple transport-layer segments\n(which might themselves each be divided into multiple network-layer datagrams).\nAt the receiving end, such a segment must then be reconstructed from its constituent\ndatagrams.\n1.6 Networks Under Attack\nThe Internet has become mission critical for many institutions today, including large\nand small companies, universities, and government agencies. Many individuals also\nrely on the Internet for many of their professional, social, and personal activities.\nBut behind all this utility and excitement, there is a dark side, a side where “bad\nguys” attempt to wreak havoc in our daily lives by damaging our Internet-connected\ncomputers, violating our privacy, and rendering inoperable the Internet services on\nwhich we depend.\nThe field of network security is about how the bad guys can attack computer\nnetworks and about how we, soon-to-be experts in computer networking, can\n1.6\n•\nNETWORKS UNDER ATTACK\n55\n\n56\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\ndefend networks against those attacks, or better yet, design new architectures that\nare immune to such attacks in the first place. Given the frequency and variety of\nexisting attacks as well as the threat of new and more destructive future attacks,\nnetwork security has become a central topic in the field of computer networking.\nOne of the features of this textbook is that it brings network security issues to the\nforefront.\nSince we don’t yet have expertise in computer networking and Internet proto-\ncols, we’ll begin here by surveying some of today’s more prevalent security-\nrelated problems. This will whet our appetite for more substantial discussions in\nthe upcoming chapters. So we begin here by simply asking, what can go wrong?\nHow are computer networks vulnerable? What are some of the more prevalent\ntypes of attacks today?\nThe bad guys can put malware into your host via the Internet\nWe attach devices to the Internet because we want to receive/send data from/to\nthe Internet. This includes all kinds of good stuff, including Web pages, e-mail\nmessages, MP3s, telephone calls, live video, search engine results, and so on.\nBut, unfortunately, along with all that good stuff comes malicious stuff—collec-\ntively known as malware—that can also enter and infect our devices. Once mal-\nware infects our device it can do all kinds of devious things, including deleting\nour files; installing spyware that collects our private information, such as social\nsecurity numbers, passwords, and keystrokes, and then sends this (over the Inter-\nnet, of course!) back to the bad guys. Our compromised host may also be\nenrolled in a network of thousands of similarly compromised devices, collec-\ntively known as a botnet, which the bad guys control and leverage for spam e-\nmail distribution or distributed denial-of-service attacks (soon to be discussed)\nagainst targeted hosts.\nMuch of the malware out there today is self-replicating: once it infects one\nhost, from that host it seeks entry into other hosts over the Internet, and from the\nnewly infected hosts, it seeks entry into yet more hosts. In this manner, self-\nreplicating malware can spread exponentially fast. Malware can spread in the\nform of a virus or a worm. Viruses are malware that require some form of user\ninteraction to infect the user’s device. The classic example is an e-mail attachment\ncontaining malicious executable code. If a user receives and opens such an attach-\nment, the user inadvertently runs the malware on the device. Typically, such e-\nmail viruses are self-replicating: once executed, the virus may send an identical\nmessage with an identical malicious attachment to, for example, every recipient in\nthe user’s address book. Worms are malware that can enter a device without any\nexplicit user interaction. For example, a user may be running a vulnerable network\napplication to which an attacker can send malware. In some cases, without any\nuser intervention, the application may accept the malware from the Internet and\n\nrun it, creating a worm. The worm in the newly infected device then scans the\nInternet, searching for other hosts running the same vulnerable network applica-\ntion. When it finds other vulnerable hosts, it sends a copy of itself to those hosts.\nToday, malware, is pervasive and costly to defend against. As you work through\nthis textbook, we encourage you to think about the following question: What can\ncomputer network designers do to defend Internet-attached devices from malware\nattacks?\nThe bad guys can attack servers and network infrastructure\nAnother broad class of security threats are known as denial-of-service (DoS)\nattacks. As the name suggests, a DoS attack renders a network, host, or other piece\nof infrastructure unusable by legitimate users. Web servers, e-mail servers, DNS\nservers (discussed in Chapter 2), and institutional networks can all be subject to DoS\nattacks. Internet DoS attacks are extremely common, with thousands of DoS attacks\noccurring every year [Moore 2001; Mirkovic 2005]. Most Internet DoS attacks fall\ninto one of three categories:\n•\nVulnerability attack. This involves sending a few well-crafted messages to a vul-\nnerable application or operating system running on a targeted host. If the right\nsequence of packets is sent to a vulnerable application or operating system, the\nservice can stop or, worse, the host can crash.\n•\nBandwidth flooding. The attacker sends a deluge of packets to the targeted\nhost—so many packets that the target’s access link becomes clogged, preventing\nlegitimate packets from reaching the server.\n•\nConnection flooding. The attacker establishes a large number of half-open or\nfully open TCP connections (TCP connections are discussed in Chapter 3) at the\ntarget host. The host can become so bogged down with these bogus connections\nthat it stops accepting legitimate connections.\nLet’s now explore the bandwidth-flooding attack in more detail. Recalling our\ndelay and loss analysis discussion in Section 1.4.2, it’s evident that if the server has an\naccess rate of R bps, then the attacker will need to send traffic at a rate of approxi-\nmately R bps to cause damage. If R is very large, a single attack source may not be\nable to generate enough traffic to harm the server. Furthermore, if all the traffic\nemanates from a single source, an upstream router may be able to detect the attack and\nblock all traffic from that source before the traffic gets near the server. In a distributed\nDoS (DDoS) attack, illustrated in Figure 1.25, the attacker controls multiple sources\nand has each source blast traffic at the target. With this approach, the aggregate traffic\nrate across all the controlled sources needs to be approximately R to cripple the\n1.6\n•\nNETWORKS UNDER ATTACK\n57\n\nservice. DDoS attacks leveraging botnets with thousands of comprised hosts are a\ncommon occurrence today [Mirkovic 2005]. DDos attacks are much harder to detect\nand defend against than a DoS attack from a single host.\nWe encourage you to consider the following question as you work your way\nthrough this book: What can computer network designers do to defend against DoS\nattacks? We will see that different defenses are needed for the three types of DoS\nattacks.\nThe bad guys can sniff packets\nMany users today access the Internet via wireless devices, such as WiFi-connected\nlaptops or handheld devices with cellular Internet connections (covered in Chapter\n6). While ubiquitous Internet access is extremely convenient and enables marvelous\nnew applications for mobile users, it also creates a major security vulnerability—by\nplacing a passive receiver in the vicinity of the wireless transmitter, that receiver can\nobtain a copy of every packet that is transmitted! These packets can contain all kinds\nof sensitive information, including passwords, social security numbers, trade\nsecrets, and private personal messages. A passive receiver that records a copy of\nevery packet that flies by is called a packet sniffer.\nSniffers can be deployed in wired environments as well. In wired broadcast envi-\nronments, as in many Ethernet LANs, a packet sniffer can obtain copies of broadcast\npackets sent over the LAN. As described in Section 1.2, cable access technologies\nalso broadcast packets and are thus vulnerable to sniffing. Furthermore, a bad guy\nwho gains access to an institution’s access router or access link to the Internet may\n58\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nAttacker\n“start\n    attack”\nSlave\nSlave\nSlave\nVictim\nSlave\nSlave\nFigure 1.25 \u0002 A distributed denial-of-service attack\n\n1.6\n•\nNETWORKS UNDER ATTACK\n59\nbe able to plant a sniffer that makes a copy of every packet going to/from the organi-\nzation. Sniffed packets can then be analyzed offline for sensitive information.\nPacket-sniffing software is freely available at various Web sites and as commercial\nproducts. Professors teaching a networking course have been known to assign lab exer-\ncises that involve writing a packet-sniffing and application-layer data reconstruction\nprogram. Indeed, the Wireshark [Wireshark 2012] labs associated with this text (see the\nintroductory Wireshark lab at the end of this chapter) use exactly such a packet sniffer!\nBecause packet sniffers are passive—that is, they do not inject packets into the\nchannel—they are difficult to detect. So, when we send packets into a wireless chan-\nnel, we must accept the possibility that some bad guy may be recording copies of\nour packets. As you may have guessed, some of the best defenses against packet\nsniffing involve cryptography. We will examine cryptography as it applies to net-\nwork security in Chapter 8.\nThe bad guys can masquerade as someone you trust\nIt is surprisingly easy (you will have the knowledge to do so shortly as you proceed\nthrough this text!) to create a packet with an arbitrary source address, packet con-\ntent, and destination address and then transmit this hand-crafted packet into the\nInternet, which will dutifully forward the packet to its destination. Imagine the\nunsuspecting receiver (say an Internet router) who receives such a packet, takes the\n(false) source address as being truthful, and then performs some command embed-\nded in the packet’s contents (say modifies its forwarding table). The ability to inject\npackets into the Internet with a false source address is known as IP spoofing, and is\nbut one of many ways in which one user can masquerade as another user.\nTo solve this problem, we will need end-point authentication, that is, a mecha-\nnism that will allow us to determine with certainty if a message originates from\nwhere we think it does. Once again, we encourage you to think about how this can\nbe done for network applications and protocols as you progress through the chapters\nof this book. We will explore mechanisms for end-point authentication in Chapter 8.\nIn closing this section, it’s worth considering how the Internet got to be such an\ninsecure place in the first place. The answer, in essence, is that the Internet was orig-\ninally designed to be that way, based on the model of “a group of mutually trusting\nusers attached to a transparent network” [Blumenthal 2001]—a model in which (by\ndefinition) there is no need for security. Many aspects of the original Internet archi-\ntecture deeply reflect this notion of mutual trust. For example, the ability for one\nuser to send a packet to any other user is the default rather than a requested/granted\ncapability, and user identity is taken at declared face value, rather than being authen-\nticated by default.\nBut today’s Internet certainly does not involve “mutually trusting users.”\nNonetheless, today’s users still need to communicate when they don’t necessarily\ntrust each other, may wish to communicate anonymously, may communicate indi-\nrectly through third parties (e.g., Web caches, which we’ll study in Chapter 2, or"
    },
    {
      "chunk_id": "129de6f5-122b-421a-bb24-24438f62f48a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.7 History of Computer Networking and the Internet",
      "original_titles": [
        "1.7 History of Computer Networking and the Internet"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.7 History of Computer Networking and the Internet",
      "start_page": 87,
      "end_page": 88,
      "token_count": 743,
      "text": "60\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nmobility-assisting agents, which we’ll study in Chapter 6), and may distrust the\nhardware, software, and even the air through which they communicate. We now\nhave many security-related challenges before us as we progress through this book:\nWe should seek defenses against sniffing, end-point masquerading, man-in-the-\nmiddle attacks, DDoS attacks, malware, and more. We should keep in mind that\ncommunication among mutually trusted users is the exception rather than the rule.\nWelcome to the world of modern computer networking!\n1.7 History of Computer Networking and \nthe Internet\nSections 1.1 through 1.6 presented an overview of the technology of computer net-\nworking and the Internet. You should know enough now to impress your family and\nfriends! However, if you really want to be a big hit at the next cocktail party, you\nshould sprinkle your discourse with tidbits about the fascinating history of the Inter-\nnet [Segaller 1998].\n1.7.1 The Development of Packet Switching: 1961–1972\nThe field of computer networking and today’s Internet trace their beginnings\nback to the early 1960s, when the telephone network was the world’s dominant\ncommunication network. Recall from Section 1.3 that the telephone network uses\ncircuit switching to transmit information from a sender to a receiver—an appro-\npriate choice given that voice is transmitted at a constant rate between sender\nand receiver. Given the increasing importance of computers in the early 1960s\nand the advent of timeshared computers, it was perhaps natural to consider how\nto hook computers together so that they could be shared among geographically\ndistributed users. The traffic generated by such users was likely to be bursty—\nintervals of activity, such as the sending of a command to a remote computer, fol-\nlowed by periods of inactivity while waiting for a reply or while contemplating\nthe received response.\nThree research groups around the world, each unaware of the others’ work\n[Leiner 1998], began inventing packet switching as an efficient and robust alterna-\ntive to circuit switching. The first published work on packet-switching techniques\nwas that of Leonard Kleinrock [Kleinrock 1961; Kleinrock 1964], then a graduate\nstudent at MIT. Using queuing theory, Kleinrock’s work elegantly demonstrated the\neffectiveness of the packet-switching approach for bursty traffic sources. In 1964,\nPaul Baran [Baran 1964] at the Rand Institute had begun investigating the use of\npacket switching for secure voice over military networks, and at the National Physi-\ncal Laboratory in England, Donald Davies and Roger Scantlebury were also devel-\noping their ideas on packet switching.\n\nThe work at MIT, Rand, and the NPL laid the foundations for today’s Inter-\nnet. But the Internet also has a long history of a let’s-build-it-and-demonstrate-it\nattitude that also dates back to the 1960s. J. C. R. Licklider [DEC 1990] and\nLawrence Roberts, both colleagues of Kleinrock’s at MIT, went on to lead the\ncomputer science program at the Advanced Research Projects Agency (ARPA) in\nthe United States. Roberts published an overall plan for the ARPAnet [Roberts\n1967], the first packet-switched computer network and a direct ancestor of today’s\npublic Internet. On Labor Day in 1969, the first packet switch was installed at\nUCLA under Kleinrock’s supervision, and three additional packet switches were\ninstalled shortly thereafter at the Stanford Research Institute (SRI), UC Santa Bar-\nbara, and the University of Utah (Figure 1.26). The fledgling precursor to the\n1.7\n•\nHISTORY OF COMPUTER NETWORKING AND THE INTERNET\n61\nFigure 1.26 \u0002 An early packet switch"
    },
    {
      "chunk_id": "b2108d9c-828f-46c9-9897-8ed74b17e207",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.7.1 The Development of Packet Switching: 1961–1972",
      "original_titles": [
        "1.7.1 The Development of Packet Switching: 1961–1972"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.7 History of Computer Networking and the Internet > 1.7.1 The Development of Packet Switching: 1961–1972",
      "start_page": 87,
      "end_page": 88,
      "token_count": 743,
      "text": "60\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nmobility-assisting agents, which we’ll study in Chapter 6), and may distrust the\nhardware, software, and even the air through which they communicate. We now\nhave many security-related challenges before us as we progress through this book:\nWe should seek defenses against sniffing, end-point masquerading, man-in-the-\nmiddle attacks, DDoS attacks, malware, and more. We should keep in mind that\ncommunication among mutually trusted users is the exception rather than the rule.\nWelcome to the world of modern computer networking!\n1.7 History of Computer Networking and \nthe Internet\nSections 1.1 through 1.6 presented an overview of the technology of computer net-\nworking and the Internet. You should know enough now to impress your family and\nfriends! However, if you really want to be a big hit at the next cocktail party, you\nshould sprinkle your discourse with tidbits about the fascinating history of the Inter-\nnet [Segaller 1998].\n1.7.1 The Development of Packet Switching: 1961–1972\nThe field of computer networking and today’s Internet trace their beginnings\nback to the early 1960s, when the telephone network was the world’s dominant\ncommunication network. Recall from Section 1.3 that the telephone network uses\ncircuit switching to transmit information from a sender to a receiver—an appro-\npriate choice given that voice is transmitted at a constant rate between sender\nand receiver. Given the increasing importance of computers in the early 1960s\nand the advent of timeshared computers, it was perhaps natural to consider how\nto hook computers together so that they could be shared among geographically\ndistributed users. The traffic generated by such users was likely to be bursty—\nintervals of activity, such as the sending of a command to a remote computer, fol-\nlowed by periods of inactivity while waiting for a reply or while contemplating\nthe received response.\nThree research groups around the world, each unaware of the others’ work\n[Leiner 1998], began inventing packet switching as an efficient and robust alterna-\ntive to circuit switching. The first published work on packet-switching techniques\nwas that of Leonard Kleinrock [Kleinrock 1961; Kleinrock 1964], then a graduate\nstudent at MIT. Using queuing theory, Kleinrock’s work elegantly demonstrated the\neffectiveness of the packet-switching approach for bursty traffic sources. In 1964,\nPaul Baran [Baran 1964] at the Rand Institute had begun investigating the use of\npacket switching for secure voice over military networks, and at the National Physi-\ncal Laboratory in England, Donald Davies and Roger Scantlebury were also devel-\noping their ideas on packet switching.\n\nThe work at MIT, Rand, and the NPL laid the foundations for today’s Inter-\nnet. But the Internet also has a long history of a let’s-build-it-and-demonstrate-it\nattitude that also dates back to the 1960s. J. C. R. Licklider [DEC 1990] and\nLawrence Roberts, both colleagues of Kleinrock’s at MIT, went on to lead the\ncomputer science program at the Advanced Research Projects Agency (ARPA) in\nthe United States. Roberts published an overall plan for the ARPAnet [Roberts\n1967], the first packet-switched computer network and a direct ancestor of today’s\npublic Internet. On Labor Day in 1969, the first packet switch was installed at\nUCLA under Kleinrock’s supervision, and three additional packet switches were\ninstalled shortly thereafter at the Stanford Research Institute (SRI), UC Santa Bar-\nbara, and the University of Utah (Figure 1.26). The fledgling precursor to the\n1.7\n•\nHISTORY OF COMPUTER NETWORKING AND THE INTERNET\n61\nFigure 1.26 \u0002 An early packet switch"
    },
    {
      "chunk_id": "208c3a4f-3b6d-4ae0-94e1-e3f3cdadcc22",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.7.2 Proprietary Networks and Internetworking: 1972–1980",
      "original_titles": [
        "1.7.2 Proprietary Networks and Internetworking: 1972–1980"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.7 History of Computer Networking and the Internet > 1.7.2 Proprietary Networks and Internetworking: 1972–1980",
      "start_page": 89,
      "end_page": 89,
      "token_count": 582,
      "text": "62\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nInternet was four nodes large by the end of 1969. Kleinrock recalls the very first\nuse of the network to perform a remote login from UCLA to SRI, crashing the sys-\ntem [Kleinrock 2004].\nBy 1972, ARPAnet had grown to approximately 15 nodes and was given its first\npublic demonstration by Robert Kahn. The first host-to-host protocol between\nARPAnet end systems, known as the network-control protocol (NCP), was com-\npleted [RFC 001]. With an end-to-end protocol available, applications could now be\nwritten. Ray Tomlinson wrote the first e-mail program in 1972.\n1.7.2 Proprietary Networks and Internetworking: 1972–1980\nThe initial ARPAnet was a single, closed network. In order to communicate with an\nARPAnet host, one had to be actually attached to another ARPAnet IMP. In the early\nto mid-1970s, additional stand-alone packet-switching networks besides ARPAnet\ncame into being: ALOHANet, a microwave network linking universities on the\nHawaiian islands [Abramson 1970], as well as DARPA’s packet-satellite [RFC 829]\nand packet-radio networks [Kahn 1978]; Telenet, a BBN commercial packet-\nswitching network based on ARPAnet technology; Cyclades, a French packet-\nswitching network pioneered by Louis Pouzin [Think 2012]; Time-sharing networks\nsuch as Tymnet and the GE Information Services network, among others, in the late\n1960s and early 1970s [Schwartz 1977]; IBM’s SNA (1969–1974), which paral-\nleled the ARPAnet work [Schwartz 1977].\nThe number of networks was growing. With perfect hindsight we can see that\nthe time was ripe for developing an encompassing architecture for connecting net-\nworks together. Pioneering work on interconnecting networks (under the sponsor-\nship of the Defense Advanced Research Projects Agency (DARPA)), in essence\ncreating a network of networks, was done by Vinton Cerf and Robert Kahn [Cerf\n1974]; the term internetting was coined to describe this work.\nThese architectural principles were embodied in TCP. The early versions of\nTCP, however, were quite different from today’s TCP. The early versions of TCP\ncombined a reliable in-sequence delivery of data via end-system retransmission\n(still part of today’s TCP) with forwarding functions (which today are performed\nby IP). Early experimentation with TCP, combined with the recognition of the\nimportance of an unreliable, non-flow-controlled, end-to-end transport service\nfor applications such as packetized voice, led to the separation of IP out of TCP\nand the development of the UDP protocol. The three key Internet protocols that\nwe see today—TCP, UDP, and IP—were conceptually in place by the end of the\n1970s.\nIn addition to the DARPA Internet-related research, many other important\nnetworking activities were underway. In Hawaii, Norman Abramson was devel-\noping ALOHAnet, a packet-based radio network that allowed multiple remote\nsites on the Hawaiian Islands to communicate with each other. The ALOHA protocol"
    },
    {
      "chunk_id": "128867c5-859b-4869-a263-c8bfa544e84a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.7.3 A Proliferation of Networks: 1980–1990",
      "original_titles": [
        "1.7.3 A Proliferation of Networks: 1980–1990"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.7 History of Computer Networking and the Internet > 1.7.3 A Proliferation of Networks: 1980–1990",
      "start_page": 90,
      "end_page": 90,
      "token_count": 608,
      "text": "[Abramson 1970] was the first multiple-access protocol, allowing geographically\ndistributed users to share a single broadcast communication medium (a radio fre-\nquency). Metcalfe and Boggs built on Abramson’s multiple-access protocol work\nwhen they developed the Ethernet protocol [Metcalfe 1976] for wire-based\nshared broadcast networks. Interestingly, Metcalfe and Boggs’ Ethernet protocol\nwas motivated by the need to connect multiple PCs, printers, and shared disks\n[Perkins 1994]. Twenty-five years ago, well before the PC revolution and the\nexplosion of networks, Metcalfe and Boggs were laying the foundation for\ntoday’s PC LANs.\n1.7.3 A Proliferation of Networks: 1980–1990\nBy the end of the 1970s, approximately two hundred hosts were connected to the\nARPAnet. By the end of the 1980s the number of hosts connected to the public\nInternet, a confederation of networks looking much like today’s Internet, would\nreach a hundred thousand. The 1980s would be a time of tremendous growth.\nMuch of that growth resulted from several distinct efforts to create computer\nnetworks linking universities together. BITNET provided e-mail and file transfers\namong several universities in the Northeast. CSNET (computer science network)\nwas formed to link university researchers who did not have access to ARPAnet. In\n1986, NSFNET was created to provide access to NSF-sponsored supercomputing\ncenters. Starting with an initial backbone speed of 56 kbps, NSFNET’s backbone\nwould be running at 1.5 Mbps by the end of the decade and would serve as a pri-\nmary backbone linking regional networks.\nIn the ARPAnet community, many of the final pieces of today’s Internet archi-\ntecture were falling into place. January 1, 1983 saw the official deployment of\nTCP/IP as the new standard host protocol for ARPAnet (replacing the NCP proto-\ncol). The transition [RFC 801] from NCP to TCP/IP was a flag day event—all\nhosts were required to transfer over to TCP/IP as of that day. In the late 1980s,\nimportant extensions were made to TCP to implement host-based congestion con-\ntrol [Jacobson 1988]. The DNS, used to map between a human-readable Internet\nname (for example, gaia.cs.umass.edu) and its 32-bit IP address, was also devel-\noped [RFC 1034].\nParalleling this development of the ARPAnet (which was for the most part a\nUS effort), in the early 1980s the French launched the Minitel project, an ambi-\ntious plan to bring data networking into everyone’s home. Sponsored by the\nFrench government, the Minitel system consisted of a public packet-switched net-\nwork (based on the X.25 protocol suite), Minitel servers, and inexpensive termi-\nnals with built-in low-speed modems. The Minitel became a huge success in 1984\nwhen the French government gave away a free Minitel terminal to each French\nhousehold that wanted one. Minitel sites included free sites—such as a telephone\ndirectory site—as well as private sites, which collected a usage-based fee from\n1.7\n•\nHISTORY OF COMPUTER NETWORKING AND THE INTERNET\n63"
    },
    {
      "chunk_id": "4d0495da-9285-4034-b8de-0f63b8ea4932",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.7.4 The Internet Explosion: The 1990s",
      "original_titles": [
        "1.7.4 The Internet Explosion: The 1990s"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.7 History of Computer Networking and the Internet > 1.7.4 The Internet Explosion: The 1990s",
      "start_page": 91,
      "end_page": 91,
      "token_count": 574,
      "text": "each user. At its peak in the mid 1990s, it offered more than 20,000 services, rang-\ning from home banking to specialized research databases. The Minitel was in a\nlarge proportion of French homes 10 years before most Americans had ever heard\nof the Internet.\n1.7.4 The Internet Explosion: The 1990s\nThe 1990s were ushered in with a number of events that symbolized the continued\nevolution and the soon-to-arrive commercialization of the Internet. ARPAnet, the\nprogenitor of the Internet, ceased to exist. In 1991, NSFNET lifted its restrictions on\nthe use of NSFNET for commercial purposes. NSFNET itself would be decommis-\nsioned in 1995, with Internet backbone traffic being carried by commercial Internet\nService Providers.\nThe main event of the 1990s was to be the emergence of the World Wide Web\napplication, which brought the Internet into the homes and businesses of millions of\npeople worldwide. The Web served as a platform for enabling and deploying hun-\ndreds of new applications that we take for granted today, including search (e.g.,\nGoogle and Bing) Internet commerce (e.g., Amazon and eBay) and social networks\n(e.g., Facebook).\nThe Web was invented at CERN by Tim Berners-Lee between 1989 and 1991\n[Berners-Lee 1989], based on ideas originating in earlier work on hypertext from\nthe 1940s by Vannevar Bush [Bush 1945] and since the 1960s by Ted Nelson\n[Xanadu 2012]. Berners-Lee and his associates developed initial versions of HTML,\nHTTP, a Web server, and a browser—the four key components of the Web. Around\nthe end of 1993 there were about two hundred Web servers in operation, this collec-\ntion of servers being just a harbinger of what was about to come. At about this time\nseveral researchers were developing Web browsers with GUI interfaces, including\nMarc Andreessen, who along with Jim Clark, formed Mosaic Communications,\nwhich later became Netscape Communications Corporation [Cusumano 1998; Quittner\n1998]. By 1995, university students were using Netscape browsers to surf the Web\non a daily basis. At about this time companies—big and small—began to operate\nWeb servers and transact commerce over the Web. In 1996, Microsoft started to\nmake browsers, which started the browser war between Netscape and Microsoft,\nwhich Microsoft won a few years later [Cusumano 1998].\nThe second half of the 1990s was a period of tremendous growth and innova-\ntion for the Internet, with major corporations and thousands of startups creating\nInternet products and services. By the end of the millennium the Internet was sup-\nporting hundreds of popular applications, including four killer applications:\n•\nE-mail, including attachments and Web-accessible e-mail\n•\nThe Web, including Web browsing and Internet commerce\n64\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET"
    },
    {
      "chunk_id": "5dc4e72f-c604-434e-8cfd-513f333f1acf",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.7.5 The New Millennium",
      "original_titles": [
        "1.7.5 The New Millennium"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.7 History of Computer Networking and the Internet > 1.7.5 The New Millennium",
      "start_page": 92,
      "end_page": 92,
      "token_count": 518,
      "text": "•\nInstant messaging, with contact lists\n•\nPeer-to-peer file sharing of MP3s, pioneered by Napster\nInterestingly, the first two killer applications came from the research community,\nwhereas the last two were created by a few young entrepreneurs.\nThe period from 1995 to 2001 was a roller-coaster ride for the Internet in the\nfinancial markets. Before they were even profitable, hundreds of Internet startups\nmade initial public offerings and started to be traded in a stock market. Many com-\npanies were valued in the billions of dollars without having any significant revenue\nstreams. The Internet stocks collapsed in 2000–2001, and many startups shut down.\nNevertheless, a number of companies emerged as big winners in the Internet space,\nincluding Microsoft, Cisco, Yahoo, e-Bay, Google, and Amazon.\n1.7.5 The New Millennium\nInnovation in computer networking continues at a rapid pace. Advances are being\nmade on all fronts, including deployments of faster routers and higher transmission\nspeeds in both access networks and in network backbones. But the following devel-\nopments merit special attention:\n•\nSince the beginning of the millennium, we have been seeing aggressive deploy-\nment of broadband Internet access to homes—not only cable modems and DSL\nbut also fiber to the home, as discussed in Section 1.2. This high-speed Internet\naccess has set the stage for a wealth of video applications, including the distribu-\ntion of user-generated video (for example, YouTube), on-demand streaming of\nmovies and television shows (e.g., Netflix) , and multi-person video conference\n(e.g., Skype).\n•\nThe increasing ubiquity of high-speed (54 Mbps and higher) public WiFi net-\nworks and medium-speed (up to a few Mbps) Internet access via 3G and 4G\ncellular telephony networks is not only making it possible to remain con-\nstantly connected while on the move, but also enabling new location-specific\napplications. The number of wireless devices connecting to the Internet sur-\npassed the number of wired devices in 2011. This high-speed wireless access\nhas set the stage for the rapid emergence of hand-held computers (iPhones,\nAndroids, iPads, and so on), which enjoy constant and untethered access to\nthe Internet.\n•\nOnline social networks, such as Facebook and Twitter, have created massive peo-\nple networks on top of the Internet. Many Internet users today “live” primarily\nwithin Facebook. Through their APIs, the online social networks create plat-\nforms for new networked applications and distributed games.\n1.7\n•\nHISTORY OF COMPUTER NETWORKING AND THE INTERNET\n65"
    },
    {
      "chunk_id": "3a81374d-239c-456b-b1ed-94c73a34018b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "1.8 Summary",
      "original_titles": [
        "1.8 Summary"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > 1.8 Summary",
      "start_page": 93,
      "end_page": 94,
      "token_count": 1202,
      "text": "66\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\n•\nAs discussed in Section 1.3.3, online service providers, such as Google and\nMicrosoft, have deployed their own extensive private networks, which not only\nconnect together their globally distributed data centers, but are used to bypass\nthe Internet as much as possible by peering directly with lower-tier ISPs. As a\nresult, Google provides search results and email access almost instantaneously,\nas if their data centers were running within one’s own computer.\n•\nMany Internet commerce companies are now running their applications in the\n“cloud”—such as in Amazon’s EC2, in Google’s Application Engine, or in\nMicrosoft’s Azure. Many companies and universities have also migrated their Inter-\nnet applications (e.g., email and Web hosting) to the cloud. Cloud companies not\nonly provide applications scalable computing and storage environments, but also\nprovide the applications implicit access to their high-performance private networks.\n1.8 Summary\nIn this chapter we’ve covered a tremendous amount of material! We’ve looked at the\nvarious pieces of hardware and software that make up the Internet in particular and\ncomputer networks in general. We started at the edge of the network, looking at end\nsystems and applications, and at the transport service provided to the applications\nrunning on the end systems. We also looked at the link-layer technologies and phys-\nical media typically found in the access network. We then dove deeper inside the\nnetwork, into the network core, identifying packet switching and circuit switching\nas the two basic approaches for transporting data through a telecommunication net-\nwork, and we examined the strengths and weaknesses of each approach. We also\nexamined the structure of the global Internet, learning that the Internet is a network\nof networks. We saw that the Internet’s hierarchical structure, consisting of higher-\nand lower-tier ISPs, has allowed it to scale to include thousands of networks.\nIn the second part of this introductory chapter, we examined several topics central\nto the field of computer networking. We first examined the causes of delay, through-\nput and packet loss in a packet-switched network. We developed simple quantitative\nmodels for transmission, propagation, and queuing delays as well as for throughput;\nwe’ll make extensive use of these delay models in the homework problems through-\nout this book. Next we examined protocol layering and service models, key architec-\ntural principles in networking that we will also refer back to throughout this book. We\nalso surveyed some of the more prevalent security attacks in the Internet day. We fin-\nished our introduction to networking with a brief history of computer networking. The\nfirst chapter in itself constitutes a mini-course in computer networking.\nSo, we have indeed covered a tremendous amount of ground in this first chapter!\nIf you’re a bit overwhelmed, don’t worry. In the following chapters we’ll revisit all of\nthese ideas, covering them in much more detail (that’s a promise, not a threat!). At this\npoint, we hope you leave this chapter with a still-developing intuition for the pieces\n\nthat make up a network, a still-developing command of the vocabulary of networking\n(don’t be shy about referring back to this chapter), and an ever-growing desire to learn\nmore about networking. That’s the task ahead of us for the rest of this book.\nRoad-Mapping This Book\nBefore starting any trip, you should always glance at a road map in order to become\nfamiliar with the major roads and junctures that lie ahead. For the trip we are about\nto embark on, the ultimate destination is a deep understanding of the how, what, and\nwhy of computer networks. Our road map is the sequence of chapters of this book:\n1. Computer Networks and the Internet\n2. Application Layer\n3. Transport Layer\n4. Network Layer\n5. Link Layer and Local Area Networks\n6. Wireless and Mobile Networks\n7. Multimedia Networking\n8. Security in Computer Networks\n9. Network Management\nChapters 2 through 5 are the four core chapters of this book. You should notice\nthat these chapters are organized around the top four layers of the five-layer Internet\nprotocol stack, one chapter for each layer. Further note that our journey will begin at\nthe top of the Internet protocol stack, namely, the application layer, and will work\nits way downward. The rationale behind this top-down journey is that once we\nunderstand the applications, we can understand the network services needed to sup-\nport these applications. We can then, in turn, examine the various ways in which\nsuch services might be implemented by a network architecture. Covering applica-\ntions early thus provides motivation for the remainder of the text.\nThe second half of the book—Chapters 6 through 9—zooms in on four enor-\nmously important (and somewhat independent) topics in modern computer network-\ning. In Chapter 6, we examine wireless and mobile networks, including wireless\nLANs (including WiFi and Bluetooth), Cellular telephony networks (including\nGSM, 3G, and 4G), and mobility (in both IP and GSM networks). In Chapter 7\n(Multimedia Networking) we examine audio and video applications such as Internet\nphone, video conferencing, and streaming of stored media. We also look at how a\npacket-switched network can be designed to provide consistent quality of service to\naudio and video applications. In Chapter 8 (Security in Computer Networks), we\nfirst look at the underpinnings of encryption and network security, and then we\nexamine how the basic theory is being applied in a broad range of Internet contexts.\nThe last chapter (Network Management) examines the key issues in network man-\nagement as well as the primary Internet protocols used for network management.\n1.8\n•\nSUMMARY\n67"
    },
    {
      "chunk_id": "8bc6cfb0-f4e2-4d15-8cbb-9f935895ffd6",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Homework Problems and Questions",
      "original_titles": [
        "Homework Problems and Questions"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > Homework Problems and Questions",
      "start_page": 95,
      "end_page": 104,
      "token_count": 5632,
      "text": "Homework Problems and Questions\nChapter 1 Review Questions\nSECTION 1.1\nR1. What is the difference between a host and an end system? List several different\ntypes of end systems. Is a Web server an end system?\nR2. The word protocol is often used to describe diplomatic relations. How does\nWikipedia describe diplomatic protocol?\nR3. Why are standards important for protocols?\nSECTION 1.2\nR4. List six access technologies. Classify each one as home access, enterprise\naccess, or wide-area wireless access.\nR5. Is HFC transmission rate dedicated or shared among users? Are collisions\npossible in a downstream HFC channel? Why or why not?\nR6. List the available residential access technologies in your city. For each type\nof access, provide the advertised downstream rate, upstream rate, and\nmonthly price.\nR7. What is the transmission rate of Ethernet LANs? \nR8. What are some of the physical media that Ethernet can run over?\nR9. Dial-up modems, HFC, DSL and FTTH are all used for residential access.\nFor each of these access technologies, provide a range of transmission rates\nand comment on whether the transmission rate is shared or dedicated.\nR10. Describe the most popular wireless Internet access technologies today. Com-\npare and contrast them.\nSECTION 1.3\nR11. Suppose there is exactly one packet switch between a sending host and a\nreceiving host. The transmission rates between the sending host and the\nswitch and between the switch and the receiving host are R1 and R2, respec-\ntively. Assuming that the switch uses store-and-forward packet switching,\nwhat is the total end-to-end delay to send a packet of length L? (Ignore\nqueuing, propagation delay, and processing delay.)\nR12. What advantage does a circuit-switched network have over a packet-switched\nnetwork? What advantages does TDM have over FDM in a circuit-switched\nnetwork?\nR13. Suppose users share a 2 Mbps link. Also suppose each user transmits continu-\nously at 1 Mbps when transmitting, but each user transmits only 20 percent of\nthe time. (See the discussion of statistical multiplexing in Section 1.3.)\n68\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\n\na. When circuit switching is used, how many users can be supported?\nb. For the remainder of this problem, suppose packet switching is used. Why\nwill there be essentially no queuing delay before the link if two or fewer\nusers transmit at the same time? Why will there be a queuing delay if\nthree users transmit at the same time?\nc. Find the probability that a given user is transmitting.\nd. Suppose now there are three users. Find the probability that at any given\ntime, all three users are transmitting simultaneously. Find the fraction of\ntime during which the queue grows.\nR14. Why will two ISPs at the same level of the hierarchy often peer with each\nother? How does an IXP earn money?\nR15. Some content providers have created their own networks. Describe Google’s\nnetwork. What motivates content providers to create these networks?\nSECTION 1.4\nR16. Consider sending a packet from a source host to a destination host over a\nfixed route. List the delay components in the end-to-end delay. Which of\nthese delays are constant and which are variable?\nR17. Visit the Transmission Versus Propagation Delay applet at the companion\nWeb site. Among the rates, propagation delay, and packet sizes available,\nfind a combination for which the sender finishes transmitting before the first\nbit of the packet reaches the receiver. Find another combination for which\nthe first bit of the packet reaches the receiver before the sender finishes\ntransmitting.\nR18. How long does it take a packet of length 1,000 bytes to propagate over a link\nof distance 2,500 km, propagation speed 2.5 · 108 m/s, and transmission rate\n2 Mbps? More generally, how long does it take a packet of length L to propa-\ngate over a link of distance d, propagation speed s, and transmission rate R\nbps? Does this delay depend on packet length? Does this delay depend on\ntransmission rate?\nR19. Suppose Host A wants to send a large file to Host B. The path from Host A to\nHost B has three links, of rates R1 = 500 kbps, R2 = 2 Mbps, and R3 = 1 Mbps.\na. Assuming no other traffic in the network, what is the throughput for the\nfile transfer?\nb. Suppose the file is 4 million bytes. Dividing the file size by the throughput,\nroughly how long will it take to transfer the file to Host B?\nc. Repeat (a) and (b), but now with R2 reduced to 100 kbps.\nR20. Suppose end system A wants to send a large file to end system B. At a very\nhigh level, describe how end system A creates packets from the file. When\nHOMEWORK PROBLEMS AND QUESTIONS\n69\n\none of these packets arrives to a packet switch, what information in the\npacket does the switch use to determine the link onto which the packet is\nforwarded? Why is packet switching in the Internet analogous to driving from\none city to another and asking directions along the way?\nR21. Visit the Queuing and Loss applet at the companion Web site. What is the\nmaximum emission rate and the minimum transmission rate? With those\nrates, what is the traffic intensity? Run the applet with these rates and deter-\nmine how long it takes for packet loss to occur. Then repeat the experiment a\nsecond time and determine again how long it takes for packet loss to occur.\nAre the values different? Why or why not?\nSECTION 1.5\nR22. List five tasks that a layer can perform. Is it possible that one (or more) of\nthese tasks could be performed by two (or more) layers?\nR23. What are the five layers in the Internet protocol stack? What are the principal\nresponsibilities of each of these layers?\nR24. What is an application-layer message? A transport-layer segment? A network-\nlayer datagram? A link-layer frame?\nR25. Which layers in the Internet protocol stack does a router process? Which\nlayers does a link-layer switch process? Which layers does a host process?\nSECTION 1.6\nR26. What is the difference between a virus and a worm?\nR27. Describe how a botnet can be created, and how it can be used for a DDoS\nattack.\nR28. Suppose Alice and Bob are sending packets to each other over a computer\nnetwork. Suppose Trudy positions herself in the network so that she can\ncapture all the packets sent by Alice and send whatever she wants to Bob;\nshe can also capture all the packets sent by Bob and send whatever she\nwants to Alice. List some of the malicious things Trudy can do from this\nposition.\nProblems\nP1. Design and describe an application-level protocol to be used between an\nautomatic teller machine and a bank’s centralized computer. Your protocol\nshould allow a user’s card and password to be verified, the account balance\n(which is maintained at the centralized computer) to be queried, and an\naccount withdrawal to be made (that is, money disbursed to the user). Your\n70\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\n\nprotocol entities should be able to handle the all-too-common case in which\nthere is not enough money in the account to cover the withdrawal. Specify\nyour protocol by listing the messages exchanged and the action taken by the\nautomatic teller machine or the bank’s centralized computer on transmission\nand receipt of messages. Sketch the operation of your protocol for the case of a\nsimple withdrawal with no errors, using a diagram similar to that in Figure 1.2.\nExplicitly state the assumptions made by your protocol about the underlying\nend-to-end transport service.\nP2. Equation 1.1 gives a formula for the end-to-end delay of sending one packet\nof length L over N links of transmission rate R. Generalize this formula for\nsending P such packets back-to-back over the N links.\nP3. Consider an application that transmits data at a steady rate (for example, the\nsender generates an N-bit unit of data every k time units, where k is small and\nfixed). Also, when such an application starts, it will continue running for a\nrelatively long period of time. Answer the following questions, briefly justi-\nfying your answer:\na. Would a packet-switched network or a circuit-switched network be more\nappropriate for this application? Why?\nb. Suppose that a packet-switched network is used and the only traffic in\nthis network comes from such applications as described above. Further-\nmore, assume that the sum of the application data rates is less than the\ncapacities of each and every link. Is some form of congestion control\nneeded? Why?\nP4. Consider the circuit-switched network in Figure 1.13. Recall that there are 4\ncircuits on each link. Label the four switches A, B, C and D, going in the\nclockwise direction.\na. What is the maximum number of simultaneous connections that can be in\nprogress at any one time in this network?\nb. Suppose that all connections are between switches A and C. What is the\nmaximum number of simultaneous connections that can be in progress?\nc. Suppose we want to make four connections between switches A and C,\nand another four connections between switches B and D. Can we route\nthese calls through the four links to accommodate all eight connections?\nP5. Review the car-caravan analogy in Section 1.4. Assume a propagation speed\nof 100 km/hour.\na. Suppose the caravan travels 150 km, beginning in front of one tollbooth,\npassing through a second tollbooth, and finishing just after a third toll-\nbooth. What is the end-to-end delay?\nb. Repeat (a), now assuming that there are eight cars in the caravan instead \nof ten.\nPROBLEMS\n71\n\nP6. This elementary problem begins to explore propagation delay and transmis-\nsion delay, two central concepts in data networking. Consider two hosts, A\nand B, connected by a single link of rate R bps. Suppose that the two hosts\nare separated by m meters, and suppose the propagation speed along the link\nis s meters/sec. Host A is to send a packet of size L bits to Host B.\na. Express the propagation delay, dprop, in terms of m and s.\nb. Determine the transmission time of the packet, dtrans, in terms of L\nand R.\nc. Ignoring processing and queuing delays, obtain an expression for the end-\nto-end delay.\nd. Suppose Host A begins to transmit the packet at time t = 0. At time t = dtrans,\nwhere is the last bit of the packet?\ne. Suppose dprop is greater than dtrans. At time t = dtrans, where is the first bit of\nthe packet?\nf. Suppose dprop is less than dtrans. At time t = dtrans, where is the first bit of\nthe packet?\ng. Suppose s = 2.5 · 108, L = 120 bits, and R = 56 kbps. Find the distance m\nso that dprop equals dtrans.\nP7. In this problem, we consider sending real-time voice from Host A to Host B\nover a packet-switched network (VoIP). Host A converts analog voice to a\ndigital 64 kbps bit stream on the fly. Host A then groups the bits into 56-byte\npackets. There is one link between Hosts A and B; its transmission rate is 2\nMbps and its propagation delay is 10 msec. As soon as Host A gathers a\npacket, it sends it to Host B. As soon as Host B receives an entire packet, it\nconverts the packet’s bits to an analog signal. How much time elapses from\nthe time a bit is created (from the original analog signal at Host A) until the\nbit is decoded (as part of the analog signal at Host B)?\nP8. Suppose users share a 3 Mbps link. Also suppose each user requires \n150 kbps when transmitting, but each user transmits only 10 percent of the\ntime. (See the discussion of packet switching versus circuit switching in\nSection 1.3.)\na. When circuit switching is used, how many users can be supported?\nb. For the remainder of this problem, suppose packet switching is used. Find\nthe probability that a given user is transmitting.\nc. Suppose there are 120 users. Find the probability that at any given time,\nexactly n users are transmitting simultaneously. (Hint: Use the binomial\ndistribution.)\nd. Find the probability that there are 21 or more users transmitting\nsimultaneously.\n72\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nVideoNote\nExploring propagation\ndelay and transmission\ndelay\n\nP9. Consider the discussion in Section 1.3 of packet switching versus circuit\nswitching in which an example is provided with a 1 Mbps link. Users are\ngenerating data at a rate of 100 kbps when busy, but are busy generating\ndata only with probability p = 0.1. Suppose that the 1 Mbps link is replaced\nby a 1 Gbps link.\na. What is N, the maximum number of users that can be supported\nsimultaneously under circuit switching?\nb. Now consider packet switching and a user population of M users. Give a\nformula (in terms of p, M, N) for the probability that more than N users are\nsending data.\nP10. Consider a packet of length L which begins at end system A and travels over\nthree links to a destination end system. These three links are connected by\ntwo packet switches. Let di, si, and Ri denote the length, propagation speed,\nand the transmission rate of link i, for i = 1, 2, 3. The packet switch delays\neach packet by dproc. Assuming no queuing delays, in terms of di, si, Ri,\n(i = 1,2,3), and L, what is the total end-to-end delay for the packet? Suppose\nnow the packet is 1,500 bytes, the propagation speed on all three links is 2.5 ·\n108 m/s, the transmission rates of all three links are 2 Mbps, the packet switch\nprocessing delay is 3 msec, the length of the first link is 5,000 km, the length\nof the second link is 4,000 km, and the length of the last link is 1,000 km. For\nthese values, what is the end-to-end delay?\nP11. In the above problem, suppose R1 = R2 = R3 = R and dproc = 0. Further sup-\npose the packet switch does not store-and-forward packets but instead imme-\ndiately transmits each bit it receives before waiting for the entire packet to\narrive. What is the end-to-end delay?\nP12. A packet switch receives a packet and determines the outbound link to which\nthe packet should be forwarded. When the packet arrives, one other packet is\nhalfway done being transmitted on this outbound link and four other packets\nare waiting to be transmitted. Packets are transmitted in order of arrival.\nSuppose all packets are 1,500 bytes and the link rate is 2 Mbps. What is the\nqueuing delay for the packet? More generally, what is the queuing delay\nwhen all packets have length L, the transmission rate is R, x bits of the\ncurrently-being-transmitted packet have been transmitted, and n packets are\nalready in the queue?\nP13. (a) Suppose N packets arrive simultaneously to a link at which no packets\nare currently being transmitted or queued. Each packet is of length L\nand the link has transmission rate R. What is the average queuing delay\nfor the N packets?\n(b)Now suppose that N such packets arrive to the link every LN/R seconds.\nWhat is the average queuing delay of a packet?\nPROBLEMS\n73\n\nP14. Consider the queuing delay in a router buffer. Let I denote traffic intensity;\nthat is, I = La/R. Suppose that the queuing delay takes the form IL/R (1 – I)\nfor I < 1.\na. Provide a formula for the total delay, that is, the queuing delay plus the\ntransmission delay.\nb. Plot the total delay as a function of L/R.\nP15. Let a denote the rate of packets arriving at a link in packets/sec, and let μ\ndenote the link’s transmission rate in packets/sec. Based on the formula for\nthe total delay (i.e., the queuing delay plus the transmission delay) derived in\nthe previous problem, derive a formula for the total delay in terms of a and μ.\nP16. Consider a router buffer preceding an outbound link. In this problem, you will\nuse Little’s formula, a famous formula from queuing theory. Let N denote the\naverage number of packets in the buffer plus the packet being transmitted. Let\na denote the rate of packets arriving at the link. Let d denote the average total\ndelay (i.e., the queuing delay plus the transmission delay) experienced by a\npacket. Little’s formula is N = a · d. Suppose that on average, the buffer con-\ntains 10 packets, and the average packet queuing delay is 10 msec. The link’s\ntransmission rate is 100 packets/sec. Using Little’s formula, what is the aver-\nage packet arrival rate, assuming there is no packet loss? \nP17. a. Generalize Equation 1.2 in Section 1.4.3 for heterogeneous processing\nrates, transmission rates, and propagation delays.\nb. Repeat (a), but now also suppose that there is an average queuing delay of\ndqueue at each node.\nP18. Perform a Traceroute between source and destination on the same continent\nat three different hours of the day.\na. Find the average and standard deviation of the round-trip delays at each of\nthe three hours.\nb. Find the number of routers in the path at each of the three hours. Did the\npaths change during any of the hours?\nc. Try to identify the number of ISP networks that the Traceroute packets pass\nthrough from source to destination. Routers with similar names and/or similar\nIP addresses should be considered as part of the same ISP. In your experiments,\ndo the largest delays occur at the peering interfaces between adjacent ISPs?\nd. Repeat the above for a source and destination on different continents.\nCompare the intra-continent and inter-continent results.\nP19. (a) Visit the site www.traceroute.org and perform  traceroutes from two different\ncities in France to the same destination host in the United States. How many\nlinks are the same in the two traceroutes? Is the transatlantic link the same? \n74\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\nVideoNote\nUsing Traceroute to\ndiscover network\npaths and measure\nnetwork delay\n\n(b) Repeat (a) but this time choose one city in France and another city in\nGermany.\n(c) Pick a city in the United States, and perform traceroutes to two hosts, each\nin a different city in China. How many links are common in the two\ntraceroutes? Do the two traceroutes diverge before reaching China?\nP20. Consider the throughput example corresponding to Figure 1.20(b). Now\nsuppose that there are M client-server pairs rather than 10. Denote Rs, Rc, and\nR for the rates of the server links, client links, and network link. Assume all\nother links have abundant capacity and that there is no other traffic in the\nnetwork besides the traffic generated by the M client-server pairs. Derive a\ngeneral expression for throughput in terms of Rs, Rc, R, and M.\nP21. Consider Figure 1.19(b). Now suppose that there are M paths between the\nserver and the client. No two paths share any link. Path k (k = 1, . . ., M ) con-\nsists of N links with transmission rates Rk\n1, Rk\n2, . . ., Rk\nN. If the server can only\nuse one path to send data to the client, what is the maximum throughput that\nthe server can achieve? If the server can use all M paths to send data, what is\nthe maximum throughput that the server can achieve?\nP22. Consider Figure 1.19(b). Suppose that each link between the server and the\nclient has a packet loss probability p, and the packet loss probabilities for\nthese links are independent. What is the probability that a packet (sent by the\nserver) is successfully received by the receiver? If a packet is lost in the path\nfrom the server to the client, then the server will re-transmit the packet. On\naverage, how many times will the server re-transmit the packet in order for\nthe client to successfully receive the packet?\nP23. Consider Figure 1.19(a). Assume that we know the bottleneck link along the\npath from the server to the client is the first link with rate Rs bits/sec. Suppose\nwe send a pair of packets back to back from the server to the client, and there\nis no other traffic on this path. Assume each packet of size L bits, and both\nlinks have the same propagation delay dprop.\na. What is the packet inter-arrival time at the destination? That is, how much\ntime elapses from when the last bit of the first packet arrives until the last\nbit of the second packet arrives?\nb. Now assume that the second link is the bottleneck link (i.e., Rc < Rs). Is it\npossible that the second packet queues at the input queue of the second\nlink? Explain. Now suppose that the server sends the second packet T sec-\nonds after sending the first packet. How large must T be to ensure no\nqueuing before the second link? Explain.\nP24. Suppose you would like to urgently deliver 40 terabytes data from Boston to\nLos Angeles. You have available a 100 Mbps dedicated link for data transfer.\nWould you prefer to transmit the data via this link or instead use FedEx over-\nnight delivery? Explain. \nPROBLEMS\n75\n\nP25. Suppose two hosts, A and B, are separated by 20,000 kilometers and are\nconnected by a direct link of R = 2 Mbps. Suppose the propagation speed\nover the link is 2.5 \u0002 108 meters/sec.\na. Calculate the bandwidth-delay product, R \u0002 dprop.\nb. Consider sending a file of 800,000 bits from Host A to Host B. Suppose\nthe file is sent continuously as one large message. What is the maximum\nnumber of bits that will be in the link at any given time?\nc. Provide an interpretation of the bandwidth-delay product.\nd. What is the width (in meters) of a bit in the link? Is it longer than a foot-\nball field?\ne. Derive a general expression for the width of a bit in terms of the propaga-\ntion speed s, the transmission rate R, and the length of the link m.\nP26. Referring to problem P25, suppose we can modify R. For what value of R is\nthe width of a bit as long as the length of the link?\nP27. Consider problem P25 but now with a link of R = 1 Gbps.\na. Calculate the bandwidth-delay product, R \u0002 dprop.\nb. Consider sending a file of 800,000 bits from Host A to Host B. Suppose\nthe file is sent continuously as one big message. What is the maximum\nnumber of bits that will be in the link at any given time?\nc. What is the width (in meters) of a bit in the link?\nP28. Refer again to problem P25.\na. How long does it take to send the file, assuming it is sent continuously?\nb. Suppose now the file is broken up into 20 packets with each packet con-\ntaining 40,000 bits. Suppose that each packet is acknowledged by the\nreceiver and the transmission time of an acknowledgment packet is\nnegligible. Finally, assume that the sender cannot send a packet until the\npreceding one is acknowledged. How long does it take to send the file?\nc. Compare the results from (a) and (b).\nP29. Suppose there is a 10 Mbps microwave link between a geostationary satellite\nand its base station on Earth. Every minute the satellite takes a digital photo and\nsends it to the base station. Assume a propagation speed of 2.4 \u0002 108 meters/sec.\na. What is the propagation delay of the link?\nb. What is the bandwidth-delay product, R · dprop?\nc. Let x denote the size of the photo. What is the minimum value of x for the\nmicrowave link to be continuously transmitting?\nP30. Consider the airline travel analogy in our discussion of layering in Section\n1.5, and the addition of headers to protocol data units as they flow down\n76\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\n\nthe protocol stack. Is there an equivalent notion of header information that\nis added to passengers and baggage as they move down the airline protocol\nstack?\nP31. In modern packet-switched networks, including the Internet, the source host\nsegments long, application-layer messages (for example, an image or a music\nfile) into smaller packets and sends the packets into the network. The receiver\nthen reassembles the packets back into the original message. We refer to this\nprocess as message segmentation. Figure 1.27 illustrates the end-to-end\ntransport of a message with and without message segmentation. Consider a\nmessage that is 8 · 106 bits long that is to be sent from source to destination in\nFigure 1.27. Suppose each link in the figure is 2 Mbps. Ignore propagation,\nqueuing, and processing delays.\na. Consider sending the message from source to destination without message\nsegmentation. How long does it take to move the message from the source\nhost to the first packet switch? Keeping in mind that each switch uses\nstore-and-forward packet switching, what is the total time to move the\nmessage from source host to destination host?\nb. Now suppose that the message is segmented into 800 packets, with each\npacket being 10,000 bits long. How long does it take to move the first\npacket from source host to the first switch? When the first packet is being\nsent from the first switch to the second switch, the second packet is being\nsent from the source host to the first switch. At what time will the second\npacket be fully received at the first switch?\nc. How long does it take to move the file from source host to destination host\nwhen message segmentation is used? Compare this result with your\nanswer in part (a) and comment.\nPROBLEMS\n77\nSource\na.\nPacket switch\nPacket switch\nDestination\nMessage\nSource\nb.\nPacket switch\nPacket\nPacket switch\nDestination\nFigure 1.27 \u0002 End-to-end message transport: (a) without message\nsegmentation; (b) with message segmentation"
    },
    {
      "chunk_id": "6429f4c7-2f45-4205-aeb5-4b8d91cabe10",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Wireshark Lab",
      "original_titles": [
        "Wireshark Lab"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > Wireshark Lab",
      "start_page": 105,
      "end_page": 106,
      "token_count": 748,
      "text": "d. In addition to reducing delay, what are reasons to use message segmentation?\ne. Discuss the drawbacks of message segmentation.\nP32. Experiment with the Message Segmentation applet at the book’s Web site. Do\nthe delays in the applet correspond to the delays in the previous problem?\nHow do link propagation delays affect the overall end-to-end delay for packet\nswitching (with message segmentation) and for message switching?\nP33. Consider sending a large file of F bits from Host A to Host B. There are three\nlinks (and two switches) between A and B, and the links are uncongested (that \nis, no queuing delays). Host A segments the file into segments of S bits each and\nadds 80 bits of header to each segment, forming packets of L = 80 + S bits. Each\nlink has a transmission rate of R bps. Find the value of S that minimizes the\ndelay of moving the file from Host A to Host B. Disregard propagation delay.\nP34. Skype offers a service that allows you to make a phone call from a PC to an\nordinary phone. This means that the voice call must pass through both the \nInternet and through a telephone network. Discuss how this might be done.\nWireshark Lab\n“Tell me and I forget. Show me and I remember. Involve me and I understand.”\nChinese proverb\nOne’s understanding of network protocols can often be greatly deepened by seeing\nthem in action and by playing around with them—observing the sequence of mes-\nsages exchanged between two protocol entities, delving into the details of protocol\noperation, causing protocols to perform certain actions, and observing these actions\nand their consequences. This can be done in simulated scenarios or in a real network\nenvironment such as the Internet. The Java applets at the textbook Web site take the\nfirst approach. In the Wireshark labs, we’ll take the latter approach. You’ll run net-\nwork applications in various scenarios using a computer on your desk, at home, or\nin a lab. You’ll observe the network protocols in your computer, interacting and\nexchanging messages with protocol entities executing elsewhere in the Internet.\nThus, you and your computer will be an integral part of these live labs. You’ll\nobserve—and you’ll learn—by doing.\nThe basic tool for observing the messages exchanged between executing\nprotocol entities is called a packet sniffer. As the name suggests, a packet sniffer\npassively copies (sniffs) messages being sent from and received by your computer;\nit also displays the contents of the various protocol fields of these captured mes-\nsages. A screenshot of the Wireshark packet sniffer is shown in Figure 1.28. Wire-\nshark is a free packet sniffer that runs on Windows, Linux/Unix, and Mac\n78\nCHAPTER 1\n•\nCOMPUTER NETWORKS AND THE INTERNET\n\nWIRESHARK LAB\n79\nCommand\nmenus\nListing of\ncaptured\npackets\nDetails of\nselected\npacket\nheader\nPacket\ncontents in\nhexadecimal\nand ASCII\nFigure 1.28 \u0002 A Wireshark screen shot (Wireshark screenshot reprinted\nby permission of the Wireshark Foundation.)\ncomputers. Throughout the textbook, you will find Wireshark labs that allow you to\nexplore a number of the protocols studied in the chapter. In this first Wireshark lab,\nyou’ll obtain and install a copy of Wireshark, access a Web site, and capture and\nexamine the protocol messages being exchanged between your Web browser and the\nWeb server.\nYou can find full details about this first Wireshark lab (including instructions\nabout how to obtain and install Wireshark) at the Web site http://www.awl.com/\nkurose-ross."
    },
    {
      "chunk_id": "f212cd74-147b-4bdf-9002-b41ccaae152d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Interview: Leonard Kleinrock",
      "original_titles": [
        "Interview: Leonard Kleinrock"
      ],
      "path": "Chapter 1 Computer Networks and the Internet > Interview: Leonard Kleinrock",
      "start_page": 107,
      "end_page": 109,
      "token_count": 1431,
      "text": "80\nLeonard Kleinrock\nLeonard Kleinrock is a professor of computer science at the University\nof California, Los Angeles. In 1969, his computer at UCLA became\nthe first node of the Internet. His creation of packet-switching princi-\nples in 1961 became the technology behind the Internet. He\nreceived his B.E.E. from the City College of New York (CCNY) and\nhis masters and PhD in electrical engineering from MIT.\nAN INTERVIEW WITH...\nWhat made you decide to specialize in networking/Internet technology?\nAs a PhD student at MIT in 1959, I looked around and found that most of my classmates\nwere doing research in the area of information theory and coding theory. At MIT, there was\nthe great researcher, Claude Shannon, who had launched these fields and had solved most of\nthe important problems already. The research problems that were left were hard and of less-\ner consequence. So I decided to launch out in a new area that no one else had yet conceived\nof. Remember that at MIT I was surrounded by lots of computers, and it was clear to me\nthat soon these machines would need to communicate with each other. At the time, there\nwas no effective way for them to do so, so I decided to develop the technology that would\npermit efficient and reliable data networks to be created.\nWhat was your first job in the computer industry? What did it entail?\nI went to the evening session at CCNY from 1951 to 1957 for my bachelor’s degree in\nelectrical engineering. During the day, I worked first as a technician and then as an engi-\nneer at a small, industrial electronics firm called Photobell. While there, I introduced\ndigital technology to their product line. Essentially, we were using photoelectric devices\nto detect the presence of certain items (boxes, people, etc.) and the use of a circuit\nknown then as a bistable multivibrator was just the kind of technology we needed to\nbring digital processing into this field of detection. These circuits happen to be the build-\ning blocks for computers, and have come to be known as flip-flops or switches in today’s\nvernacular.\nWhat was going through your mind when you sent the first host-to-host message (from\nUCLA to the Stanford Research Institute)?\nFrankly, we had no idea of the importance of that event. We had not prepared a special mes-\nsage of historic significance, as did so many inventors of the past (Samuel Morse with “What\nhath God wrought.” or Alexander Graham Bell with “Watson, come here! I want you.” or\nNeal Amstrong with “That’s one small step for a man, one giant leap for mankind.”) Those\nguys were smart! They understood media and public relations. All we wanted to do was to\nlogin to the SRI computer. So we typed the “L”, which was correctly received, we typed the\n“o” which was received, and then we typed the “g” which caused the SRI host computer to\n\ncrash! So, it turned out that our message was the shortest and perhaps the most prophetic\nmessage ever, namely “Lo!” as in “Lo and behold!”\nEarlier that year, I was quoted in a UCLA press release saying that once the network was\nup and running, it would be possible to gain access to computer utilities from our homes and\noffices as easily as we gain access to electricity and telephone connectivity. So my vision at\nthat time was that the Internet would be ubiquitous, always on, always available, anyone with\nany device could connect from any location, and it would be invisible. However, I never\nanticipated that my 99-year-old mother would use the Internet—and indeed she did!\nWhat is your vision for the future of networking?\nThe easy part of the vision is to predict the infrastructure itself. I anticipate that we see con-\nsiderable deployment of nomadic computing, mobile devices, and smart spaces. Indeed, the\navailability of lightweight, inexpensive, high-performance, portable computing, and commu-\nnication devices (plus the ubiquity of the Internet) has enabled us to become nomads.\nNomadic computing refers to the technology that enables end users who travel from place to\nplace to gain access to Internet services in a transparent fashion, no matter where they travel\nand no matter what device they carry or gain access to. The harder part of the vision is to\npredict the applications and services, which have consistently surprised us in dramatic ways\n(email, search technologies, the world-wide-web, blogs, social networks, user generation, and\nsharing of music, photos, and videos, etc.). We are on the verge of a new class of surprising\nand innovative mobile applications delivered to our hand-held devices. \nThe next step will enable us to move out from the netherworld of cyberspace to the\nphysical world of smart spaces. Our environments (desks, walls, vehicles, watches, belts, and\nso on) will come alive with technology, through actuators, sensors, logic, processing, storage,\ncameras, microphones, speakers, displays, and communication. This embedded technology\nwill allow our environment to provide the IP services we want. When I walk into a room, the\nroom will know I entered. I will be able to communicate with my environment naturally, as\nin spoken English; my requests will generate replies that present Web pages to me from wall\ndisplays, through my eyeglasses, as speech, holograms, and so forth.\nLooking a bit further out, I see a networking future that includes the following addi-\ntional key components. I see intelligent software agents deployed across the network\nwhose function it is to mine data, act on that data, observe trends, and carry out tasks\ndynamically and adaptively. I see considerably more network traffic generated not so much\nby humans, but by these embedded devices and these intelligent software agents. I see\nlarge collections of self-organizing systems controlling this vast, fast network. I see huge\namounts of information flashing across this network instantaneously with this information\nundergoing enormous processing and filtering. The Internet will essentially be a pervasive\nglobal nervous system. I see all these things and more as we move headlong through the\ntwenty-first century.\n81\n\nWhat people have inspired you professionally?\nBy far, it was Claude Shannon from MIT, a brilliant researcher who had the ability to relate\nhis mathematical ideas to the physical world in highly intuitive ways. He was on my PhD\nthesis committee.\nDo you have any advice for students entering the networking/Internet field?\nThe Internet and all that it enables is a vast new frontier, full of amazing challenges. There\nis room for great innovation. Don’t be constrained by today’s technology. Reach out and\nimagine what could be and then make it happen.\n82"
    },
    {
      "chunk_id": "7793f5cd-6815-4fb1-b6a9-45998377f3cc",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Chapter 2 Application Layer",
      "original_titles": [
        "Chapter 2 Application Layer"
      ],
      "path": "Chapter 2 Application Layer",
      "start_page": 110,
      "end_page": 110,
      "token_count": 325,
      "text": "CHAPTER 2\nApplication\nLayer\n83\nNetwork applications are the raisons d’être of a computer network—if we couldn’t\nconceive of any useful applications, there wouldn’t be any need for networking proto-\ncols that support these applications. Since the Internet’s inception, numerous useful and\nentertaining applications have indeed been created. These applications have been the\ndriving force behind the Internet’s success, motivating people in homes, schools, gov-\nernments, and businesses to make the Internet an integral part of their daily activities.\nInternet applications include the classic text-based applications that became\npopular in the 1970s and 1980s: text email, remote access to computers, file trans-\nfers, and newsgroups. They include the killer application of the mid-1990s, the\nWorld Wide Web, encompassing Web surfing, search, and electronic commerce.\nThey include instant messaging and P2P file sharing, the two killer applications\nintroduced at the end of the millennium. Since 2000, we have seen an explosion of\npopular voice and video applications, including: voice-over-IP (VoIP) and video\nconferencing over IP such as Skype; user-generated video distribution such as\nYouTube; and movies on demand such as Netflix. During this same period we have\nalso seen the immergence of highly engaging multi-player online games, including\nSecond Life and World of Warcraft. And most recently, we have seen the emergence\nof a new generation of social networking applications, such as Facebook and Twitter,\nwhich have created engaging human networks on top of the Internet’s network of\nrouters and communication links. Clearly, there has been no slowing down of new"
    },
    {
      "chunk_id": "ecfcbf57-c3da-4e5c-a039-56e2524b0d4a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.1 Principles of Network Applications",
      "original_titles": [
        "2.1 Principles of Network Applications"
      ],
      "path": "Chapter 2 Application Layer > 2.1 Principles of Network Applications",
      "start_page": 111,
      "end_page": 112,
      "token_count": 713,
      "text": "and exciting Internet applications. Perhaps some of the readers of this text will cre-\nate the next generation of killer Internet applications!\nIn this chapter we study the conceptual and implementation aspects of network\napplications. We begin by defining key application-layer concepts, including network\nservices required by applications, clients and servers, processes, and transport-layer\ninterfaces. We examine several network applications in detail, including the Web,\ne-mail, DNS, and peer-to-peer (P2P) file distribution (Chapter 8 focuses on multime-\ndia applications, including streaming video and VoIP). We then cover network applica-\ntion development, over both TCP and UDP. In particular, we study the socket API\nand walk through some simple client-server applications in Python. We also provide\nseveral fun and interesting socket programming assignments at the end of the chapter.\nThe application layer is a particularly good place to start our study of protocols.\nIt’s familiar ground. We’re acquainted with many of the applications that rely on the\nprotocols we’ll study. It will give us a good feel for what protocols are all about and\nwill introduce us to many of the same issues that we’ll see again when we study trans-\nport, network, and link layer protocols.\n2.1 Principles of Network Applications\nSuppose you have an idea for a new network application. Perhaps this application\nwill be a great service to humanity, or will please your professor, or will bring you\ngreat wealth, or will simply be fun to develop. Whatever the motivation may be, let’s\nnow examine how you transform the idea into a real-world network application.\nAt the core of network application development is writing programs that run on\ndifferent end systems and communicate with each other over the network. For\nexample, in the Web application there are two distinct programs that communicate\nwith each other: the browser program running in the user’s host (desktop, laptop,\ntablet, smartphone, and so on); and the Web server program running in the Web\nserver host. As another example, in a P2P file-sharing system there is a program in\neach host that participates in the file-sharing community. In this case, the programs\nin the various hosts may be similar or identical.\nThus, when developing your new application, you need to write software that\nwill run on multiple end systems. This software could be written, for example, in C,\nJava, or Python. Importantly, you do not need to write software that runs on network-\ncore devices, such as routers or link-layer switches. Even if you wanted to write\napplication software for these network-core devices, you wouldn’t be able to do so.\nAs we learned in Chapter 1, and as shown earlier in Figure 1.24, network-core\ndevices do not function at the application layer but instead function at lower layers—\nspecifically at the network layer and below. This basic design—namely, confining\napplication software to the end systems—as shown in Figure 2.1, has facilitated the\nrapid development and deployment of a vast array of network applications.\n84\nCHAPTER 2\n•\nAPPLICATION LAYER\n\n2.1\n•\nPRINCIPLES OF NETWORK APPLICATIONS\n85\nMobile Network\nTransport\nNetwork\nLink\nPhysical\nApplication\nNational or\nGlobal ISP\nLocal or\nRegional ISP\nEnterprise Network\nHome Network\nTransport\nNetwork\nLink\nApplication\nPhysical\nTransport\nNetwork\nLink\nPhysical\nApplication\nFigure 2.1 \u0002 Communication for a network application takes place\nbetween end systems at the application layer"
    },
    {
      "chunk_id": "2c3f50c5-15df-4b4c-8894-57a86d1f199d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.1.1 Network Application Architectures",
      "original_titles": [
        "2.1.1 Network Application Architectures"
      ],
      "path": "Chapter 2 Application Layer > 2.1 Principles of Network Applications > 2.1.1 Network Application Architectures",
      "start_page": 113,
      "end_page": 114,
      "token_count": 959,
      "text": "2.1.1 Network Application Architectures\nBefore diving into software coding, you should have a broad architectural plan for\nyour application. Keep in mind that an application’s architecture is distinctly differ-\nent from the network architecture (e.g., the five-layer Internet architecture discussed\nin Chapter 1). From the application developer’s perspective, the network architec-\nture is fixed and provides a specific set of services to applications. The application\narchitecture, on the other hand, is designed by the application developer and dic-\ntates how the application is structured over the various end systems. In choosing the\napplication architecture, an application developer will likely draw on one of the two\npredominant architectural paradigms used in modern network applications: the\nclient-server architecture or the peer-to-peer (P2P) architecture\nIn a client-server architecture, there is an always-on host, called the server,\nwhich services requests from many other hosts, called clients. A classic example is the\nWeb application for which an always-on Web server services requests from browsers\nrunning on client hosts. When a Web server receives a request for an object from a\nclient host, it responds by sending the requested object to the client host. Note that\nwith the client-server architecture, clients do not directly communicate with each\nother; for example, in the Web application, two browsers do not directly communi-\ncate. Another characteristic of the client-server architecture is that the server has a\nfixed, well-known address, called an IP address (which we’ll discuss soon). Because\nthe server has a fixed, well-known address, and because the server is always on, a\nclient can always contact the server by sending a packet to the server’s IP address.\nSome of the better-known applications with a client-server architecture include the\nWeb, FTP, Telnet, and e-mail. The client-server architecture is shown in Figure 2.2(a).\nOften in a client-server application, a single-server host is incapable of keeping up\nwith all the requests from clients. For example, a popular social-networking site can\nquickly become overwhelmed if it has only one server handling all of its requests. For\nthis reason, a data center, housing a large number of hosts, is often used to create a\npowerful virtual server. The most popular Internet services—such as search engines\n(e.g., Google and Bing), Internet commerce (e.g., Amazon and e-Bay), Web-based\nemail (e.g., Gmail and Yahoo Mail), social networking (e.g., Facebook and Twitter)—\nemploy one or more data centers. As discussed in Section 1.3.3, Google has 30 to 50\ndata centers distributed around the world, which collectively handle search, YouTube,\nGmail, and other services. A data center can have hundreds of thousands of servers,\nwhich must be powered and maintained. Additionally, the service providers must pay\nrecurring interconnection and bandwidth costs for sending data from their data centers.\nIn a P2P architecture, there is minimal (or no) reliance on dedicated servers in\ndata centers. Instead the application exploits direct communication between pairs of\nintermittently connected hosts, called peers. The peers are not owned by the service\nprovider, but are instead desktops and laptops controlled by users, with most of the\npeers residing in homes, universities, and offices. Because the peers communicate\nwithout passing through a dedicated server, the architecture is called peer-to-peer.\nMany of today’s most popular and traffic-intensive applications are based on P2P\narchitectures. These applications include file sharing (e.g., BitTorrent), peer-assisted\n86\nCHAPTER 2\n•\nAPPLICATION LAYER\n\ndownload acceleration (e.g., Xunlei), Internet Telephony (e.g., Skype), and IPTV (e.g.,\nKankan and PPstream). The P2P architecture is illustrated in Figure 2.2(b). We men-\ntion that some applications have hybrid architectures, combining both client-server\nand P2P elements. For example, for many instant messaging applications, servers are\nused to track the IP addresses of users, but user-to-user messages are sent directly\nbetween user hosts (without passing through intermediate servers).\nOne of the most compelling features of P2P architectures is their self-scalability.\nFor example, in a P2P file-sharing application, although each peer generates\nworkload by requesting files, each peer also adds service capacity to the system\nby distributing files to other peers. P2P architectures are also cost effective, since\nthey normally don’t require significant server infrastructure and server bandwidth\n(in contrast with clients-server designs with datacenters). However, future P2P\napplications face three major challenges:\n1. ISP Friendly. Most residential ISPs (including DSL and cable ISPs) have been\ndimensioned for “asymmetrical” bandwidth usage, that is, for much more\n2.1\n•\nPRINCIPLES OF NETWORK APPLICATIONS\n87\nFigure 2.2 \u0002 (a) Client-server architecture; (b) P2P architecture\na. Client-server architecture\nb. Peer-to-peer architecture"
    },
    {
      "chunk_id": "6154d27c-3ee8-48a2-a1fe-8f9dfdc9cb34",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.1.2 Processes Communicating",
      "original_titles": [
        "2.1.2 Processes Communicating"
      ],
      "path": "Chapter 2 Application Layer > 2.1 Principles of Network Applications > 2.1.2 Processes Communicating",
      "start_page": 115,
      "end_page": 117,
      "token_count": 1713,
      "text": "downstream than upstream traffic. But P2P video streaming and file distribu-\ntion applications shift upstream traffic from servers to residential ISPs, thereby\nputting significant stress on the ISPs. Future P2P applications need to be\ndesigned so that they are friendly to ISPs [Xie 2008].\n2. Security. Because of their highly distributed and open nature, P2P applications\ncan be a challenge to secure [Doucer 2002; Yu 2006; Liang 2006; Naoumov\n2006; Dhungel 2008; LeBlond 2011].\n3. Incentives. The success of future P2P applications also depends on convincing\nusers to volunteer bandwidth, storage, and computation resources to the appli-\ncations, which is the challenge of incentive design [Feldman 2005; Piatek\n2008; Aperjis 2008; Liu 2010].\n2.1.2 Processes Communicating\nBefore building your network application, you also need a basic understanding of\nhow the programs, running in multiple end systems, communicate with each other.\nIn the jargon of operating systems, it is not actually programs but processes that\ncommunicate. A process can be thought of as a program that is running within an\nend system. When processes are running on the same end system, they can com-\nmunicate with each other with interprocess communication, using rules that are\ngoverned by the end system’s operating system. But in this book we are not par-\nticularly interested in how processes in the same host communicate, but instead in\nhow processes running on different hosts (with potentially different operating sys-\ntems) communicate.\nProcesses on two different end systems communicate with each other by exchang-\ning messages across the computer network. A sending process creates and sends mes-\nsages into the network; a receiving process receives these messages and possibly\nresponds by sending messages back. Figure 2.1 illustrates that processes communicat-\ning with each other reside in the application layer of the five-layer protocol stack.\nClient and Server Processes\nA network application consists of pairs of processes that send messages to each\nother over a network. For example, in the Web application a client browser\nprocess exchanges messages with a Web server process. In a P2P file-sharing sys-\ntem, a file is transferred from a process in one peer to a process in another peer.\nFor each pair of communicating processes, we typically label one of the two\nprocesses as the client and the other process as the server. With the Web, a\nbrowser is a client process and a Web server is a server process. With P2P file\nsharing, the peer that is downloading the file is labeled as the client, and the peer\nthat is uploading the file is labeled as the server.\nYou may have observed that in some applications, such as in P2P file sharing, a\nprocess can be both a client and a server. Indeed, a process in a P2P file-sharing sys-\ntem can both upload and download files. Nevertheless, in the context of any given\n88\nCHAPTER 2\n•\nAPPLICATION LAYER\n\ncommunication session between a pair of processes, we can still label one process\nas the client and the other process as the server. We define the client and server\nprocesses as follows:\nIn the context of a communication session between a pair of processes, the\nprocess that initiates the communication (that is, initially contacts the other\nprocess at the beginning of the session) is labeled as the client. The process\nthat waits to be contacted to begin the session is the server.\nIn the Web, a browser process initializes contact with a Web server process;\nhence the browser process is the client and the Web server process is the server. In\nP2P file sharing, when Peer A asks Peer B to send a specific file, Peer A is the client\nand Peer B is the server in the context of this specific communication session. When\nthere’s no confusion, we’ll sometimes also use the terminology “client side and\nserver side of an application.” At the end of this chapter, we’ll step through simple\ncode for both the client and server sides of network applications.\nThe Interface Between the Process and the Computer Network\nAs noted above, most applications consist of pairs of communicating processes,\nwith the two processes in each pair sending messages to each other. Any message\nsent from one process to another must go through the underlying network. A process\nsends messages into, and receives messages from, the network through a software\ninterface called a socket. Let’s consider an analogy to help us understand processes\nand sockets. A process is analogous to a house and its socket is analogous to its door.\nWhen a process wants to send a message to another process on another host, it\nshoves the message out its door (socket). This sending process assumes that there is\na transportation infrastructure on the other side of its door that will transport the\nmessage to the door of the destination process. Once the message arrives at the des-\ntination host, the message passes through the receiving process’s door (socket), and\nthe receiving process then acts on the message\nFigure 2.3 illustrates socket communication between two processes that com-\nmunicate over the Internet. (Figure 2.3 assumes that the underlying transport pro-\ntocol used by the processes is the Internet’s TCP protocol.) As shown in this\nfigure, a socket is the interface between the application layer and the transport\nlayer within a host. It is also referred to as the Application Programming Inter-\nface (API) between the application and the network, since the socket is the pro-\ngramming interface with which network applications are built. The application\ndeveloper has control of everything on the application-layer side of the socket but\nhas little control of the transport-layer side of the socket. The only control that the\napplication developer has on the transport-layer side is (1) the choice of transport\nprotocol and (2) perhaps the ability to fix a few transport-layer parameters such as\nmaximum buffer and maximum segment sizes (to be covered in Chapter 3). Once\nthe application developer chooses a transport protocol (if a choice is available),\n2.1\n•\nPRINCIPLES OF NETWORK APPLICATIONS\n89\n\nthe application is built using the transport-layer services provided by that proto-\ncol. We’ll explore sockets in some detail in Section 2.7.\nAddressing Processes\nIn order to send postal mail to a particular destination, the destination needs to have\nan address. Similarly, in order for a process running on one host to send packets to a\nprocess running on another host, the receiving process needs to have an address. \nTo identify the receiving process, two pieces of information need to be specified:\n(1) the address of the host and (2) an identifier that specifies the receiving process\nin the destination host.\nIn the Internet, the host is identified by its IP address. We’ll discuss IP\naddresses in great detail in Chapter 4. For now, all we need to know is that an IP\naddress is a 32-bit quantity that we can think of as uniquely identifying the host.\nIn addition to knowing the address of the host to which a message is destined, the\nsending process must also identify the receiving process (more specifically, the\nreceiving socket) running in the host. This information is needed because in gen-\neral a host could be running many network applications. A destination port num-\nber serves this purpose. Popular applications have been assigned specific port\nnumbers. For example, a Web server is identified by port number 80. A mail\nserver process (using the SMTP protocol) is identified by port number 25. A list\nof well-known port numbers for all Internet standard protocols can be found at\nhttp://www.iana.org. We’ll examine port numbers in detail in Chapter 3.\n90\nCHAPTER 2\n•\nAPPLICATION LAYER\nProcess\nHost or\nserver\nHost or\nserver\nControlled\nby application\ndeveloper\nControlled\nby application\ndeveloper\nProcess\nTCP with\nbuffers,\nvariables\nInternet\nControlled\nby operating\nsystem\nControlled\nby operating\nsystem\nTCP with\nbuffers,\nvariables\nSocket\nSocket\nFigure 2.3 \u0002 Application processes, sockets, and underlying transport protocol"
    },
    {
      "chunk_id": "3429daf0-d06c-48c6-a9f2-2ae4301f1069",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.1.3 Transport Services Available to Applications",
      "original_titles": [
        "2.1.3 Transport Services Available to Applications"
      ],
      "path": "Chapter 2 Application Layer > 2.1 Principles of Network Applications > 2.1.3 Transport Services Available to Applications",
      "start_page": 118,
      "end_page": 119,
      "token_count": 1207,
      "text": "2.1.3 Transport Services Available to Applications\nRecall that a socket is the interface between the application process and the \ntransport-layer protocol. The application at the sending side pushes messages\nthrough the socket. At the other side of the socket, the transport-layer protocol\nhas the responsibility of getting the messages to the socket of the receiving\nprocess.\nMany networks, including the Internet, provide more than one transport-layer\nprotocol. When you develop an application, you must choose one of the available\ntransport-layer protocols. How do you make this choice? Most likely, you would\nstudy the services provided by the available transport-layer protocols, and then pick\nthe protocol with the services that best match your application’s needs. The situa-\ntion is similar to choosing either train or airplane transport for travel between two\ncities. You have to choose one or the other, and each transportation mode offers dif-\nferent services. (For example, the train offers downtown pickup and drop-off,\nwhereas the plane offers shorter travel time.)\nWhat are the services that a transport-layer protocol can offer to applications\ninvoking it? We can broadly classify the possible services along four dimensions:\nreliable data transfer, throughput, timing, and security.\nReliable Data Transfer\nAs discussed in Chapter 1, packets can get lost within a computer network. For\nexample, a packet can overflow a buffer in a router, or can be discarded by a host\nor router after having some of its bits corrupted. For many applications—such as\nelectronic mail, file transfer, remote host access, Web document transfers, and\nfinancial applications—data loss can have devastating consequences (in the latter\ncase, for either the bank or the customer!). Thus, to support these applications,\nsomething has to be done to guarantee that the data sent by one end of the appli-\ncation is delivered correctly and completely to the other end of the application. If\na protocol provides such a guaranteed data delivery service, it is said to provide\nreliable data transfer. One important service that a transport-layer protocol can\npotentially provide to an application is process-to-process reliable data transfer.\nWhen a transport protocol provides this service, the sending process can just pass\nits data into the socket and know with complete confidence that the data will\narrive without errors at the receiving process.\nWhen a transport-layer protocol doesn’t provide reliable data transfer, some of\nthe data sent by the sending process may never arrive at the receiving process. This\nmay be acceptable for loss-tolerant applications, most notably multimedia applica-\ntions such as conversational audio/video that can tolerate some amount of data loss.\nIn these multimedia applications, lost data might result in a small glitch in the\naudio/video—not a crucial impairment.\n2.1\n•\nPRINCIPLES OF NETWORK APPLICATIONS\n91\n\nThroughput\nIn Chapter 1 we introduced the concept of available throughput, which, in the con-\ntext of a communication session between two processes along a network path, is\nthe rate at which the sending process can deliver bits to the receiving process.\nBecause other sessions will be sharing the bandwidth along the network path, and\nbecause these other sessions will be coming and going, the available throughput\ncan fluctuate with time. These observations lead to another natural service that a\ntransport-layer protocol could provide, namely, guaranteed available throughput\nat some specified rate. With such a service, the application could request a guar-\nanteed throughput of r bits/sec, and the transport protocol would then ensure that\nthe available throughput is always at least r bits/sec. Such a guaranteed through-\nput service would appeal to many applications. For example, if an Internet teleph-\nony application encodes voice at 32 kbps, it needs to send data into the network\nand have data delivered to the receiving application at this rate. If the transport\nprotocol cannot provide this throughput, the application would need to encode at\na lower rate (and receive enough throughput to sustain this lower coding rate) or\nmay have to give up, since receiving, say, half of the needed throughput is of little\nor no use to this Internet telephony application. Applications that have throughput\nrequirements are said to be bandwidth-sensitive applications. Many current\nmultimedia applications are bandwidth sensitive, although some multimedia\napplications may use adaptive coding techniques to encode digitized voice or\nvideo at a rate that matches the currently available throughput.\nWhile bandwidth-sensitive applications have specific throughput require-\nments, elastic applications can make use of as much, or as little, throughput \nas happens to be available. Electronic mail, file transfer, and Web transfers are\nall elastic applications. Of course, the more throughput, the better. There’s\nan adage that says that one cannot be too rich, too thin, or have too much\nthroughput!\nTiming\nA transport-layer protocol can also provide timing guarantees. As with throughput\nguarantees, timing guarantees can come in many shapes and forms. An example\nguarantee might be that every bit that the sender pumps into the socket arrives at the\nreceiver’s socket no more than 100 msec later. Such a service would be appealing to\ninteractive real-time applications, such as Internet telephony, virtual environments,\nteleconferencing, and multiplayer games, all of which require tight timing con-\nstraints on data delivery in order to be effective. (See Chapter 7, [Gauthier 1999;\nRamjee 1994].) Long delays in Internet telephony, for example, tend to result in\nunnatural pauses in the conversation; in a multiplayer game or virtual interactive\nenvironment, a long delay between taking an action and seeing the response from\nthe environment (for example, from another player at the end of an end-to-end con-\nnection) makes the application feel less realistic. For non-real-time applications,\n92\nCHAPTER 2\n•\nAPPLICATION LAYER"
    },
    {
      "chunk_id": "a7ea5b42-45cf-402b-a28b-aeb5a7b51f3b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.1.4 Transport Services Provided by the Internet",
      "original_titles": [
        "2.1.4 Transport Services Provided by the Internet"
      ],
      "path": "Chapter 2 Application Layer > 2.1 Principles of Network Applications > 2.1.4 Transport Services Provided by the Internet",
      "start_page": 120,
      "end_page": 122,
      "token_count": 1582,
      "text": "lower delay is always preferable to higher delay, but no tight constraint is placed on\nthe end-to-end delays.\nSecurity\nFinally, a transport protocol can provide an application with one or more security\nservices. For example, in the sending host, a transport protocol can encrypt all data\ntransmitted by the sending process, and in the receiving host, the transport-layer\nprotocol can decrypt the data before delivering the data to the receiving process.\nSuch a service would provide confidentiality between the two processes, even if the\ndata is somehow observed between sending and receiving processes. A transport\nprotocol can also provide other security services in addition to confidentiality,\nincluding data integrity and end-point authentication, topics that we’ll cover in\ndetail in Chapter 8.\n2.1.4 Transport Services Provided by the Internet\nUp until this point, we have been considering transport services that a computer\nnetwork could provide in general. Let’s now get more specific and examine the\ntype of transport services provided by the Internet. The Internet (and, more gen-\nerally, TCP/IP networks) makes two transport protocols available to applications,\nUDP and TCP. When you (as an application developer) create a new network\napplication for the Internet, one of the first decisions you have to make is\nwhether to use UDP or TCP. Each of these protocols offers a different set of serv-\nices to the invoking applications. Figure 2.4 shows the service requirements for\nsome selected applications.\n2.1\n•\nPRINCIPLES OF NETWORK APPLICATIONS\n93\nApplication\nData Loss\nThroughput\nTime-Sensitive\nFile transfer/download\nNo loss\nElastic\nNo\nE-mail\nNo loss\nElastic\nNo\nWeb documents\nNo loss\nElastic (few kbps)\nNo\nFigure 2.4 \u0002 Requirements of selected network applications\nInternet telephony/\nVideo conferencing\nLoss-tolerant\nAudio: few kbps–1Mbps\nVideo: 10 kbps–5 Mbps\nYes: 100s of msec\nStreaming stored \nLoss-tolerant\nSame as above\nYes: few seconds\naudio/video\nInteractive games\nLoss-tolerant\nFew kbps–10 kbps\nYes: 100s of msec\nInstant messaging\nNo loss\nElastic\nYes and no\n\nTCP Services\nThe TCP service model includes a connection-oriented service and a reliable data\ntransfer service. When an application invokes TCP as its transport protocol, the\napplication receives both of these services from TCP.\n•\nConnection-oriented service. TCP has the client and server exchange transport-\nlayer control information with each other before the application-level messages\nbegin to flow. This so-called handshaking procedure alerts the client and server,\nallowing them to prepare for an onslaught of packets. After the handshaking phase,\na TCP connection is said to exist between the sockets of the two processes. The\nconnection is a full-duplex connection in that the two processes can send messages\nto each other over the connection at the same time. When the application finishes\nsending messages, it must tear down the connection. In Chapter 3 we’ll discuss\nconnection-oriented service in detail and examine how it is implemented.\n94\nCHAPTER 2\n•\nAPPLICATION LAYER\nSECURING TCP\nNeither TCP nor UDP provide any encryption—the data that the sending process pass-\nes into its socket is the same data that travels over the network to the destination\nprocess. So, for example, if the sending process sends a password in cleartext (i.e.,\nunencrypted) into its socket, the cleartext password will travel over all the links between\nsender and receiver, potentially getting sniffed and discovered at any of the intervening\nlinks. Because privacy and other security issues have become critical for many applica-\ntions, the Internet community has developed an enhancement for TCP, called Secure\nSockets Layer (SSL). TCP-enhanced-with-SSL not only does everything that traditional\nTCP does but also provides critical process-to-process security services, including\nencryption, data integrity, and end-point authentication. We emphasize that SSL is not\na third Internet transport protocol, on the same level as TCP and UDP, but instead is an\nenhancement of TCP, with the enhancements being implemented in the application\nlayer. In particular, if an application wants to use the services of SSL, it needs to\ninclude SSL code (existing, highly optimized libraries and classes) in both the client and\nserver sides of the application. SSL has its own socket API that is similar to the tradition-\nal TCP socket API. When an application uses SSL, the sending process passes cleartext\ndata to the SSL socket; SSL in the sending host then encrypts the data and passes the\nencrypted data to the TCP socket. The encrypted data travels over the Internet to the\nTCP socket in the receiving process. The receiving socket passes the encrypted data to\nSSL, which decrypts the data. Finally, SSL passes the cleartext data through its SSL\nsocket to the receiving process. We’ll cover SSL in some detail in Chapter 8.\nFOCUS ON SECURITY\n\n•\nReliable data transfer service. The communicating processes can rely on TCP\nto deliver all data sent without error and in the proper order. When one side of\nthe application passes a stream of bytes into a socket, it can count on TCP to\ndeliver the same stream of bytes to the receiving socket, with no missing or\nduplicate bytes.\nTCP also includes a congestion-control mechanism, a service for the general\nwelfare of the Internet rather than for the direct benefit of the communicating\nprocesses. The TCP congestion-control mechanism throttles a sending process (client\nor server) when the network is congested between sender and receiver. As we will\nsee in Chapter 3, TCP congestion control also attempts to limit each TCP connection\nto its fair share of network bandwidth.\nUDP Services\nUDP is a no-frills, lightweight transport protocol, providing minimal services. UDP\nis connectionless, so there is no handshaking before the two processes start to\ncommunicate. UDP provides an unreliable data transfer service—that is, when a process\nsends a message into a UDP socket, UDP provides no guarantee that the message\nwill ever reach the receiving process. Furthermore, messages that do arrive at the\nreceiving process may arrive out of order.\nUDP does not include a congestion-control mechanism, so the sending side of\nUDP can pump data into the layer below (the network layer) at any rate it pleases.\n(Note, however, that the actual end-to-end throughput may be less than this rate due\nto the limited transmission capacity of intervening links or due to congestion). \nServices Not Provided by Internet Transport Protocols\nWe have organized transport protocol services along four dimensions: reliable data\ntransfer, throughput, timing, and security. Which of these services are provided by\nTCP and UDP? We have already noted that TCP provides reliable end-to-end data\ntransfer. And we also know that TCP can be easily enhanced at the application layer\nwith SSL to provide security services. But in our brief description of TCP and UDP,\nconspicuously missing was any mention of throughput or timing guarantees—serv-\nices not provided by today’s Internet transport protocols. Does this mean that time-\nsensitive applications such as Internet telephony cannot run in today’s Internet? The\nanswer is clearly no—the Internet has been hosting time-sensitive applications for\nmany years. These applications often work fairly well because they have been\ndesigned to cope, to the greatest extent possible, with this lack of guarantee. We’ll\ninvestigate several of these design tricks in Chapter 7. Nevertheless, clever design\nhas its limitations when delay is excessive, or the end-to-end throughput is limited.\nIn summary, today’s Internet can often provide satisfactory service to time-sensitive\napplications, but it cannot provide any timing or throughput guarantees.\n2.1\n•\nPRINCIPLES OF NETWORK APPLICATIONS\n95"
    },
    {
      "chunk_id": "50a5c5ed-9f50-4ba4-bb11-f8d71a5a577d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.1.5 Application-Layer Protocols",
      "original_titles": [
        "2.1.5 Application-Layer Protocols"
      ],
      "path": "Chapter 2 Application Layer > 2.1 Principles of Network Applications > 2.1.5 Application-Layer Protocols",
      "start_page": 123,
      "end_page": 123,
      "token_count": 401,
      "text": "Figure 2.5 indicates the transport protocols used by some popular Internet\napplications. We see that e-mail, remote terminal access, the Web, and file trans-\nfer all use TCP. These applications have chosen TCP primarily because TCP pro-\nvides  reliable data transfer, guaranteeing that all data will eventually get to its\ndestination. Because Internet telephony applications (such as Skype) can often\ntolerate some loss but require a minimal rate to be effective, developers of Inter-\nnet telephony applications usually prefer to run their applications over UDP,\nthereby circumventing TCP’s congestion control mechanism and packet over-\nheads. But because many firewalls are configured to block (most types of) UDP\ntraffic, Internet telephony applications often are designed to use TCP as a backup\nif UDP communication fails.\n2.1.5 Application-Layer Protocols\nWe have just learned that network processes communicate with each other by send-\ning messages into sockets. But how are these messages structured? What are the\nmeanings of the various fields in the messages? When do the processes send the mes-\nsages? These questions bring us into the realm of application-layer protocols. An\napplication-layer protocol defines how an application’s processes, running on dif-\nferent end systems, pass messages to each other. In particular, an application-layer\nprotocol defines:\n•\nThe types of messages exchanged, for example, request messages and response\nmessages\n•\nThe syntax of the various message types, such as the fields in the message and\nhow the fields are delineated\n96\nCHAPTER 2\n•\nAPPLICATION LAYER\nApplication\nApplication-Layer Protocol\nUnderlying Transport Protocol\nElectronic mail\nSMTP [RFC 5321]\nTCP\nRemote terminal access\nTelnet [RFC 854]\nTCP\nWeb\nHTTP [RFC 2616]\nTCP\nFile transfer\nFTP [RFC 959]\nTCP\nStreaming multimedia\nHTTP (e.g., YouTube)\nTCP\nInternet telephony\nSIP [RFC 3261], RTP [RFC 3550], or proprietary \nUDP or TCP\n(e.g., Skype)\nFigure 2.5 \u0002 Popular Internet applications, their application-layer \nprotocols, and their underlying transport protocols"
    },
    {
      "chunk_id": "ba4857c8-939f-4bfd-979f-746e450f2a0a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.1.6 Network Applications Covered in This Book",
      "original_titles": [
        "2.1.6 Network Applications Covered in This Book"
      ],
      "path": "Chapter 2 Application Layer > 2.1 Principles of Network Applications > 2.1.6 Network Applications Covered in This Book",
      "start_page": 124,
      "end_page": 124,
      "token_count": 652,
      "text": "•\nThe semantics of the fields, that is, the meaning of the information in the fields\n•\nRules for determining when and how a process sends messages and responds to\nmessages\nSome application-layer protocols are specified in RFCs and are therefore in the public\ndomain. For example, the Web’s application-layer protocol, HTTP (the HyperText\nTransfer Protocol [RFC 2616]), is available as an RFC. If a browser developer follows\nthe rules of the HTTP RFC, the browser will be able to retrieve Web pages from \nany Web server that has also followed the rules of the HTTP RFC. Many other\napplication-layer protocols are proprietary and intentionally not available in the public\ndomain. For example, Skype uses proprietary application-layer protocols.\nIt is important to distinguish between network applications and application-layer\nprotocols. An application-layer protocol is only one piece of a network application\n(albeit, a very important piece of the application from our point of view!). Let’s look at\na couple of examples. The Web is a client-server application that allows users to\nobtain documents from Web servers on demand. The Web application consists of\nmany components, including a standard for document formats (that is, HTML), Web\nbrowsers (for example, Firefox and Microsoft Internet Explorer), Web servers (for\nexample, Apache and Microsoft servers), and an application-layer protocol. The\nWeb’s application-layer protocol, HTTP, defines the format and sequence of messages\nexchanged between browser and Web server. Thus, HTTP is only one piece (albeit, an\nimportant piece) of the Web application. As another example, an Internet e-mail appli-\ncation also has many components, including mail servers that house user mailboxes;\nmail clients (such as Microsoft Outlook) that allow users to read and create messages; a\nstandard for defining the structure of an e-mail message; and application-layer proto-\ncols that define how messages are passed between servers, how messages are passed\nbetween servers and mail clients, and how the contents of message headers are to be\ninterpreted. The principal application-layer protocol for electronic mail is SMTP\n(Simple Mail Transfer Protocol) [RFC 5321]. Thus, e-mail’s principal application-layer\nprotocol, SMTP, is only one piece (albeit, an important piece) of the e-mail application.\n2.1.6 Network Applications Covered in This Book\nNew public domain and proprietary Internet applications are being developed every\nday. Rather than covering a large number of Internet applications in an encyclope-\ndic manner, we have chosen to focus on a small number of applications that are both\npervasive and important. In this chapter we discuss five important applications: the\nWeb, file transfer, electronic mail, directory service, and P2P applications. We first\ndiscuss the Web, not only because it is an enormously popular application, but also\nbecause its application-layer protocol, HTTP, is straightforward and easy to under-\nstand. After covering the Web, we briefly examine FTP, because it provides a nice\ncontrast to HTTP. We then discuss electronic mail, the Internet’s first killer applica-\ntion. E-mail is more complex than the Web in the sense that it makes use of not one\n2.1\n•\nPRINCIPLES OF NETWORK APPLICATIONS\n97"
    },
    {
      "chunk_id": "0b54d285-6e9d-4170-9d77-d068cc5b5742",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.2 The Web and HTTP",
      "original_titles": [
        "2.2 The Web and HTTP"
      ],
      "path": "Chapter 2 Application Layer > 2.2 The Web and HTTP",
      "start_page": 125,
      "end_page": 126,
      "token_count": 1095,
      "text": "but several application-layer protocols. After e-mail, we cover DNS, which provides\na directory service for the Internet. Most users do not interact with DNS directly;\ninstead, users invoke DNS indirectly through other applications (including the Web,\nfile transfer, and electronic mail). DNS illustrates nicely how a piece of core net-\nwork functionality (network-name to network-address translation) can be imple-\nmented at the application layer in the Internet. Finally, we discuss in this chapter\nseveral P2P applications, focusing on file sharing applications, and distributed\nlookup services. In Chapter 7, we’ll cover multimedia applications, including\nstreaming video and voice-over-IP.\n2.2 The Web and HTTP\nUntil the early 1990s the Internet was used primarily by researchers, academics, and\nuniversity students to log in to remote hosts, to transfer files from local hosts to remote\nhosts and vice versa, to receive and send news, and to receive and send electronic\nmail. Although these applications were (and continue to be) extremely useful, the\nInternet was essentially unknown outside of the academic and research communities.\nThen, in the early 1990s, a major new application arrived on the scene—the World\nWide Web [Berners-Lee 1994]. The Web was the first Internet application that caught\nthe general public’s eye. It dramatically changed, and continues to change, how peo-\nple interact inside and outside their work environments. It elevated the Internet from\njust one of many data networks to essentially the one and only data network.\nPerhaps what appeals the most to users is that the Web operates on demand.\nUsers receive what they want, when they want it. This is unlike traditional broad-\ncast radio and television, which force users to tune in when the content provider\nmakes the content available. In addition to being available on demand, the Web has\nmany other wonderful features that people love and cherish. It is enormously easy\nfor any individual to make information available over the Web—everyone can\nbecome a publisher at extremely low cost. Hyperlinks and search engines help us\nnavigate through an ocean of Web sites. Graphics stimulate our senses. Forms,\nJavaScript, Java applets, and many other devices enable us to interact with pages\nand sites. And the Web serves as a platform for many killer applications emerging\nafter 2003, including YouTube, Gmail, and Facebook.\n2.2.1 Overview of HTTP\nThe HyperText Transfer Protocol (HTTP), the Web’s application-layer protocol,\nis at the heart of the Web. It is defined in [RFC 1945] and [RFC 2616]. HTTP is\nimplemented in two programs: a client program and a server program. The client\nprogram and server program, executing on different end systems, talk to each other\nby exchanging HTTP messages. HTTP defines the structure of these messages and\nhow the client and server exchange the messages. Before explaining HTTP in detail,\nwe should review some Web terminology.\n98\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nA Web page (also called a document) consists of objects. An object is simply a\nfile—such as an HTML file, a JPEG image, a Java applet, or a video clip—that is\naddressable by a single URL. Most Web pages consist of a base HTML file and\nseveral referenced objects. For example, if a Web page contains HTML text and five\nJPEG images, then the Web page has six objects: the base HTML file plus the five\nimages. The base HTML file references the other objects in the page with the\nobjects’ URLs. Each URL has two components: the hostname of the server that\nhouses the object and the object’s path name. For example, the URL\nhttp://www.someSchool.edu/someDepartment/picture.gif\nhas www.someSchool.edu for a hostname and /someDepartment/\npicture.gif for a path name. Because Web browsers (such as Internet Explorer\nand Firefox) implement the client side of HTTP, in the context of the Web, we will use\nthe words browser and client interchangeably. Web servers, which implement the\nserver side of HTTP, house Web objects, each addressable by a URL. Popular Web\nservers include Apache and Microsoft Internet Information Server.\nHTTP defines how Web clients request Web pages from Web servers and how\nservers transfer Web pages to clients. We discuss the interaction between client and\nserver in detail later, but the general idea is illustrated in Figure 2.6. When a user\nrequests a Web page (for example, clicks on a hyperlink), the browser sends HTTP\nrequest messages for the objects in the page to the server. The server receives the\nrequests and responds with HTTP response messages that contain the objects.\nHTTP uses TCP as its underlying transport protocol (rather than running on top of\nUDP). The HTTP client first initiates a TCP connection with the server. Once the con-\nnection is established, the browser and the server processes access TCP through their\nsocket interfaces. As described in Section 2.1, on the client side the socket interface is\nthe door between the client process and the TCP connection; on the server side it is the\n2.2\n•\nTHE WEB AND HTTP\n99\nHTTP request\nHTTP response\nHTTP response\nHTTP request\nPC running\nInternet Explorer\nLinux running\nFirefox\nServer running\nApache Web server\nFigure 2.6 \u0002 HTTP request-response behavior"
    },
    {
      "chunk_id": "8a39cb91-66b8-43ac-b419-f0b80ff1fac5",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.2.1 Overview of HTTP",
      "original_titles": [
        "2.2.1 Overview of HTTP"
      ],
      "path": "Chapter 2 Application Layer > 2.2 The Web and HTTP > 2.2.1 Overview of HTTP",
      "start_page": 125,
      "end_page": 126,
      "token_count": 1095,
      "text": "but several application-layer protocols. After e-mail, we cover DNS, which provides\na directory service for the Internet. Most users do not interact with DNS directly;\ninstead, users invoke DNS indirectly through other applications (including the Web,\nfile transfer, and electronic mail). DNS illustrates nicely how a piece of core net-\nwork functionality (network-name to network-address translation) can be imple-\nmented at the application layer in the Internet. Finally, we discuss in this chapter\nseveral P2P applications, focusing on file sharing applications, and distributed\nlookup services. In Chapter 7, we’ll cover multimedia applications, including\nstreaming video and voice-over-IP.\n2.2 The Web and HTTP\nUntil the early 1990s the Internet was used primarily by researchers, academics, and\nuniversity students to log in to remote hosts, to transfer files from local hosts to remote\nhosts and vice versa, to receive and send news, and to receive and send electronic\nmail. Although these applications were (and continue to be) extremely useful, the\nInternet was essentially unknown outside of the academic and research communities.\nThen, in the early 1990s, a major new application arrived on the scene—the World\nWide Web [Berners-Lee 1994]. The Web was the first Internet application that caught\nthe general public’s eye. It dramatically changed, and continues to change, how peo-\nple interact inside and outside their work environments. It elevated the Internet from\njust one of many data networks to essentially the one and only data network.\nPerhaps what appeals the most to users is that the Web operates on demand.\nUsers receive what they want, when they want it. This is unlike traditional broad-\ncast radio and television, which force users to tune in when the content provider\nmakes the content available. In addition to being available on demand, the Web has\nmany other wonderful features that people love and cherish. It is enormously easy\nfor any individual to make information available over the Web—everyone can\nbecome a publisher at extremely low cost. Hyperlinks and search engines help us\nnavigate through an ocean of Web sites. Graphics stimulate our senses. Forms,\nJavaScript, Java applets, and many other devices enable us to interact with pages\nand sites. And the Web serves as a platform for many killer applications emerging\nafter 2003, including YouTube, Gmail, and Facebook.\n2.2.1 Overview of HTTP\nThe HyperText Transfer Protocol (HTTP), the Web’s application-layer protocol,\nis at the heart of the Web. It is defined in [RFC 1945] and [RFC 2616]. HTTP is\nimplemented in two programs: a client program and a server program. The client\nprogram and server program, executing on different end systems, talk to each other\nby exchanging HTTP messages. HTTP defines the structure of these messages and\nhow the client and server exchange the messages. Before explaining HTTP in detail,\nwe should review some Web terminology.\n98\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nA Web page (also called a document) consists of objects. An object is simply a\nfile—such as an HTML file, a JPEG image, a Java applet, or a video clip—that is\naddressable by a single URL. Most Web pages consist of a base HTML file and\nseveral referenced objects. For example, if a Web page contains HTML text and five\nJPEG images, then the Web page has six objects: the base HTML file plus the five\nimages. The base HTML file references the other objects in the page with the\nobjects’ URLs. Each URL has two components: the hostname of the server that\nhouses the object and the object’s path name. For example, the URL\nhttp://www.someSchool.edu/someDepartment/picture.gif\nhas www.someSchool.edu for a hostname and /someDepartment/\npicture.gif for a path name. Because Web browsers (such as Internet Explorer\nand Firefox) implement the client side of HTTP, in the context of the Web, we will use\nthe words browser and client interchangeably. Web servers, which implement the\nserver side of HTTP, house Web objects, each addressable by a URL. Popular Web\nservers include Apache and Microsoft Internet Information Server.\nHTTP defines how Web clients request Web pages from Web servers and how\nservers transfer Web pages to clients. We discuss the interaction between client and\nserver in detail later, but the general idea is illustrated in Figure 2.6. When a user\nrequests a Web page (for example, clicks on a hyperlink), the browser sends HTTP\nrequest messages for the objects in the page to the server. The server receives the\nrequests and responds with HTTP response messages that contain the objects.\nHTTP uses TCP as its underlying transport protocol (rather than running on top of\nUDP). The HTTP client first initiates a TCP connection with the server. Once the con-\nnection is established, the browser and the server processes access TCP through their\nsocket interfaces. As described in Section 2.1, on the client side the socket interface is\nthe door between the client process and the TCP connection; on the server side it is the\n2.2\n•\nTHE WEB AND HTTP\n99\nHTTP request\nHTTP response\nHTTP response\nHTTP request\nPC running\nInternet Explorer\nLinux running\nFirefox\nServer running\nApache Web server\nFigure 2.6 \u0002 HTTP request-response behavior"
    },
    {
      "chunk_id": "30fa9dac-8401-490b-95c1-7190a9ee2ff9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.2.2 Non-Persistent and Persistent Connections",
      "original_titles": [
        "2.2.2 Non-Persistent and Persistent Connections"
      ],
      "path": "Chapter 2 Application Layer > 2.2 The Web and HTTP > 2.2.2 Non-Persistent and Persistent Connections",
      "start_page": 127,
      "end_page": 129,
      "token_count": 1621,
      "text": "door between the server process and the TCP connection. The client sends HTTP\nrequest messages into its socket interface and receives HTTP response messages from\nits socket interface. Similarly, the HTTP server receives request messages from its\nsocket interface and sends response messages into its socket interface. Once the client\nsends a message into its socket interface, the message is out of the client’s hands and is\n“in the hands” of TCP. Recall from Section 2.1 that TCP provides a reliable data trans-\nfer service to HTTP. This implies that each HTTP request message sent by a client\nprocess eventually arrives intact at the server; similarly, each HTTP response message\nsent by the server process eventually arrives intact at the client. Here we see one of the\ngreat advantages of a layered architecture—HTTP need not worry about lost data or the\ndetails of how TCP recovers from loss or reordering of data within the network. That is\nthe job of TCP and the protocols in the lower layers of the protocol stack.\nIt is important to note that the server sends requested files to clients without stor-\ning any state information about the client. If a particular client asks for the same object\ntwice in a period of a few seconds, the server does not respond by saying that it just\nserved the object to the client; instead, the server resends the object, as it has com-\npletely forgotten what it did earlier. Because an HTTP server maintains no informa-\ntion about the clients, HTTP is said to be a stateless protocol. We also remark that the\nWeb uses the client-server application architecture, as described in Section 2.1. A Web\nserver is always on, with a fixed IP address, and it services requests from potentially\nmillions of different browsers.\n2.2.2 Non-Persistent and Persistent Connections\nIn many Internet applications, the client and server communicate for an extended\nperiod of time, with the client making a series of requests and the server responding to\neach of the requests. Depending on the application and on how the application is being\nused, the series of requests may be made back-to-back, periodically at regular intervals,\nor intermittently. When this client-server interaction is taking place over TCP, the appli-\ncation developer needs to make an important decision––should each request/response\npair be sent over a separate TCP connection, or should all of the requests and their cor-\nresponding responses be sent over the same TCP connection? In the former approach,\nthe application is said to use non-persistent connections; and in the latter approach,\npersistent connections. To gain a deep understanding of this design issue, let’s exam-\nine the advantages and disadvantages of persistent connections in the context of a spe-\ncific application, namely, HTTP, which can use both non-persistent connections and\npersistent connections. Although HTTP uses persistent connections in its default mode,\nHTTP clients and servers can be configured to use non-persistent connections instead.\nHTTP with Non-Persistent Connections\nLet’s walk through the steps of transferring a Web page from server to client for the\ncase of non-persistent connections. Let’s suppose the page consists of a base HTML\n100\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nfile and 10 JPEG images, and that all 11 of these objects reside on the same server.\nFurther suppose the URL for the base HTML file is\nhttp://www.someSchool.edu/someDepartment/home.index\nHere is what happens:\n1. The HTTP client process initiates a TCP connection to the server\nwww.someSchool.edu on port number 80, which is the default port num-\nber for HTTP. Associated with the TCP connection, there will be a socket at the\nclient and a socket at the server.\n2. The HTTP client sends an HTTP request message to the server via its socket. The\nrequest message includes the path name /someDepartment/home.index.\n(We will discuss HTTP messages in some detail below.)\n3. The HTTP server process receives the request message via its socket, retrieves\nthe object /someDepartment/home.index from its storage (RAM or\ndisk), encapsulates the object in an HTTP response message, and sends the\nresponse message to the client via its socket.\n4. The HTTP server process tells TCP to close the TCP connection. (But TCP\ndoesn’t actually terminate the connection until it knows for sure that the client\nhas received the response message intact.)\n5. The HTTP client receives the response message. The TCP connection termi-\nnates. The message indicates that the encapsulated object is an HTML file. The\nclient extracts the file from the response message, examines the HTML file,\nand finds references to the 10 JPEG objects.\n6. The first four steps are then repeated for each of the referenced JPEG objects.\nAs the browser receives the Web page, it displays the page to the user. Two differ-\nent browsers may interpret (that is, display to the user) a Web page in somewhat differ-\nent ways. HTTP has nothing to do with how a Web page is interpreted by a client. The\nHTTP specifications ([RFC 1945] and [RFC 2616]) define only the communication\nprotocol between the client HTTP program and the server HTTP program.\nThe steps above illustrate the use of non-persistent connections, where each TCP\nconnection is closed after the server sends the object—the connection does not persist\nfor other objects. Note that each TCP connection transports exactly one request mes-\nsage and one response message. Thus, in this example, when a user requests the Web\npage, 11 TCP connections are generated.\nIn the steps described above, we were intentionally vague about whether the\nclient obtains the 10 JPEGs over 10 serial TCP connections, or whether some of the\nJPEGs are obtained over parallel TCP connections. Indeed, users can configure\nmodern browsers to control the degree of parallelism. In their default modes, most\nbrowsers open 5 to 10 parallel TCP connections, and each of these connections han-\ndles one request-response transaction. If the user prefers, the maximum number of\n2.2\n•\nTHE WEB AND HTTP\n101\n\nparallel connections can be set to one, in which case the 10 connections are estab-\nlished serially. As we’ll see in the next chapter, the use of parallel connections short-\nens the response time.\nBefore continuing, let’s do a back-of-the-envelope calculation to estimate the\namount of time that elapses from when a client requests the base HTML file until\nthe entire file is received by the client. To this end, we define the round-trip time\n(RTT), which is the time it takes for a small packet to travel from client to server\nand then back to the client. The RTT includes packet-propagation delays, packet-\nqueuing delays in intermediate routers and switches, and packet-processing\ndelays. (These delays were discussed in Section 1.4.) Now consider what happens\nwhen a user clicks on a hyperlink. As shown in Figure 2.7, this causes the browser\nto initiate a TCP connection between the browser and the Web server; this\ninvolves a “three-way handshake”—the client sends a small TCP segment to the\nserver, the server acknowledges and responds with a small TCP segment, and,\nfinally, the client acknowledges back to the server. The first two parts of the three-\nway handshake take one RTT. After completing the first two parts of the hand-\nshake, the client sends the HTTP request message combined with the third part of\n102\nCHAPTER 2\n•\nAPPLICATION LAYER\nTime\nat client\nTime\nat server\nInitiate TCP\nconnection\nRTT\nRequest file\nRTT\nEntire file received\nTime to transmit file\nFigure 2.7 \u0002 Back-of-the-envelope calculation for the time needed to\nrequest and receive an HTML file"
    },
    {
      "chunk_id": "1b7025d2-3e6d-442e-b088-c2d4af0d496f",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.2.3 HTTP Message Format",
      "original_titles": [
        "2.2.3 HTTP Message Format"
      ],
      "path": "Chapter 2 Application Layer > 2.2 The Web and HTTP > 2.2.3 HTTP Message Format",
      "start_page": 130,
      "end_page": 134,
      "token_count": 2557,
      "text": "the three-way handshake (the acknowledgment) into the TCP connection. Once\nthe request message arrives at the server, the server sends the HTML file into the\nTCP connection. This HTTP request/response eats up another RTT. Thus, roughly,\nthe total response time is two RTTs plus the transmission time at the server of the\nHTML file.\nHTTP with Persistent Connections\nNon-persistent connections have some shortcomings. First, a brand-new connec-\ntion must be established and maintained for each requested object. For each of\nthese connections, TCP buffers must be allocated and TCP variables must be kept\nin both the client and server. This can place a significant burden on the Web server,\nwhich may be serving requests from hundreds of different clients simultaneously.\nSecond, as we just described, each object suffers a delivery delay of two RTTs—\none RTT to establish the TCP connection and one RTT to request and receive an\nobject.\nWith persistent connections, the server leaves the TCP connection open after\nsending a response. Subsequent requests and responses between the same client and\nserver can be sent over the same connection. In particular, an entire Web page (in\nthe example above, the base HTML file and the 10 images) can be sent over a single\npersistent TCP connection. Moreover, multiple Web pages residing on the same\nserver can be sent from the server to the same client over a single persistent TCP\nconnection. These requests for objects can be made back-to-back, without waiting\nfor replies to pending requests (pipelining). Typically, the HTTP server closes a con-\nnection when it isn’t used for a certain time (a configurable timeout interval). When\nthe server receives the back-to-back requests, it sends the objects back-to-back. The\ndefault mode of HTTP uses persistent connections with pipelining. We’ll quantita-\ntively compare the performance of non-persistent and persistent connections in the\nhomework problems of Chapters 2 and 3. You are also encouraged to see [Heide-\nmann 1997; Nielsen 1997].\n2.2.3 HTTP Message Format\nThe HTTP specifications [RFC 1945; RFC 2616] include the definitions of the\nHTTP message formats. There are two types of HTTP messages, request messages\nand response messages, both of which are discussed below.\nHTTP Request Message\nBelow we provide a typical HTTP request message:\nGET /somedir/page.html HTTP/1.1\nHost: www.someschool.edu\n2.2\n•\nTHE WEB AND HTTP\n103\n\nConnection: close\nUser-agent: Mozilla/5.0\nAccept-language: fr\nWe can learn a lot by taking a close look at this simple request message. First of\nall, we see that the message is written in ordinary ASCII text, so that your ordinary\ncomputer-literate human being can read it. Second, we see that the message consists\nof five lines, each followed by a carriage return and a line feed. The last line is fol-\nlowed by an additional carriage return and line feed. Although this particular request\nmessage has five lines, a request message can have many more lines or as few as\none line. The first line of an HTTP request message is called the request line; the\nsubsequent lines are called the header lines. The request line has three fields: the\nmethod field, the URL field, and the HTTP version field. The method field can take\non several different values, including GET, POST, HEAD, PUT, and DELETE.\nThe great majority of HTTP request messages use the GET method. The GET\nmethod is used when the browser requests an object, with the requested object iden-\ntified in the URL field. In this example, the browser is requesting the object\n/somedir/page.html. The version is self-explanatory; in this example, the\nbrowser implements version HTTP/1.1.\nNow let’s look at the header lines in the example. The header line Host:\nwww.someschool.edu specifies the host on which the object resides. You might\nthink that this header line is unnecessary, as there is already a TCP connection in\nplace to the host. But, as we’ll see in Section 2.2.5, the information provided by the\nhost header line is required by Web proxy caches. By including the Connection:\nclose header line, the browser is telling the server that it doesn’t want to bother\nwith persistent connections; it wants the server to close the connection after sending\nthe requested object. The User-agent: header line specifies the user agent, that\nis, the browser type that is making the request to the server. Here the user agent is\nMozilla/5.0, a Firefox browser. This header line is useful because the server can\nactually send different versions of the same object to different types of user agents.\n(Each of the versions is addressed by the same URL.) Finally, the Accept-\nlanguage: header indicates that the user prefers to receive a French version of\nthe object, if such an object exists on the server; otherwise, the server should send\nits default version. The Accept-language: header is just one of many content\nnegotiation headers available in HTTP.\nHaving looked at an example, let’s now look at the general format of a request\nmessage, as shown in Figure 2.8. We see that the general format closely follows our\nearlier example. You may have noticed, however, that after the header lines (and the\nadditional carriage return and line feed) there is an “entity body.” The entity body is\nempty with the GET method, but is used with the POST method. An HTTP client\noften uses the POST method when the user fills out a form—for example, when a\nuser provides search words to a search engine. With a POST message, the user is still\nrequesting a Web page from the server, but the specific contents of the Web page\n104\nCHAPTER 2\n•\nAPPLICATION LAYER\n\ndepend on what the user entered into the form fields. If the value of the method field\nis POST, then the entity body contains what the user entered into the form fields.\nWe would be remiss if we didn’t mention that a request generated with a form\ndoes not necessarily use the POST method. Instead, HTML forms often use the GET\nmethod and include the inputted data (in the form fields) in the requested URL. For\nexample, if a form uses the GET method, has two fields, and the inputs to the two\nfields are monkeys and bananas, then the URL will have the structure\nwww.somesite.com/animalsearch?monkeys&bananas. In your day-to-\nday Web surfing, you have probably noticed extended URLs of this sort.\nThe HEAD method is similar to the GET method. When a server receives a\nrequest with the HEAD method, it responds with an HTTP message but it leaves out\nthe requested object. Application developers often use the HEAD method for debug-\nging. The PUT method is often used in conjunction with Web publishing tools. It\nallows a user to upload an object to a specific path (directory) on a specific Web\nserver. The PUT method is also used by applications that need to upload objects to\nWeb servers. The DELETE method allows a user, or an application, to delete an\nobject on a Web server.\nHTTP Response Message\nBelow we provide a typical HTTP response message. This response message could\nbe the response to the example request message just discussed.\nHTTP/1.1 200 OK\nConnection: close\n2.2\n•\nTHE WEB AND HTTP\n105\nmethod\nsp\nsp\ncr\nlf\ncr\nlf\nheader field name:\nHeader lines\nBlank line\nEntity body\nRequest line\nvalue\nsp\ncr\nlf\ncr\nlf\nheader field name:\nvalue\nsp\nURL\nVersion\nFigure 2.8 \u0002 General format of an HTTP request message\n\nDate: Tue, 09 Aug 2011 15:44:04 GMT\nServer: Apache/2.2.3 (CentOS)\nLast-Modified: Tue, 09 Aug 2011 15:11:03 GMT\nContent-Length: 6821\nContent-Type: text/html\n(data data data data data ...)\nLet’s take a careful look at this response message. It has three sections: an ini-\ntial status line, six header lines, and then the entity body. The entity body is the\nmeat of the message—it contains the requested object itself (represented by data\ndata data data data ...). The status line has three fields: the protocol ver-\nsion field, a status code, and a corresponding status message. In this example, the\nstatus line indicates that the server is using HTTP/1.1 and that everything is OK\n(that is, the server has found, and is sending, the requested object).\nNow let’s look at the header lines. The server uses the Connection: close\nheader line to tell the client that it is going to close the TCP connection after sending\nthe message. The Date: header line indicates the time and date when the HTTP\nresponse was created and sent by the server. Note that this is not the time when the\nobject was created or last modified; it is the time when the server retrieves the\nobject from its file system, inserts the object into the response message, and sends\nthe response message. The Server: header line indicates that the message was gen-\nerated by an Apache Web server; it is analogous to the User-agent: header line\nin the HTTP request message. The Last-Modified: header line indicates the\ntime and date when the object was created or last modified. The Last-Modified:\nheader, which we will soon cover in more detail, is critical for object caching, both in\nthe local client and in network cache servers (also known as proxy servers). The\nContent-Length: header line indicates the number of bytes in the object being\nsent. The Content-Type: header line indicates that the object in the entity body is\nHTML text. (The object type is officially indicated by the Content-Type: header\nand not by the file extension.)\nHaving looked at an example, let’s now examine the general format of a\nresponse message, which is shown in Figure 2.9. This general format of the response\nmessage matches the previous example of a response message. Let’s say a few addi-\ntional words about status codes and their phrases. The status code and associated\nphrase indicate the result of the request. Some common status codes and associated\nphrases include:\n•\n200 OK: Request succeeded and the information is returned in the response.\n•\n301 Moved Permanently: Requested object has been permanently moved;\nthe new URL is specified in Location: header of the response message. The\nclient software will automatically retrieve the new URL.\n106\nCHAPTER 2\n•\nAPPLICATION LAYER\n\n•\n400 Bad Request: This is a generic error code indicating that the request\ncould not be understood by the server.\n•\n404 Not Found: The requested document does not exist on this server.\n•\n505 HTTP Version Not Supported: The requested HTTP protocol\nversion is not supported by the server.\nHow would you like to see a real HTTP response message? This is highly rec-\nommended and very easy to do! First Telnet into your favorite Web server. Then\ntype in a one-line request message for some object that is housed on the server. For\nexample, if you have access to a command prompt, type:\ntelnet cis.poly.edu 80\nGET /~ross/ HTTP/1.1\nHost: cis.poly.edu\n(Press the carriage return twice after typing the last line.) This opens a TCP connec-\ntion to port 80 of the host cis.poly.edu and then sends the HTTP request mes-\nsage. You should see a response message that includes the base HTML file of\nProfessor Ross’s homepage. If you’d rather just see the HTTP message lines and not\nreceive the object itself, replace GET with HEAD. Finally, replace /~ross/ with\n/~banana/ and see what kind of response message you get.\nIn this section we discussed a number of header lines that can be used within\nHTTP request and response messages. The HTTP specification defines many, many\n2.2\n•\nTHE WEB AND HTTP\n107\nversion\nsp\nsp\ncr\nlf\ncr\nlf\nheader field name:\nHeader lines\nBlank line\nEntity body\nStatus line\nvalue\ncr\nsp\nsp\nlf\ncr\nlf\nheader field name:\nvalue\nstatus code\nphrase\nFigure 2.9 \u0002 General format of an HTTP response message\nVideoNote\nUsing Wireshark to\ninvestigate the \nHTTP protocol"
    },
    {
      "chunk_id": "5d4e1112-4474-437a-b0ed-76bfddb2c2fc",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.2.4 User-Server Interaction: Cookies",
      "original_titles": [
        "2.2.4 User-Server Interaction: Cookies"
      ],
      "path": "Chapter 2 Application Layer > 2.2 The Web and HTTP > 2.2.4 User-Server Interaction: Cookies",
      "start_page": 135,
      "end_page": 136,
      "token_count": 837,
      "text": "more header lines that can be inserted by browsers, Web servers, and network cache\nservers. We have covered only a small number of the totality of header lines. We’ll\ncover a few more below and another small number when we discuss network Web\ncaching in Section 2.2.5. Ahighly readable and comprehensive discussion of the HTTP\nprotocol, including its headers and status codes, is given in [Krishnamurthy 2001].\nHow does a browser decide which header lines to include in a request mes-\nsage? How does a Web server decide which header lines to include in a response\nmessage? A browser will generate header lines as a function of the browser type\nand version (for example, an HTTP/1.0 browser will not generate any 1.1 header\nlines), the user configuration of the browser (for example, preferred language), and\nwhether the browser currently has a cached, but possibly out-of-date, version of the\nobject. Web servers behave similarly: There are different products, versions, and\nconfigurations, all of which influence which header lines are included in response\nmessages.\n2.2.4 User-Server Interaction: Cookies\nWe mentioned above that an HTTP server is stateless. This simplifies server design\nand has permitted engineers to develop high-performance Web servers that can han-\ndle thousands of simultaneous TCP connections. However, it is often desirable for a\nWeb site to identify users, either because the server wishes to restrict user access or\nbecause it wants to serve content as a function of the user identity. For these pur-\nposes, HTTP uses cookies. Cookies, defined in [RFC 6265], allow sites to keep\ntrack of users. Most major commercial Web sites use cookies today.\nAs shown in Figure 2.10, cookie technology has four components: (1) a cookie\nheader line in the HTTP response message; (2) a cookie header line in the HTTP\nrequest message; (3) a cookie file kept on the user’s end system and managed by the\nuser’s browser; and (4) a back-end database at the Web site. Using Figure 2.10, let’s\nwalk through an example of how cookies work. Suppose Susan, who always\naccesses the Web using Internet Explorer from her home PC, contacts Amazon.com\nfor the first time. Let us suppose that in the past she has already visited the eBay site.\nWhen the request comes into the Amazon Web server, the server creates a unique\nidentification number and creates an entry in its back-end database that is indexed\nby the identification number. The Amazon Web server then responds to Susan’s\nbrowser, including in the HTTP response a Set-cookie: header, which contains\nthe identification number. For example, the header line might be:\nSet-cookie: 1678\nWhen Susan’s browser receives the HTTP response message, it sees the Set-\ncookie: header. The browser then appends a line to the special cookie file that it\nmanages. This line includes the hostname of the server and the identification num-\nber in the Set-cookie: header. Note that the cookie file already has an entry for\n108\nCHAPTER 2\n•\nAPPLICATION LAYER\n\neBay, since Susan has visited that site in the past. As Susan continues to browse the\nAmazon site, each time she requests a Web page, her browser consults her cookie\nfile, extracts her identification number for this site, and puts a cookie header line\nthat includes the identification number in the HTTP request. Specifically, each of\nher HTTP requests to the Amazon server includes the header line:\nCookie: 1678\n2.2\n•\nTHE WEB AND HTTP\n109\nClient host\nServer host\nusual http request msg\nusual http response\nSet-cookie: 1678\nusual http request msg\ncookie: 1678\nusual http response msg\nusual http request msg\ncookie: 1678\nusual http response msg\nTime\nOne week later\nebay: 8734\nServer creates\nID 1678 for user\nTime\nCookie file\nKey:\namazon: 1678\nebay: 8734\namazon: 1678\nebay: 8734\nCookie-specific\naction\naccess\naccess\nentry in backend\ndatabase\nCookie-specific\naction\nFigure 2.10 \u0002 Keeping user state with cookies"
    },
    {
      "chunk_id": "9f44ee83-3e2d-4e7e-b3ca-312c3c07747d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.2.5 Web Caching",
      "original_titles": [
        "2.2.5 Web Caching"
      ],
      "path": "Chapter 2 Application Layer > 2.2 The Web and HTTP > 2.2.5 Web Caching",
      "start_page": 137,
      "end_page": 140,
      "token_count": 2181,
      "text": "In this manner, the Amazon server is able to track Susan’s activity at the Amazon\nsite. Although the Amazon Web site does not necessarily know Susan’s name, it\nknows exactly which pages user 1678 visited, in which order, and at what times!\nAmazon uses cookies to provide its shopping cart service—Amazon can maintain a\nlist of all of Susan’s intended purchases, so that she can pay for them collectively at\nthe end of the session.\nIf Susan returns to Amazon’s site, say, one week later, her browser will continue\nto put the header line Cookie: 1678 in the request messages. Amazon also rec-\nommends products to Susan based on Web pages she has visited at Amazon in the\npast. If Susan also registers herself with Amazon—providing full name, e-mail\naddress, postal address, and credit card information—Amazon can then include this\ninformation in its database, thereby associating Susan’s name with her identification\nnumber (and all of the pages she has visited at the site in the past!). This is how\nAmazon and other e-commerce sites provide “one-click shopping”—when Susan\nchooses to purchase an item during a subsequent visit, she doesn’t need to re-enter\nher name, credit card number, or address.\nFrom this discussion we see that cookies can be used to identify a user. The first\ntime a user visits a site, the user can provide a user identification (possibly his or her\nname). During the subsequent sessions, the browser passes a cookie header to the\nserver, thereby identifying the user to the server. Cookies can thus be used to create\na user session layer on top of stateless HTTP. For example, when a user logs in to a\nWeb-based e-mail application (such as Hotmail), the browser sends cookie informa-\ntion to the server, permitting the server to identify the user throughout the user’s ses-\nsion with the application.\nAlthough cookies often simplify the Internet shopping experience for the user,\nthey are controversial because they can also be considered as an invasion of privacy.\nAs we just saw, using a combination of cookies and user-supplied account informa-\ntion, a Web site can learn a lot about a user and potentially sell this information to a\nthird party. Cookie Central [Cookie Central 2012] includes extensive information\non the cookie controversy.\n2.2.5 Web Caching\nA Web cache—also called a proxy server—is a network entity that satisfies HTTP\nrequests on the behalf of an origin Web server. The Web cache has its own disk storage\nand keeps copies of recently requested objects in this storage. As shown in Figure 2.11, a\nuser’s browser can be configured so that all of the user’s HTTP requests are first directed\nto the Web cache. Once a browser is configured, each browser request for an object is\nfirst directed to the Web cache. As an example, suppose a browser is requesting the\nobject http://www.someschool.edu/campus.gif. Here is what happens:\n1. The browser establishes a TCP connection to the Web cache and sends an\nHTTP request for the object to the Web cache.\n110\nCHAPTER 2\n•\nAPPLICATION LAYER\n\n2. The Web cache checks to see if it has a copy of the object stored locally. If it\ndoes, the Web cache returns the object within an HTTP response message to\nthe client browser.\n3. If the Web cache does not have the object, the Web cache opens a TCP connec-\ntion to the origin server, that is, to www.someschool.edu. The Web cache\nthen sends an HTTP request for the object into the cache-to-server TCP con-\nnection. After receiving this request, the origin server sends the object within\nan HTTP response to the Web cache.\n4. When the Web cache receives the object, it stores a copy in its local storage and\nsends a copy, within an HTTP response message, to the client browser (over the\nexisting TCP connection between the client browser and the Web cache).\nNote that a cache is both a server and a client at the same time. When it receives\nrequests from and sends responses to a browser, it is a server. When it sends requests\nto and receives responses from an origin server, it is a client.\nTypically a Web cache is purchased and installed by an ISP. For example, a uni-\nversity might install a cache on its campus network and configure all of the campus\nbrowsers to point to the cache. Or a major residential ISP (such as AOL) might\ninstall one or more caches in its network and preconfigure its shipped browsers to\npoint to the installed caches.\nWeb caching has seen deployment in the Internet for two reasons. First, a Web\ncache can substantially reduce the response time for a client request, particularly if the\nbottleneck bandwidth between the client and the origin server is much less than the bot-\ntleneck bandwidth between the client and the cache. If there is a high-speed connection\nbetween the client and the cache, as there often is, and if the cache has the requested\nobject, then the cache will be able to deliver the object rapidly to the client. Second, as\nwe will soon illustrate with an example, Web caches can substantially reduce traffic on\n2.2\n•\nTHE WEB AND HTTP\n111\nHTTP request\nHTTP response\nHTTP request\nHTTP response\nHTTP request\nHTTP response\nHTTP request\nHTTP response\nClient\nOrigin\nserver\nOrigin\nserver\nClient\nProxy\nserver\nFigure 2.11 \u0002 Clients requesting objects through a Web cache\n\nan institution’s access link to the Internet. By reducing traffic, the institution (for exam-\nple, a company or a university) does not have to upgrade bandwidth as quickly, thereby\nreducing costs. Furthermore, Web caches can substantially reduce Web traffic in the\nInternet as a whole, thereby improving performance for all applications.\nTo gain a deeper understanding of the benefits of caches, let’s consider an exam-\nple in the context of Figure 2.12. This figure shows two networks—the institutional\nnetwork and the rest of the public Internet. The institutional network is a high-speed\nLAN. A router in the institutional network and a router in the Internet are connected\nby a 15 Mbps link. The origin servers are attached to the Internet but are located all\nover the globe. Suppose that the average object size is 1 Mbits and that the average\nrequest rate from the institution’s browsers to the origin servers is 15 requests per\nsecond. Suppose that the HTTP request messages are negligibly small and thus cre-\nate no traffic in the networks or in the access link (from institutional router to Inter-\nnet router). Also suppose that the amount of time it takes from when the router on the\nInternet side of the access link in Figure 2.12 forwards an HTTP request (within an\nIP datagram) until it receives the response (typically within many IP datagrams) is\ntwo seconds on average. Informally, we refer to this last delay as the “Internet delay.”\n112\nCHAPTER 2\n•\nAPPLICATION LAYER\nPublic Internet\nInstitutional network\n15 Mbps access link\n100 Mbps LAN\nOrigin servers\nFigure 2.12 \u0002 Bottleneck between an institutional network and the Internet\n\nThe total response time—that is, the time from the browser’s request of an object\nuntil its receipt of the object—is the sum of the LAN delay, the access delay (that is,\nthe delay between the two routers), and the Internet delay. Let’s now do a very crude\ncalculation to estimate this delay. The traffic intensity on the LAN (see Section 1.4.2) is\n(15 requests/sec) \u0002 (1 Mbits/request)/(100 Mbps) = 0.15\nwhereas the traffic intensity on the access link (from the Internet router to institution\nrouter) is\n(15 requests/sec) \u0002 (1 Mbits/request)/(15 Mbps) = 1\nA traffic intensity of 0.15 on a LAN typically results in, at most, tens of millisec-\nonds of delay; hence, we can neglect the LAN delay. However, as discussed in\nSection 1.4.2, as the traffic intensity approaches 1 (as is the case of the access link\nin Figure 2.12), the delay on a link becomes very large and grows without bound.\nThus, the average response time to satisfy requests is going to be on the order of\nminutes, if not more, which is unacceptable for the institution’s users. Clearly some-\nthing must be done.\nOne possible solution is to increase the access rate from 15 Mbps to, say, 100\nMbps. This will lower the traffic intensity on the access link to 0.15, which trans-\nlates to negligible delays between the two routers. In this case, the total response\ntime will roughly be two seconds, that is, the Internet delay. But this solution also\nmeans that the institution must upgrade its access link from 15 Mbps to 100 Mbps, a\ncostly proposition.\nNow consider the alternative solution of not upgrading the access link but\ninstead installing a Web cache in the institutional network. This solution is illus-\ntrated in Figure 2.13. Hit rates—the fraction of requests that are satisfied by a\ncache—typically range from 0.2 to 0.7 in practice. For illustrative purposes, let’s\nsuppose that the cache provides a hit rate of 0.4 for this institution. Because the\nclients and the cache are connected to the same high-speed LAN, 40 percent of\nthe requests will be satisfied almost immediately, say, within 10 milliseconds, by the\ncache. Nevertheless, the remaining 60 percent of the requests still need to be satis-\nfied by the origin servers. But with only 60 percent of the requested objects passing\nthrough the access link, the traffic intensity on the access link is reduced from 1.0 to\n0.6. Typically, a traffic intensity less than 0.8 corresponds to a small delay, say, tens\nof milliseconds, on a 15 Mbps link. This delay is negligible compared with the two-\nsecond Internet delay. Given these considerations, average delay therefore is\n0.4 \u0002 (0.01 seconds) + 0.6 \u0002 (2.01 seconds)\nwhich is just slightly greater than 1.2 seconds. Thus, this second solution provides an\neven lower response time than the first solution, and it doesn’t require the institution\nto upgrade its link to the Internet. The institution does, of course, have to purchase\n2.2\n•\nTHE WEB AND HTTP\n113"
    },
    {
      "chunk_id": "a4e251de-4862-402f-84eb-f8b29ab35116",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.2.6 The Conditional GET",
      "original_titles": [
        "2.2.6 The Conditional GET"
      ],
      "path": "Chapter 2 Application Layer > 2.2 The Web and HTTP > 2.2.6 The Conditional GET",
      "start_page": 141,
      "end_page": 142,
      "token_count": 629,
      "text": "and install a Web cache. But this cost is low—many caches use public-domain soft-\nware that runs on inexpensive PCs.\nThrough the use of Content Distribution Networks (CDNs), Web caches are\nincreasingly playing an important role in the Internet. A CDN company installs many\ngeographically distributed caches throughout the Internet, thereby localizing much of\nthe traffic. There are shared CDNs (such as Akamai and Limelight) and dedicated CDNs\n(such as Google and Microsoft). We will discuss CDNs in more detail in Chapter 7.\n2.2.6 The Conditional GET\nAlthough caching can reduce user-perceived response times, it introduces a new prob-\nlem—the copy of an object residing in the cache may be stale. In other words, the\nobject housed in the Web server may have been modified since the copy was cached\nat the client. Fortunately, HTTP has a mechanism that allows a cache to verify that its\nobjects are up to date. This mechanism is called the conditional GET. An HTTP\n114\nCHAPTER 2\n•\nAPPLICATION LAYER\nPublic Internet\nInstitutional network\n15 Mbps access link\nInstitutional\ncache\n100 Mbps LAN\nOrigin servers\nFigure 2.13 \u0002 Adding a cache to the institutional network\n\nrequest message is a so-called conditional GET message if (1) the request message\nuses the GET method and (2) the request message includes an If-Modified-\nSince: header line.\nTo illustrate how the conditional GET operates, let’s walk through an example.\nFirst, on the behalf of a requesting browser, a proxy cache sends a request message\nto a Web server:\nGET /fruit/kiwi.gif HTTP/1.1\nHost: www.exotiquecuisine.com\nSecond, the Web server sends a response message with the requested object to the\ncache:\nHTTP/1.1 200 OK\nDate: Sat, 8 Oct 2011 15:39:29\nServer: Apache/1.3.0 (Unix)\nLast-Modified: Wed, 7 Sep 2011 09:23:24\nContent-Type: image/gif\n(data data data data data ...)\nThe cache forwards the object to the requesting browser but also caches the object\nlocally. Importantly, the cache also stores the last-modified date along with the\nobject. Third, one week later, another browser requests the same object via the\ncache, and the object is still in the cache. Since this object may have been modified\nat the Web server in the past week, the cache performs an up-to-date check by issu-\ning a conditional GET. Specifically, the cache sends:\nGET /fruit/kiwi.gif HTTP/1.1\nHost: www.exotiquecuisine.com\nIf-modified-since: Wed, 7 Sep 2011 09:23:24\nNote that the value of the If-modified-since: header line is exactly equal to\nthe value of the Last-Modified: header line that was sent by the server one\nweek ago. This conditional GET is telling the server to send the object only if the\nobject has been modified since the specified date. Suppose the object has not been\nmodified since 7 Sep 2011 09:23:24. Then, fourth, the Web server sends a response\nmessage to the cache:\nHTTP/1.1 304 Not Modified\nDate: Sat, 15 Oct 2011 15:39:29\nServer: Apache/1.3.0 (Unix)\n(empty entity body)\n2.2\n•\nTHE WEB AND HTTP\n115"
    },
    {
      "chunk_id": "42b75abc-44ef-4676-9906-cb67604b2191",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.3 File Transfer: FTP",
      "original_titles": [
        "2.3 File Transfer: FTP"
      ],
      "path": "Chapter 2 Application Layer > 2.3 File Transfer: FTP",
      "start_page": 143,
      "end_page": 144,
      "token_count": 1075,
      "text": "We see that in response to the conditional GET, the Web server still sends a response\nmessage but does not include the requested object in the response message. Including\nthe requested object would only waste bandwidth and increase user-perceived response\ntime, particularly if the object is large. Note that this last response message has 304\nNot Modified in the status line, which tells the cache that it can go ahead and for-\nward its (the proxy cache’s) cached copy of the object to the requesting browser.\nThis ends our discussion of HTTP, the first Internet protocol (an application-layer\nprotocol) that we’ve studied in detail. We’ve seen the format of HTTP messages and\nthe actions taken by the Web client and server as these messages are sent and received.\nWe’ve also studied a bit of the Web’s application infrastructure, including caches, cook-\nies, and back-end databases, all of which are tied in some way to the HTTP protocol.\n2.3 File Transfer: FTP\nIn a typical FTP session, the user is sitting in front of one host (the local host)\nand wants to transfer files to or from a remote host. In order for the user to \naccess the remote account, the user must provide a user identification and a pass-\nword. After providing this authorization information, the user can transfer files\nfrom the local file system to the remote file system and vice versa. As shown in\nFigure 2.14, the user interacts with FTP through an FTP user agent. The user first\nprovides the hostname of the remote host, causing the FTP client process in the\nlocal host to establish a TCP connection with the FTP server process in the\nremote host. The user then provides the user identification and password, which\nare sent over the TCP connection as part of FTP commands. Once the server has\nauthorized the user, the user copies one or more files stored in the local file sys-\ntem into the remote file system (or vice versa).\n116\nCHAPTER 2\n•\nAPPLICATION LAYER\nFTP user\ninterface\nLocal file\nsystem\nUser\nor host\nRemote file\nsystem\nFTP\nclient\nFTP\nserver\nFile transfer\nFigure 2.14 \u0002 FTP moves files between local and remote file systems\n\nHTTP and FTP are both file transfer protocols and have many common charac-\nteristics; for example, they both run on top of TCP. However, the two application-layer\nprotocols have some important differences. The most striking difference is that FTP\nuses two parallel TCP connections to transfer a file, a control connection and a data\nconnection. The control connection is used for sending control information between\nthe two hosts—information such as user identification, password, commands to\nchange remote directory, and commands to “put” and “get” files. The data connection\nis used to actually send a file. Because FTP uses a separate control connection, FTP is\nsaid to send its control information out-of-band. HTTP, as you recall, sends request\nand response header lines into the same TCP connection that carries the transferred\nfile itself. For this reason, HTTP is said to send its control information in-band. In the\nnext section, we’ll see that SMTP, the main protocol for electronic mail, also sends\ncontrol information in-band. The FTP control and data connections are illustrated in\nFigure 2.15.\nWhen a user starts an FTP session with a remote host, the client side of FTP\n(user) first initiates a control TCP connection with the server side (remote host) on\nserver port number 21. The client side of FTP sends the user identification and\npassword over this control connection. The client side of FTP also sends, over the\ncontrol connection, commands to change the remote directory. When the server\nside receives a command for a file transfer over the control connection (either to,\nor from, the remote host), the server side initiates a TCP data connection to the\nclient side. FTP sends exactly one file over the data connection and then closes the\ndata connection. If, during the same session, the user wants to transfer another file,\nFTP opens another data connection. Thus, with FTP, the control connection\nremains open throughout the duration of the user session, but a new data connec-\ntion is created for each file transferred within a session (that is, the data connec-\ntions are non-persistent).\nThroughout a session, the FTP server must maintain state about the user. In par-\nticular, the server must associate the control connection with a specific user account,\nand the server must keep track of the user’s current directory as the user wanders\nabout the remote directory tree. Keeping track of this state information for each\nongoing user session significantly constrains the total number of sessions that FTP\ncan maintain simultaneously. Recall that HTTP, on the other hand, is stateless—it\ndoes not have to keep track of any user state.\n2.3\n•\nFILE TRANSFER: FTP\n117\nTCP control connection port 21\nTCP data connection port 20\nFTP\nclient\nFTP\nserver\nFigure 2.15 \u0002 Control and data connections"
    },
    {
      "chunk_id": "e861eeae-4da6-4b3c-a644-3ad689ace658",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.3.1 FTP Commands and Replies",
      "original_titles": [
        "2.3.1 FTP Commands and Replies"
      ],
      "path": "Chapter 2 Application Layer > 2.3 File Transfer: FTP > 2.3.1 FTP Commands and Replies",
      "start_page": 145,
      "end_page": 147,
      "token_count": 1309,
      "text": "2.3.1 FTP Commands and Replies\nWe end this section with a brief discussion of some of the more common FTP com-\nmands and replies. The commands, from client to server, and replies, from server to\nclient, are sent across the control connection in 7-bit ASCII format. Thus, like HTTP\ncommands, FTP commands are readable by people. In order to delineate successive\ncommands, a carriage return and line feed end each command. Each command con-\nsists of four uppercase ASCII characters, some with optional arguments. Some of\nthe more common commands are given below:\n•\nUSER username: Used to send the user identification to the server.\n•\nPASS password: Used to send the user password to the server.\n•\nLIST: Used to ask the server to send back a list of all the files in the current\nremote directory. The list of files is sent over a (new and non-persistent) data\nconnection rather than the control TCP connection.\n•\nRETR filename: Used to retrieve (that is, get) a file from the current direc-\ntory of the remote host. This command causes the remote host to initiate a data\nconnection and to send the requested file over the data connection.\n•\nSTOR filename: Used to store (that is, put) a file into the current directory\nof the remote host.\nThere is typically a one-to-one correspondence between the command that the\nuser issues and the FTP command sent across the control connection. Each com-\nmand is followed by a reply, sent from server to client. The replies are three-digit\nnumbers, with an optional message following the number. This is similar in struc-\nture to the status code and phrase in the status line of the HTTP response message.\nSome typical replies, along with their possible messages, are as follows:\n•\n331 Username OK, password required\n•\n125 Data connection already open; transfer starting\n•\n425 Can’t open data connection\n•\n452 Error writing file\nReaders who are interested in learning about the other FTP commands and replies\nare encouraged to read RFC 959.\n2.4 Electronic Mail in the Internet\nElectronic mail has been around since the beginning of the Internet. It was the most\npopular application when the Internet was in its infancy [Segaller 1998], and has\n118\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nOutgoing\nmessage queue \nKey:\nUser mailbox\nSMTP\nUser agent\nUser agent\nUser agent\nUser agent\nUser agent\nUser agent\nMail server\nMail server\nMail server\nSMTP\nSMTP\nbecome more and more elaborate and powerful over the years. It remains one of the\nInternet’s most important and utilized applications.\nAs with ordinary postal mail, e-mail is an asynchronous communication\nmedium—people send and read messages when it is convenient for them, without\nhaving to coordinate with other people’s schedules. In contrast with postal mail, elec-\ntronic mail is fast, easy to distribute, and inexpensive. Modern e-mail has many pow-\nerful features, including messages with attachments, hyperlinks, HTML-formatted\ntext, and embedded photos.\nIn this section, we examine the application-layer protocols that are at the heart\nof Internet e-mail. But before we jump into an in-depth discussion of these proto-\ncols, let’s take a high-level view of the Internet mail system and its key components.\nFigure 2.16 presents a high-level view of the Internet mail system. We see from\nthis diagram that it has three major components: user agents, mail servers, and the\n2.4\n•\nELECTRONIC MAIL IN THE INTERNET\n119\nFigure 2.16 \u0002 A high-level view of the Internet e-mail system\n\nSimple Mail Transfer Protocol (SMTP). We now describe each of these compo-\nnents in the context of a sender, Alice, sending an e-mail message to a recipient,\nBob. User agents allow users to read, reply to, forward, save, and compose mes-\nsages. Microsoft Outlook and Apple Mail are examples of user agents for e-mail.\nWhen Alice is finished composing her message, her user agent sends the message to\nher mail server, where the message is placed in the mail server’s outgoing message\nqueue. When Bob wants to read a message, his user agent retrieves the message\nfrom his mailbox in his mail server.\nMail servers form the core of the e-mail infrastructure. Each recipient, such as\nBob, has a mailbox located in one of the mail servers. Bob’s mailbox manages and\nmaintains the messages that have been sent to him. A typical message starts its jour-\nney in the sender’s user agent, travels to the sender’s mail server, and \ntravels to the recipient’s mail server, where it is deposited in the recipient’s mailbox.\n120\nCHAPTER 2\n•\nAPPLICATION LAYER\nWEB E-MAIL\nIn December 1995, just a few years after the Web was “invented,” Sabeer Bhatia\nand Jack Smith visited the Internet venture capitalist Draper Fisher Jurvetson and\nproposed developing a free Web-based e-mail system. The idea was to give a free\ne-mail account to anyone who wanted one, and to make the accounts accessible\nfrom the Web. In exchange for 15 percent of the company, Draper Fisher\nJurvetson financed Bhatia and Smith, who formed a company called Hotmail.\nWith three full-time people and 14 part-time people who worked for stock options,\nthey were able to develop and launch the service in July 1996. Within a month\nafter launch, they had 100,000 subscribers. In December 1997, less than 18\nmonths after launching the service, Hotmail had over 12 million subscribers and\nwas acquired by Microsoft, reportedly for $400 million. The success of Hotmail is\noften attributed to its “first-mover advantage” and to the intrinsic “viral marketing”\nof e-mail. (Perhaps some of the students reading this book will be among the new\nentrepreneurs who conceive and develop first-mover Internet services with inherent\nviral marketing.)\nWeb e-mail continues to thrive, becoming more sophisticated and powerful every\nyear. One of the most popular services today is Google’s gmail, which offers giga-\nbytes of free storage, advanced spam filtering and virus detection, e-mail encryption\n(using SSL), mail fetching from third-party e-mail services, and a search-oriented inter-\nface. Asynchronous messaging within social networks, such as Facebook, has also\nbecome popular in recent years.\nCASE HISTORY"
    },
    {
      "chunk_id": "37c8913a-ca50-4604-ac48-1d83a921818a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.4 Electronic Mail in the Internet",
      "original_titles": [
        "2.4 Electronic Mail in the Internet"
      ],
      "path": "Chapter 2 Application Layer > 2.4 Electronic Mail in the Internet",
      "start_page": 145,
      "end_page": 147,
      "token_count": 1309,
      "text": "2.3.1 FTP Commands and Replies\nWe end this section with a brief discussion of some of the more common FTP com-\nmands and replies. The commands, from client to server, and replies, from server to\nclient, are sent across the control connection in 7-bit ASCII format. Thus, like HTTP\ncommands, FTP commands are readable by people. In order to delineate successive\ncommands, a carriage return and line feed end each command. Each command con-\nsists of four uppercase ASCII characters, some with optional arguments. Some of\nthe more common commands are given below:\n•\nUSER username: Used to send the user identification to the server.\n•\nPASS password: Used to send the user password to the server.\n•\nLIST: Used to ask the server to send back a list of all the files in the current\nremote directory. The list of files is sent over a (new and non-persistent) data\nconnection rather than the control TCP connection.\n•\nRETR filename: Used to retrieve (that is, get) a file from the current direc-\ntory of the remote host. This command causes the remote host to initiate a data\nconnection and to send the requested file over the data connection.\n•\nSTOR filename: Used to store (that is, put) a file into the current directory\nof the remote host.\nThere is typically a one-to-one correspondence between the command that the\nuser issues and the FTP command sent across the control connection. Each com-\nmand is followed by a reply, sent from server to client. The replies are three-digit\nnumbers, with an optional message following the number. This is similar in struc-\nture to the status code and phrase in the status line of the HTTP response message.\nSome typical replies, along with their possible messages, are as follows:\n•\n331 Username OK, password required\n•\n125 Data connection already open; transfer starting\n•\n425 Can’t open data connection\n•\n452 Error writing file\nReaders who are interested in learning about the other FTP commands and replies\nare encouraged to read RFC 959.\n2.4 Electronic Mail in the Internet\nElectronic mail has been around since the beginning of the Internet. It was the most\npopular application when the Internet was in its infancy [Segaller 1998], and has\n118\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nOutgoing\nmessage queue \nKey:\nUser mailbox\nSMTP\nUser agent\nUser agent\nUser agent\nUser agent\nUser agent\nUser agent\nMail server\nMail server\nMail server\nSMTP\nSMTP\nbecome more and more elaborate and powerful over the years. It remains one of the\nInternet’s most important and utilized applications.\nAs with ordinary postal mail, e-mail is an asynchronous communication\nmedium—people send and read messages when it is convenient for them, without\nhaving to coordinate with other people’s schedules. In contrast with postal mail, elec-\ntronic mail is fast, easy to distribute, and inexpensive. Modern e-mail has many pow-\nerful features, including messages with attachments, hyperlinks, HTML-formatted\ntext, and embedded photos.\nIn this section, we examine the application-layer protocols that are at the heart\nof Internet e-mail. But before we jump into an in-depth discussion of these proto-\ncols, let’s take a high-level view of the Internet mail system and its key components.\nFigure 2.16 presents a high-level view of the Internet mail system. We see from\nthis diagram that it has three major components: user agents, mail servers, and the\n2.4\n•\nELECTRONIC MAIL IN THE INTERNET\n119\nFigure 2.16 \u0002 A high-level view of the Internet e-mail system\n\nSimple Mail Transfer Protocol (SMTP). We now describe each of these compo-\nnents in the context of a sender, Alice, sending an e-mail message to a recipient,\nBob. User agents allow users to read, reply to, forward, save, and compose mes-\nsages. Microsoft Outlook and Apple Mail are examples of user agents for e-mail.\nWhen Alice is finished composing her message, her user agent sends the message to\nher mail server, where the message is placed in the mail server’s outgoing message\nqueue. When Bob wants to read a message, his user agent retrieves the message\nfrom his mailbox in his mail server.\nMail servers form the core of the e-mail infrastructure. Each recipient, such as\nBob, has a mailbox located in one of the mail servers. Bob’s mailbox manages and\nmaintains the messages that have been sent to him. A typical message starts its jour-\nney in the sender’s user agent, travels to the sender’s mail server, and \ntravels to the recipient’s mail server, where it is deposited in the recipient’s mailbox.\n120\nCHAPTER 2\n•\nAPPLICATION LAYER\nWEB E-MAIL\nIn December 1995, just a few years after the Web was “invented,” Sabeer Bhatia\nand Jack Smith visited the Internet venture capitalist Draper Fisher Jurvetson and\nproposed developing a free Web-based e-mail system. The idea was to give a free\ne-mail account to anyone who wanted one, and to make the accounts accessible\nfrom the Web. In exchange for 15 percent of the company, Draper Fisher\nJurvetson financed Bhatia and Smith, who formed a company called Hotmail.\nWith three full-time people and 14 part-time people who worked for stock options,\nthey were able to develop and launch the service in July 1996. Within a month\nafter launch, they had 100,000 subscribers. In December 1997, less than 18\nmonths after launching the service, Hotmail had over 12 million subscribers and\nwas acquired by Microsoft, reportedly for $400 million. The success of Hotmail is\noften attributed to its “first-mover advantage” and to the intrinsic “viral marketing”\nof e-mail. (Perhaps some of the students reading this book will be among the new\nentrepreneurs who conceive and develop first-mover Internet services with inherent\nviral marketing.)\nWeb e-mail continues to thrive, becoming more sophisticated and powerful every\nyear. One of the most popular services today is Google’s gmail, which offers giga-\nbytes of free storage, advanced spam filtering and virus detection, e-mail encryption\n(using SSL), mail fetching from third-party e-mail services, and a search-oriented inter-\nface. Asynchronous messaging within social networks, such as Facebook, has also\nbecome popular in recent years.\nCASE HISTORY"
    },
    {
      "chunk_id": "a9469eaf-1232-4db9-b4f7-2813f6d6e64a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.4.1 SMTP",
      "original_titles": [
        "2.4.1 SMTP"
      ],
      "path": "Chapter 2 Application Layer > 2.4 Electronic Mail in the Internet > 2.4.1 SMTP",
      "start_page": 148,
      "end_page": 150,
      "token_count": 1584,
      "text": "When Bob wants to access the messages in his mailbox, the mail server \ncontaining his mailbox authenticates Bob (with usernames and passwords). Alice’s\nmail server must also deal with failures in Bob’s mail server. If Alice’s server can-\nnot deliver mail to Bob’s server, Alice’s server holds the message in a message\nqueue and attempts to transfer the message later. Reattempts are often done every\n30 minutes or so; if there is no success after several days, the server removes the\nmessage and notifies the sender (Alice) with an e-mail message.\nSMTP is the principal application-layer protocol for Internet electronic mail. It\nuses the reliable data transfer service of TCP to transfer mail from the sender’s mail\nserver to the recipient’s mail server. As with most application-layer protocols,\nSMTP has two sides: a client side, which executes on the sender’s mail server, and a\nserver side, which executes on the recipient’s mail server. Both the client and server\nsides of SMTP run on every mail server. When a mail server sends mail to other\nmail servers, it acts as an SMTP client. When a mail server receives mail from other\nmail servers, it acts as an SMTP server.\n2.4.1 SMTP\nSMTP, defined in RFC 5321, is at the heart of Internet electronic mail. As men-\ntioned above, SMTP transfers messages from senders’ mail servers to the recipi-\nents’ mail servers. SMTP is much older than HTTP. (The original SMTP RFC\ndates back to 1982, and SMTP was around long before that.) Although SMTP has\nnumerous wonderful qualities, as evidenced by its ubiquity in the Internet, it is\nnevertheless a legacy technology that possesses certain archaic characteristics.\nFor example, it restricts the body (not just the headers) of all mail messages to\nsimple 7-bit ASCII. This restriction made sense in the early 1980s when trans-\nmission capacity was scarce and no one was e-mailing large attachments or large\nimage, audio, or video files. But today, in the multimedia era, the 7-bit ASCII\nrestriction is a bit of a pain—it requires binary multimedia data to be encoded to\nASCII before being sent over SMTP; and it requires the corresponding ASCII\nmessage to be decoded back to binary after SMTP transport. Recall from Section\n2.2 that HTTP does not require multimedia data to be ASCII encoded before\ntransfer.\nTo illustrate the basic operation of SMTP, let’s walk through a common sce-\nnario. Suppose Alice wants to send Bob a simple ASCII message.\n1. Alice invokes her user agent for e-mail, provides Bob’s e-mail address (for\nexample, bob@someschool.edu), composes a message, and instructs the\nuser agent to send the message.\n2. Alice’s user agent sends the message to her mail server, where it is placed in a\nmessage queue.\n2.4\n•\nELECTRONIC MAIL IN THE INTERNET\n121\n\n3. The client side of SMTP, running on Alice’s mail server, sees the message in\nthe message queue. It opens a TCP connection to an SMTP server, running on\nBob’s mail server.\n4. After some initial SMTP handshaking, the SMTP client sends Alice’s message\ninto the TCP connection.\n5. At Bob’s mail server, the server side of SMTP receives the message. Bob’s\nmail server then places the message in Bob’s mailbox.\n6. Bob invokes his user agent to read the message at his convenience.\nThe scenario is summarized in Figure 2.17.\nIt is important to observe that SMTP does not normally use intermediate mail\nservers for sending mail, even when the two mail servers are located at opposite\nends of the world. If Alice’s server is in Hong Kong and Bob’s server is in St. Louis,\nthe TCP connection is a direct connection between the Hong Kong and St. Louis\nservers. In particular, if Bob’s mail server is down, the message remains in Alice’s\nmail server and waits for a new attempt—the message does not get placed in some\nintermediate mail server.\nLet’s now take a closer look at how SMTP transfers a message from a send-\ning mail server to a receiving mail server. We will see that the SMTP protocol has\nmany similarities with protocols that are used for face-to-face human interaction.\nFirst, the client SMTP (running on the sending mail server host) has TCP estab-\nlish a connection to port 25 at the server SMTP (running on the receiving mail\nserver host). If the server is down, the client tries again later. Once this connec-\ntion is established, the server and client perform some application-layer\nhandshaking—just as humans often introduce themselves before transferring\ninformation from one to another, SMTP clients and servers introduce themselves\nbefore transferring information. During this SMTP handshaking phase, the\n122\nCHAPTER 2\n•\nAPPLICATION LAYER\nSMTP\nAlice’s\nmail server\nBob’s\nmail server\nAlice’s\nagent\nBob’s\nagent\n1\n2\n4\n6\n5\nMessage queue \nKey:\nUser mailbox\n3\nFigure 2.17 \u0002 Alice sends a message to Bob\n\nSMTP client indicates the e-mail address of the sender (the person who generated\nthe message) and the e-mail address of the recipient. Once the SMTP client and\nserver have introduced themselves to each other, the client sends the message.\nSMTP can count on the reliable data transfer service of TCP to get the message\nto the server without errors. The client then repeats this process over the same\nTCP connection if it has other messages to send to the server; otherwise, it\ninstructs TCP to close the connection.\nLet’s next take a look at an example transcript of messages exchanged\nbetween an SMTP client (C) and an SMTP server (S). The hostname of the client\nis crepes.fr\nand the hostname of the server is hamburger.edu. The\nASCII text lines prefaced with C: are exactly the lines the client sends into its\nTCP socket, and the ASCII text lines prefaced with S: are exactly the lines the\nserver sends into its TCP socket. The following transcript begins as soon as the\nTCP connection is established.\nS: 220 hamburger.edu\nC: HELO crepes.fr\nS: 250 Hello crepes.fr, pleased to meet you\nC: MAIL FROM: <alice@crepes.fr>\nS: 250 alice@crepes.fr ... Sender ok\nC: RCPT TO: <bob@hamburger.edu>\nS: 250 bob@hamburger.edu ... Recipient ok\nC: DATA\nS: 354 Enter mail, end with “.” on a line by itself\nC: Do you like ketchup?\nC: How about pickles?\nC: .\nS: 250 Message accepted for delivery\nC: QUIT\nS: 221 hamburger.edu closing connection\nIn the example above, the client sends a message (“Do you like ketchup?\nHow about pickles?”) from mail server crepes.fr to mail server ham-\nburger.edu. As part of the dialogue, the client issued five commands: HELO (an\nabbreviation for HELLO), MAIL FROM, RCPT TO, DATA, and QUIT. These com-\nmands are self-explanatory. The client also sends a line consisting of a single period,\nwhich indicates the end of the message to the server. (In ASCII jargon, each mes-\nsage ends with CRLF.CRLF, where CR and LF stand for carriage return and line\nfeed, respectively.) The server issues replies to each command, with each reply hav-\ning a reply code and some (optional) English-language explanation. We mention\nhere that SMTP uses persistent connections: If the sending mail server has several\nmessages to send to the same receiving mail server, it can send all of the messages\nover the same TCP connection. For each message, the client begins the process with\n2.4\n•\nELECTRONIC MAIL IN THE INTERNET\n123"
    },
    {
      "chunk_id": "c9cf3d02-de32-4976-bb8a-6a04cbe668c8",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.4.2 Comparison with HTTP",
      "original_titles": [
        "2.4.2 Comparison with HTTP"
      ],
      "path": "Chapter 2 Application Layer > 2.4 Electronic Mail in the Internet > 2.4.2 Comparison with HTTP",
      "start_page": 151,
      "end_page": 151,
      "token_count": 578,
      "text": "a new MAIL FROM: crepes.fr, designates the end of message with an isolated\nperiod, and issues QUIT only after all messages have been sent.\nIt is highly recommended that you use Telnet to carry out a direct dialogue with\nan SMTP server. To do this, issue\ntelnet serverName 25\nwhere serverName is the name of a local mail server. When you do this, you are\nsimply establishing a TCP connection between your local host and the mail server.\nAfter typing this line, you should immediately receive the 220 reply from the\nserver. Then issue the SMTP commands HELO, MAIL FROM, RCPT TO, DATA,\nCRLF.CRLF, and QUIT at the appropriate times. It is also highly recommended\nthat you do Programming Assignment 3 at the end of this chapter. In that assign-\nment, you’ll build a simple user agent that implements the client side of SMTP.\nIt will allow you to send an e-mail message to an arbitrary recipient via a local\nmail server.\n2.4.2 Comparison with HTTP\nLet’s now briefly compare SMTP with HTTP. Both protocols are used to transfer\nfiles from one host to another: HTTP transfers files (also called objects) from a Web\nserver to a Web client (typically a browser); SMTP transfers files (that is, e-mail\nmessages) from one mail server to another mail server. When transferring the files,\nboth persistent HTTP and SMTP use persistent connections. Thus, the two protocols\nhave common characteristics. However, there are important differences. First,\nHTTP is mainly a pull protocol—someone loads information on a Web server and\nusers use HTTP to pull the information from the server at their convenience. In par-\nticular, the TCP connection is initiated by the machine that wants to receive the file.\nOn the other hand, SMTP is primarily a push protocol—the sending mail server\npushes the file to the receiving mail server. In particular, the TCP connection is ini-\ntiated by the machine that wants to send the file.\nA second difference, which we alluded to earlier, is that SMTP requires\neach message, including the body of each message, to be in 7-bit ASCII format.\nIf the message contains characters that are not 7-bit ASCII (for example, French\ncharacters with accents) or contains binary data (such as an image file), then the\nmessage has to be encoded into 7-bit ASCII. HTTP data does not impose this\nrestriction.\nA third important difference concerns how a document consisting of text and\nimages (along with possibly other media types) is handled. As we learned in Section\n2.2, HTTP encapsulates each object in its own HTTP response message. Internet\nmail places all of the message’s objects into one message.\n124\nCHAPTER 2\n•\nAPPLICATION LAYER"
    },
    {
      "chunk_id": "76bf9d8f-2fed-4c49-9581-c7d020c7b64f",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.4.3 Mail Message Format",
      "original_titles": [
        "2.4.3 Mail Message Format"
      ],
      "path": "Chapter 2 Application Layer > 2.4 Electronic Mail in the Internet > 2.4.3 Mail Message Format",
      "start_page": 152,
      "end_page": 156,
      "token_count": 2796,
      "text": "2.4.3 Mail Message Formats\nWhen Alice writes an ordinary snail-mail letter to Bob, she may include all kinds of\nperipheral header information at the top of the letter, such as Bob’s address, her own\nreturn address, and the date. Similarly, when an e-mail message is sent from one per-\nson to another, a header containing peripheral information precedes the body of the\nmessage itself. This peripheral information is contained in a series of header lines,\nwhich are defined in RFC 5322. The header lines and the body of the message are\nseparated by a blank line (that is, by CRLF). RFC 5322 specifies the exact format for\nmail header lines as well as their semantic interpretations. As with HTTP, each header\nline contains readable text, consisting of a keyword followed by a colon followed by\na value. Some of the keywords are required and others are optional. Every header\nmust have a From: header line and a To: header line; a header may include a Sub-\nject: header line as well as other optional header lines. It is important to note that\nthese header lines are different from the SMTP commands we studied in Section 2.4.1\n(even though they contain some common words such as “from” and “to”). The com-\nmands in that section were part of the SMTP handshaking protocol; the header lines\nexamined in this section are part of the mail message itself.\nA typical message header looks like this:\nFrom: alice@crepes.fr\nTo: bob@hamburger.edu\nSubject: Searching for the meaning of life.\nAfter the message header, a blank line follows; then the message body (in ASCII)\nfollows. You should use Telnet to send a message to a mail server that contains some\nheader lines, including the Subject: header line. To do this, issue telnet\nserverName 25, as discussed in Section 2.4.1.\n2.4.4 Mail Access Protocols\nOnce SMTP delivers the message from Alice’s mail server to Bob’s mail server, the\nmessage is placed in Bob’s mailbox. Throughout this discussion we have tacitly\nassumed that Bob reads his mail by logging onto the server host and then executing\na mail reader that runs on that host. Up until the early 1990s this was the standard\nway of doing things. But today, mail access uses a client-server architecture—the\ntypical user reads e-mail with a client that executes on the user’s end system, for\nexample, on an office PC, a laptop, or a smartphone. By executing a mail client on a\nlocal PC, users enjoy a rich set of features, including the ability to view multimedia\nmessages and attachments.\nGiven that Bob (the recipient) executes his user agent on his local PC, it is nat-\nural to consider placing a mail server on his local PC as well. With this approach,\n2.4\n•\nELECTRONIC MAIL IN THE INTERNET\n125\n\nAlice’s mail server would dialogue directly with Bob’s PC. There is a problem with\nthis approach, however. Recall that a mail server manages mailboxes and runs the\nclient and server sides of SMTP. If Bob’s mail server were to reside on his local PC,\nthen Bob’s PC would have to remain always on, and connected to the Internet, in\norder to receive new mail, which can arrive at any time. This is impractical for many\nInternet users. Instead, a typical user runs a user agent on the local PC but accesses\nits mailbox stored on an always-on shared mail server. This mail server is shared\nwith other users and is typically maintained by the user’s ISP (for example, univer-\nsity or company).\nNow let’s consider the path an e-mail message takes when it is sent from Alice\nto Bob. We just learned that at some point along the path the e-mail message needs\nto be deposited in Bob’s mail server. This could be done simply by having Alice’s\nuser agent send the message directly to Bob’s mail server. And this could be done\nwith SMTP—indeed, SMTP has been designed for pushing e-mail from one host to\nanother. However, typically the sender’s user agent does not dialogue directly with\nthe recipient’s mail server. Instead, as shown in Figure 2.18, Alice’s user agent uses\nSMTP to push the e-mail message into her mail server, then Alice’s mail server uses\nSMTP (as an SMTP client) to relay the e-mail message to Bob’s mail server. Why\nthe two-step procedure? Primarily because without relaying through Alice’s mail\nserver, Alice’s user agent doesn’t have any recourse to an unreachable destination\nmail server. By having Alice first deposit the e-mail in her own mail server, Alice’s\nmail server can repeatedly try to send the message to Bob’s mail server, say every\n30 minutes, until Bob’s mail server becomes operational. (And if Alice’s mail server\nis down, then she has the recourse of complaining to her system administrator!) The\nSMTP RFC defines how the SMTP commands can be used to relay a message\nacross multiple SMTP servers.\nBut there is still one missing piece to the puzzle! How does a recipient like Bob,\nrunning a user agent on his local PC, obtain his messages, which are sitting in a mail\nserver within Bob’s ISP? Note that Bob’s user agent can’t use SMTP to obtain the\nmessages because obtaining the messages is a pull operation, whereas SMTP is a\n126\nCHAPTER 2\n•\nAPPLICATION LAYER\nSMTP\nAlice’s\nmail server\nBob’s\nmail server\nAlice’s\nagent\nBob’s\nagent\nSMTP\nPOP3,\nIMAP, or\nHTTP\nFigure 2.18 \u0002 E-mail protocols and their communicating entities\n\npush protocol. The puzzle is completed by introducing a special mail access proto-\ncol that transfers messages from Bob’s mail server to his local PC. There are cur-\nrently a number of popular mail access protocols, including Post Office\nProtocol—Version 3 (POP3), Internet Mail Access Protocol (IMAP), and HTTP.\nFigure 2.18 provides a summary of the protocols that are used for Internet mail:\nSMTP is used to transfer mail from the sender’s mail server to the recipient’s mail\nserver; SMTP is also used to transfer mail from the sender’s user agent to the\nsender’s mail server. A mail access protocol, such as POP3, is used to transfer mail\nfrom the recipient’s mail server to the recipient’s user agent.\nPOP3\nPOP3 is an extremely simple mail access protocol. It is defined in [RFC 1939], which\nis short and quite readable. Because the protocol is so simple, its functionality is\nrather limited. POP3 begins when the user agent (the client) opens a TCP connec-\ntion to the mail server (the server) on port 110. With the TCP connection estab-\nlished, POP3 progresses through three phases: authorization, transaction, and update.\nDuring the first phase, authorization, the user agent sends a username and a password\n(in the clear) to authenticate the user. During the second phase, transaction, the user\nagent retrieves messages; also during this phase, the user agent can mark messages\nfor deletion, remove deletion marks, and obtain mail statistics. The third phase,\nupdate, occurs after the client has issued the quit command, ending the POP3\nsession; at this time, the mail server deletes the messages that were marked for\ndeletion.\nIn a POP3 transaction, the user agent issues commands, and the server responds\nto each command with a reply. There are two possible responses: +OK (sometimes\nfollowed by server-to-client data), used by the server to indicate that the previous\ncommand was fine; and -ERR, used by the server to indicate that something was\nwrong with the previous command.\nThe authorization phase has two principal commands: user <username> and\npass <password>. To illustrate these two commands, we suggest that you Telnet\ndirectly into a POP3 server, using port 110, and issue these commands. Suppose that\nmailServer is the name of your mail server. You will see something like:\ntelnet mailServer 110\n+OK POP3 server ready\nuser bob\n+OK\npass hungry\n+OK user successfully logged on\nIf you misspell a command, the POP3 server will reply with an -ERR message.\n2.4\n•\nELECTRONIC MAIL IN THE INTERNET\n127\n\nNow let’s take a look at the transaction phase. A user agent using POP3 can\noften be configured (by the user) to “download and delete” or to “download and\nkeep.” The sequence of commands issued by a POP3 user agent depends on which\nof these two modes the user agent is operating in. In the download-and-delete mode,\nthe user agent will issue the list, retr, and dele commands. As an example,\nsuppose the user has two messages in his or her mailbox. In the dialogue below, C:\n(standing for client) is the user agent and S: (standing for server) is the mail server.\nThe transaction will look something like:\nC: list\nS: 1 498\nS: 2 912\nS: .\nC: retr 1\nS: (blah blah ...\nS: .................\nS: ..........blah)\nS: .\nC: dele 1\nC: retr 2\nS: (blah blah ...\nS: .................\nS: ..........blah)\nS: .\nC: dele 2\nC: quit\nS: +OK POP3 server signing off\nThe user agent first asks the mail server to list the size of each of the stored mes-\nsages. The user agent then retrieves and deletes each message from the server. Note\nthat after the authorization phase, the user agent employed only four commands:\nlist, retr, dele, and quit. The syntax for these commands is defined in RFC\n1939. After processing the quit command, the POP3 server enters the update\nphase and removes messages 1 and 2 from the mailbox.\nA problem with this download-and-delete mode is that the recipient, Bob, may\nbe nomadic and may want to access his mail messages from multiple machines, for\nexample, his office PC, his home PC, and his portable computer. The download-\nand-delete mode partitions Bob’s mail messages over these three machines; in par-\nticular, if Bob first reads a message on his office PC, he will not be able to reread\nthe message from his portable at home later in the evening. In the download-and-\nkeep mode, the user agent leaves the messages on the mail server after downloading\nthem. In this case, Bob can reread messages from different machines; he can access\na message from work and access it again later in the week from home.\n128\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nDuring a POP3 session between a user agent and the mail server, the POP3\nserver maintains some state information; in particular, it keeps track of which user\nmessages have been marked deleted. However, the POP3 server does not carry state\ninformation across POP3 sessions. This lack of state information across sessions\ngreatly simplifies the implementation of a POP3 server.\nIMAP\nWith POP3 access, once Bob has downloaded his messages to the local machine,\nhe can create mail folders and move the downloaded messages into the folders.\nBob can then delete messages, move messages across folders, and search for\nmessages (by sender name or subject). But this paradigm—namely, folders and\nmessages in the local machine—poses a problem for the nomadic user, who\nwould prefer to maintain a folder hierarchy on a remote server that can be\naccessed from any computer. This is not possible with POP3—the POP3 protocol\ndoes not provide any means for a user to create remote folders and assign mes-\nsages to folders.\nTo solve this and other problems, the IMAP protocol, defined in [RFC 3501],\nwas invented. Like POP3, IMAP is a mail access protocol. It has many more fea-\ntures than POP3, but it is also significantly more complex. (And thus the client and\nserver side implementations are significantly more complex.)\nAn IMAP server will associate each message with a folder; when a message first\narrives at the server, it is associated with the recipient’s INBOX folder. The recipient\ncan then move the message into a new, user-created folder, read the message, delete\nthe message, and so on. The IMAP protocol provides commands to allow users to\ncreate folders and move messages from one folder to another. IMAP also provides\ncommands that allow users to search remote folders for messages matching specific\ncriteria. Note that, unlike POP3, an IMAP server maintains user state information\nacross IMAP sessions—for example, the names of the folders and which messages\nare associated with which folders.\nAnother important feature of IMAP is that it has commands that permit a user\nagent to obtain components of messages. For example, a user agent can obtain just\nthe message header of a message or just one part of a multipart MIME message.\nThis feature is useful when there is a low-bandwidth connection (for example, a\nslow-speed modem link) between the user agent and its mail server. With a low-\nbandwidth connection, the user may not want to download all of the messages in\nits mailbox, particularly avoiding long messages that might contain, for example,\nan audio or video clip.\nWeb-Based E-Mail\nMore and more users today are sending and accessing their e-mail through their Web\nbrowsers. Hotmail introduced Web-based access in the mid 1990s. Now Web-based\n2.4\n•\nELECTRONIC MAIL IN THE INTERNET\n129"
    },
    {
      "chunk_id": "1417c9aa-1882-4785-b6fc-fc59d4c1a6d1",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.4.4 Mail Access Protocols",
      "original_titles": [
        "2.4.4 Mail Access Protocols"
      ],
      "path": "Chapter 2 Application Layer > 2.4 Electronic Mail in the Internet > 2.4.4 Mail Access Protocols",
      "start_page": 152,
      "end_page": 156,
      "token_count": 2796,
      "text": "2.4.3 Mail Message Formats\nWhen Alice writes an ordinary snail-mail letter to Bob, she may include all kinds of\nperipheral header information at the top of the letter, such as Bob’s address, her own\nreturn address, and the date. Similarly, when an e-mail message is sent from one per-\nson to another, a header containing peripheral information precedes the body of the\nmessage itself. This peripheral information is contained in a series of header lines,\nwhich are defined in RFC 5322. The header lines and the body of the message are\nseparated by a blank line (that is, by CRLF). RFC 5322 specifies the exact format for\nmail header lines as well as their semantic interpretations. As with HTTP, each header\nline contains readable text, consisting of a keyword followed by a colon followed by\na value. Some of the keywords are required and others are optional. Every header\nmust have a From: header line and a To: header line; a header may include a Sub-\nject: header line as well as other optional header lines. It is important to note that\nthese header lines are different from the SMTP commands we studied in Section 2.4.1\n(even though they contain some common words such as “from” and “to”). The com-\nmands in that section were part of the SMTP handshaking protocol; the header lines\nexamined in this section are part of the mail message itself.\nA typical message header looks like this:\nFrom: alice@crepes.fr\nTo: bob@hamburger.edu\nSubject: Searching for the meaning of life.\nAfter the message header, a blank line follows; then the message body (in ASCII)\nfollows. You should use Telnet to send a message to a mail server that contains some\nheader lines, including the Subject: header line. To do this, issue telnet\nserverName 25, as discussed in Section 2.4.1.\n2.4.4 Mail Access Protocols\nOnce SMTP delivers the message from Alice’s mail server to Bob’s mail server, the\nmessage is placed in Bob’s mailbox. Throughout this discussion we have tacitly\nassumed that Bob reads his mail by logging onto the server host and then executing\na mail reader that runs on that host. Up until the early 1990s this was the standard\nway of doing things. But today, mail access uses a client-server architecture—the\ntypical user reads e-mail with a client that executes on the user’s end system, for\nexample, on an office PC, a laptop, or a smartphone. By executing a mail client on a\nlocal PC, users enjoy a rich set of features, including the ability to view multimedia\nmessages and attachments.\nGiven that Bob (the recipient) executes his user agent on his local PC, it is nat-\nural to consider placing a mail server on his local PC as well. With this approach,\n2.4\n•\nELECTRONIC MAIL IN THE INTERNET\n125\n\nAlice’s mail server would dialogue directly with Bob’s PC. There is a problem with\nthis approach, however. Recall that a mail server manages mailboxes and runs the\nclient and server sides of SMTP. If Bob’s mail server were to reside on his local PC,\nthen Bob’s PC would have to remain always on, and connected to the Internet, in\norder to receive new mail, which can arrive at any time. This is impractical for many\nInternet users. Instead, a typical user runs a user agent on the local PC but accesses\nits mailbox stored on an always-on shared mail server. This mail server is shared\nwith other users and is typically maintained by the user’s ISP (for example, univer-\nsity or company).\nNow let’s consider the path an e-mail message takes when it is sent from Alice\nto Bob. We just learned that at some point along the path the e-mail message needs\nto be deposited in Bob’s mail server. This could be done simply by having Alice’s\nuser agent send the message directly to Bob’s mail server. And this could be done\nwith SMTP—indeed, SMTP has been designed for pushing e-mail from one host to\nanother. However, typically the sender’s user agent does not dialogue directly with\nthe recipient’s mail server. Instead, as shown in Figure 2.18, Alice’s user agent uses\nSMTP to push the e-mail message into her mail server, then Alice’s mail server uses\nSMTP (as an SMTP client) to relay the e-mail message to Bob’s mail server. Why\nthe two-step procedure? Primarily because without relaying through Alice’s mail\nserver, Alice’s user agent doesn’t have any recourse to an unreachable destination\nmail server. By having Alice first deposit the e-mail in her own mail server, Alice’s\nmail server can repeatedly try to send the message to Bob’s mail server, say every\n30 minutes, until Bob’s mail server becomes operational. (And if Alice’s mail server\nis down, then she has the recourse of complaining to her system administrator!) The\nSMTP RFC defines how the SMTP commands can be used to relay a message\nacross multiple SMTP servers.\nBut there is still one missing piece to the puzzle! How does a recipient like Bob,\nrunning a user agent on his local PC, obtain his messages, which are sitting in a mail\nserver within Bob’s ISP? Note that Bob’s user agent can’t use SMTP to obtain the\nmessages because obtaining the messages is a pull operation, whereas SMTP is a\n126\nCHAPTER 2\n•\nAPPLICATION LAYER\nSMTP\nAlice’s\nmail server\nBob’s\nmail server\nAlice’s\nagent\nBob’s\nagent\nSMTP\nPOP3,\nIMAP, or\nHTTP\nFigure 2.18 \u0002 E-mail protocols and their communicating entities\n\npush protocol. The puzzle is completed by introducing a special mail access proto-\ncol that transfers messages from Bob’s mail server to his local PC. There are cur-\nrently a number of popular mail access protocols, including Post Office\nProtocol—Version 3 (POP3), Internet Mail Access Protocol (IMAP), and HTTP.\nFigure 2.18 provides a summary of the protocols that are used for Internet mail:\nSMTP is used to transfer mail from the sender’s mail server to the recipient’s mail\nserver; SMTP is also used to transfer mail from the sender’s user agent to the\nsender’s mail server. A mail access protocol, such as POP3, is used to transfer mail\nfrom the recipient’s mail server to the recipient’s user agent.\nPOP3\nPOP3 is an extremely simple mail access protocol. It is defined in [RFC 1939], which\nis short and quite readable. Because the protocol is so simple, its functionality is\nrather limited. POP3 begins when the user agent (the client) opens a TCP connec-\ntion to the mail server (the server) on port 110. With the TCP connection estab-\nlished, POP3 progresses through three phases: authorization, transaction, and update.\nDuring the first phase, authorization, the user agent sends a username and a password\n(in the clear) to authenticate the user. During the second phase, transaction, the user\nagent retrieves messages; also during this phase, the user agent can mark messages\nfor deletion, remove deletion marks, and obtain mail statistics. The third phase,\nupdate, occurs after the client has issued the quit command, ending the POP3\nsession; at this time, the mail server deletes the messages that were marked for\ndeletion.\nIn a POP3 transaction, the user agent issues commands, and the server responds\nto each command with a reply. There are two possible responses: +OK (sometimes\nfollowed by server-to-client data), used by the server to indicate that the previous\ncommand was fine; and -ERR, used by the server to indicate that something was\nwrong with the previous command.\nThe authorization phase has two principal commands: user <username> and\npass <password>. To illustrate these two commands, we suggest that you Telnet\ndirectly into a POP3 server, using port 110, and issue these commands. Suppose that\nmailServer is the name of your mail server. You will see something like:\ntelnet mailServer 110\n+OK POP3 server ready\nuser bob\n+OK\npass hungry\n+OK user successfully logged on\nIf you misspell a command, the POP3 server will reply with an -ERR message.\n2.4\n•\nELECTRONIC MAIL IN THE INTERNET\n127\n\nNow let’s take a look at the transaction phase. A user agent using POP3 can\noften be configured (by the user) to “download and delete” or to “download and\nkeep.” The sequence of commands issued by a POP3 user agent depends on which\nof these two modes the user agent is operating in. In the download-and-delete mode,\nthe user agent will issue the list, retr, and dele commands. As an example,\nsuppose the user has two messages in his or her mailbox. In the dialogue below, C:\n(standing for client) is the user agent and S: (standing for server) is the mail server.\nThe transaction will look something like:\nC: list\nS: 1 498\nS: 2 912\nS: .\nC: retr 1\nS: (blah blah ...\nS: .................\nS: ..........blah)\nS: .\nC: dele 1\nC: retr 2\nS: (blah blah ...\nS: .................\nS: ..........blah)\nS: .\nC: dele 2\nC: quit\nS: +OK POP3 server signing off\nThe user agent first asks the mail server to list the size of each of the stored mes-\nsages. The user agent then retrieves and deletes each message from the server. Note\nthat after the authorization phase, the user agent employed only four commands:\nlist, retr, dele, and quit. The syntax for these commands is defined in RFC\n1939. After processing the quit command, the POP3 server enters the update\nphase and removes messages 1 and 2 from the mailbox.\nA problem with this download-and-delete mode is that the recipient, Bob, may\nbe nomadic and may want to access his mail messages from multiple machines, for\nexample, his office PC, his home PC, and his portable computer. The download-\nand-delete mode partitions Bob’s mail messages over these three machines; in par-\nticular, if Bob first reads a message on his office PC, he will not be able to reread\nthe message from his portable at home later in the evening. In the download-and-\nkeep mode, the user agent leaves the messages on the mail server after downloading\nthem. In this case, Bob can reread messages from different machines; he can access\na message from work and access it again later in the week from home.\n128\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nDuring a POP3 session between a user agent and the mail server, the POP3\nserver maintains some state information; in particular, it keeps track of which user\nmessages have been marked deleted. However, the POP3 server does not carry state\ninformation across POP3 sessions. This lack of state information across sessions\ngreatly simplifies the implementation of a POP3 server.\nIMAP\nWith POP3 access, once Bob has downloaded his messages to the local machine,\nhe can create mail folders and move the downloaded messages into the folders.\nBob can then delete messages, move messages across folders, and search for\nmessages (by sender name or subject). But this paradigm—namely, folders and\nmessages in the local machine—poses a problem for the nomadic user, who\nwould prefer to maintain a folder hierarchy on a remote server that can be\naccessed from any computer. This is not possible with POP3—the POP3 protocol\ndoes not provide any means for a user to create remote folders and assign mes-\nsages to folders.\nTo solve this and other problems, the IMAP protocol, defined in [RFC 3501],\nwas invented. Like POP3, IMAP is a mail access protocol. It has many more fea-\ntures than POP3, but it is also significantly more complex. (And thus the client and\nserver side implementations are significantly more complex.)\nAn IMAP server will associate each message with a folder; when a message first\narrives at the server, it is associated with the recipient’s INBOX folder. The recipient\ncan then move the message into a new, user-created folder, read the message, delete\nthe message, and so on. The IMAP protocol provides commands to allow users to\ncreate folders and move messages from one folder to another. IMAP also provides\ncommands that allow users to search remote folders for messages matching specific\ncriteria. Note that, unlike POP3, an IMAP server maintains user state information\nacross IMAP sessions—for example, the names of the folders and which messages\nare associated with which folders.\nAnother important feature of IMAP is that it has commands that permit a user\nagent to obtain components of messages. For example, a user agent can obtain just\nthe message header of a message or just one part of a multipart MIME message.\nThis feature is useful when there is a low-bandwidth connection (for example, a\nslow-speed modem link) between the user agent and its mail server. With a low-\nbandwidth connection, the user may not want to download all of the messages in\nits mailbox, particularly avoiding long messages that might contain, for example,\nan audio or video clip.\nWeb-Based E-Mail\nMore and more users today are sending and accessing their e-mail through their Web\nbrowsers. Hotmail introduced Web-based access in the mid 1990s. Now Web-based\n2.4\n•\nELECTRONIC MAIL IN THE INTERNET\n129"
    },
    {
      "chunk_id": "674ecc2f-eee3-419a-9fee-695323f54df3",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.5 DNS—The Internet’s Directory Service",
      "original_titles": [
        "2.5 DNS—The Internet’s Directory Service"
      ],
      "path": "Chapter 2 Application Layer > 2.5 DNS—The Internet’s Directory Service",
      "start_page": 157,
      "end_page": 157,
      "token_count": 642,
      "text": "e-mail is also provided by Google, Yahoo!, as well as just about every major univer-\nsity and corporation. With this service, the user agent is an ordinary Web browser,\nand the user communicates with its remote mailbox via HTTP. When a recipient,\nsuch as Bob, wants to access a message in his mailbox, the e-mail message is sent\nfrom Bob’s mail server to Bob’s browser using the HTTP protocol rather than the\nPOP3 or IMAP protocol. When a sender, such as Alice, wants to send an e-mail\nmessage, the e-mail message is sent from her browser to her mail server over HTTP\nrather than over SMTP. Alice’s mail server, however, still sends messages to, and\nreceives messages from, other mail servers using SMTP.\n2.5 DNS—The Internet’s Directory Service\nWe human beings can be identified in many ways. For example, we can be identi-\nfied by the names that appear on our birth certificates. We can be identified by our\nsocial security numbers. We can be identified by our driver’s license numbers.\nAlthough each of these identifiers can be used to identify people, within a given\ncontext one identifier may be more appropriate than another. For example, the com-\nputers at the IRS (the infamous tax-collecting agency in the United States) prefer to\nuse fixed-length social security numbers rather than birth certificate names. On the\nother hand, ordinary people prefer the more mnemonic birth certificate names rather\nthan social security numbers. (Indeed, can you imagine saying, “Hi. My name is\n132-67-9875. Please meet my husband, 178-87-1146.”)\nJust as humans can be identified in many ways, so too can Internet hosts. One identi-\nfier for a host is its hostname. Hostnames—such as cnn.com, www.yahoo.\ncom, gaia.cs.umass.edu, and cis.poly.edu—are mnemonic and are there-\nfore appreciated by humans. However, hostnames provide little, if any, information about\nthe location within the Internet of the host. (A hostname such as www.eurecom.fr,\nwhich ends with the country code .fr, tells us that the host is probably in France, but\ndoesn’t say much more.) Furthermore, because hostnames can consist of variable-\nlength alphanumeric characters, they would be difficult to process by routers. For these\nreasons, hosts are also identified by so-called IP addresses.\nWe discuss IP addresses in some detail in Chapter 4, but it is useful to say a few\nbrief words about them now. An IP address consists of four bytes and has a rigid\nhierarchical structure. An IP address looks like 121.7.106.83, where each\nperiod separates one of the bytes expressed in decimal notation from 0 to 255. An IP\naddress is hierarchical because as we scan the address from left to right, we obtain\nmore and more specific information about where the host is located in the Internet\n(that is, within which network, in the network of networks). Similarly, when we scan\na postal address from bottom to top, we obtain more and more specific information\nabout where the addressee is located.\n130\nCHAPTER 2\n•\nAPPLICATION LAYER"
    },
    {
      "chunk_id": "39d468e6-489b-40c0-98ab-11298950bef8",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.5.1 Services Provided by DNS",
      "original_titles": [
        "2.5.1 Services Provided by DNS"
      ],
      "path": "Chapter 2 Application Layer > 2.5 DNS—The Internet’s Directory Service > 2.5.1 Services Provided by DNS",
      "start_page": 158,
      "end_page": 159,
      "token_count": 1150,
      "text": "2.5.1 Services Provided by DNS\nWe have just seen that there are two ways to identify a host—by a hostname and by\nan IP address. People prefer the more mnemonic hostname identifier, while routers\nprefer fixed-length, hierarchically structured IP addresses. In order to reconcile\nthese preferences, we need a directory service that translates hostnames to IP\naddresses. This is the main task of the Internet’s domain name system (DNS). The\nDNS is (1) a distributed database implemented in a hierarchy of DNS servers, and\n(2) an application-layer protocol that allows hosts to query the distributed database.\nThe DNS servers are often UNIX machines running the Berkeley Internet Name\nDomain (BIND) software [BIND 2012]. The DNS protocol runs over UDP and uses\nport 53.\nDNS is commonly employed by other application-layer protocols—including\nHTTP, SMTP, and FTP—to translate user-supplied hostnames to IP addresses. As\nan example, consider what happens when a browser (that is, an HTTP client),\nrunning on some user’s host, requests the URL www.someschool.edu/\nindex.html. In order for the user’s host to be able to send an HTTP request mes-\nsage to the Web server www.someschool.edu, the user’s host must first obtain\nthe IP address of www.someschool.edu. This is done as follows.\n1. The same user machine runs the client side of the DNS application.\n2. The browser extracts the hostname, www.someschool.edu, from the URL\nand passes the hostname to the client side of the DNS application.\n3. The DNS client sends a query containing the hostname to a DNS server.\n4. The DNS client eventually receives a reply, which includes the IP address for\nthe hostname.\n5. Once the browser receives the IP address from DNS, it can initiate a TCP con-\nnection to the HTTP server process located at port 80 at that IP address.\nWe see from this example that DNS adds an additional delay—sometimes substan-\ntial—to the Internet applications that use it. Fortunately, as we discuss below, the\ndesired IP address is often cached in a “nearby” DNS server, which helps to reduce\nDNS network traffic as well as the average DNS delay.\nDNS provides a few other important services in addition to translating host-\nnames to IP addresses:\n•\nHost aliasing. A host with a complicated hostname can have one or more alias\nnames. For example, a hostname such as relay1.west-coast.enter-\nprise.com could have, say, two aliases such as enterprise.com and\nwww.enterprise.com. In this case, the hostname relay1.west-\ncoast.enterprise.com is said to be a canonical hostname. Alias host-\nnames, when present, are typically more mnemonic than canonical hostnames.\n2.5\n•\nDNS—THE INTERNET’S DIRECTORY SERVICE\n131\n\nDNS can be invoked by an application to obtain the canonical hostname for a\nsupplied alias hostname as well as the IP address of the host.\n•\nMail server aliasing. For obvious reasons, it is highly desirable that e-mail\naddresses be mnemonic. For example, if Bob has an account with Hotmail, Bob’s\ne-mail address might be as simple as bob@hotmail.com. However, the host-\nname of the Hotmail mail server is more complicated and much less mnemonic\nthan simply hotmail.com (for example, the canonical hostname might be\nsomething like relay1.west-coast.hotmail.com). DNS can be\ninvoked by a mail application to obtain the canonical hostname for a supplied\nalias hostname as well as the IP address of the host. In fact, the MX record (see\nbelow) permits a company’s mail server and Web server to have identical\n(aliased) hostnames; for example, a company’s Web server and mail server can\nboth be called enterprise.com.\n•\nLoad distribution. DNS is also used to perform load distribution among repli-\ncated servers, such as replicated Web servers. Busy sites, such as cnn.com, are\nreplicated over multiple servers, with each server running on a different end sys-\ntem and each having a different IP address. For replicated Web servers, a set of\nIP addresses is thus associated with one canonical hostname. The DNS database\ncontains this set of IP addresses. When clients make a DNS query for a name\nmapped to a set of addresses, the server responds with the entire set of IP\naddresses, but rotates the ordering of the addresses within each reply. Because a\nclient typically sends its HTTP request message to the IP address that is listed\nfirst in the set, DNS rotation distributes the traffic among the replicated servers.\n132\nCHAPTER 2\n•\nAPPLICATION LAYER\nDNS: CRITICAL NETWORK FUNCTIONS VIA THE CLIENT-SERVER \nPARADIGM\nLike HTTP, FTP, and SMTP, the DNS protocol is an application-layer protocol since it (1) runs\nbetween communicating end systems using the client-server paradigm and (2) relies on an\nunderlying end-to-end transport protocol to transfer DNS messages between communicating\nend systems. In another sense, however, the role of the DNS is quite different from Web,\nfile transfer, and e-mail applications. Unlike these applications, the DNS is not an applica-\ntion with which a user directly interacts. Instead, the DNS provides a core Internet func-\ntion—namely, translating hostnames to their underlying IP addresses, for user applications\nand other software in the Internet. We noted in Section 1.2 that much of the complexity in\nthe Internet architecture is located at the “edges” of the network. The DNS, which imple-\nments the critical name-to-address translation process using clients and servers located at\nthe edge of the network, is yet another example of that design philosophy.\nPRINCIPLES IN PRACTICE"
    },
    {
      "chunk_id": "0b455ec8-7e0a-46b0-ae07-37a5d09ae96c",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.5.2 Overview of How DNS Works",
      "original_titles": [
        "2.5.2 Overview of How DNS Works"
      ],
      "path": "Chapter 2 Application Layer > 2.5 DNS—The Internet’s Directory Service > 2.5.2 Overview of How DNS Works",
      "start_page": 160,
      "end_page": 165,
      "token_count": 2671,
      "text": "DNS rotation is also used for e-mail so that multiple mail servers can have the\nsame alias name. Also, content distribution companies such as Akamai have used\nDNS in more sophisticated ways [Dilley 2002] to provide Web content distribu-\ntion (see Chapter 7).\nThe DNS is specified in RFC 1034 and RFC 1035, and updated in several\nadditional RFCs. It is a complex system, and we only touch upon key aspects of\nits operation here. The interested reader is referred to these RFCs and the book\nby Albitz and Liu [Albitz 1993]; see also the retrospective paper [Mockapetris\n1988], which provides a nice description of the what and why of DNS, and\n[Mockapetris 2005].\n2.5.2 Overview of How DNS Works\nWe now present a high-level overview of how DNS works. Our discussion will\nfocus on the hostname-to-IP-address translation service.\nSuppose that some application (such as a Web browser or a mail reader) run-\nning in a user’s host needs to translate a hostname to an IP address. The applica-\ntion will invoke the client side of DNS, specifying the hostname that needs to be\ntranslated. (On many UNIX-based machines, gethostbyname() is the func-\ntion call that an application calls in order to perform the translation.) DNS in the\nuser’s host then takes over, sending a query message into the network. All DNS\nquery and reply messages are sent within UDP datagrams to port 53. After a delay,\nranging from milliseconds to seconds, DNS in the user’s host receives a DNS\nreply message that provides the desired mapping. This mapping is then passed to\nthe invoking application. Thus, from the perspective of the invoking application\nin the user’s host, DNS is a black box providing a simple, straightforward transla-\ntion service. But in fact, the black box that implements the service is complex,\nconsisting of a large number of DNS servers distributed around the globe, as well\nas an application-layer protocol that specifies how the DNS servers and querying\nhosts communicate.\nA simple design for DNS would have one DNS server that contains all the map-\npings. In this centralized design, clients simply direct all queries to the single DNS\nserver, and the DNS server responds directly to the querying clients. Although the\nsimplicity of this design is attractive, it is inappropriate for today’s Internet, with its\nvast (and growing) number of hosts. The problems with a centralized design\ninclude:\n•\nA single point of failure. If the DNS server crashes, so does the entire Internet!\n•\nTraffic volume. A single DNS server would have to handle all DNS queries (for\nall the HTTP requests and e-mail messages generated from hundreds of millions\nof hosts).\n2.5\n•\nDNS—THE INTERNET’S DIRECTORY SERVICE\n133\n\n•\nDistant centralized database. A single DNS server cannot be “close to” all the\nquerying clients. If we put the single DNS server in New York City, then all\nqueries from Australia must travel to the other side of the globe, perhaps over\nslow and congested links. This can lead to significant delays.\n•\nMaintenance. The single DNS server would have to keep records for all Internet\nhosts. Not only would this centralized database be huge, but it would have to be\nupdated frequently to account for every new host.\nIn summary, a centralized database in a single DNS server simply doesn’t scale.\nConsequently, the DNS is distributed by design. In fact, the DNS is a wonderful\nexample of how a distributed database can be implemented in the Internet.\nA Distributed, Hierarchical Database\nIn order to deal with the issue of scale, the DNS uses a large number of servers,\norganized in a hierarchical fashion and distributed around the world. No single DNS\nserver has all of the mappings for all of the hosts in the Internet. Instead, the map-\npings are distributed across the DNS servers. To a first approximation, there are\nthree classes of DNS servers—root DNS servers, top-level domain (TLD) DNS\nservers, and authoritative DNS servers—organized in a hierarchy as shown in Fig-\nure 2.19. To understand how these three classes of servers interact, suppose a DNS\nclient wants to determine the IP address for the hostname www.amazon.com. To\na first approximation, the following events will take place. The client first contacts\none of the root servers, which returns IP addresses for TLD servers for the top-level\ndomain com. The client then contacts one of these TLD servers, which returns the\nIP address of an authoritative server for amazon.com. Finally, the client contacts\none of the authoritative servers for amazon.com, which returns the IP address\n134\nCHAPTER 2\n•\nAPPLICATION LAYER\nedu DNS servers\norg DNS servers\ncom DNS servers\npoly.edu\nDNS servers\nyahoo.com\nDNS servers\namazon.com\nDNS servers\npbs.org\nDNS servers\numass.edu\nDNS servers\nRoot DNS servers\nFigure 2.19 \u0002 Portion of the hierarchy of DNS servers\n\nfor the hostname www.amazon.com. We’ll soon examine this DNS lookup\nprocess in more detail. But let’s first take a closer look at these three classes of\nDNS servers:\n•\nRoot DNS servers. In the Internet there are 13 root DNS servers (labeled A\nthrough M), most of which are located in North America. An October 2006 map\nof the root DNS servers is shown in Figure 2.20; a list of the current root DNS\nservers is available via [Root-servers 2012]. Although we have referred to each\nof the 13 root DNS servers as if it were a single server, each “server” is actually\na network of replicated servers, for both security and reliability purposes. All\ntogether, there are 247 root servers as of fall 2011.\n•\nTop-level domain (TLD) servers. These servers are responsible for top-level\ndomains such as com, org, net, edu, and gov, and all of the country top-level domains\nsuch as uk, fr, ca, and jp. The company Verisign Global Registry Services\nmaintains the TLD servers for the com top-level domain, and the company\nEducause maintains the TLD servers for the edu top-level domain. See [IANA\nTLD 2012] for a list of all top-level domains.\n•\nAuthoritative DNS servers. Every organization with publicly accessible hosts\n(such as Web servers and mail servers) on the Internet must provide publicly acces-\nsible DNS records that map the names of those hosts to IP addresses. An organiza-\ntion’s authoritative DNS server houses these DNS records. An organization can\n2.5\n•\nDNS—THE INTERNET’S DIRECTORY SERVICE\n135\nc.\nd.\nh.\nj.\nCogent, Herndon, VA (5 other sites)\nU Maryland College Park, MD\nARL Aberdeen, MD\nVerisign, Dulles VA (69 other sites )\ni. Netnod, Stockholm\n(37 other sites)\nk. RIPE London \n(17 other sites)\nm. WIDE Tokyo\n     (5 other sites)\ng. US DoD Columbus, OH\n     (5 other sites)\ne.\nf.\nNASA Mt View, CA\nInternet Software C.\nPalo Alto, CA\n(and 48 other sites)\na.\nb.\n l.\nVerisign, Los Angeles CA \n(5 other sites)\nUSC-ISI Marina del Rey, CA\nICANN Los Angeles, CA\n(41 other sites)\nFigure 2.20 \u0002 DNS root servers in 2012 (name, organization, location)\n\nchoose to implement its own authoritative DNS server to hold these records; alter-\nnatively, the organization can pay to have these records stored in an authoritative\nDNS server of some service provider. Most universities and large companies\nimplement and maintain their own primary and secondary (backup) authoritative\nDNS server.\nThe root, TLD, and authoritative DNS servers all belong to the hierarchy of\nDNS servers, as shown in Figure 2.19. There is another important type of DNS\nserver called the local DNS server. A local DNS server does not strictly belong to\nthe hierarchy of servers but is nevertheless central to the DNS architecture. Each\nISP—such as a university, an academic department, an employee’s company, or a\nresidential ISP—has a local DNS server (also called a default name server). When a\nhost connects to an ISP, the ISP provides the host with the IP addresses of one or\nmore of its local DNS servers (typically through DHCP, which is discussed in Chap-\nter 4). You can easily determine the IP address of your local DNS server by access-\ning network status windows in Windows or UNIX. A host’s local DNS server is\ntypically “close to” the host. For an institutional ISP, the local DNS server may be\non the same LAN as the host; for a residential ISP, it is typically separated from the\nhost by no more than a few routers. When a host makes a DNS query, the query is\nsent to the local DNS server, which acts a proxy, forwarding the query into the DNS\nserver hierarchy, as we’ll discuss in more detail below.\nLet’s take a look at a simple example. Suppose the host cis.poly.edu\ndesires the IP address of gaia.cs.umass.edu. Also suppose that Polytechnic’s\nlocal DNS server is called dns.poly.edu and that an authoritative DNS server\nfor gaia.cs.umass.edu is called dns.umass.edu. As shown in Figure\n2.21, the host cis.poly.edu first sends a DNS query message to its local DNS\nserver, dns.poly.edu. The query message contains the hostname to be trans-\nlated, namely, gaia.cs.umass.edu. The local DNS server forwards the query\nmessage to a root DNS server. The root DNS server takes note of the edu suffix and\nreturns to the local DNS server a list of IP addresses for TLD servers responsible for\nedu. The local DNS server then resends the query message to one of these TLD\nservers. The TLD server takes note of the umass.edu suffix and responds with the\nIP address of the authoritative DNS server for the University of Massachusetts,\nnamely, dns.umass.edu. Finally, the local DNS server resends the query\nmessage directly to dns.umass.edu, which responds with the IP address of\ngaia.cs.umass.edu. Note that in this example, in order to obtain the mapping\nfor one hostname, eight DNS messages were sent: four query messages and four\nreply messages! We’ll soon see how DNS caching reduces this query traffic.\nOur previous example assumed that the TLD server knows the authoritative\nDNS server for the hostname. In general this not always true. Instead, the TLD server\nmay know only of an intermediate DNS server, which in turn knows the authoritative\nDNS server for the hostname. For example, suppose again that the University of\n136\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nMassachusetts has a DNS server for the university, called dns.umass.edu. Also\nsuppose that each of the departments at the University of Massachusetts has its own\nDNS server, and that each departmental DNS server is authoritative for all hosts in\nthe department. In this case, when the intermediate DNS server, dns.umass.edu,\nreceives a query for a host with a hostname ending with cs.umass.edu, it returns\nto dns.poly.edu the IP address of dns.cs.umass.edu, which is authorita-\ntive for all hostnames ending with cs.umass.edu. The local DNS server\ndns.poly.edu then sends the query to the authoritative DNS server, which\nreturns the desired mapping to the local DNS server, which in turn returns the map-\nping to the requesting host. In this case, a total of 10 DNS messages are sent!\nThe example shown in Figure 2.21 makes use of both recursive queries and\niterative queries. The query sent from cis.poly.edu to dns.poly.edu is a\nrecursive query, since the query asks dns.poly.edu to obtain the mapping on its\n2.5\n•\nDNS—THE INTERNET’S DIRECTORY SERVICE\n137\nRequesting host\ncis.poly.edu\nLocal DNS server\nTLD DNS server\ndns.poly.edu\nRoot DNS server\n1\n8\n2\n7\n4\n5\n3\n6\nAuthoritative DNS server\ndns.umass.edu\ngaia.cs.umass.edu\nFigure 2.21 \u0002 Interaction of the various DNS servers\n\nbehalf. But the subsequent three queries are iterative since all of the replies are\ndirectly returned to dns.poly.edu. In theory, any DNS query can be iterative or\nrecursive. For example, Figure 2.22 shows a DNS query chain for which all of \nthe queries are recursive. In practice, the queries typically follow the pattern in \nFigure 2.21: The query from the requesting host to the local DNS server is recur-\nsive, and the remaining queries are iterative.\nDNS Caching\nOur discussion thus far has ignored DNS caching, a critically important feature of the\nDNS system. In truth, DNS extensively exploits DNS caching in order to improve \nthe delay performance and to reduce the number of DNS messages ricocheting around\n138\nCHAPTER 2\n•\nAPPLICATION LAYER\nRequesting host\ncis.poly.edu\nLocal DNS server\nTLD DNS server\ndns.poly.edu\nRoot DNS server\n1\n8\n5\n4\n2\n7\nAuthoritative DNS server\ndns.umass.edu\ngaia.cs.umass.edu\n6\n3\nFigure 2.22 \u0002 Recursive queries in DNS"
    },
    {
      "chunk_id": "b11ab05f-63ed-4415-a485-659f69ef1c00",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.5.3 DNS Records and Messages",
      "original_titles": [
        "2.5.3 DNS Records and Messages"
      ],
      "path": "Chapter 2 Application Layer > 2.5 DNS—The Internet’s Directory Service > 2.5.3 DNS Records and Messages",
      "start_page": 166,
      "end_page": 170,
      "token_count": 2965,
      "text": "the Internet. The idea behind DNS caching is very simple. In a query chain, when a\nDNS server receives a DNS reply (containing, for example, a mapping from a host-\nname to an IP address), it can cache the mapping in its local memory. For example,\nin Figure 2.21, each time the local DNS server dns.poly.edu receives a reply\nfrom some DNS server, it can cache any of the information contained in the reply. If a\nhostname/IP address pair is cached in a DNS server and another query arrives to the\nDNS server for the same hostname, the DNS server can provide the desired IP address,\neven if it is not authoritative for the hostname. Because hosts and mappings between\nhostnames and IP addresses are by no means permanent, DNS servers discard cached\ninformation after a period of time (often set to two days).\nAs an example, suppose that a host apricot.poly.edu queries\ndns.poly.edu for the IP address for the hostname cnn.com. Furthermore, sup-\npose that a few hours later, another Polytechnic University host, say, kiwi.poly.fr,\nalso queries dns.poly.edu with the same hostname. Because of caching, the local\nDNS server will be able to immediately return the IP address of cnn.com to this sec-\nond requesting host without having to query any other DNS servers. A local DNS\nserver can also cache the IP addresses of TLD servers, thereby allowing the local DNS\nserver to bypass the root DNS servers in a query chain (this often happens).\n2.5.3 DNS Records and Messages\nThe DNS servers that together implement the DNS distributed database store\nresource records (RRs), including RRs that provide hostname-to-IP address map-\npings. Each DNS reply message carries one or more resource records. In this and\nthe following subsection, we provide a brief overview of DNS resource records and\nmessages; more details can be found in [Abitz 1993] or in the DNS RFCs [RFC\n1034; RFC 1035].\nA resource record is a four-tuple that contains the following fields:\n(Name, Value, Type, TTL)\nTTL is the time to live of the resource record; it determines when a resource should\nbe removed from a cache. In the example records given below, we ignore the TTL\nfield. The meaning of Name and Value depend on Type:\n•\nIf Type=A, then Name is a hostname and Value is the IP address for the host-\nname. Thus, a Type A record provides the standard hostname-to-IP address map-\nping. As an example, (relay1.bar.foo.com, 145.37.93.126, A)\nis a Type A record.\n•\nIf Type=NS, then Name is a domain (such as foo.com) and Value is the host-\nname of an authoritative DNS server that knows how to obtain the IP addresses\nfor hosts in the domain. This record is used to route DNS queries further along in\n2.5\n•\nDNS—THE INTERNET’S DIRECTORY SERVICE\n139\n\nthe query chain. As an example, (foo.com, dns.foo.com, NS) is a Type\nNS record.\n•\nIf Type=CNAME, then Value is a canonical hostname for the alias hostname\nName. This record can provide querying hosts the canonical name for a host-\nname. As an example, (foo.com, relay1.bar.foo.com, CNAME) is a\nCNAME record.\n•\nIf Type=MX, then Value is the canonical name of a mail server that has an alias\nhostname Name. As an example, (foo.com, mail.bar.foo.com, MX)\nis an MX record. MX records allow the hostnames of mail servers to have sim-\nple aliases. Note that by using the MX record, a company can have the same\naliased name for its mail server and for one of its other servers (such as its Web\nserver). To obtain the canonical name for the mail server, a DNS client would\nquery for an MX record; to obtain the canonical name for the other server, the\nDNS client would query for the CNAME record.\nIf a DNS server is authoritative for a particular hostname, then the DNS server will\ncontain a Type A record for the hostname. (Even if the DNS server is not authoritative,\nit may contain a Type A record in its cache.) If a server is not authoritative for a host-\nname, then the server will contain a Type NS record for the domain that includes the\nhostname; it will also contain a Type A record that provides the IP address of the DNS\nserver in the Value field of the NS record. As an example, suppose an edu TLD server\nis not authoritative for the host gaia.cs.umass.edu. Then this server will contain\na record for a domain that includes the host gaia.cs.umass.edu, for example,\n(umass.edu, dns.umass.edu, NS). The edu TLD server would also contain\na Type A record, which maps the DNS server dns.umass.edu to an IP address, for\nexample, (dns.umass.edu, 128.119.40.111, A).\nDNS Messages\nEarlier in this section, we referred to DNS query and reply messages. These are the\nonly two kinds of DNS messages. Furthermore, both query and reply messages have\nthe same format, as shown in Figure 2.23.The semantics of the various fields in a\nDNS message are as follows:\n•\nThe first 12 bytes is the header section, which has a number of fields. The first field\nis a 16-bit number that identifies the query. This identifier is copied into the reply\nmessage to a query, allowing the client to match received replies with sent queries.\nThere are a number of flags in the flag field. A 1-bit query/reply flag indicates\nwhether the message is a query (0) or a reply (1). A 1-bit authoritative flag is set in a\nreply message when a DNS server is an authoritative server for a queried name. A\n1-bit recursion-desired flag is set when a client (host or DNS server) desires that the\nDNS server perform recursion when it doesn’t have the record. A 1-bit recursion-\navailable field is set in a reply if the DNS server supports recursion. In the header,\n140\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nthere are also four number-of fields. These fields indicate the number of occurrences\nof the four types of data sections that follow the header.\n•\nThe question section contains information about the query that is being made.\nThis section includes (1) a name field that contains the name that is being\nqueried, and (2) a type field that indicates the type of question being asked about\nthe name—for example, a host address associated with a name (Type A) or the\nmail server for a name (Type MX).\n•\nIn a reply from a DNS server, the answer section contains the resource records\nfor the name that was originally queried. Recall that in each resource record there\nis the Type (for example, A, NS, CNAME, and MX), the Value, and the TTL.\nA reply can return multiple RRs in the answer, since a hostname can have multi-\nple IP addresses (for example, for replicated Web servers, as discussed earlier in\nthis section).\n•\nThe authority section contains records of other authoritative servers.\n•\nThe additional section contains other helpful records. For example, the answer\nfield in a reply to an MX query contains a resource record providing the canoni-\ncal hostname of a mail server. The additional section contains a Type A record\nproviding the IP address for the canonical hostname of the mail server.\nHow would you like to send a DNS query message directly from the host\nyou’re working on to some DNS server? This can easily be done with the nslookup\n2.5\n•\nDNS—THE INTERNET’S DIRECTORY SERVICE\n141\nIdentification\nNumber of questions\nNumber of authority RRs\nName, type fields for\na query\n12 bytes\nRRs in response to query\nRecords for\nauthoritative servers\nAdditional “helpful”\ninfo that may be used\nFlags\nNumber of answer RRs\nNumber of additional RRs\nAuthority\n(variable number of resource records)\nAdditional information\n(variable number of resource records)\nAnswers\n(variable number of resource records)\nQuestions\n(variable number of questions)\nFigure 2.23 \u0002 DNS message format\n\nprogram, which is available from most Windows and UNIX platforms. For exam-\nple, from a Windows host, open the Command Prompt and invoke the nslookup pro-\ngram by simply typing “nslookup.” After invoking nslookup, you can send a DNS\nquery to any DNS server (root, TLD, or authoritative). After receiving the reply\nmessage from the DNS server, nslookup will display the records included in the\nreply (in a human-readable format). As an alternative to running nslookup from your\nown host, you can visit one of many Web sites that allow you to remotely employ\nnslookup. (Just type “nslookup” into a search engine and you’ll be brought to one of\nthese sites.) The DNS Wireshark lab at the end of this chapter will allow you to\nexplore the DNS in much more detail.\nInserting Records into the DNS Database\nThe discussion above focused on how records are retrieved from the DNS database.\nYou might be wondering how records get into the database in the first place. Let’s look\nat how this is done in the context of a specific example. Suppose you have just created\nan exciting new startup company called Network Utopia. The first thing you’ll surely\nwant to do is register the domain name networkutopia.com at a registrar. A\nregistrar is a commercial entity that verifies the uniqueness of the domain name,\nenters the domain name into the DNS database (as discussed below), and collects a\nsmall fee from you for its services. Prior to 1999, a single registrar, Network Solutions,\nhad a monopoly on domain name registration for com, net, and org domains. But\nnow there are many registrars competing for customers, and the Internet Corporation\nfor Assigned Names and Numbers (ICANN) accredits the various registrars. A com-\nplete list of accredited registrars is available at http://www.internic.net.\nWhen you register the domain name networkutopia.com with some reg-\nistrar, you also need to provide the registrar with the names and IP addresses of your\nprimary and secondary authoritative DNS servers. Suppose the names and IP\naddresses are dns1.networkutopia.com, dns2.networkutopia.com,\n212.212.212.1, and 212.212.212.2. For each of these two authoritative\nDNS servers, the registrar would then make sure that a Type NS and a Type A record\nare entered into the TLD com servers. Specifically, for the primary authoritative\nserver for networkutopia.com, the registrar would insert the following two\nresource records into the DNS system:\n(networkutopia.com, dns1.networkutopia.com, NS)\n(dns1.networkutopia.com, 212.212.212.1, A)\nYou’ll also have to make sure that the Type A resource record for your Web server\nwww.networkutopia.com and the Type MX resource record for your mail\nserver mail.networkutopia.com are entered into your authoritative DNS\nservers. (Until recently, the contents of each DNS server were configured statically,\n142\nCHAPTER 2\n•\nAPPLICATION LAYER\n\n2.5\n•\nDNS—THE INTERNET’S DIRECTORY SERVICE\n143\nDNS VULNERABILITIES\nWe have seen that DNS is a critical component of the Internet infrastructure, with\nmany important services - including the Web and e-mail - simply incapable of func-\ntioning without it. We therefore naturally ask, how can DNS be attacked? Is DNS a\nsitting duck, waiting to be knocked out of service, while taking most Internet applica-\ntions down with it?\nThe first type of attack that comes to mind is a DDoS bandwidth-flooding attack (see\nSection 1.6) against DNS servers. For example, an attacker could attempt to send to\neach DNS root server a deluge of packets, so many that the majority of legitimate DNS\nqueries never get answered. Such a large-scale DDoS attack against DNS root servers\nactually took place on October 21, 2002. In this attack, the attackers leveraged a bot-\nnet to send truck loads of ICMP ping messages to each of the 13 DNS root servers.\n(ICMP messages are discussed in Chapter 4. For now, it suffices to know that ICMP pack-\nets are special types of IP datagrams.) Fortunately, this large-scale attack caused minimal\ndamage, having little or no impact on users’ Internet experience. The attackers did\nsucceed at directing a deluge of packets at the root servers. But many of the DNS root\nservers were protected by packet filters, configured to always block all ICMP ping\nmessages directed at the root servers. These protected servers were thus spared and\nfunctioned as normal. Furthermore, most local DNS servers cache the IP addresses of top-\nlevel-domain servers, allowing the query process to often bypass the DNS root servers.\nA potentially more effective DDoS attack against DNS would be send a deluge of\nDNS queries to top-level-domain servers, for example, to all the top-level-domain\nservers that handle the .com domain. It would be harder to filter DNS queries direct-\ned to DNS servers; and top-level-domain servers are not as easily bypassed as are\nroot servers. But the severity of such an attack would be partially mitigated by\ncaching in local DNS servers.\nDNS could potentially be attacked in other ways. In a man-in-the-middle attack,\nthe attacker intercepts queries from hosts and returns bogus replies. In the DNS poi-\nsoning attack, the attacker sends bogus replies to a DNS server, tricking the server\ninto accepting bogus records into its cache. Either of these attacks could be used, for\nexample, to redirect an unsuspecting Web user to the attacker’s Web site. These\nattacks, however, are difficult to implement, as they require intercepting packets or\nthrottling servers [Skoudis 2006].\nAnother important DNS attack is not an attack on the DNS service per se, but\ninstead exploits the DNS infrastructure to launch a DDoS attack against a targeted host\n(for example, your university’s mail server). In this attack, the attacker sends DNS\nqueries to many authoritative DNS servers, with each query having the spoofed source\naddress of the targeted host. The DNS servers then send their replies directly to the tar-\ngeted host. If the queries can be crafted in such a way that a response is much larger\nFOCUS ON SECURITY"
    },
    {
      "chunk_id": "adff2a08-787c-4466-acc7-0ccb7b20fd18",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.6 Peer-to-Peer Applications",
      "original_titles": [
        "2.6 Peer-to-Peer Applications"
      ],
      "path": "Chapter 2 Application Layer > 2.6 Peer-to-Peer Applications",
      "start_page": 171,
      "end_page": 171,
      "token_count": 575,
      "text": "for example, from a configuration file created by a system manager. More recently,\nan UPDATE option has been added to the DNS protocol to allow data to be dynam-\nically added or deleted from the database via DNS messages. [RFC 2136] and [RFC\n3007] specify DNS dynamic updates.)\nOnce all of these steps are completed, people will be able to visit your Web site\nand send e-mail to the employees at your company. Let’s conclude our discussion of\nDNS by verifying that this statement is true. This verification also helps to solidify\nwhat we have learned about DNS. Suppose Alice in Australia wants to view the Web\npage www.networkutopia.com. As discussed earlier, her host will first send a\nDNS query to her local DNS server. The local DNS server will then contact a TLD\ncom server. (The local DNS server will also have to contact a root DNS server if the\naddress of a TLD com server is not cached.) This TLD server contains the Type NS\nand Type A resource records listed above, because the registrar had these resource\nrecords inserted into all of the TLD com servers. The TLD com server sends a reply\nto Alice’s local DNS server, with the reply containing the two resource records. The\nlocal DNS server then sends a DNS query to 212.212.212.1, asking for the\nType A record corresponding to www.networkutopia.com. This record pro-\nvides the IP address of the desired Web server, say, 212.212.71.4, which the\nlocal DNS server passes back to Alice’s host. Alice’s browser can now initiate a TCP\nconnection to the host 212.212.71.4 and send an HTTP request over the con-\nnection. Whew! There’s a lot more going on than what meets the eye when one surfs\nthe Web!\n2.6 Peer-to-Peer Applications\nThe applications described in this chapter thus far—including the Web, e-mail, and\nDNS—all employ client-server architectures with significant reliance on always-on\ninfrastructure servers. Recall from Section 2.1.1 that with a P2P architecture, there\nis minimal (or no) reliance on always-on infrastructure servers. Instead, pairs of\nintermittently connected hosts, called peers, communicate directly with each other.\n144\nCHAPTER 2\n•\nAPPLICATION LAYER\n(in bytes) than a query (so-called amplification), then the attacker can potentially over-\nwhelm the target without having to generate much of its own traffic. Such reflection\nattacks exploiting DNS have had limited success to date [Mirkovic 2005].\nIn summary, DNS has demonstrated itself to be surprisingly robust against attacks.\nTo date, there hasn’t been an attack that has successfully impeded the DNS service.\nThere have been successful reflector attacks; however, these attacks can be (and are\nbeing) addressed by appropriate configuration of DNS servers.\nFOCUS ON SECURITY"
    },
    {
      "chunk_id": "21f37c35-f4b9-4b1c-8c10-8bd9d90045bf",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.6.1 P2P File Distribution",
      "original_titles": [
        "2.6.1 P2P File Distribution"
      ],
      "path": "Chapter 2 Application Layer > 2.6 Peer-to-Peer Applications > 2.6.1 P2P File Distribution",
      "start_page": 172,
      "end_page": 177,
      "token_count": 3131,
      "text": "The peers are not owned by a service provider, but are instead desktops and laptops\ncontrolled by users.\nIn this section we’ll examine two different applications that are particularly\nwell-suited for P2P designs. The first is file distribution, where the application dis-\ntributes a file from a single source to a large number of peers. File distribution is a\nnice place to start our investigation of P2P, as it clearly exposes the self-scalability\nof P2P architectures. As a specific example for file distribution, we’ll describe\nthe popular BitTorrent system. The second P2P application we’ll examine is a\ndatabase distributed over a large community of peers. For this application, we’ll\nexplore the concept of a Distributed Hash Table (DHT).\n2.6.1 P2P File Distribution\nWe begin our foray into P2P by considering a very natural application, namely,\ndistributing a large file from a single server to a large number of hosts (called\npeers). The file might be a new version of the Linux operating system, a software\npatch for an existing operating system or application, an MP3 music file, or an\nMPEG video file. In client-server file distribution, the server must send a copy of\nthe file to each of the peers—placing an enormous burden on the server and con-\nsuming a large amount of server bandwidth. In P2P file distribution, each peer can\nredistribute any portion of the file it has received to any other peers, thereby\nassisting the server in the distribution process. As of 2012, the most popular P2P\nfile distribution protocol is BitTorrent. Originally developed by Bram Cohen,\nthere are now many different independent BitTorrent clients conforming to the\nBitTorrent protocol, just as there are a number of Web browser clients that \nconform to the HTTP protocol. In this subsection, we first examine the self-\nscalability of P2P architectures in the context of file distribution. We then describe\nBitTorrent in some detail, highlighting its most important characteristics and\nfeatures.\nScalability of P2P Architectures\nTo compare client-server architectures with peer-to-peer architectures, and illustrate\nthe inherent self-scalability of P2P, we now consider a simple quantitative model for\ndistributing a file to a fixed set of peers for both architecture types. As shown in Fig-\nure 2.24, the server and the peers are connected to the Internet with access links.\nDenote the upload rate of the server’s access link by us, the upload rate of the ith\npeer’s access link by ui, and the download rate of the ith peer’s access link by di.\nAlso denote the size of the file to be distributed (in bits) by F and the number of\npeers that want to obtain a copy of the file by N. The distribution time is the time it\ntakes to get a copy of the file to all N peers. In our analysis of the distribution time\nbelow, for both client-server and P2P architectures, we make the simplifying (and\ngenerally accurate [Akella 2003]) assumption that the Internet core has abundant\n2.6\n•\nPEER-TO-PEER APPLICATIONS\n145\n\nbandwidth, implying that all of the bottlenecks are in access networks. We also sup-\npose that the server and clients are not participating in any other network applica-\ntions, so that all of their upload and download access bandwidth can be fully\ndevoted to distributing this file.\nLet’s first determine the distribution time for the client-server architecture,\nwhich we denote by Dcs. In the client-server architecture, none of the peers aids in\ndistributing the file. We make the following observations:\n•\nThe server must transmit one copy of the file to each of the N peers. Thus the\nserver must transmit NF bits. Since the server’s upload rate is us, the time to dis-\ntribute the file must be at least NF/us.\n•\nLet dmin denote the download rate of the peer with the lowest download rate, that\nis, dmin = min{d1,dp,...,dN}. The peer with the lowest download rate cannot\nobtain all F bits of the file in less than F/dmin seconds. Thus the minimum distri-\nbution time is at least F/dmin.\nPutting these two observations together, we obtain\n.\nDcs Ú maxb NF\nus\n, F\ndmin\nr\n146\nCHAPTER 2\n•\nAPPLICATION LAYER\nInternet\nFile: F\nServer\nus\nu1\nu2\nu3\nd1\nd2\nd3\nu4\nu5\nu6\nd4\nd5\nd6\nuN\ndN\nFigure 2.24 \u0002 An illustrative file distribution problem\n\nThis provides a lower bound on the minimum distribution time for the client-server\narchitecture. In the homework problems you will be asked to show that the server\ncan schedule its transmissions so that the lower bound is actually achieved. So let’s\ntake this lower bound provided above as the actual distribution time, that is,\n(2.1)\nWe see from Equation 2.1 that for N large enough, the client-server distribution time\nis given by NF/us. Thus, the distribution time increases linearly with the number of\npeers N. So, for example, if the number of peers from one week to the next increases\na thousand-fold from a thousand to a million, the time required to distribute the file\nto all peers increases by 1,000.\nLet’s now go through a similar analysis for the P2P architecture, where each\npeer can assist the server in distributing the file. In particular, when a peer receives\nsome file data, it can use its own upload capacity to redistribute the data to other\npeers. Calculating the distribution time for the P2P architecture is somewhat more\ncomplicated than for the client-server architecture, since the distribution time\ndepends on how each peer distributes portions of the file to the other peers. Never-\ntheless, a simple expression for the minimal distribution time can be obtained\n[Kumar 2006]. To this end, we first make the following observations:\n•\nAt the beginning of the distribution, only the server has the file. To get this file\ninto the community of peers, the server must send each bit of the file at least once\ninto its access link. Thus, the minimum distribution time is at least F/us. (Unlike\nthe client-server scheme, a bit sent once by the server may not have to be sent by\nthe server again, as the peers may redistribute the bit among themselves.)\n•\nAs with the client-server architecture, the peer with the lowest download rate\ncannot obtain all F bits of the file in less than F/dmin seconds. Thus the minimum\ndistribution time is at least F/dmin.\n•\nFinally, observe that the total upload capacity of the system as a whole is equal\nto the upload rate of the server plus the upload rates of each of the individual\npeers, that is, utotal = us + u1 + … + uN. The system must deliver (upload) F bits\nto each of the N peers, thus delivering a total of NF bits. This cannot be done at a\nrate faster than utotal. Thus, the minimum distribution time is also at least\nNF/(us + u1 + … + uN).\nPutting these three observations together, we obtain the minimum distribution time\nfor P2P, denoted by DP2P.\n(2.2)\nDP2P Ú max c\nF\nus\n, F\ndmin\n,\nNF\nus + a\nN\ni=1\nui\ns\nDcs = maxb NF\nus\n, F\ndmin\nr\n2.6\n•\nPEER-TO-PEER APPLICATIONS\n147\n\nEquation 2.2 provides a lower bound for the minimum distribution time for the P2P\narchitecture. It turns out that if we imagine that each peer can redistribute a bit as\nsoon as it receives the bit, then there is a redistribution scheme that actually achieves\nthis lower bound [Kumar 2006]. (We will prove a special case of this result in the\nhomework.) In reality, where chunks of the file are redistributed rather than individ-\nual bits, Equation 2.2 serves as a good approximation of the actual minimum distri-\nbution time. Thus, let’s take the lower bound provided by Equation 2.2 as the actual\nminimum distribution time, that is,\n(2.3)\nFigure 2.25 compares the minimum distribution time for the client-server and\nP2P architectures assuming that all peers have the same upload rate u. In Figure\n2.25, we have set F/u = 1 hour, us = 10u, and dmin ≥us. Thus, a peer can transmit the\nentire file in one hour, the server transmission rate is 10 times the peer upload rate,\nand (for simplicity) the peer download rates are set large enough so as not to have\nan effect. We see from Figure 2.25 that for the client-server architecture, the dis-\ntribution time increases linearly and without bound as the number of peers\nincreases. However, for the P2P architecture, the minimal distribution time is not\nonly always less than the distribution time of the client-server architecture; it is also\nless than one hour for any number of peers N. Thus, applications with the P2P\narchitecture can be self-scaling. This scalability is a direct consequence of peers\nbeing redistributors as well as consumers of bits.\nDP2P = max c\nF\nus\n, F\ndmin\n,\nNF\nus + a\nN\ni=1\nui\ns\n148\nCHAPTER 2\n•\nAPPLICATION LAYER\n0\n5\n10\n15\n20\n25\n30\n0\nN\nMinimum distributioin tiime\n35\n0.5\n1.5\n2.5\n1.0\n3.0\n2.0\n3.5\nClient-Server\nP2P\nFigure 2.25 \u0002 Distribution time for P2P and client-server architectures\n\nBitTorrent\nBitTorrent is a popular P2P protocol for file distribution [Chao 2011]. In BitTor-\nrent lingo, the collection of all peers participating in the distribution of a particular\nfile is called a torrent. Peers in a torrent download equal-size chunks of the file\nfrom one another, with a typical chunk size of 256 KBytes. When a peer first joins\na torrent, it has no chunks. Over time it accumulates more and more chunks. While\nit downloads chunks it also uploads chunks to other peers. Once a peer has\nacquired the entire file, it may (selfishly) leave the torrent, or (altruistically) remain\nin the torrent and continue to upload chunks to other peers. Also, any peer may leave\nthe torrent at any time with only a subset of chunks, and later rejoin the torrent.\nLet’s now take a closer look at how BitTorrent operates. Since BitTorrent is a\nrather complicated protocol and system, we’ll only describe its most important\nmechanisms, sweeping some of the details under the rug; this will allow us to see\nthe forest through the trees. Each torrent has an infrastructure node called a tracker.\nWhen a peer joins a torrent, it registers itself with the tracker and periodically\ninforms the tracker that it is still in the torrent. In this manner, the tracker keeps\ntrack of the peers that are participating in the torrent. A given torrent may have\nfewer than ten or more than a thousand peers participating at any instant of time.\nAs shown in Figure 2.26, when a new peer, Alice, joins the torrent, the tracker\nrandomly selects a subset of peers (for concreteness, say 50) from the set of participat-\ning peers, and sends the IP addresses of these 50 peers to Alice. Possessing this list of\npeers, Alice attempts to establish concurrent TCP connections with all the peers on this\nlist. Let’s call all the peers with which Alice succeeds in establishing a TCP connec-\ntion “neighboring peers.” (In Figure 2.26, Alice is shown to have only three neighbor-\ning peers. Normally, she would have many more.) As time evolves, some of these\npeers may leave and other peers (outside the initial 50) may attempt to establish TCP\nconnections with Alice. So a peer’s neighboring peers will fluctuate over time.\nAt any given time, each peer will have a subset of chunks from the file, with dif-\nferent peers having different subsets. Periodically, Alice will ask each of her neighbor-\ning peers (over the TCP connections) for the list of the chunks they have. If Alice has L\ndifferent neighbors, she will obtain L lists of chunks. With this knowledge, Alice will\nissue requests (again over the TCP connections) for chunks she currently does not have.\nSo at any given instant of time, Alice will have a subset of chunks and will\nknow which chunks her neighbors have. With this information, Alice will have two\nimportant decisions to make. First, which chunks should she request first from her\nneighbors? And second, to which of her neighbors should she send requested\nchunks? In deciding which chunks to request, Alice uses a technique called rarest\nfirst. The idea is to determine, from among the chunks she does not have, the\nchunks that are the rarest among her neighbors (that is, the chunks that have the\nfewest repeated copies among her neighbors) and then request those rarest chunks\nfirst. In this manner, the rarest chunks get more quickly redistributed, aiming to\n(roughly) equalize the numbers of copies of each chunk in the torrent.\n2.6\n•\nPEER-TO-PEER APPLICATIONS\n149\n\nTo determine which requests she responds to, BitTorrent uses a clever trading\nalgorithm. The basic idea is that Alice gives priority to the neighbors that are cur-\nrently supplying her data at the highest rate. Specifically, for each of her neighbors,\nAlice continually measures the rate at which she receives bits and determines the four\npeers that are feeding her bits at the highest rate. She then reciprocates by sending\nchunks to these same four peers. Every 10 seconds, she recalculates the rates and pos-\nsibly modifies the set of four peers. In BitTorrent lingo, these four peers are said to\nbe unchoked. Importantly, every 30 seconds, she also picks one additional neighbor\nat random and sends it chunks. Let’s call the randomly chosen peer Bob. In BitTor-\nrent lingo, Bob is said to be optimistically unchoked. Because Alice is sending data\nto Bob, she may become one of Bob’s top four uploaders, in which case Bob would\nstart to send data to Alice. If the rate at which Bob sends data to Alice is high enough,\nBob could then, in turn, become one of Alice’s top four uploaders. In other words,\nevery 30 seconds, Alice will randomly choose a new trading partner and initiate trad-\ning with that partner. If the two peers are satisfied with the trading, they will put each\nother in their top four lists and continue trading with each other until one of the peers\nfinds a better partner. The effect is that peers capable of uploading at compatible rates\ntend to find each other. The random neighbor selection also allows new peers to get\n150\nCHAPTER 2\n•\nAPPLICATION LAYER\nTracker\nTrading chunks\nPeer\nObtain\nlist of\npeers\nAlice\nFigure 2.26 \u0002 File distribution with BitTorrent"
    },
    {
      "chunk_id": "1844e67c-8e1d-4e61-8f03-46bf75873b09",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.6.2 Distributed Hash Tables (DHTs)",
      "original_titles": [
        "2.6.2 Distributed Hash Tables (DHTs)"
      ],
      "path": "Chapter 2 Application Layer > 2.6 Peer-to-Peer Applications > 2.6.2 Distributed Hash Tables (DHTs)",
      "start_page": 178,
      "end_page": 182,
      "token_count": 3630,
      "text": "chunks, so that they can have something to trade. All other neighboring peers besides\nthese five peers (four “top” peers and one probing peer) are “choked,” that is, they do\nnot receive any chunks from Alice. BitTorrent has a number of interesting mecha-\nnisms that are not discussed here, including pieces (mini-chunks), pipelining, random\nfirst selection, endgame mode, and anti-snubbing [Cohen 2003].\nThe incentive mechanism for trading just described is often referred to as tit-for-tat\n[Cohen 2003]. It has been shown that this incentive scheme can be circumvented\n[Liogkas 2006; Locher 2006; Piatek 2007]. Nevertheless, the BitTorrent ecosystem is\nwildly successful, with millions of simultaneous peers actively sharing files in hun-\ndreds of thousands of torrents. If BitTorrent had been designed without tit-for-tat (or a\nvariant), but otherwise exactly the same, BitTorrent would likely not even exist now, as\nthe majority of the users would have been freeriders [Saroiu 2002].\nInteresting variants of the BitTorrent protocol are proposed [Guo 2005; Piatek\n2007]. Also, many of the P2P live streaming applications, such as PPLive and\nppstream, have been inspired by BitTorrent [Hei 2007].\n2.6.2 Distributed Hash Tables (DHTs)\nIn this section, we will consider how to implement a simple database in a P2P net-\nwork. Let’s begin by describing a centralized version of this simple database, which\nwill simply contain (key, value) pairs. For example, the keys could be social secu-\nrity numbers and the values could be the corresponding human names; in this case,\nan example key-value pair is (156-45-7081, Johnny Wu). Or the keys could be con-\ntent names (e.g., names of movies, albums, and software), and the value could be\nthe IP address at which the content is stored; in this case, an example key-value pair\nis (Led Zeppelin IV, 128.17.123.38). We query the database with a key. If there are\none or more key-value pairs in the database that match the query key, the database\nreturns the corresponding values. So, for example, if the database stores social secu-\nrity numbers and their corresponding human names, we can query with a specific\nsocial security number, and the database returns the name of the human who has that\nsocial security number. Or, if the database stores content names and their correspon-\nding IP addresses, we can query with a specific content name, and the database\nreturns the IP addresses that store the specific content.\nBuilding such a database is straightforward with a client-server architecture that\nstores all the (key, value) pairs in one central server. So in this section, we’ll instead\nconsider how to build a distributed, P2P version of this database that will store the\n(key, value) pairs over millions of peers. In the P2P system, each peer will only hold a\nsmall subset of the totality of the (key, value) pairs. We’ll allow any peer to query the\ndistributed database with a particular key. The distributed database will then locate the\npeers that have the corresponding (key, value) pairs and return the key-value pairs to\nthe querying peer. Any peer will also be allowed to insert new key-value pairs into the\ndatabase. Such a distributed database is referred to as a distributed hash table\n(DHT).\n2.6\n•\nPEER-TO-PEER APPLICATIONS\n151\n\nBefore describing how we can create a DHT, let’s first describe a specific\nexample DHT service in the context of P2P file sharing. In this case, a key is the\ncontent name and the value is the IP address of a peer that has a copy of the content.\nSo, if Bob and Charlie each have a copy of the latest Linux distribution, then the\nDHT database will include the following two key-value pairs: (Linux, IPBob) and\n(Linux, IPCharlie). More specifically, since the DHT database is distributed over the\npeers, some peer, say Dave, will be responsible for the key “Linux” and will have\nthe corresponding key-value pairs. Now suppose Alice wants to obtain a copy of\nLinux. Clearly, she first needs to know which peers have a copy of Linux before she\ncan begin to download it. To this end, she queries the DHT with “Linux” as the key.\nThe DHT then determines that the peer Dave is responsible for the key “Linux.” The\nDHT then contacts peer Dave, obtains from Dave the key-value pairs (Linux, IPBob)\nand (Linux, IPCharlie), and passes them on to Alice. Alice can then download the lat-\nest Linux distribution from either IPBob or IPCharlie.\nNow let’s return to the general problem of designing a DHT for general key-\nvalue pairs. One naïve approach to building a DHT is to randomly scatter the (key,\nvalue) pairs across all the peers and have each peer maintain a list of the IP\naddresses of all participating peers. In this design, the querying peer sends its query\nto all other peers, and the peers containing the (key, value) pairs that match the key\ncan respond with their matching pairs. Such an approach is completely unscalable,\nof course, as it would require each peer to not only know about all other peers (pos-\nsibly millions of such peers!) but even worse, have each query sent to all peers.\nWe now describe an elegant approach to designing a DHT. To this end, let’s first\nassign an identifier to each peer, where each identifier is an integer in the range [0, \n2n \u0003 1] for some fixed n. Note that each such identifier can be expressed by an n-bit\nrepresentation. Let’s also require each key to be an integer in the same range. The\nastute reader may have observed that the example keys described a little earlier (social\nsecurity numbers and content names) are not integers. To create integers out of such\nkeys, we will use a hash function that maps each key (e.g., social security number) to\nan integer in the range [0, 2n\u0003 1]. A hash function is a many-to-one function for which\ntwo different inputs can have the same output (same integer), but the likelihood of the\nhaving the same output is extremely small. (Readers who are unfamiliar with hash\nfunctions may want to visit Chapter 7, in which hash functions are discussed in some\ndetail.) The hash function is assumed to be available to all peers in the system. Hence-\nforth, when we refer to the “key,” we are referring to the hash of the original key. So,\nfor example, if the original key is “Led Zeppelin IV,” the key used in the DHT will be\nthe integer that equals the hash of “Led Zeppelin IV.” As you may have guessed, this\nis why “Hash” is used in the term “Distributed Hash Function.”\nLet’s now consider the problem of storing the (key, value) pairs in the DHT. The\ncentral issue here is defining a rule for assigning keys to peers. Given that each peer\nhas an integer identifier and that each key is also an integer in the same range, a natu-\nral approach is to assign each (key, value) pair to the peer whose identifier is the\nclosest to the key. To implement such a scheme, we’ll need to define what is meant by\n“closest,” for which many conventions are possible. For convenience, let’s define the\n152\nCHAPTER 2\n•\nAPPLICATION LAYER\nVideoNote\nWalking through \ndistributed hash tables\n\nclosest peer as the closest successor of the key. To gain some insight here, let’s take a\nlook at a specific example. Suppose n \u0004 4 so that all the peer and key identifiers are in\nthe range [0, 15]. Further suppose that there are eight peers in the system with identi-\nfiers 1, 3, 4, 5, 8, 10, 12, and 15. Finally, suppose we want to store the (key, value) pair\n(11, Johnny Wu) in one of the eight peers. But in which peer? Using our closest con-\nvention, since peer 12 is the closest successor for key 11, we therefore store the pair\n(11, Johnny Wu) in the peer 12. [To complete our definition of closest, if the key is\nexactly equal to one of the peer identifiers, we store the (key, value) pair in that match-\ning peer; and if the key is larger than all the peer identifiers, we use a modulo-2n con-\nvention, storing the (key, value) pair in the peer with the smallest identifier.]\nNow suppose a peer, Alice, wants to insert a (key, value) pair into the DHT.\nConceptually, this is straightforward: She first determines the peer whose identifier\nis closest to the key; she then sends a message to that peer, instructing it to store the\n(key, value) pair. But how does Alice determine the peer that is closest to the key? If\nAlice were to keep track of all the peers in the system (peer IDs and corresponding\nIP addresses), she could locally determine the closest peer. But such an approach\nrequires each peer to keep track of all other peers in the DHT—which is completely\nimpractical for a large-scale system with millions of peers.\nCircular DHT\nTo address this problem of scale, let’s now consider organizing the peers into a\ncircle. In this circular arrangement, each peer only keeps track of its immediate suc-\ncessor and immediate predecessor (modulo 2n). An example of such a circle is\nshown in Figure 2.27(a). In this example, n is again 4 and there are the same eight\n2.6\n•\nPEER-TO-PEER APPLICATIONS\n153\n1\n3\nWho is\nresponsible\nfor key 11?\n4\n5\n8\na.\nb.\n10\n12\n15\n1\n3\n4\n5\n8\n10\n12\n15\nFigure 2.27 \u0002 (a) A circular DHT. Peer 3 wants to determine who is\nresponsible for key 11. (b) A circular DHT with shortcuts\n\npeers from the previous example. Each peer is only aware of its immediate succes-\nsor and predecessor; for example, peer 5 knows the IP address and identifier for\npeers 8 and 4 but does not necessarily know anything about any other peers that may\nbe in the DHT. This circular arrangement of the peers is a special case of an overlay\nnetwork. In an overlay network, the peers form an abstract logical network which\nresides above the “underlay” computer network consisting of physical links, routers,\nand hosts. The links in an overlay network are not physical links, but are simply vir-\ntual liaisons between pairs of peers. In the overlay in Figure 2.27(a), there are eight\npeers and eight overlay links; in the overlay in Figure 2.27(b) there are eight peers\nand 16 overlay links. A single overlay link typically uses many physical links and\nphysical routers in the underlay network.\nUsing the circular overlay in Figure 2.27(a), now suppose that peer 3 wants to\ndetermine which peer in the DHT is responsible for key 11. Using the circular overlay,\nthe origin peer (peer 3) creates a message saying “Who is responsible for key 11?” and\nsends this message clockwise around the circle. Whenever a peer receives such a mes-\nsage, because it knows the identifier of its successor and predecessor, it can determine\nwhether it is responsible for (that is, closest to) the key in question. If a peer is not\nresponsible for the key, it simply sends the message to its successor. So, for example,\nwhen peer 4 receives the message asking about key 11, it determines that it is not\nresponsible for the key (because its successor is closer to the key), so it just passes the\nmessage along to peer 5. This process continues until the message arrives at peer 12,\nwho determines that it is the closest peer to key 11. At this point, peer 12 can send a\nmessage back to the querying peer, peer 3, indicating that it is responsible for key 11.\nThe circular DHT provides a very elegant solution for reducing the amount of\noverlay information each peer must manage. In particular, each peer needs only to\nbe aware of two peers, its immediate successor and its immediate predecessor. But\nthis solution introduces yet a new problem. Although each peer is only aware of two\nneighboring peers, to find the node responsible for a key (in the worst case), all N\nnodes in the DHT will have to forward a message around the circle; N/2 messages\nare sent on average.\nThus, in designing a DHT, there is tradeoff between the number of neighbors each\npeer has to track and the number of messages that the DHT needs to send to resolve a\nsingle query. On one hand, if each peer tracks all other peers (mesh overlay), then only\none message is sent per query, but each peer has to keep track of N peers. On the other\nhand, with a circular DHT, each peer is only aware of two peers, but N/2 messages are\nsent on average for each query. Fortunately, we can refine our designs of DHTs so that\nthe number of neighbors per peer as well as the number of messages per query is kept\nto an acceptable size. One such refinement is to use the circular overlay as a founda-\ntion, but add “shortcuts” so that each peer not only keeps track of its immediate suc-\ncessor and predecessor, but also of a relatively small number of shortcut peers\nscattered about the circle. An example of such a circular DHT with some shortcuts is\nshown in Figure 2.27(b). Shortcuts are used to expedite the routing of query messages.\nSpecifically, when a peer receives a message that is querying for a key, it forwards the\n154\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nmessage to the neighbor (successor neighbor or one of the shortcut neighbors) which\nis the closet to the key. Thus, in Figure 2.27(b), when peer 4 receives the message ask-\ning about key 11, it determines that the closet peer to the key (among its neighbors) is\nits shortcut neighbor 10 and then forwards the message directly to peer 10. Clearly,\nshortcuts can significantly reduce the number of messages used to process a query.\nThe next natural question is “How many shortcut neighbors should a peer have,\nand which peers should be these shortcut neighbors? This question has received sig-\nnificant attention in the research community [Balakrishnan 2003; Androutsellis-\nTheotokis  2004]. Importantly, it has been shown that the DHT can be designed so that\nboth the number of neighbors per peer as well as the number of messages per query is\nO(log N), where N is the number of peers. Such designs strike a satisfactory compro-\nmise between the extreme solutions of using mesh and circular overlay topologies.\nPeer Churn\nIn P2P systems, a peer can come or go without warning. Thus, when designing a\nDHT, we also must be concerned about maintaining the DHT overlay in the pres-\nence of such peer churn. To get a big-picture understanding of how this could be\naccomplished, let’s once again consider the circular DHT in Figure 2.27(a). To han-\ndle peer churn, we will now require each peer to track (that is, know the IP address\nof) its first and second successors; for example, peer 4 now tracks both peer 5 and\npeer 8. We also require each peer to periodically verify that its two successors are\nalive (for example, by periodically sending ping messages to them and asking for\nresponses). Let’s now consider how the DHT is maintained when a peer abruptly\nleaves. For example, suppose peer 5 in Figure 2.27(a) abruptly leaves. In this case,\nthe two peers preceding the departed peer (4 and 3) learn that 5 has departed, since\nit no longer responds to ping messages. Peers 4 and 3 thus need to update their suc-\ncessor state information. Let’s consider how peer 4 updates its state:\n1. Peer 4 replaces its first successor (peer 5) with its second successor (peer 8).\n2. Peer 4 then asks its new first successor (peer 8) for the identifier and IP address of\nits immediate successor (peer 10). Peer 4 then makes peer 10 its second successor.\nIn the homework problems, you will be asked to determine how peer 3 updates its\noverlay routing information.\nHaving briefly addressed what has to be done when a peer leaves, let’s now\nconsider what happens when a peer wants to join the DHT. Let’s say a peer with\nidentifier 13 wants to join the DHT, and at the time of joining, it only knows about\npeer 1’s existence in the DHT. Peer 13 would first send peer 1 a message, saying\n“what will be 13’s predecessor and successor?” This message gets forwarded\nthrough the DHT until it reaches peer 12, who realizes that it will be 13’s predeces-\nsor and that its current successor, peer 15, will become 13’s successor. Next, peer 12\nsends this predecessor and successor information to peer 13. Peer 13 can now join\n2.6\n•\nPEER-TO-PEER APPLICATIONS\n155"
    },
    {
      "chunk_id": "84946417-249d-4597-b0a9-bc707141b127",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.7 Socket Programming: Creating Network Applications",
      "original_titles": [
        "2.7 Socket Programming: Creating Network Applications"
      ],
      "path": "Chapter 2 Application Layer > 2.7 Socket Programming: Creating Network Applications",
      "start_page": 183,
      "end_page": 183,
      "token_count": 604,
      "text": "the DHT by making peer 15 its successor and by notifying peer 12 that it should\nchange its immediate successor to 13.\nDHTs have been finding widespread use in practice. For example, BitTorrent\nuses the Kademlia DHT to create a distributed tracker. In the BitTorrent, the key is\nthe torrent identifier and the value is the IP addresses of all the peers currently par-\nticipating in the torrent [Falkner 2007, Neglia 2007]. In this manner, by querying\nthe DHT with a torrent identifier, a newly arriving BitTorrent peer can determine the\npeer that is responsible for the identifier (that is, for tracking the peers in the tor-\nrent). After having found that peer, the arriving peer can query it for a list of other\npeers in the torrent.\n2.7 Socket Programming: Creating Network\nApplications\nNow that we’ve looked at a number of important network applications, let’s explore\nhow network application programs are actually created. Recall from Section 2.1 that\na typical network application consists of a pair of programs—a client program and a\nserver program—residing in two different end systems. When these two programs\nare executed, a client process and a server process are created, and these processes\ncommunicate with each other by reading from, and writing to, sockets. When creat-\ning a network application, the developer’s main task is therefore to write the code\nfor both the client and server programs.\nThere are two types of network applications. One type is an implementation\nwhose operation is specified in a protocol standard, such as an RFC or some\nother standards document; such an application is sometimes referred to as\n“open,” since the rules specifying its operation are known to all. For such an\nimplementation, the client and server programs must conform to the rules dic-\ntated by the RFC. For example, the client program could be an implementation\nof the client side of the FTP protocol, described in Section 2.3 and explicitly\ndefined in RFC 959; similarly, the server program could be an implementation of\nthe FTP server protocol, also explicitly defined in RFC 959. If one developer\nwrites code for the client program and another developer writes code for the\nserver program, and both developers carefully follow the rules of the RFC, then\nthe two programs will be able to interoperate. Indeed, many of today’s network\napplications involve communication between client and server programs that\nhave been created by independent developers—for example, a Firefox browser\ncommunicating with an Apache Web server, or a BitTorrent client communicat-\ning with BitTorrent tracker.\nThe other type of network application is a proprietary network application. \nIn this case the client and server programs employ an application-layer protocol that\nhas not been openly published in an RFC or elsewhere. A single developer (or\n156\nCHAPTER 2\n•\nAPPLICATION LAYER"
    },
    {
      "chunk_id": "6ca552fd-f555-48d6-8993-e862b6d19878",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.7.1 Socket Programming with UDP",
      "original_titles": [
        "2.7.1 Socket Programming with UDP"
      ],
      "path": "Chapter 2 Application Layer > 2.7 Socket Programming: Creating Network Applications > 2.7.1 Socket Programming with UDP",
      "start_page": 184,
      "end_page": 189,
      "token_count": 2939,
      "text": "development team) creates both the client and server programs, and the developer\nhas complete control over what goes in the code. But because the code does not\nimplement an open protocol, other independent developers will not be able to\ndevelop code that interoperates with the application.\nIn this section, we’ll examine the key issues in developing a client-server appli-\ncation, and we’ll “get our hands dirty” by looking at code that implements a very\nsimple client-server application. During the development phase, one of the first\ndecisions the developer must make is whether the application is to run over TCP\nor over UDP. Recall that TCP is connection oriented and provides a reliable byte-\nstream channel through which data flows between two end systems. UDP is\nconnectionless and sends independent packets of data from one end system to the\nother, without any guarantees about delivery. Recall also that when a client or server\nprogram implements a protocol defined by an RFC, it should use the well-known\nport number associated with the protocol; conversely, when developing a propri-\netary application, the developer must be careful to avoid using such well-known\nport numbers. (Port numbers were briefly discussed in Section 2.1. They are cov-\nered in more detail in Chapter 3.)\nWe introduce UDP and TCP socket programming by way of a simple UDP appli-\ncation and a simple TCP application. We present the simple UDP and TCP applica-\ntions in Python. We could have written the code in Java, C, or C++, but we chose\nPython mostly because Python clearly exposes the key socket concepts. With Python\nthere are fewer lines of code, and each line can be explained to the novice program-\nmer without difficulty. But there’s no need to be frightened if you are not familiar with\nPython. You should be able to easily follow the code if you have experience program-\nming in Java, C, or C++.\nIf you are interested in client-server programming with Java, you are encour-\naged to see the companion Web site for this textbook; in fact, you can find there all\nthe examples in this section (and associated labs) in Java. For readers who are inter-\nested in client-server programming in C, there are several good references available\n[Donahoo 2001; Stevens 1997; Frost 1994; Kurose 1996]; our Python examples\nbelow have a similar look and feel to C.\n2.7.1 Socket Programming with UDP\nIn this subsection, we’ll write simple client-server programs that use UDP; in the\nfollowing section, we’ll write similar programs that use TCP.\nRecall from Section 2.1 that processes running on different machines communi-\ncate with each other by sending messages into sockets. We said that each process is\nanalogous to a house and the process’s socket is analogous to a door. The application\nresides on one side of the door in the house; the transport-layer protocol resides on\nthe other side of the door in the outside world. The application developer has control\nof everything on the application-layer side of the socket; however, it has little control\nof the transport-layer side.\n2.7\n•\nSOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS\n157\n\nNow let’s take a closer look at the interaction between two communicating\nprocesses that use UDP sockets. Before the sending process can push a packet of\ndata out the socket door, when using UDP, it must first attach a destination address\nto the packet. After the packet passes through the sender’s socket, the Internet will\nuse this destination address to route the packet through the Internet to the socket in\nthe receiving process. When the packet arrives at the receiving socket, the receiving\nprocess will retrieve the packet through the socket, and then inspect the packet’s\ncontents and take appropriate action.\nSo you may be now wondering, what goes into the destination address that is\nattached to the packet? As you might expect, the destination host’s IP address is part of\nthe destination address. By including the destination IP address in the packet, the\nrouters in the Internet will be able to route the packet through the Internet to the desti-\nnation host. But because a host may be running many network application processes,\neach with one or more sockets, it is also necessary to identify the particular socket in\nthe destination host. When a socket is created, an identifier, called a port number, is\nassigned to it. So, as you might expect, the packet’s destination address also includes\nthe socket’s port number. In summary, the sending process attaches to the packet a des-\ntination address which consists of the destination host’s IP address and the destination\nsocket’s port number. Moreover, as we shall soon see, the sender’s source address—\nconsisting of the IP address of the source host and the port number of the source\nsocket—are also attached to the packet. However, attaching the source address to the\npacket is typically not done by the UDP application code; instead it is automatically\ndone by the underlying operating system.\nWe’ll use the following simple client-server application to demonstrate socket\nprogramming for both UDP and TCP:\n1. The client reads a line of characters (data) from its keyboard and sends the data\nto the server.\n2. The server receives the data and converts the characters to uppercase.\n3. The server sends the modified data to the client.\n4. The client receives the modified data and displays the line on its screen.\nFigure 2.28 highlights the main socket-related activity of the client and server that\ncommunicate over the UDP transport service.\nNow let’s get our hands dirty and take a look at the client-server program pair\nfor a UDP implementation of this simple application. We also provide a detailed,\nline-by-line analysis after each program. We’ll begin with the UDP client, which\nwill send a simple application-level message to the server. In order for the server to\nbe able to receive and reply to the client’s message, it must be ready and running—\nthat is, it must be running as a process before the client sends its message.\nThe client program is called UDPClient.py, and the server program is called\nUDPServer.py. In order to emphasize the key issues, we intentionally provide code\nthat is minimal. “Good code” would certainly have a few more auxiliary lines, in\n158\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nparticular for handling error cases. For this application, we have arbitrarily chosen\n12000 for the server port number.\n2.7\n•\nSOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS\n159\nCreate  socket, port=x:\nServer\nserverSocket =\nsocket(AF_INET,SOCK_DGRAM)\n(Running on serverIP)\nClient\nRead UDP segment from\nserverSocket\nWrite reply to\nspecifying client address,\nport number\nserverSocket\nCreate datagram with serverIP\nand port=x;\nsend datagram via\nclientSocket\nCreate socket:\nclientSocket =\nsocket(AF_INET,SOCK_DGRAM)\nRead datagram from\nclientSocket\nClose\nclientSocket\nFigure 2.28 \u0002 The client-server application using UDP\nUDPClient.py\nHere is the code for the client side of the application:\nfrom socket import *\nserverName = ‘hostname’\nserverPort = 12000\nclientSocket = socket(socket.AF_INET, socket.SOCK_DGRAM)\nmessage = raw_input(’Input lowercase sentence:’)\nclientSocket.sendto(message,(serverName, serverPort))\nmodifiedMessage, serverAddress = clientSocket.recvfrom(2048)\nprint modifiedMessage\nclientSocket.close()\n\nNow let’s take a look at the various lines of code in UDPClient.py.\nfrom socket import *\nThe socket module forms the basis of all network communications in Python. By\nincluding this line, we will be able to create sockets within our program.\nserverName = ‘hostname’\nserverPort = 12000\nThe first line sets the string serverName to hostname. Here, we provide a string\ncontaining either the IP address of the server (e.g., “128.138.32.126”) or the host-\nname of the server (e.g., “cis.poly.edu”). If we use the hostname, then a DNS lookup\nwill automatically be performed to get the IP address.) The second line sets the inte-\nger variable serverPort to 12000.\nclientSocket = socket(socket.AF_INET, socket.SOCK_DGRAM)\nThis line creates the client’s socket, called clientSocket. The first parameter\nindicates the address family; in particular, AF_INET indicates that the underlying\nnetwork is using IPv4. (Do not worry about this now—we will discuss IPv4 in\nChapter 4.) The second parameter indicates that the socket is of type SOCK_DGRAM,\nwhich means it is a UDP socket (rather than a TCP socket). Note that we are not\nspecifying the port number of the client socket when we create it; we are instead let-\nting the operating system do this for us. Now that the client process’s door has been\ncreated, we will want to create a message to send through the door.\n160\nCHAPTER 2\n•\nAPPLICATION LAYER\nmessage = raw_input(’Input lowercase sentence:’)\nraw_input() is a built-in function in Python. When this command is executed,\nthe user at the client is prompted with the words “Input data:” The user then uses her\nkeyboard to input a line, which is put into the variable message. Now that we have\na socket and a message, we will want to send the message through the socket to the\ndestination host.\nclientSocket.sendto(message,(serverName, serverPort))\nIn the above line, the method sendto() attaches the destination address\n(serverName, serverPort) to the message and sends the resulting packet into\nthe process’s socket, clientSocket. (As mentioned earlier, the source address is\nalso attached to the packet, although this is done automatically rather than explicitly\nby the code.) Sending a client-to-server message via a UDP socket is that simple!\nAfter sending the packet, the client waits to receive data from the server.\n\nmodifiedMessage, serverAddress = clientSocket.recvfrom(2048)\n2.7\n•\nSOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS\n161\nWith the above line, when a packet arrives from the Internet at the client’s socket,\nthe packet’s data is put into the variable modifiedMessage and the packet’s\nsource address is put into the variable serverAddress. The variable\nserverAddress contains both the server’s IP address and the server’s port\nnumber. The program UDPClient doesn’t actually need this server address infor-\nmation, since it already knows the server address from the outset; but this line of\nPython provides the server address nevertheless. The method recvfrom also\ntakes the buffer size 2048 as input. (This buffer size works for most purposes.)\nprint modifiedMessage\nThis line prints out modifiedMessage on the user’s display. It should be the original\nline that the user typed, but now capitalized.\nclientSocket.close()\nThis line closes the socket. The process then terminates.\nUDPServer.py\nLet’s now take a look at the server side of the application:\nfrom socket import *\nserverPort = 12000\nserverSocket = socket(AF_INET, SOCK_DGRAM)\nserverSocket.bind((’’, serverPort))\nprint ”The server is ready to receive”\nwhile 1:\nmessage, clientAddress = serverSocket.recvfrom(2048)\nmodifiedMessage = message.upper()\nserverSocket.sendto(modifiedMessage, clientAddress)\nNote that the beginning of UDPServer is similar to UDPClient. It also imports the\nsocket module, also sets the integer variable serverPort to 12000, and also cre-\nates a socket of type SOCK_DGRAM (a UDP socket). The first line of code that is\nsignificantly different from UDPClient is:\nserverSocket.bind((’’, serverPort))\nThe above line binds (that is, assigns) the port number 12000 to the server’s socket.\nThus in UDPServer, the code (written by the application developer) is explicitly\n\nassigning a port number to the socket. In this manner, when anyone sends a packet\nto port 12000 at the IP address of the server, that packet will be directed to this\nsocket. UDPServer then enters a while loop; the while loop will allow UDPServer\nto receive and process packets from clients indefinitely. In the while loop,\nUDPServer waits for a packet to arrive.\nmessage, clientAddress = serverSocket.recvfrom(2048)\nThis line of code is similar to what we saw in UDPClient. When a packet arrives at\nthe server’s socket, the packet’s data is put into the variable message and the\npacket’s source address is put into the variable clientAddress. The variable\nclientAddress contains both the client’s IP address and the client’s port number.\nHere, UDPServer will make use of this address information, as it provides a return\naddress, similar to the return address with ordinary postal mail. With this source\naddress information, the server now knows to where it should direct its reply.\nmodifiedMessage = message.upper()\nThis line is the heart of our simple application. It takes the line sent by the client and\nuses the method upper() to capitalize it.\nserverSocket.sendto(modifiedMessage, clientAddress)\nThis last line attaches the client’s address (IP address and port number) to the capi-\ntalized message, and sends the resulting packet into the server’s socket. (As men-\ntioned earlier, the server address is also attached to the packet, although this is done\nautomatically rather than explicitly by the code.) The Internet will then deliver the\npacket to this client address. After the server sends the packet, it remains in the\nwhile loop, waiting for another UDP packet to arrive (from any client running on\nany host).\nTo test the pair of programs, you install and compile UDPClient.py in one host\nand UDPServer.py in another host. Be sure to include the proper hostname or IP\naddress of the server in UDPClient.py. Next, you execute UDPServer.py, the com-\npiled server program, in the server host. This creates a process in the server that\nidles until it is contacted by some client. Then you execute UDPClient.py, the com-\npiled client program, in the client. This creates a process in the client. Finally, to use\nthe application at the client, you type a sentence followed by a carriage return.\nTo develop your own UDP client-server application, you can begin by\nslightly modifying the client or server programs. For example, instead of con-\nverting all the letters to uppercase, the server could count the number of times the\nletter s appears and return this number. Or you can modify the client so that after\nreceiving a capitalized sentence, the user can continue to send more sentences to\nthe server.\n162\nCHAPTER 2\n•\nAPPLICATION LAYER"
    },
    {
      "chunk_id": "db716047-3262-4f35-bf22-f9e62e7f5fbf",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.7.2 Socket Programming with TCP",
      "original_titles": [
        "2.7.2 Socket Programming with TCP"
      ],
      "path": "Chapter 2 Application Layer > 2.7 Socket Programming: Creating Network Applications > 2.7.2 Socket Programming with TCP",
      "start_page": 190,
      "end_page": 194,
      "token_count": 1881,
      "text": "2.7.2 Socket Programming with TCP\nUnlike UDP, TCP is a connection-oriented protocol. This means that before the client\nand server can start to send data to each other, they first need to handshake and estab-\nlish a TCP connection. One end of the TCP connection is attached to the client socket\nand the other end is attached to a server socket. When creating the TCP connection,\nwe associate with it the client socket address (IP address and port number) and the\nserver socket address (IP address and port number). With the TCP connection estab-\nlished, when one side wants to send data to the other side, it just drops the data into\nthe TCP connection via its socket. This is different from UDP, for which the server\nmust attach a destination address to the packet before dropping it into the socket.\nNow let’s take a closer look at the interaction of client and server programs in\nTCP. The client has the job of initiating contact with the server. In order for the\nserver to be able to react to the client’s initial contact, the server has to be ready.\nThis implies two things. First, as in the case of UDP, the TCP server must be run-\nning as a process before the client attempts to initiate contact. Second, the server\nprogram must have a special door—more precisely, a special socket—that wel-\ncomes some initial contact from a client process running on an arbitrary host. Using\nour house/door analogy for a process/socket, we will sometimes refer to the client’s\ninitial contact as “knocking on the welcoming door.”\nWith the server process running, the client process can initiate a TCP connec-\ntion to the server. This is done in the client program by creating a TCP socket. When\nthe client creates its TCP socket, it specifies the address of the welcoming socket in\nthe server, namely, the IP address of the server host and the port number of the\nsocket. After creating its socket, the client initiates a three-way handshake and\nestablishes a TCP connection with the server. The three-way handshake, which takes\nplace within the transport layer, is completely invisible to the client and server pro-\ngrams.\nDuring the three-way handshake, the client process knocks on the welcoming door\nof the server process. When the server “hears” the knocking, it creates a new door—\nmore precisely, a new socket that is dedicated to that particular client. In our example\nbelow, the welcoming door is a TCP socket object that we call serverSocket; the\nnewly created socket dedicated to the client making the connection is called connec-\ntionSocket. Students who are encountering TCP sockets for the first time some-\ntimes confuse the welcoming socket (which is the initial point of contact for all clients\nwanting to communicate with the server), and each newly created server-side connec-\ntion socket that is subsequently created for communicating with each client.\nFrom the application’s perspective, the client’s socket and the server’s connec-\ntion socket are directly connected by a pipe. As shown in Figure 2.29, the client\nprocess can send arbitrary bytes into its socket, and TCP guarantees that the server\nprocess will receive (through the connection socket) each byte in the order sent. TCP\nthus provides a reliable service between the client and server processes. Furthermore,\njust as people can go in and out the same door, the client process not only sends bytes\n2.7\n•\nSOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS\n163\n\ninto but also receives bytes from its socket; similarly, the server process not only\nreceives bytes from but also sends bytes into its connection socket.\nWe use the same simple client-server application to demonstrate socket program-\nming with TCP: The client sends one line of data to the server, the server capitalizes\nthe line and sends it back to the client. Figure 2.30 highlights the main socket-related\nactivity of the client and server that communicate over the TCP transport service.\nTCPClient.py\nHere is the code for the client side of the application:\n164\nCHAPTER 2\n•\nAPPLICATION LAYER\nClient process\nServer process\nClient\nsocket\nWelcoming\nsocket\nThree-way handshake\nConnection\nsocket\nbytes\nbytes\nFigure 2.29 \u0002 The TCPServer process has two sockets\nfrom socket import *\nserverName = ’servername’\nserverPort = 12000\nclientSocket = socket(AF_INET, SOCK_STREAM)\nclientSocket.connect((serverName,serverPort))\nsentence = raw_input(‘Input lowercase sentence:’)\nclientSocket.send(sentence)\nmodifiedSentence = clientSocket.recv(1024)\nprint ‘From Server:’, modifiedSentence\nclientSocket.close()\n\nLet’s now take a look at the various lines in the code that differ significantly\nfrom the UDP implementation. The first such line is the creation of the client\nsocket.\nclientSocket = socket(AF_INET, SOCK_STREAM)\nThis line creates the client’s socket, called clientSocket. The first parameter\nagain indicates that the underlying network is using IPv4. The second parameter\nindicates that the socket is of type SOCK_STREAM, which means it is a TCP socket\n(rather than a UDP socket). Note that we are again not specifying the port number\n2.7\n•\nSOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS\n165\nClose\nconnectionSocket\nWrite reply to\nconnectionSocket\nRead request from\nconnectionSocket\nCreate  socket, port=x,\nfor incoming request:\nServer\nserverSocket =\nsocket()\nWait for incoming\nconnection request:\nconnectionSocket =\nserverSocket.accept()\n(Running on serverIP)\nClient\nTCP\nconnection setup\nCreate socket, connect\nto serverIP, port=x:\nclientSocket =\nsocket()\nRead reply from\nclientSocket\nSend request using\nclientSocket\nClose\nclientSocket\nFigure 2.30 \u0002 The client-server application using TCP\n\nof the client socket when we create it; we are instead letting the operating system\ndo this for us. Now the next line of code is very different from what we saw in\nUDPClient:\nclientSocket.connect((serverName,serverPort))\nRecall that before the client can send data to the server (or vice versa) using a TCP\nsocket, a TCP connection must first be established between the client and server.\nThe above line initiates the TCP connection between the client and server. The\nparameter of the connect() method is the address of the server side of the con-\nnection. After this line of code is executed, the three-way handshake is performed\nand a TCP connection is established between the client and server.\n166\nCHAPTER 2\n•\nAPPLICATION LAYER\nsentence = raw_input(‘Input lowercase sentence:’)\nAs with UDPClient, the above obtains a sentence from the user. The string sentence\ncontinues to gather characters until the user ends the line by typing a carriage return.\nThe next line of code is also very different from UDPClient:\nclientSocket.send(sentence)\nThe above line sends the string sentence through the client’s socket and into the\nTCP connection. Note that the program does not explicitly create a packet and attach\nthe destination address to the packet, as was the case with UDP sockets. Instead the\nclient program simply drops the bytes in the string sentence into the TCP con-\nnection. The client then waits to receive bytes from the server.\nmodifiedSentence = clientSocket.recv(2048)\nWhen characters arrive from the server, they get placed into the string modified-\nSentence. Characters continue to accumulate in modifiedSentence until the\nline ends with a carriage return character. After printing the capitalized sentence, we\nclose the client’s socket:\nclientSocket.close()\nThis last line closes the socket and, hence, closes the TCP connection between the\nclient and the server. It causes TCP in the client to send a TCP message to TCP in\nthe server (see Section 3.5).\nTCPServer.py\nNow let’s take a look at the server program.\n\nfrom socket import *\nserverPort = 12000\nserverSocket = socket(AF_INET,SOCK_STREAM)\nserverSocket.bind((‘’,serverPort))\nserverSocket.listen(1)\nprint ‘The server is ready to receive’\nwhile 1:\nconnectionSocket, addr = serverSocket.accept()\nsentence = connectionSocket.recv(1024)\ncapitalizedSentence = sentence.upper()\nconnectionSocket.send(capitalizedSentence)\nconnectionSocket.close()\nLet’s now take a look at the lines that differ significantly from UDPServer and TCP-\nClient. As with TCPClient, the server creates a TCP socket with:\nserverSocket=socket(AF_INET,SOCK_STREAM)\nSimilar to UDPServer, we associate the server port number, serverPort, with\nthis socket:\nserverSocket.bind((‘’,serverPort))\nBut with TCP, serverSocket will be our welcoming socket. After establish-\ning this welcoming door, we will wait and listen for some client to knock on the\ndoor:\nserverSocket.listen(1)\nThis line has the server listen for TCP connection requests from the client. The\nparameter specifies the maximum number of queued connections (at least 1).\nconnectionSocket, addr = serverSocket.accept()\nWhen a client knocks on this door, the program invokes the accept() method for\nserverSocket, which creates a new socket in the server, called connec-\ntionSocket, dedicated to this particular client. The client and server then complete\nthe handshaking, creating a TCP connection between the client’s clientSocket\nand the server’s connectionSocket. With the TCP connection established, the\nclient and server can now send bytes to each other over the connection. With TCP, all\nbytes sent from one side not are not only guaranteed to arrive at the other side but also\nguaranteed arrive in order.\nconnectionSocket.close()\n2.7\n•\nSOCKET PROGRAMMING: CREATING NETWORK APPLICATIONS\n167"
    },
    {
      "chunk_id": "3574f5eb-dd18-4935-8d5c-b322591e1d11",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "2.8 Summary",
      "original_titles": [
        "2.8 Summary"
      ],
      "path": "Chapter 2 Application Layer > 2.8 Summary",
      "start_page": 195,
      "end_page": 195,
      "token_count": 625,
      "text": "In this program, after sending the modified sentence to the client, we close the con-\nnection socket. But since serverSocket remains open, another client can now\nknock on the door and send the server a sentence to modify.\nThis completes our discussion of socket programming in TCP. You are encour-\naged to run the two programs in two separate hosts, and also to modify them to\nachieve slightly different goals. You should compare the UDP program pair with the\nTCP program pair and see how they differ. You should also do many of the socket\nprogramming assignments described at the ends of Chapters 2, 4, and 7. Finally, we\nhope someday, after mastering these and more advanced socket programs, you will\nwrite your own popular network application, become very rich and famous, and\nremember the authors of this textbook!\n2.8 Summary\nIn this chapter, we’ve studied the conceptual and the implementation aspects of net-\nwork applications. We’ve learned about the ubiquitous client-server architecture\nadopted by many Internet applications and seen its use in the HTTP, FTP, SMTP,\nPOP3, and DNS protocols. We’ve studied these important application-level proto-\ncols, and their corresponding associated applications (the Web, file transfer, e-mail,\nand DNS) in some detail. We’ve also learned about the increasingly prevalent P2P\narchitecture and how it is used in many applications. We’ve examined how the\nsocket API can be used to build network applications. We’ve walked through the use\nof sockets for connection-oriented (TCP) and connectionless (UDP) end-to-end\ntransport services. The first step in our journey down the layered network architec-\nture is now complete!\nAt the very beginning of this book, in Section 1.1, we gave a rather vague, bare-\nbones definition of a protocol: “the format and the order of messages exchanged\nbetween two or more communicating entities, as well as the actions taken on the trans-\nmission and/or receipt of a message or other event.” The material in this chapter, and\nin particular our detailed study of the HTTP, FTP, SMTP, POP3, and DNS protocols,\nhas now added considerable substance to this definition. Protocols are a key concept\nin networking; our study of application protocols has now given us the opportunity to\ndevelop a more intuitive feel for what protocols are all about.\nIn Section 2.1, we described the service models that TCP and UDP offer to\napplications that invoke them. We took an even closer look at these service models\nwhen we developed simple applications that run over TCP and UDP in Section 2.7.\nHowever, we have said little about how TCP and UDP provide these service mod-\nels. For example, we know that TCP provides a reliable data service, but we haven’t\nsaid yet how it does so. In the next chapter we’ll take a careful look at not only the\nwhat, but also the how and why of transport protocols.\n168\nCHAPTER 2\n•\nAPPLICATION LAYER"
    },
    {
      "chunk_id": "59a2084e-a637-4b30-956a-b9171af8dc84",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Homework Problems and Questions",
      "original_titles": [
        "Homework Problems and Questions"
      ],
      "path": "Chapter 2 Application Layer > Homework Problems and Questions",
      "start_page": 196,
      "end_page": 205,
      "token_count": 4878,
      "text": "Equipped with knowledge about Internet application structure and application-\nlevel protocols, we’re now ready to head further down the protocol stack and exam-\nine the transport layer in Chapter 3.\nHomework Problems and Questions\nChapter 2 Review Questions\nSECTION 2.1\nR1. List five nonproprietary Internet applications and the application-layer \nprotocols that they use.\nR2. What is the difference between network architecture and application \narchitecture?\nR3. For a communication session between a pair of processes, which process is\nthe client and which is the server?\nR4. For a P2P file-sharing application, do you agree with the statement, “There is\nno notion of client and server sides of a communication session”? Why or\nwhy not?\nR5. What information is used by a process running on one host to identify a\nprocess running on another host?\nR6. Suppose you wanted to do a transaction from a remote client to a server as\nfast as possible. Would you use UDP or TCP? Why?\nR7. Referring to Figure 2.4, we see that none of the applications listed in Figure\n2.4 requires both no data loss and timing. Can you conceive of an application\nthat requires no data loss and that is also highly time-sensitive?\nR8. List the four broad classes of services that a transport protocol can provide.\nFor each of the service classes, indicate if either UDP or TCP (or both) pro-\nvides such a service.\nR9. Recall that TCP can be enhanced with SSL to provide process-to-process\nsecurity services, including encryption. Does SSL operate at the transport\nlayer or the application layer? If the application developer wants TCP to be\nenhanced with SSL, what does the developer have to do?\nSECTIONS 2.2–2.5\nR10. What is meant by a handshaking protocol?\nR11. Why do HTTP, FTP, SMTP, and POP3 run on top of TCP rather than on UDP?\nR12. Consider an e-commerce site that wants to keep a purchase record for each of\nits customers. Describe how this can be done with cookies.\nHOMEWORK PROBLEMS AND QUESTIONS\n169\n\nR13. Describe how Web caching can reduce the delay in receiving a requested\nobject. Will Web caching reduce the delay for all objects requested by a user\nor for only some of the objects? Why?\nR14. Telnet into a Web server and send a multiline request message. Include in the\nrequest message the If-modified-since: header line to force a\nresponse message with the 304 Not Modified status code.\nR15. Why is it said that FTP sends control information “out-of-band”?\nR16. Suppose Alice, with a Web-based e-mail account (such as Hotmail or gmail),\nsends a message to Bob, who accesses his mail from his mail server using\nPOP3. Discuss how the message gets from Alice’s host to Bob’s host. Be sure\nto list the series of application-layer protocols that are used to move the mes-\nsage between the two hosts.\nR17. Print out the header of an e-mail message you have recently received. How\nmany Received: header lines are there? Analyze each of the header lines\nin the message.\nR18. From a user’s perspective, what is the difference between the download-and-\ndelete mode and the download-and-keep mode in POP3?\nR19. Is it possible for an organization’s Web server and mail server to have exactly\nthe same alias for a hostname (for example, foo.com)? What would be the\ntype for the RR that contains the hostname of the mail server?\nR20. Look over your received emails, and examine the header of a message sent\nfrom a user with an .edu email address. Is it possible to determine from the\nheader the IP address of the host from which the message was sent? Do the\nsame for a message sent from a gmail account. \nSECTION 2.6\nR21. In BitTorrent, suppose Alice provides chunks to Bob throughout a 30-second\ninterval. Will Bob necessarily return the favor and provide chunks to Alice in\nthis same interval? Why or why not?\nR22. Consider a new peer Alice that joins BitTorrent without possessing any\nchunks. Without any chunks, she cannot become a top-four uploader for any\nof the other peers, since she has nothing to upload. How then will Alice get\nher first chunk?\nR23. What is an overlay network? Does it include routers? What are the edges in\nthe overlay network?\nR24. Consider a DHT with a mesh overlay topology (that is, every peer tracks all\npeers in the system). What are the advantages and disadvantages of such a\ndesign? What are the advantages and disadvantages of a circular DHT (with\nno shortcuts)?\n170\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nR25. List at least four different applications that are naturally suitable for P2P\narchitectures. (Hint: File distribution and instant messaging are two.)\nSECTION 2.7\nR26. In Section 2.7, the UDP server described needed only one socket, whereas the\nTCP server needed two sockets. Why? If the TCP server were to support n\nsimultaneous connections, each from a different client host, how many sockets\nwould the TCP server need?\nR27. For the client-server application over TCP described in Section 2.7, why must\nthe server program be executed before the client program? For the client-\nserver application over UDP, why may the client program be executed before\nthe server program?\nProblems\nP1. True or false?\na. A user requests a Web page that consists of some text and three images.\nFor this page, the client will send one request message and receive four\nresponse messages.\nb. Two distinct Web pages (for example, www.mit.edu/research.html\nand www.mit.edu/students.html) can be sent over the same per-\nsistent connection.\nc. With nonpersistent connections between browser and origin server, it is pos-\nsible for a single TCP segment to carry two distinct HTTP request messages.\nd. The Date: header in the HTTP response message indicates when the\nobject in the response was last modified.\ne. HTTP response messages never have an empty message body.\nP2. Read RFC 959 for FTP. List all of the client commands that are supported by\nthe RFC.\nP3. Consider an HTTP client that wants to retrieve a Web document at a given\nURL. The IP address of the HTTP server is initially unknown. What transport\nand application-layer protocols besides HTTP are needed in this scenario?\nP4. Consider the following string of ASCII characters that were captured by\nWireshark when the browser sent an HTTP GET message (i.e., this is the actual\ncontent of an HTTP GET message). The characters <cr><lf> are carriage\nreturn and line-feed characters (that is, the italized character string <cr> in\nthe text below represents the single carriage-return character that was con-\ntained at that point in the HTTP header). Answer the following questions,\nindicating where in the HTTP GET message below you find the answer.\nPROBLEMS\n171\n\nGET /cs453/index.html HTTP/1.1<cr><lf>Host: gai\na.cs.umass.edu<cr><lf>User-Agent: Mozilla/5.0 (\nWindows;U; Windows NT 5.1; en-US; rv:1.7.2) Gec\nko/20040804 Netscape/7.2 (ax) <cr><lf>Accept:ex\nt/xml, application/xml, application/xhtml+xml, text\n/html;q=0.9, text/plain;q=0.8,image/png,*/*;q=0.5\n<cr><lf>Accept-Language: en-us,en;q=0.5<cr><lf>Accept-\nEncoding: zip,deflate<cr><lf>Accept-Charset: ISO\n-8859-1,utf-8;q=0.7,*;q=0.7<cr><lf>Keep-Alive: 300<cr>\n<lf>Connection:keep-alive<cr><lf><cr><lf>\na. What is the URL of the document requested by the browser?\nb. What version of HTTP is the browser running?\nc. Does the browser request a non-persistent or a persistent connection?\nd. What is the IP address of the host on which the browser is running?\ne. What type of browser initiates this message? Why is the browser type\nneeded in an HTTP request message?\nP5. The text below shows the reply sent from the server in response to the HTTP\nGET message in the question above. Answer the following questions, indicat-\ning where in the message below you find the answer.\n172\nCHAPTER 2\n•\nAPPLICATION LAYER\nHTTP/1.1 200 OK<cr><lf>Date: Tue, 07 Mar 2008\n12:39:45GMT<cr><lf>Server: Apache/2.0.52 (Fedora)\n<cr><lf>Last-Modified: Sat, 10 Dec2005 18:27:46\nGMT<cr><lf>ETag: “526c3-f22-a88a4c80”<cr><lf>Accept-\nRanges: bytes<cr><lf>Content-Length: 3874<cr><lf>\nKeep-Alive: timeout=max=100<cr><lf>Connection:\nKeep-Alive<cr><lf>Content-Type: text/html; charset=\nISO-8859-1<cr><lf><cr><lf><!doctype html public “-\n//w3c//dtd html 4.0 transitional//en”><lf><html><lf>\n<head><lf> <meta http-equiv=”Content-Type”\ncontent=”text/html; charset=iso-8859-1”><lf> <meta\nname=”GENERATOR” content=”Mozilla/4.79 [en] (Windows NT\n5.0; U) Netscape]”><lf> <title>CMPSCI 453 / 591 / \nNTU-ST550A Spring 2005 homepage</title><lf></head><lf>\n<much more document text following here (not shown)>\na. Was the server able to successfully find the document or not? What time\nwas the document reply provided?\nb. When was the document last modified?\nc. How many bytes are there in the document being returned?\nd. What are the first 5 bytes of the document being returned? Did the server\nagree to a persistent connection?\n\nP6. Obtain the HTTP/1.1 specification (RFC 2616). Answer the following questions:\na. Explain the mechanism used for signaling between the client and server\nto indicate that a persistent connection is being closed. Can the client, the\nserver, or both signal the close of a connection?\nb. What encryption services are provided by HTTP?\nc. Can a client open three or more simultaneous connections with a given\nserver?\nd. Either a server or a client may close a transport connection between them\nif either one detects the connection has been idle for some time. Is it pos-\nsible that one side starts closing a connection while the other side is trans-\nmitting data via this connection? Explain. \nP7. Suppose within your Web browser you click on a link to obtain a Web page.\nThe IP address for the associated URL is not cached in your local host, so a\nDNS lookup is necessary to obtain the IP address. Suppose that n DNS\nservers are visited before your host receives the IP address from DNS; the\nsuccessive visits incur an RTT of RTT1, . . ., RTTn. Further suppose that the\nWeb page associated with the link contains exactly one object, consisting of a\nsmall amount of HTML text. Let RTT0 denote the RTT between the local host\nand the server containing the object. Assuming zero transmission time of the\nobject, how much time elapses from when the client clicks on the link until\nthe client receives the object?\nP8. Referring to Problem P7, suppose the HTML file references eight very small\nobjects on the same server. Neglecting transmission times, how much time\nelapses with\na. Non-persistent HTTP with no parallel TCP connections?\nb. Non-persistent HTTP with the browser configured for 5 parallel connections?\nc. Persistent HTTP?\nP9. Consider Figure 2.12, for which there is an institutional network connected to\nthe Internet. Suppose that the average object size is 850,000 bits and that the\naverage request rate from the institution’s browsers to the origin servers is 16\nrequests per second. Also suppose that the amount of time it takes from when\nthe router on the Internet side of the access link forwards an HTTP request\nuntil it receives the response is three seconds on average (see Section 2.2.5).\nModel the total average response time as the sum of the average access delay\n(that is, the delay from Internet router to institution router) and the average\nInternet delay. For the average access delay, use Δ/(1 – Δ\u0005), where Δ is the\naverage time required to send an object over the access link and \u0005 is the\narrival rate of objects to the access link.\na. Find the total average response time.\nb. Now suppose a cache is installed in the institutional LAN. Suppose the\nmiss rate is 0.4. Find the total response time.\nPROBLEMS\n173\n\nP10. Consider a short, 10-meter link, over which a sender can transmit at a rate of\n150 bits/sec in both directions. Suppose that packets containing data are\n100,000 bits long, and packets containing only control (e.g., ACK or hand-\nshaking) are 200 bits long. Assume that N parallel connections each get 1/N\nof the link bandwidth. Now consider the HTTP protocol, and suppose that\neach downloaded object is 100 Kbits long, and that the initial downloaded\nobject contains 10 referenced objects from the same sender. Would parallel\ndownloads via parallel instances of non-persistent HTTP make sense in this\ncase? Now consider persistent HTTP. Do you expect significant gains over\nthe non-persistent case? Justify and explain your answer.\nP11. Consider the scenario introduced in the previous problem. Now suppose that\nthe link is shared by Bob with four other users. Bob uses parallel instances of\nnon-persistent HTTP, and the other four users use non-persistent HTTP with-\nout parallel downloads. \na. Do Bob’s parallel connections help him get Web pages more quickly?\nWhy or why not? \nb. If all five users open five parallel instances of non-persistent HTTP, then\nwould Bob’s parallel connections still be beneficial? Why or why not?\nP12. Write a simple TCP program for a server that accepts lines of input from a\nclient and prints the lines onto the server’s standard output. (You can do this by\nmodifying the TCPServer.py program in the text.) Compile and execute your\nprogram. On any other machine that contains a Web browser, set the proxy\nserver in the browser to the host that is running your server program; also con-\nfigure the port number appropriately. Your browser should now send its GET\nrequest messages to your server, and your server should display the messages\non its standard output. Use this platform to determine whether your browser\ngenerates conditional GET messages for objects that are locally cached.\nP13. What is the difference between MAIL FROM: in SMTP and From: in the\nmail message itself?\nP14. How does SMTP mark the end of a message body? How about HTTP? Can\nHTTP use the same method as SMTP to mark the end of a message body?\nExplain.\nP15. Read RFC 5321 for SMTP. What does MTA stand for? Consider the follow-\ning received spam email (modified from a real spam email). Assuming only\nthe originator of this spam email is malacious and all other hosts are honest,\nidentify the malacious host that has generated this spam email.\nFrom - Fri Nov 07 13:41:30 2008\nReturn-Path: <tennis5@pp33head.com>\nReceived: from barmail.cs.umass.edu\n(barmail.cs.umass.edu [128.119.240.3]) by cs.umass.edu\n(8.13.1/8.12.6) for <hg@cs.umass.edu>; Fri, 7 Nov 2008\n13:27:10 -0500\n174\nCHAPTER 2\n•\nAPPLICATION LAYER\n\nReceived: from asusus-4b96 (localhost [127.0.0.1]) by\nbarmail.cs.umass.edu (Spam Firewall) for\n<hg@cs.umass.edu>; Fri,  7 Nov 2008 13:27:07 -0500\n(EST)\nReceived: from asusus-4b96 ([58.88.21.177]) by \nbarmail.cs.umass.edu for <hg@cs.umass.edu>; Fri, \n07 Nov 2008 13:27:07 -0500 (EST)\nReceived: from [58.88.21.177] by\ninbnd55.exchangeddd.com; Sat, 8 Nov 2008 01:27:07 +0700\nFrom: \"Jonny\" <tennis5@pp33head.com>\nTo: <hg@cs.umass.edu>\nSubject: How to secure your savings\nP16. Read the POP3 RFC, RFC 1939. What is the purpose of the UIDL POP3\ncommand?\nP17. Consider accessing your e-mail with POP3.\na. Suppose you have configured your POP mail client to operate in the\ndownload-and-delete mode. Complete the following transaction:\nC: list\nS: 1 498\nS: 2 912\nS: .\nC: retr 1\nS: blah blah ...\nS: ..........blah\nS: .\n?\n?\nb. Suppose you have configured your POP mail client to operate in the\ndownload-and-keep mode. Complete the following transaction:\nC: list\nS: 1 498\nS: 2 912\nS: .\nC: retr 1\nS: blah blah ...\nS: ..........blah\nS: .\n?\n?\nPROBLEMS\n175\n\n176\nCHAPTER 2\n•\nAPPLICATION LAYER\nc. Suppose you have configured your POP mail client to operate in the\ndownload-and-keep mode. Using your transcript in part (b), suppose you\nretrieve messages 1 and 2, exit POP, and then five minutes later you again\naccess POP to retrieve new e-mail. Suppose that in the five-minute inter-\nval no new messages have been sent to you. Provide a transcript of this\nsecond POP session.\nP18. a. What is a whois database?\nb. Use various whois databases on the Internet to obtain the names of two\nDNS servers. Indicate which whois databases you used.\nc. Use nslookup on your local host to send DNS queries to three DNS servers:\nyour local DNS server and the two DNS servers you found in part (b). Try\nquerying for Type A, NS, and MX reports. Summarize your findings.\nd. Use nslookup to find a Web server that has multiple IP addresses. Does\nthe Web server of your institution (school or company) have multiple IP\naddresses?\ne. Use the ARIN whois database to determine the IP address range used by\nyour university.\nf. Describe how an attacker can use whois databases and the nslookup tool\nto perform reconnaissance on an institution before launching an attack.\ng. Discuss why whois databases should be publicly available.\nP19. In this problem, we use the useful dig tool available on Unix and Linux hosts\nto explore the hierarchy of DNS servers. Recall that in Figure 2.21, a DNS\nserver higher in the DNS hierarchy delegates a DNS query to a DNS server\nlower in the hierarchy, by sending back to the DNS client the name of that\nlower-level DNS server. First read the man page for dig, and then answer the\nfollowing questions. \na. Starting with a root DNS server (from one of  the root servers [a-m].root-\nservers.net), initiate a sequence of queries for the IP address for your\ndepartment’s Web server by using dig. Show the list of the names of DNS\nservers in the delegation chain in answering your query.\nb. Repeat part a) for several popular Web sites, such as google.com,\nyahoo.com, or amazon.com. \nP20. Suppose you can access the caches in the local DNS servers of your department.\nCan you propose a way to roughly determine the Web servers (outside your\ndepartment) that are most popular among the users in your department? Explain. \nP21. Suppose that your department has a local DNS server for all computers in the\ndepartment. You are an ordinary user (i.e., not a network/system administra-\ntor). Can you determine if an external Web site was likely accessed from a\ncomputer in your department a couple of seconds ago? Explain. \n\nPROBLEMS\n177\nP22. Consider distributing a file of F = 15 Gbits to N peers. The server has an upload\nrate of us = 30 Mbps, and each peer has a download rate of di = 2 Mbps and an\nupload rate of u. For N = 10, 100, and 1,000 and u = 300 Kbps, 700 Kbps, and\n2 Mbps, prepare a chart giving the minimum distribution time for each of \nthe combinations of N and u for both client-server distribution and P2P\ndistribution.\nP23. Consider distributing a file of F bits to N peers using a client-server architec-\nture. Assume a fluid model where the server can simultaneously transmit to\nmultiple peers, transmitting to each peer at different rates, as long as the com-\nbined rate does not exceed us.\na. Suppose that us/N ≤dmin. Specify a distribution scheme that has a distri-\nbution time of NF/us.\nb. Suppose that us/N ≥dmin. Specify a distribution scheme that has a distri-\nbution time of F/ dmin.\nc. Conclude that the minimum distribution time is in general given by\nmax{NF/us, F/ dmin}.\nP24. Consider distributing a file of F bits to N peers using a P2P architecture.\nAssume a fluid model. For simplicity assume that dmin is very large, so that\npeer download bandwidth is never a bottleneck.\na. Suppose that us ≤(us + u1 + ... + uN)/N. Specify a distribution scheme\nthat has a distribution time of F/us.\nb. Suppose that us ≥(us + u1 + ... + uN)/N. Specify a distribution scheme\nthat has a distribution time of NF/(us + u1 + ... + uN).\nc. Conclude that the minimum distribution time is in general given by\nmax{F/us, NF/(us + u1 + ... + uN)}.\nP25. Consider an overlay network with N active peers, with each pair of peers hav-\ning an active TCP connection. Additionally, suppose that the TCP connections\npass through a total of M routers. How many nodes and edges are there in the\ncorresponding overlay network?\nP26. Suppose Bob joins a BitTorrent torrent, but he does not want to upload any\ndata to any other peers (so called free-riding). \na. Bob claims that he can receive a complete copy of the file that is shared\nby the swarm. Is Bob’s claim possible? Why or why not? \nb. Bob further claims that he can further make his “free-riding” more effi-\ncient by using a collection of multiple computers (with distinct IP\naddresses) in the computer lab in his department. How can he do that?\nP27. In the circular DHT example in Section 2.6.2, suppose that peer 3 learns that\npeer 5 has left. How does peer 3 update its successor state information?\nWhich peer is now its first successor? Its second successor?\nVideoNote\nWalking through \ndistributed hash tables\n\n178\nCHAPTER 2\n•\nAPPLICATION LAYER\nP28. In the circular DHT example in Section 2.6.2, suppose that a new peer 6\nwants to join the DHT and peer 6 initially only knows peer 15’s IP address.\nWhat steps are taken?\nP29. Because an integer in [0, 2n\u0003 1] can be expressed as an n-bit binary number in\na DHT, each key can be expressed as k = (k0, k1, . . . , kn–1), and each peer iden-\ntifier can be expressed p = (p0, p1, . . . , pn–1). Let’s now define the XOR dis-\ntance between a key k and peer p as\nDescribe how this metric can be used to assign (key, value) pairs to peers. \n(To learn about how to build an efficient DHT using this natural metric, see\n[Maymounkov 2002] in which the Kademlia DHT is described.)\nP30. As DHTs are overlay networks, they may not necessarily match the under-\nlay physical network well in the sense that two neighboring peers might be\nphysically very far away; for example, one peer could be in Asia and its\nneighbor could be in North America. If we randomly and uniformly assign\nidentifiers to newly joined peers, would this assignment scheme cause such\na mismatch? Explain. And how would such a mismatch affect the DHT’s\nperformance?\nP31. Install and compile the Python programs TCPClient and UDPClient on one\nhost and TCPServer and UDPServer on another host.\na. Suppose you run TCPClient before you run TCPServer. What happens?\nWhy?\nb. Suppose you run UDPClient before you run UDPServer. What happens?\nWhy?\nc. What happens if you use different port numbers for the client and server\nsides?\nP32. Suppose that in UDPClient.py, after we create the socket, we add the line: \nclientSocket.bind(('', 5432))\nWill it become necessary to change UDPServer.py? What are the port num-\nbers for the sockets in UDPClient and UDPServer? What were they before\nmaking this change?\nP33. Can you configure your browser to open multiple simultaneous connections\nto a Web site? What are the advantages and disadvantages of having a large\nnumber of simultaneous TCP connections?\nP34 We have seen that Internet TCP sockets treat the data being sent as a byte\nstream but UDP sockets recognize message boundaries. What are one\nd1k, p2 = a\nn-1\nj=0\n\u0002 kj - pj\u00022j"
    },
    {
      "chunk_id": "87718a21-bf82-4f38-9277-ee2e0655a754",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Socket Programming Assignments",
      "original_titles": [
        "Socket Programming Assignments"
      ],
      "path": "Chapter 2 Application Layer > Socket Programming Assignments",
      "start_page": 206,
      "end_page": 207,
      "token_count": 1163,
      "text": "advantage and one disadvantage of byte-oriented API versus having the API\nexplicitly recognize and preserve application-defined message boundaries?\nP35. What is the Apache Web server? How much does it cost? What functionality\ndoes it currently have? You may want to look at Wikipedia to answer this\nquestion.\nP36. Many BitTorrent clients use DHTs to create a distributed tracker. For these\nDHTs, what is the “key” and what is the “value”?\nSocket Programming Assignments\nThe companion Web site includes six socket programming assignments. The first\nfour assignments are summarized below. The fifth assignment makes use of the\nICMP protocol and is summarized at the end of Chapter 4. The sixth assignment\nemploys multimedia protocols and is summarized at the end of Chapter 7. It is\nhighly recommended that students complete several, if not all, of these assignments.\nStudents can find full details of these assignments, as well as important snippets of\nthe Python code, at the Web site http://www.awl.com/kurose-ross.\nAssignment 1: Web Server\nIn this assignment, you will develop a simple Web server in Python that is capable\nof processing only one request. Specifically, your Web server will (i) create a con-\nnection socket when contacted by a client (browser); (ii) receive the HTTP request\nfrom this connection; (iii) parse the request to determine the specific file being\nrequested; (iv) get the requested file from the server’s file system; (v) create an\nHTTP response message consisting of the requested file preceded by header lines;\nand (vi) send the response over the TCP connection to the requesting browser. If a\nbrowser requests a file that is not present in your server, your server should return a\n“404 Not Found” error message.\nIn the companion Web site, we provide the skeleton code for your server. Your\njob is to complete the code, run your server, and then test your server by sending\nrequests from browsers running on different hosts. If you run your server on a host\nthat already has a Web server running on it, then you should use a different port than\nport 80 for your Web server.\nAssignment 2: UDP Pinger\nIn this programming assignment, you will write a client ping program in Python.\nYour client will send a simple ping message to a server, receive a corresponding\npong message back from the server, and determine the delay between when the\nclient sent the ping message and received the pong message. This delay is called the\nRound Trip Time (RTT). The functionality provided by the client and server is\nSOCKET PROGRAMMING ASSIGNMENTS\n179\n\nsimilar to the functionality provided by standard ping program available in modern\noperating systems. However, standard ping programs use the Internet Control Mes-\nsage Protocol (ICMP) (which we will study in Chapter 4). Here we will create a\nnonstandard (but simple!) UDP-based ping program.\nYour ping program is to send 10 ping messages to the target server over UDP.\nFor each message, your client is to determine and print the RTT when the correspon-\nding pong message is returned. Because UDP is an unreliable protocol, a packet sent\nby the client or server may be lost. For this reason, the client cannot wait indefinitely\nfor a reply to a ping message. You should have the client wait up to one second for a\nreply from the server; if no reply is received, the client should assume that the\npacket was lost and print a message accordingly.\nIn this assignment, you will be given the complete code for the server (avail-\nable in the companion Web site). Your job is to write the client code, which will be\nvery similar to the server code. It is recommended that you first study carefully the\nserver code. You can then write your client code, liberally cutting and pasting lines\nfrom the server code.\nAssignment 3: Mail Client\nThe goal of this programming assignment is to create a simple mail client that sends\nemail to any recipient. Your client will need to establish a TCP connection with a\nmail server (e.g., a Google mail server), dialogue with the mail server using the\nSMTP protocol, send an email message to a recipient (e.g., your friend) via the mail\nserver, and finally close the TCP connection with the mail server.\nFor this assignment, the companion Web site provides the skeleton code for\nyour client. Your job is to complete the code and test your client by sending\nemail to different user accounts. You may also try sending through different\nservers (for example, through a Google mail server and through your university\nmail server).\nAssignment 4: Multi-Threaded Web Proxy\nIn this assignment, you will develop a Web proxy. When your proxy receives an\nHTTP request for an object from a browser, it generates a new HTTP request for\nthe same object and sends it to the origin server. When the proxy receives the\ncorresponding HTTP response with the object from the origin server, it creates a\nnew HTTP response, including the object, and sends it to the client. This proxy\nwill be multi-threaded, so that it will be able to handle multiple requests at the\nsame time.\nFor this assignment, the companion Web site provides the skeleton code for the\nproxy server. Your job is to complete the code, and then test it by having different\nbrowsers request Web objects via your proxy.\n180\nCHAPTER 2\n•\nAPPLICATION LAYER"
    },
    {
      "chunk_id": "a9563204-549d-48f0-83eb-8be9e77282b1",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Wireshark Labs: HTTP, DNS",
      "original_titles": [
        "Wireshark Labs: HTTP, DNS"
      ],
      "path": "Chapter 2 Application Layer > Wireshark Labs: HTTP, DNS",
      "start_page": 208,
      "end_page": 208,
      "token_count": 314,
      "text": "Wireshark Lab: HTTP\nHaving gotten our feet wet with the Wireshark packet sniffer in Lab 1, we’re now\nready to use Wireshark to investigate protocols in operation. In this lab, we’ll explore\nseveral aspects of the HTTP protocol: the basic GET/reply interaction, HTTP message\nformats, retrieving large HTML files, retrieving HTML files with embedded URLs,\npersistent and non-persistent connections, and HTTP authentication and security.\nAs is the case with all Wireshark labs, the full description of this lab is available\nat this book’s Web site, http://www.awl.com/kurose-ross.\nWireshark Lab: DNS\nIn this lab, we take a closer look at the client side of the DNS, the protocol that trans-\nlates Internet hostnames to IP addresses. Recall from Section 2.5 that the client’s role in\nthe DNS is relatively simple—a client sends a query to its local DNS server and\nreceives a response back. Much can go on under the covers, invisible to the DNS\nclients, as the hierarchical DNS servers communicate with each other to either recur-\nsively or iteratively resolve the client’s DNS query. From the DNS client’s standpoint,\nhowever, the protocol is quite simple—a query is formulated to the local DNS server\nand a response is received from that server. We observe DNS in action in this lab.\nAs is the case with all Wireshark labs, the full description of this lab is available at\nthis book’s Web site, http://www.awl.com/kurose-ross.\nWIRESHARK LABS\n181\nVideoNote\nUsing Wireshark to\ninvestigate the \nHTTP protocol"
    },
    {
      "chunk_id": "7f3443ff-15c8-42aa-a805-d4ec4149c7ac",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Interview: Marc Andreessen",
      "original_titles": [
        "Interview: Marc Andreessen"
      ],
      "path": "Chapter 2 Application Layer > Interview: Marc Andreessen",
      "start_page": 209,
      "end_page": 211,
      "token_count": 733,
      "text": "182\nHow did you become interested in computing? Did you always know that you wanted to\nwork in information technology?\nThe video game and personal computing revolutions hit right when I was growing up—\npersonal computing was the new technology frontier in the late 70’s and early 80’s. And it\nwasn’t just Apple and the IBM PC, but hundreds of new companies like Commodore and\nAtari as well. I taught myself to program out of a book called “Instant Freeze-Dried BASIC”\nat age 10, and got my first computer (a TRS-80 Color Computer—look it up!) at age 12.\nPlease describe one or two of the most exciting projects you have worked on during your\ncareer. What were the biggest challenges?\nUndoubtedly the most exciting project was the original Mosaic web browser in ’92–’93—\nand the biggest challenge was getting anyone to take it seriously back then. At the time,\neveryone thought the interactive future would be delivered as “interactive television” by\nhuge companies, not as the Internet by startups.\nWhat excites you about the future of networking and the Internet? What are your biggest\nconcerns?\nThe most exciting thing is the huge unexplored frontier of applications and services that\nprogrammers and entrepreneurs are able to explore—the Internet has unleashed creativity at\nMarc Andreessen\nMarc Andreessen is the co-creator of Mosaic, the Web browser that\npopularized the World Wide Web in 1993. Mosaic had a clean,\neasily understood interface and was the first browser to display\nimages in-line with text. In 1994, Marc Andreessen and Jim Clark\nfounded Netscape, whose browser was by far the most popular\nbrowser through the mid-1990s. Netscape also developed the Secure\nSockets Layer (SSL) protocol and many Internet server products, includ-\ning mail servers and SSL-based Web servers. He is now a co-founder\nand general partner of venture capital firm Andreessen Horowitz, over-\nseeing portfolio development with holdings that include Facebook,\nFoursquare, Groupon, Jawbone, Twitter, and Zynga. He serves on\nnumerous boards, including Bump, eBay, Glam Media, Facebook,\nand Hewlett-Packard. He holds a BS in Computer Science from the\nUniversity of Illinois at Urbana-Champaign.\nAN INTERVIEW WITH...\n\n183\na level that I don’t think we’ve ever seen before. My biggest concern is the principle of\nunintended consequences—we don’t always know the implications of what we do, such as\nthe Internet being used by governments to run a new level of surveillance on citizens.\nIs there anything in particular students should be aware of as Web technology advances?\nThe rate of change—the most important thing to learn is how to learn—how to flexibly\nadapt to changes in the specific technologies, and how to keep an open mind on the new\nopportunities and possibilities as you move through your career.\nWhat people inspired you professionally?\nVannevar Bush, Ted Nelson, Doug Engelbart, Nolan Bushnell, Bill Hewlett and Dave\nPackard, Ken Olsen, Steve Jobs, Steve Wozniak, Andy Grove, Grace Hopper, Hedy Lamarr,\nAlan Turing, Richard Stallman.\nWhat are your recommendations for students who want to pursue careers in computing\nand information technology?\nGo as deep as you possibly can on understanding how technology is created, and then com-\nplement with learning how business works.\nCan technology solve the world’s problems?\nNo, but we advance the standard of living of people through economic growth, and most\neconomic growth throughout history has come from technology—so that’s as good as it gets.\n\nThis page intentionally left blank"
    },
    {
      "chunk_id": "f8ee9d41-40c4-412f-b327-1b927d998e14",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Chapter 3 Transport Layer",
      "original_titles": [
        "Chapter 3 Transport Layer"
      ],
      "path": "Chapter 3 Transport Layer",
      "start_page": 212,
      "end_page": 212,
      "token_count": 321,
      "text": "CHAPTER 3\nTransport\nLayer\nResiding between the application and network layers, the transport layer is a central\npiece of the layered network architecture. It has the critical role of providing com-\nmunication services directly to the application processes running on different hosts.\nThe pedagogic approach we take in this chapter is to alternate between discussions\nof transport-layer principles and discussions of how these principles are imple-\nmented in existing protocols; as usual, particular emphasis will be given to Internet\nprotocols, in particular the TCP and UDP transport-layer protocols.\nWe’ll begin by discussing the relationship between the transport and network\nlayers. This sets the stage for examining the first critical function of the transport\nlayer—extending the network layer’s delivery service between two end systems to a\ndelivery service between two application-layer processes running on the end sys-\ntems. We’ll illustrate this function in our coverage of the Internet’s connectionless\ntransport protocol, UDP.\nWe’ll then return to principles and confront one of the most fundamental prob-\nlems in computer networking—how two entities can communicate reliably over a\nmedium that may lose and corrupt data. Through a series of increasingly compli-\ncated (and realistic!) scenarios, we’ll build up an array of techniques that transport\nprotocols use to solve this problem. We’ll then show how these principles are\nembodied in TCP, the Internet’s connection-oriented transport protocol.\nWe’ll next move on to a second fundamentally important problem in networking—\ncontrolling the transmission rate of transport-layer entities in order to avoid, or\n185"
    },
    {
      "chunk_id": "2085de1f-f8b4-46a8-9693-8d6e814a5891",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.1 Introduction and Transport-Layer Services",
      "original_titles": [
        "3.1 Introduction and Transport-Layer Services"
      ],
      "path": "Chapter 3 Transport Layer > 3.1 Introduction and Transport-Layer Services",
      "start_page": 213,
      "end_page": 215,
      "token_count": 1363,
      "text": "recover from, congestion within the network. We’ll consider the causes and conse-\nquences of congestion, as well as commonly used congestion-control techniques.\nAfter obtaining a solid understanding of the issues behind congestion control, we’ll\nstudy TCP’s approach to congestion control.\n3.1 Introduction and Transport-Layer Services\nIn the previous two chapters we touched on the role of the transport layer and the\nservices that it provides. Let’s quickly review what we have already learned about\nthe transport layer.\nA transport-layer protocol provides for logical communication between appli-\ncation processes running on different hosts. By logical communication, we mean\nthat from an application’s perspective, it is as if the hosts running the processes were\ndirectly connected; in reality, the hosts may be on opposite sides of the planet, con-\nnected via numerous routers and a wide range of link types. Application processes\nuse the logical communication provided by the transport layer to send messages to\neach other, free from the worry of the details of the physical infrastructure used to\ncarry these messages. Figure 3.1 illustrates the notion of logical communication.\nAs shown in Figure 3.1, transport-layer protocols are implemented in the end\nsystems but not in network routers. On the sending side, the transport layer converts\nthe application-layer messages it receives from a sending application process into\ntransport-layer packets, known as transport-layer segments in Internet terminology.\nThis is done by (possibly) breaking the application messages into smaller chunks and\nadding a transport-layer header to each chunk to create the transport-layer segment.\nThe transport layer then passes the segment to the network layer at the sending end\nsystem, where the segment is encapsulated within a network-layer packet (a data-\ngram) and sent to the destination. It’s important to note that network routers act only\non the network-layer fields of the datagram; that is, they do not examine the fields of\nthe transport-layer segment encapsulated with the datagram. On the receiving side,\nthe network layer extracts the transport-layer segment from the datagram and passes\nthe segment up to the transport layer. The transport layer then processes the received\nsegment, making the data in the segment available to the receiving application.\nMore than one transport-layer protocol may be available to network applications.\nFor example, the Internet has two protocols—TCP and UDP. Each of these protocols\nprovides a different set of transport-layer services to the invoking application.\n3.1.1 Relationship Between Transport and Network Layers\nRecall that the transport layer lies just above the network layer in the protocol stack.\nWhereas a transport-layer protocol provides logical communication between\nprocesses running on different hosts, a network-layer protocol provides logical\n186\nCHAPTER 3\n•\nTRANSPORT LAYER\n\n3.1\n•\nINTRODUCTION AND TRANSPORT-LAYER SERVICES\n187\nMobile Network\nNational or\nGlobal ISP\nLocal or\nRegional ISP\nEnterprise Network\nHome Network\nNetwork\nData link\nPhysical\nApplication\nTransport\nNetwork\nData link\nPhysical\nApplication\nTransport\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nLogical end-to-end transport\nFigure 3.1 \u0002 The transport layer provides logical rather than physical \ncommunication between application processes\n\ncommunication between hosts. This distinction is subtle but important. Let’s exam-\nine this distinction with the aid of a household analogy.\nConsider two houses, one on the East Coast and the other on the West Coast, with\neach house being home to a dozen kids. The kids in the East Coast household are\ncousins of the kids in the West Coast household. The kids in the two households love\nto write to each other—each kid writes each cousin every week, with each letter deliv-\nered by the traditional postal service in a separate envelope. Thus, each household\nsends 144 letters to the other household every week. (These kids would save a lot of\nmoney if they had e-mail!) In each of the households there is one kid—Ann in the\nWest Coast house and Bill in the East Coast house—responsible for mail collection\nand mail distribution. Each week Ann visits all her brothers and sisters, collects the\nmail, and gives the mail to a postal-service mail carrier, who makes daily visits to the\nhouse. When letters arrive at the West Coast house, Ann also has the job of distribut-\ning the mail to her brothers and sisters. Bill has a similar job on the East Coast.\nIn this example, the postal service provides logical communication between the\ntwo houses—the postal service moves mail from house to house, not from person to\nperson. On the other hand, Ann and Bill provide logical communication among the\ncousins—Ann and Bill pick up mail from, and deliver mail to, their brothers and sis-\nters. Note that from the cousins’ perspective, Ann and Bill are the mail service, even\nthough Ann and Bill are only a part (the end-system part) of the end-to-end delivery\nprocess. This household example serves as a nice analogy for explaining how the\ntransport layer relates to the network layer:\napplication messages = letters in envelopes\nprocesses = cousins\nhosts (also called end systems) = houses\ntransport-layer protocol = Ann and Bill\nnetwork-layer protocol = postal service (including mail carriers)\nContinuing with this analogy, note that Ann and Bill do all their work within\ntheir respective homes; they are not involved, for example, in sorting mail in any\nintermediate mail center or in moving mail from one mail center to another. Simi-\nlarly, transport-layer protocols live in the end systems. Within an end system, a\ntransport protocol moves messages from application processes to the network edge\n(that is, the network layer) and vice versa, but it doesn’t have any say about how the\nmessages are moved within the network core. In fact, as illustrated in Figure 3.1,\nintermediate routers neither act on, nor recognize, any information that the transport\nlayer may have added to the application messages.\nContinuing with our family saga, suppose now that when Ann and Bill go on\nvacation, another cousin pair—say, Susan and Harvey—substitute for them and pro-\nvide the household-internal collection and delivery of mail. Unfortunately for the\ntwo families, Susan and Harvey do not do the collection and delivery in exactly the\nsame way as Ann and Bill. Being younger kids, Susan and Harvey pick up and drop\noff the mail less frequently and occasionally lose letters (which are sometimes\n188\nCHAPTER 3\n•\nTRANSPORT LAYER"
    },
    {
      "chunk_id": "4754581b-8784-477d-bc53-276ec81378c4",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.1.1 Relationship Between Transport and Network Layers",
      "original_titles": [
        "3.1.1 Relationship Between Transport and Network Layers"
      ],
      "path": "Chapter 3 Transport Layer > 3.1 Introduction and Transport-Layer Services > 3.1.1 Relationship Between Transport and Network Layers",
      "start_page": 213,
      "end_page": 215,
      "token_count": 1363,
      "text": "recover from, congestion within the network. We’ll consider the causes and conse-\nquences of congestion, as well as commonly used congestion-control techniques.\nAfter obtaining a solid understanding of the issues behind congestion control, we’ll\nstudy TCP’s approach to congestion control.\n3.1 Introduction and Transport-Layer Services\nIn the previous two chapters we touched on the role of the transport layer and the\nservices that it provides. Let’s quickly review what we have already learned about\nthe transport layer.\nA transport-layer protocol provides for logical communication between appli-\ncation processes running on different hosts. By logical communication, we mean\nthat from an application’s perspective, it is as if the hosts running the processes were\ndirectly connected; in reality, the hosts may be on opposite sides of the planet, con-\nnected via numerous routers and a wide range of link types. Application processes\nuse the logical communication provided by the transport layer to send messages to\neach other, free from the worry of the details of the physical infrastructure used to\ncarry these messages. Figure 3.1 illustrates the notion of logical communication.\nAs shown in Figure 3.1, transport-layer protocols are implemented in the end\nsystems but not in network routers. On the sending side, the transport layer converts\nthe application-layer messages it receives from a sending application process into\ntransport-layer packets, known as transport-layer segments in Internet terminology.\nThis is done by (possibly) breaking the application messages into smaller chunks and\nadding a transport-layer header to each chunk to create the transport-layer segment.\nThe transport layer then passes the segment to the network layer at the sending end\nsystem, where the segment is encapsulated within a network-layer packet (a data-\ngram) and sent to the destination. It’s important to note that network routers act only\non the network-layer fields of the datagram; that is, they do not examine the fields of\nthe transport-layer segment encapsulated with the datagram. On the receiving side,\nthe network layer extracts the transport-layer segment from the datagram and passes\nthe segment up to the transport layer. The transport layer then processes the received\nsegment, making the data in the segment available to the receiving application.\nMore than one transport-layer protocol may be available to network applications.\nFor example, the Internet has two protocols—TCP and UDP. Each of these protocols\nprovides a different set of transport-layer services to the invoking application.\n3.1.1 Relationship Between Transport and Network Layers\nRecall that the transport layer lies just above the network layer in the protocol stack.\nWhereas a transport-layer protocol provides logical communication between\nprocesses running on different hosts, a network-layer protocol provides logical\n186\nCHAPTER 3\n•\nTRANSPORT LAYER\n\n3.1\n•\nINTRODUCTION AND TRANSPORT-LAYER SERVICES\n187\nMobile Network\nNational or\nGlobal ISP\nLocal or\nRegional ISP\nEnterprise Network\nHome Network\nNetwork\nData link\nPhysical\nApplication\nTransport\nNetwork\nData link\nPhysical\nApplication\nTransport\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nLogical end-to-end transport\nFigure 3.1 \u0002 The transport layer provides logical rather than physical \ncommunication between application processes\n\ncommunication between hosts. This distinction is subtle but important. Let’s exam-\nine this distinction with the aid of a household analogy.\nConsider two houses, one on the East Coast and the other on the West Coast, with\neach house being home to a dozen kids. The kids in the East Coast household are\ncousins of the kids in the West Coast household. The kids in the two households love\nto write to each other—each kid writes each cousin every week, with each letter deliv-\nered by the traditional postal service in a separate envelope. Thus, each household\nsends 144 letters to the other household every week. (These kids would save a lot of\nmoney if they had e-mail!) In each of the households there is one kid—Ann in the\nWest Coast house and Bill in the East Coast house—responsible for mail collection\nand mail distribution. Each week Ann visits all her brothers and sisters, collects the\nmail, and gives the mail to a postal-service mail carrier, who makes daily visits to the\nhouse. When letters arrive at the West Coast house, Ann also has the job of distribut-\ning the mail to her brothers and sisters. Bill has a similar job on the East Coast.\nIn this example, the postal service provides logical communication between the\ntwo houses—the postal service moves mail from house to house, not from person to\nperson. On the other hand, Ann and Bill provide logical communication among the\ncousins—Ann and Bill pick up mail from, and deliver mail to, their brothers and sis-\nters. Note that from the cousins’ perspective, Ann and Bill are the mail service, even\nthough Ann and Bill are only a part (the end-system part) of the end-to-end delivery\nprocess. This household example serves as a nice analogy for explaining how the\ntransport layer relates to the network layer:\napplication messages = letters in envelopes\nprocesses = cousins\nhosts (also called end systems) = houses\ntransport-layer protocol = Ann and Bill\nnetwork-layer protocol = postal service (including mail carriers)\nContinuing with this analogy, note that Ann and Bill do all their work within\ntheir respective homes; they are not involved, for example, in sorting mail in any\nintermediate mail center or in moving mail from one mail center to another. Simi-\nlarly, transport-layer protocols live in the end systems. Within an end system, a\ntransport protocol moves messages from application processes to the network edge\n(that is, the network layer) and vice versa, but it doesn’t have any say about how the\nmessages are moved within the network core. In fact, as illustrated in Figure 3.1,\nintermediate routers neither act on, nor recognize, any information that the transport\nlayer may have added to the application messages.\nContinuing with our family saga, suppose now that when Ann and Bill go on\nvacation, another cousin pair—say, Susan and Harvey—substitute for them and pro-\nvide the household-internal collection and delivery of mail. Unfortunately for the\ntwo families, Susan and Harvey do not do the collection and delivery in exactly the\nsame way as Ann and Bill. Being younger kids, Susan and Harvey pick up and drop\noff the mail less frequently and occasionally lose letters (which are sometimes\n188\nCHAPTER 3\n•\nTRANSPORT LAYER"
    },
    {
      "chunk_id": "24957844-72c5-4bc5-b6a2-709a63dfd6bc",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.1.2 Overview of the Transport Layer in the Internet",
      "original_titles": [
        "3.1.2 Overview of the Transport Layer in the Internet"
      ],
      "path": "Chapter 3 Transport Layer > 3.1 Introduction and Transport-Layer Services > 3.1.2 Overview of the Transport Layer in the Internet",
      "start_page": 216,
      "end_page": 217,
      "token_count": 1433,
      "text": "chewed up by the family dog). Thus, the cousin-pair Susan and Harvey do not pro-\nvide the same set of services (that is, the same service model) as Ann and Bill. In an\nanalogous manner, a computer network may make available multiple transport pro-\ntocols, with each protocol offering a different service model to applications.\nThe possible services that Ann and Bill can provide are clearly constrained by\nthe possible services that the postal service provides. For example, if the postal serv-\nice doesn’t provide a maximum bound on how long it can take to deliver mail\nbetween the two houses (for example, three days), then there is no way that Ann and\nBill can guarantee a maximum delay for mail delivery between any of the cousin\npairs. In a similar manner, the services that a transport protocol can provide are often\nconstrained by the service model of the underlying network-layer protocol. If the\nnetwork-layer protocol cannot provide delay or bandwidth guarantees for transport-\nlayer segments sent between hosts, then the transport-layer protocol cannot provide\ndelay or bandwidth guarantees for application messages sent between processes.\nNevertheless, certain services can be offered by a transport protocol even when\nthe underlying network protocol doesn’t offer the corresponding service at the net-\nwork layer. For example, as we’ll see in this chapter, a transport protocol can offer\nreliable data transfer service to an application even when the underlying network\nprotocol is unreliable, that is, even when the network protocol loses, garbles, or\nduplicates packets. As another example (which we’ll explore in Chapter 8 when we\ndiscuss network security), a transport protocol can use encryption to guarantee that\napplication messages are not read by intruders, even when the network layer cannot\nguarantee the confidentiality of transport-layer segments.\n3.1.2 Overview of the Transport Layer in the Internet\nRecall that the Internet, and more generally a TCP/IP network, makes two distinct\ntransport-layer protocols available to the application layer. One of these protocols is\nUDP (User Datagram Protocol), which provides an unreliable, connectionless service\nto the invoking application. The second of these protocols is TCP (Transmission Con-\ntrol Protocol), which provides a reliable, connection-oriented service to the invoking\napplication. When designing a network application, the application developer must\nspecify one of these two transport protocols. As we saw in Section 2.7, the application\ndeveloper selects between UDP and TCP when creating sockets.\nTo simplify terminology, when in an Internet context, we refer to the transport-\nlayer packet as a segment. We mention, however, that the Internet literature (for exam-\nple, the RFCs) also refers to the transport-layer packet for TCP as a segment but often\nrefers to the packet for UDP as a datagram. But this same Internet literature also uses\nthe term datagram for the network-layer packet! For an introductory book on computer\nnetworking such as this, we believe that it is less confusing to refer to both TCP and\nUDP packets as segments, and reserve the term datagram for the network-layer packet.\nBefore proceeding with our brief introduction of UDP and TCP, it will be use-\nful to say a few words about the Internet’s network layer. (We’ll learn about the net-\nwork layer in detail in Chapter 4.) The Internet’s network-layer protocol has a\n3.1\n•\nINTRODUCTION AND TRANSPORT-LAYER SERVICES\n189\n\nname—IP, for Internet Protocol. IP provides logical communication between hosts.\nThe IP service model is a best-effort delivery service. This means that IP makes its\n“best effort” to deliver segments between communicating hosts, but it makes no\nguarantees. In particular, it does not guarantee segment delivery, it does not guaran-\ntee orderly delivery of segments, and it does not guarantee the integrity of the data\nin the segments. For these reasons, IP is said to be an unreliable service. We also\nmention here that every host has at least one network-layer address, a so-called IP\naddress. We’ll examine IP addressing in detail in Chapter 4; for this chapter we need\nonly keep in mind that each host has an IP address.\nHaving taken a glimpse at the IP service model, let’s now summarize the serv-\nice models provided by UDP and TCP. The most fundamental responsibility of UDP\nand TCP is to extend IP’s delivery service between two end systems to a delivery\nservice between two processes running on the end systems. Extending host-to-host\ndelivery to process-to-process delivery is called transport-layer multiplexing and\ndemultiplexing. We’ll discuss transport-layer multiplexing and demultiplexing in\nthe next section. UDP and TCP also provide integrity checking by including error-\ndetection fields in their segments’ headers. These two minimal transport-layer serv-\nices—process-to-process data delivery and error checking—are the only two\nservices that UDP provides! In particular, like IP, UDP is an unreliable service—it\ndoes not guarantee that data sent by one process will arrive intact (or at all!) to the\ndestination process. UDP is discussed in detail in Section 3.3.\nTCP, on the other hand, offers several additional services to applications. First\nand foremost, it provides reliable data transfer. Using flow control, sequence num-\nbers, acknowledgments, and timers (techniques we’ll explore in detail in this chap-\nter), TCP ensures that data is delivered from sending process to receiving process,\ncorrectly and in order. TCP thus converts IP’s unreliable service between end sys-\ntems into a reliable data transport service between processes. TCP also provides\ncongestion control. Congestion control is not so much a service provided to the\ninvoking application as it is a service for the Internet as a whole, a service for the\ngeneral good. Loosely speaking, TCP congestion control prevents any one TCP con-\nnection from swamping the links and routers between communicating hosts with an\nexcessive amount of traffic. TCP strives to give each connection traversing a con-\ngested link an equal share of the link bandwidth. This is done by regulating the rate\nat which the sending sides of TCP connections can send traffic into the network.\nUDP traffic, on the other hand, is unregulated. An application using UDP transport\ncan send at any rate it pleases, for as long as it pleases.\nA protocol that provides reliable data transfer and congestion control is neces-\nsarily complex. We’ll need several sections to cover the principles of reliable data\ntransfer and congestion control, and additional sections to cover the TCP protocol\nitself. These topics are investigated in Sections 3.4 through 3.8. The approach taken\nin this chapter is to alternate between basic principles and the TCP protocol. For\nexample, we’ll first discuss reliable data transfer in a general setting and then dis-\ncuss how TCP specifically provides reliable data transfer. Similarly, we’ll first\n190\nCHAPTER 3\n•\nTRANSPORT LAYER"
    },
    {
      "chunk_id": "812fb1f2-eeed-4b3a-bc05-b770276e26d5",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.2 Multiplexing and Demultiplexing",
      "original_titles": [
        "3.2 Multiplexing and Demultiplexing"
      ],
      "path": "Chapter 3 Transport Layer > 3.2 Multiplexing and Demultiplexing",
      "start_page": 218,
      "end_page": 224,
      "token_count": 3455,
      "text": "discuss congestion control in a general setting and then discuss how TCP performs\ncongestion control. But before getting into all this good stuff, let’s first look at\ntransport-layer multiplexing and demultiplexing.\n3.2 Multiplexing and Demultiplexing\nIn this section, we discuss transport-layer multiplexing and demultiplexing, that is,\nextending the host-to-host delivery service provided by the network layer to a\nprocess-to-process delivery service for applications running on the hosts. In order to\nkeep the discussion concrete, we’ll discuss this basic transport-layer service in the\ncontext of the Internet. We emphasize, however, that a multiplexing/demultiplexing\nservice is needed for all computer networks.\nAt the destination host, the transport layer receives segments from the network\nlayer just below. The transport layer has the responsibility of delivering the data in\nthese segments to the appropriate application process running in the host. Let’s take\na look at an example. Suppose you are sitting in front of your computer, and you are\ndownloading Web pages while running one FTP session and two Telnet sessions.\nYou therefore have four network application processes running—two Telnet\nprocesses, one FTP process, and one HTTP process. When the transport layer in\nyour computer receives data from the network layer below, it needs to direct the\nreceived data to one of these four processes. Let’s now examine how this is done.\nFirst recall from Section 2.7 that a process (as part of a network application) can\nhave one or more sockets, doors through which data passes from the network to the\nprocess and through which data passes from the process to the network. Thus, as\nshown in Figure 3.2, the transport layer in the receiving host does not actually\ndeliver data directly to a process, but instead to an intermediary socket. Because at\nany given time there can be more than one socket in the receiving host, each socket\nhas a unique identifier. The format of the identifier depends on whether the socket is\na UDP or a TCP socket, as we’ll discuss shortly.\nNow let’s consider how a receiving host directs an incoming transport-layer seg-\nment to the appropriate socket. Each transport-layer segment has a set of fields in the\nsegment for this purpose. At the receiving end, the transport layer examines these\nfields to identify the receiving socket and then directs the segment to that socket. This\njob of delivering the data in a transport-layer segment to the correct socket is called\ndemultiplexing. The job of gathering data chunks at the source host from different\nsockets, encapsulating each data chunk with header information (that will later be\nused in demultiplexing) to create segments, and passing the segments to the network\nlayer is called multiplexing. Note that the transport layer in the middle host in Fig-\nure 3.2 must demultiplex segments arriving from the network layer below to either\nprocess P1 or P2 above; this is done by directing the arriving segment’s data to the\ncorresponding process’s socket. The transport layer in the middle host must also\n3.2\n•\nMULTIPLEXING AND DEMULTIPLEXING\n191\n\ngather outgoing data from these sockets, form transport-layer segments, and pass\nthese segments down to the network layer. Although we have introduced multiplex-\ning and demultiplexing in the context of the Internet transport protocols, it’s impor-\ntant to realize that they are concerns whenever a single protocol at one layer (at the\ntransport layer or elsewhere) is used by multiple protocols at the next higher layer.\nTo illustrate the demultiplexing job, recall the household analogy in the previ-\nous section. Each of the kids is identified by his or her name. When Bill receives a\nbatch of mail from the mail carrier, he performs a demultiplexing operation by\nobserving to whom the letters are addressed and then hand delivering the mail to his\nbrothers and sisters. Ann performs a multiplexing operation when she collects let-\nters from her brothers and sisters and gives the collected mail to the mail person.\nNow that we understand the roles of transport-layer multiplexing and demulti-\nplexing, let us examine how it is actually done in a host. From the discussion above,\nwe know that transport-layer multiplexing requires (1) that sockets have unique\nidentifiers, and (2) that each segment have special fields that indicate the socket to\nwhich the segment is to be delivered. These special fields, illustrated in Figure 3.3,\nare the source port number field and the destination port number field. (The\nUDP and TCP segments have other fields as well, as discussed in the subsequent\nsections of this chapter.) Each port number is a 16-bit number, ranging from 0 to\n65535. The port numbers ranging from 0 to 1023 are called well-known port num-\nbers and are restricted, which means that they are reserved for use by well-known\napplication protocols such as HTTP (which uses port number 80) and FTP (which\nuses port number 21). The list of well-known port numbers is given in RFC 1700\nand is updated at http://www.iana.org [RFC 3232]. When we develop a new\n192\nCHAPTER 3\n•\nTRANSPORT LAYER\nNetwork\nKey:\nProcess\nSocket\nData link\nPhysical\nTransport\nApplication\nNetwork\nApplication\nData link\nPhysical\nTransport\nNetwork\nData link\nPhysical\nTransport\nP3\nP2\nP1\nP4\nApplication\nFigure 3.2 \u0002 Transport-layer multiplexing and demultiplexing\n\napplication (such as the simple application developed in Section 2.7), we must\nassign the application a port number.\nIt should now be clear how the transport layer could implement the demultiplex-\ning service: Each socket in the host could be assigned a port number, and when a seg-\nment arrives at the host, the transport layer examines the destination port number in\nthe segment and directs the segment to the corresponding socket. The segment’s data\nthen passes through the socket into the attached process. As we’ll see, this is basi-\ncally how UDP does it. However, we’ll also see that multiplexing/demultiplexing in\nTCP is yet more subtle.\nConnectionless Multiplexing and Demultiplexing\nRecall from Section 2.7.1 that the Python program running in a host can create a\nUDP socket with the line\nclientSocket = socket(socket.AF_INET, socket.SOCK_DGRAM)\nWhen a UDP socket is created in this manner, the transport layer automatically\nassigns a port number to the socket. In particular, the transport layer assigns a port\nnumber in the range 1024 to 65535 that is currently not being used by any other UDP\nport in the host. Alternatively, we can add a line into our Python program after we\ncreate the socket to associate a specific port number (say, 19157) to this UDP socket\nvia the socket bind() method:\nclientSocket.bind((‘’, 19157))\nIf the application developer writing the code were implementing the server side of a\n“well-known protocol,” then the developer would have to assign the corresponding\n3.2\n•\nMULTIPLEXING AND DEMULTIPLEXING\n193\nSource port #\n32 bits\nDest. port #\nOther header fields\nApplication\ndata\n(message)\nFigure 3.3 \u0002 Source and destination port-number fields in a transport-layer\nsegment\n\nwell-known port number. Typically, the client side of the application lets the trans-\nport layer automatically (and transparently) assign the port number, whereas the\nserver side of the application assigns a specific port number.\nWith port numbers assigned to UDP sockets, we can now precisely describe\nUDP multiplexing/demultiplexing. Suppose a process in Host A, with UDP port\n19157, wants to send a chunk of application data to a process with UDP port 46428\nin Host B. The transport layer in Host A creates a transport-layer segment that\nincludes the application data, the source port number (19157), the destination port\nnumber (46428), and two other values (which will be discussed later, but are unim-\nportant for the current discussion). The transport layer then passes the resulting seg-\nment to the network layer. The network layer encapsulates the segment in an IP\ndatagram and makes a best-effort attempt to deliver the segment to the receiving host.\nIf the segment arrives at the receiving Host B, the transport layer at the receiving\nhost examines the destination port number in the segment (46428) and delivers the\nsegment to its socket identified by port 46428. Note that Host B could be running\nmultiple processes, each with its own UDP socket and associated port number. As\nUDP segments arrive from the network, Host B directs (demultiplexes) each segment\nto the appropriate socket by examining the segment’s destination port number.\nIt is important to note that a UDP socket is fully identified by a two-tuple consist-\ning of a destination IP address and a destination port number. As a consequence, if two\nUDP segments have different source IP addresses and/or source port numbers, but have\nthe same destination IP address and destination port number, then the two segments\nwill be directed to the same destination process via the same destination socket.\nYou may be wondering now, what is the purpose of the source port number? As\nshown in Figure 3.4, in the A-to-B segment the source port number serves as part of\na “return address”—when B wants to send a segment back to A, the destination port\nin the B-to-A segment will take its value from the source port value of the A-to-B\nsegment. (The complete return address is A’s IP address and the source port num-\nber.) As an example, recall the UDP server program studied in Section 2.7. In\nUDPServer.py, the server uses the recvfrom() method to extract the client-\nside (source) port number from the segment it receives from the client; it then sends\na new segment to the client, with the extracted source port number serving as the\ndestination port number in this new segment.\nConnection-Oriented Multiplexing and Demultiplexing\nIn order to understand TCP demultiplexing, we have to take a close look at TCP\nsockets and TCP connection establishment. One subtle difference between a TCP\nsocket and a UDP socket is that a TCP socket is identified by a four-tuple: (source\nIP address, source port number, destination IP address, destination port number).\nThus, when a TCP segment arrives from the network to a host, the host uses all four\nvalues to direct (demultiplex) the segment to the appropriate socket. In particular,\nand in contrast with UDP, two arriving TCP segments with different source IP\n194\nCHAPTER 3\n•\nTRANSPORT LAYER\n\naddresses or source port numbers will (with the exception of a TCP segment carry-\ning the original connection-establishment request) be directed to two different sock-\nets. To gain further insight, let’s reconsider the TCP client-server programming\nexample in Section 2.7.2:\n•\nThe TCP server application has a “welcoming socket,” that waits for connection-\nestablishment requests from TCP clients (see Figure 2.29) on port number 12000.\n•\nThe TCP client creates a socket and sends a connection establishment request\nsegment with the lines:\nclientSocket = socket(AF_INET, SOCK_STREAM)\nclientSocket.connect((serverName,12000))\n•\nA connection-establishment request is nothing more than a TCP segment with desti-\nnation port number 12000 and a special connection-establishment bit set in the TCP\nheader (discussed in Section 3.5). The segment also includes a source port number\nthat was chosen by the client.\n•\nWhen the host operating system of the computer running the server process\nreceives the incoming connection-request segment with destination port 12000,\nit locates the server process that is waiting to accept a connection on port num-\nber 12000. The server process then creates a new socket:\nconnectionSocket, addr = serverSocket.accept()\n3.2\n•\nMULTIPLEXING AND DEMULTIPLEXING\n195\nHost A\nClient process\nSocket\nServer B\nsource port:\n19157\ndest. port:\n46428\nsource port:\n46428\ndest. port:\n19157\nFigure 3.4 \u0002 The inversion of source and destination port numbers\n\n•\nAlso, the transport layer at the server notes the following four values in the con-\nnection-request segment: (1) the source port number in the segment, (2) the IP\naddress of the source host, (3) the destination port number in the segment, and\n(4) its own IP address. The newly created connection socket is identified by these\nfour values; all subsequently arriving segments whose source port, source IP\naddress, destination port, and destination IP address match these four values will\nbe demultiplexed to this socket. With the TCP connection now in place, the client\nand server can now send data to each other.\nThe server host may support many simultaneous TCP connection sockets, with\neach socket attached to a process, and with each socket identified by its own four-\ntuple. When a TCP segment arrives at the host, all four fields (source IP address,\nsource port, destination IP address, destination port) are used to direct (demultiplex)\nthe segment to the appropriate socket.\n196\nCHAPTER 3\n•\nTRANSPORT LAYER\nPORT SCANNING\nWe’ve seen that a server process waits patiently on an open port for contact by a\nremote client. Some ports are reserved for well-known applications (e.g., Web, FTP,\nDNS, and SMTP servers); other ports are used by convention by popular applications\n(e.g., the Microsoft 2000 SQL server listens for requests on UDP port 1434). Thus, if\nwe determine that a port is open on a host, we may be able to map that port to a\nspecific application running on the host. This is very useful for system administrators,\nwho are often interested in knowing which network applications are running on the\nhosts in their networks. But attackers, in order to “case the joint,” also want to know\nwhich ports are open on target hosts. If a host is found to be running an application\nwith a known security flaw (e.g., a SQL server listening on port 1434 was subject to\na buffer overflow, allowing a remote user to execute arbitrary code on the vulnerable\nhost, a flaw exploited by the Slammer worm [CERT 2003–04]), then that host is ripe\nfor attack.\nDetermining which applications are listening on which ports is a relatively easy\ntask. Indeed there are a number of public domain programs, called port scanners,\nthat do just that. Perhaps the most widely used of these is nmap, freely available at\nhttp://nmap.org and included in most Linux distributions. For TCP, nmap sequentially\nscans ports, looking for ports that are accepting TCP connections. For UDP, nmap\nagain sequentially scans ports, looking for UDP ports that respond to transmitted \nUDP segments. In both cases, nmap returns a list of open, closed, or unreachable\nports. A host running nmap can attempt to scan any target host anywhere in the\nInternet. We’ll revisit nmap in Section 3.5.6, when we discuss TCP connection\nmanagement.\nFOCUS ON SECURITY\n\nThe situation is illustrated in Figure 3.5, in which Host C initiates two HTTP ses-\nsions to server B, and Host Ainitiates one HTTP session to B. Hosts Aand C and server\nB each have their own unique IP address—A, C, and B, respectively. Host C assigns\ntwo different source port numbers (26145 and 7532) to its two HTTP connections.\nBecause Host A is choosing source port numbers independently of C, it might also\nassign a source port of 26145 to its HTTP connection. But this is not a problem—server\nB will still be able to correctly demultiplex the two connections having the same source\nport number, since the two connections have different source IP addresses.\nWeb Servers and TCP\nBefore closing this discussion, it’s instructive to say a few additional words about\nWeb servers and how they use port numbers. Consider a host running a Web server,\nsuch as an Apache Web server, on port 80. When clients (for example, browsers)\nsend segments to the server, all segments will have destination port 80. In particu-\nlar, both the initial connection-establishment segments and the segments carrying \nHTTP request messages will have destination port 80. As we have just described,\n3.2\n•\nMULTIPLEXING AND DEMULTIPLEXING\n197\nsource port:\n7532\ndest. port:\n80\nsource IP:\nC\ndest. IP:\nB\nsource port:\n26145\ndest. port:\n80\nsource IP:\nC\ndest. IP:\nB\nsource port:\n26145\ndest. port:\n80\nsource IP:\nA\ndest. IP:\nB\nPer-connection\nHTTP\nprocesses\nTransport-\nlayer\ndemultiplexing\nWeb \nserver B\nWeb client\nhost C\nWeb client\nhost A\nFigure 3.5 \u0002 Two clients, using the same destination port number (80) to\ncommunicate with the same Web server application"
    },
    {
      "chunk_id": "750db850-5d39-4850-8b58-df377220507d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.3 Connectionless Transport: UDP",
      "original_titles": [
        "3.3 Connectionless Transport: UDP"
      ],
      "path": "Chapter 3 Transport Layer > 3.3 Connectionless Transport: UDP",
      "start_page": 225,
      "end_page": 228,
      "token_count": 2428,
      "text": "the server distinguishes the segments from the different clients using source IP\naddresses and source port numbers.\nFigure 3.5 shows a Web server that spawns a new process for each connection.\nAs shown in Figure 3.5, each of these processes has its own connection socket\nthrough which HTTP requests arrive and HTTP responses are sent. We mention,\nhowever, that there is not always a one-to-one correspondence between connection\nsockets and processes. In fact, today’s high-performing Web servers often use only\none process, and create a new thread with a new connection socket for each new\nclient connection. (A thread can be viewed as a lightweight subprocess.) If you did\nthe first programming assignment in Chapter 2, you built a Web server that does just\nthis. For such a server, at any given time there may be many connection sockets\n(with different identifiers) attached to the same process.\nIf the client and server are using persistent HTTP, then throughout the duration\nof the persistent connection the client and server exchange HTTP messages via the\nsame server socket. However, if the client and server use non-persistent HTTP, then\na new TCP connection is created and closed for every request/response, and hence\na new socket is created and later closed for every request/response. This frequent\ncreating and closing of sockets can severely impact the performance of a busy Web\nserver (although a number of operating system tricks can be used to mitigate\nthe problem). Readers interested in the operating system issues surrounding per-\nsistent and non-persistent HTTP are encouraged to see [Nielsen 1997; Nahum\n2002].\nNow that we’ve discussed transport-layer multiplexing and demultiplexing,\nlet’s move on and discuss one of the Internet’s transport protocols, UDP. In the next\nsection we’ll see that UDP adds little more to the network-layer protocol than a mul-\ntiplexing/demultiplexing service.\n3.3 Connectionless Transport: UDP\nIn this section, we’ll take a close look at UDP, how it works, and what it does.\nWe encourage you to refer back to Section 2.1, which includes an overview of\nthe UDP service model, and to Section 2.7.1, which discusses socket program-\nming using UDP.\nTo motivate our discussion about UDP, suppose you were interested in design-\ning a no-frills, bare-bones transport protocol. How might you go about doing this?\nYou might first consider using a vacuous transport protocol. In particular, on the\nsending side, you might consider taking the messages from the application process\nand passing them directly to the network layer; and on the receiving side, you might\nconsider taking the messages arriving from the network layer and passing them\ndirectly to the application process. But as we learned in the previous section, we\nhave to do a little more than nothing! At the very least, the transport layer has to\n198\nCHAPTER 3\n•\nTRANSPORT LAYER\n\nprovide a multiplexing/demultiplexing service in order to pass data between the\nnetwork layer and the correct application-level process.\nUDP, defined in [RFC 768], does just about as little as a transport protocol can\ndo. Aside from the multiplexing/demultiplexing function and some light error\nchecking, it adds nothing to IP. In fact, if the application developer chooses UDP\ninstead of TCP, then the application is almost directly talking with IP. UDP takes\nmessages from the application process, attaches source and destination port number\nfields for the multiplexing/demultiplexing service, adds two other small fields, and\npasses the resulting segment to the network layer. The network layer encapsulates\nthe transport-layer segment into an IP datagram and then makes a best-effort attempt\nto deliver the segment to the receiving host. If the segment arrives at the receiving\nhost, UDP uses the destination port number to deliver the segment’s data to the cor-\nrect application process. Note that with UDP there is no handshaking between send-\ning and receiving transport-layer entities before sending a segment. For this reason,\nUDP is said to be connectionless.\nDNS is an example of an application-layer protocol that typically uses UDP.\nWhen the DNS application in a host wants to make a query, it constructs a DNS\nquery message and passes the message to UDP. Without performing any handshak-\ning with the UDP entity running on the destination end system, the host-side UDP\nadds header fields to the message and passes the resulting segment to the network\nlayer. The network layer encapsulates the UDP segment into a datagram and sends\nthe datagram to a name server. The DNS application at the querying host then waits\nfor a reply to its query. If it doesn’t receive a reply (possibly because the underlying\nnetwork lost the query or the reply), either it tries sending the query to another name\nserver, or it informs the invoking application that it can’t get a reply.\nNow you might be wondering why an application developer would ever choose\nto build an application over UDP rather than over TCP. Isn’t TCP always preferable,\nsince TCP provides a reliable data transfer service, while UDP does not? The answer\nis no, as many applications are better suited for UDP for the following reasons:\n•\nFiner application-level control over what data is sent, and when. Under UDP, as\nsoon as an application process passes data to UDP, UDP will package the data\ninside a UDP segment and immediately pass the segment to the network layer.\nTCP, on the other hand, has a congestion-control mechanism that throttles the\ntransport-layer TCP sender when one or more links between the source and des-\ntination hosts become excessively congested. TCP will also continue to resend a\nsegment until the receipt of the segment has been acknowledged by the destina-\ntion, regardless of how long reliable delivery takes. Since real-time applications\noften require a minimum sending rate, do not want to overly delay segment \ntransmission, and can tolerate some data loss, TCP’s service model is not partic-\nularly well matched to these applications’ needs. As discussed below, these appli-\ncations can use UDP and implement, as part of the application, any additional\nfunctionality that is needed beyond UDP’s no-frills segment-delivery service.\n3.3\n•\nCONNECTIONLESS TRANSPORT: UDP\n199\n\n•\nNo connection establishment. As we’ll discuss later, TCP uses a three-way hand-\nshake before it starts to transfer data. UDP just blasts away without any formal pre-\nliminaries. Thus UDP does not introduce any delay to establish a connection. This\nis probably the principal reason why DNS runs over UDP rather than TCP—DNS\nwould be much slower if it ran over TCP. HTTP uses TCP rather than UDP, since\nreliability is critical for Web pages with text. But, as we briefly discussed in Sec-\ntion 2.2, the TCP connection-establishment delay in HTTP is an important contrib-\nutor to the delays associated with downloading Web documents.\n•\nNo connection state. TCP maintains connection state in the end systems. This\nconnection state includes receive and send buffers, congestion-control parame-\nters, and sequence and acknowledgment number parameters. We will see in Sec-\ntion 3.5 that this state information is needed to implement TCP’s reliable data\ntransfer service and to provide congestion control. UDP, on the other hand, does\nnot maintain connection state and does not track any of these parameters. For this\nreason, a server devoted to a particular application can typically support many\nmore active clients when the application runs over UDP rather than TCP.\n•\nSmall packet header overhead. The TCP segment has 20 bytes of header over-\nhead in every segment, whereas UDP has only 8 bytes of overhead.\nFigure 3.6 lists popular Internet applications and the transport protocols that they\nuse. As we expect, e-mail, remote terminal access, the Web, and file transfer run over\nTCP—all these applications need the reliable data transfer service of TCP. Neverthe-\nless, many important applications run over UDP rather than TCP. UDP is used for RIP\nrouting table updates (see Section 4.6.1). Since RIP updates are sent periodically (typi-\ncally every five minutes), lost updates will be replaced by more recent updates, thus\nmaking the lost, out-of-date update useless. UDP is also used to carry network manage-\nment (SNMP; see Chapter 9) data. UDP is preferred to TCP in this case, since network\nmanagement applications must often run when the network is in a stressed state—pre-\ncisely when reliable, congestion-controlled data transfer is difficult to achieve. Also,\nas we mentioned earlier, DNS runs over UDP, thereby avoiding TCP’s connection-\nestablishment delays.\nAs shown in Figure 3.6, both UDP and TCP are used today with multimedia\napplications, such as Internet phone, real-time video conferencing, and streaming of\nstored audio and video. We’ll take a close look at these applications in Chapter 7. We\njust mention now that all of these applications can tolerate a small amount of packet\nloss, so that reliable data transfer is not absolutely critical for the application’s suc-\ncess. Furthermore, real-time applications, like Internet phone and video conferenc-\ning, react very poorly to TCP’s congestion control. For these reasons, developers of\nmultimedia applications may choose to run their applications over UDP instead of\nTCP. However, TCP is increasingly being used for streaming media transport. For\nexample, [Sripanidkulchai 2004] found that nearly 75% of on-demand and live\nstreaming used TCP. When packet loss rates are low, and with some organizations\n200\nCHAPTER 3\n•\nTRANSPORT LAYER\n\nblocking UDP traffic for security reasons (see Chapter 8), TCP becomes an increas-\ningly attractive protocol for streaming media transport.\nAlthough commonly done today, running multimedia applications over UDP is\ncontroversial. As we mentioned above, UDP has no congestion control. But conges-\ntion control is needed to prevent the network from entering a congested state in\nwhich very little useful work is done. If everyone were to start streaming high-bit-\nrate video without using any congestion control, there would be so much packet\noverflow at routers that very few UDP packets would successfully traverse the\nsource-to-destination path. Moreover, the high loss rates induced by the uncon-\ntrolled UDP senders would cause the TCP senders (which, as we’ll see, do decrease\ntheir sending rates in the face of congestion) to dramatically decrease their rates.\nThus, the lack of congestion control in UDP can result in high loss rates between a\nUDP sender and receiver, and the crowding out of TCP sessions—a potentially seri-\nous problem [Floyd 1999]. Many researchers have proposed new mechanisms to\nforce all sources, including UDP sources, to perform adaptive congestion control\n[Mahdavi 1997; Floyd 2000; Kohler 2006: RFC 4340].\nBefore discussing the UDP segment structure, we mention that it is possible for\nan application to have reliable data transfer when using UDP. This can be done if reli-\nability is built into the application itself (for example, by adding acknowledgment\nand retransmission mechanisms, such as those we’ll study in the next section). But\nthis is a nontrivial task that would keep an application developer busy debugging for\n3.3\n•\nCONNECTIONLESS TRANSPORT: UDP\n201\nApplication-Layer\nUnderlying Transport\nApplication\nProtocol\nProtocol\nElectronic mail\nSMTP\nTCP\nRemote terminal access\nTelnet\nTCP\nWeb\nHTTP\nTCP\nFile transfer\nFTP\nTCP\nRemote file server\nNFS\nTypically UDP\nStreaming multimedia\ntypically proprietary\nUDP or TCP\nInternet telephony\ntypically proprietary\nUDP or TCP\nNetwork management\nSNMP\nTypically UDP\nRouting protocol\nRIP\nTypically UDP\nName translation\nDNS\nTypically UDP\nFigure 3.6 \u0002 Popular Internet applications and their underlying transport\nprotocols"
    },
    {
      "chunk_id": "d5af461e-57bf-4084-831d-27a71e080e07",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.3.1 UDP Segment Structure",
      "original_titles": [
        "3.3.1 UDP Segment Structure"
      ],
      "path": "Chapter 3 Transport Layer > 3.3 Connectionless Transport: UDP > 3.3.1 UDP Segment Structure",
      "start_page": 229,
      "end_page": 230,
      "token_count": 995,
      "text": "a long time. Nevertheless, building reliability directly into the application allows the\napplication to “have its cake and eat it too.” That is, application processes can com-\nmunicate reliably without being subjected to the transmission-rate constraints\nimposed by TCP’s congestion-control mechanism.\n3.3.1 UDP Segment Structure\nThe UDP segment structure, shown in Figure 3.7, is defined in RFC 768. The applica-\ntion data occupies the data field of the UDP segment. For example, for DNS, the data\nfield contains either a query message or a response message. For a streaming audio\napplication, audio samples fill the data field. The UDP header has only four fields,\neach consisting of two bytes. As discussed in the previous section, the port numbers\nallow the destination host to pass the application data to the correct process running\non the destination end system (that is, to perform the demultiplexing function). The\nlength field specifies the number of bytes in the UDP segment (header plus data). An\nexplicit length value is needed since the size of the data field may differ from one UDP\nsegment to the next. The checksum is used by the receiving host to check whether\nerrors have been introduced into the segment. In truth, the checksum is also calculated\nover a few of the fields in the IP header in addition to the UDP segment. But we ignore\nthis detail in order to see the forest through the trees. We’ll discuss the checksum cal-\nculation below. Basic principles of error detection are described in Section 5.2. The\nlength field specifies the length of the UDP segment, including the header, in bytes.\n3.3.2 UDP Checksum\nThe UDP checksum provides for error detection. That is, the checksum is used to\ndetermine whether bits within the UDP segment have been altered (for example, by\nnoise in the links or while stored in a router) as it moved from source to destination.\nUDP at the sender side performs the 1s complement of the sum of all the 16-bit\nwords in the segment, with any overflow encountered during the sum being \n202\nCHAPTER 3\n•\nTRANSPORT LAYER\nSource port #\n32 bits\nDest. port #\nLength\nChecksum\nApplication\ndata\n(message)\nFigure 3.7 \u0002 UDP segment structure\n\nwrapped around. This result is put in the checksum field of the UDP segment. Here\nwe give a simple example of the checksum calculation. You can find details about\nefficient implementation of the calculation in RFC 1071 and performance over real\ndata in [Stone 1998; Stone 2000]. As an example, suppose that we have the follow-\ning three 16-bit words:\n0110011001100000\n0101010101010101\n1000111100001100\nThe sum of first two of these 16-bit words is\n0110011001100000\n0101010101010101\n1011101110110101\nAdding the third word to the above sum gives\n1011101110110101\n1000111100001100\n0100101011000010\nNote that this last addition had overflow, which was wrapped around. The 1s com-\nplement is obtained by converting all the 0s to 1s and converting all the 1s to 0s.\nThus the 1s complement of the sum 0100101011000010 is 1011010100111101,\nwhich becomes the checksum. At the receiver, all four 16-bit words are added,\nincluding the checksum. If no errors are introduced into the packet, then clearly the\nsum at the receiver will be 1111111111111111. If one of the bits is a 0, then we know\nthat errors have been introduced into the packet.\nYou may wonder why UDP provides a checksum in the first place, as many link-\nlayer protocols (including the popular Ethernet protocol) also provide error checking.\nThe reason is that there is no guarantee that all the links between source and destination\nprovide error checking; that is, one of the links may use a link-layer protocol that does\nnot provide error checking. Furthermore, even if segments are correctly transferred\nacross a link, it’s possible that bit errors could be introduced when a segment is stored\nin a router’s memory. Given that neither link-by-link reliability nor in-memory error\ndetection is guaranteed, UDP must provide error detection at the transport layer, on an\nend-end basis, if the end-end data transfer service is to provide error detection. This is\nan example of the celebrated end-end principle in system design [Saltzer 1984], which\nstates that since certain functionality (error detection, in this case) must be implemented\non an end-end basis: “functions placed at the lower levels may be redundant or of little\nvalue when compared to the cost of providing them at the higher level.”\nBecause IP is supposed to run over just about any layer-2 protocol, it is useful\nfor the transport layer to provide error checking as a safety measure. Although UDP\n3.3\n•\nCONNECTIONLESS TRANSPORT: UDP\n203"
    },
    {
      "chunk_id": "10b3be2f-55f6-417e-b637-a86e16846693",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.3.2 UDP Checksum",
      "original_titles": [
        "3.3.2 UDP Checksum"
      ],
      "path": "Chapter 3 Transport Layer > 3.3 Connectionless Transport: UDP > 3.3.2 UDP Checksum",
      "start_page": 229,
      "end_page": 230,
      "token_count": 995,
      "text": "a long time. Nevertheless, building reliability directly into the application allows the\napplication to “have its cake and eat it too.” That is, application processes can com-\nmunicate reliably without being subjected to the transmission-rate constraints\nimposed by TCP’s congestion-control mechanism.\n3.3.1 UDP Segment Structure\nThe UDP segment structure, shown in Figure 3.7, is defined in RFC 768. The applica-\ntion data occupies the data field of the UDP segment. For example, for DNS, the data\nfield contains either a query message or a response message. For a streaming audio\napplication, audio samples fill the data field. The UDP header has only four fields,\neach consisting of two bytes. As discussed in the previous section, the port numbers\nallow the destination host to pass the application data to the correct process running\non the destination end system (that is, to perform the demultiplexing function). The\nlength field specifies the number of bytes in the UDP segment (header plus data). An\nexplicit length value is needed since the size of the data field may differ from one UDP\nsegment to the next. The checksum is used by the receiving host to check whether\nerrors have been introduced into the segment. In truth, the checksum is also calculated\nover a few of the fields in the IP header in addition to the UDP segment. But we ignore\nthis detail in order to see the forest through the trees. We’ll discuss the checksum cal-\nculation below. Basic principles of error detection are described in Section 5.2. The\nlength field specifies the length of the UDP segment, including the header, in bytes.\n3.3.2 UDP Checksum\nThe UDP checksum provides for error detection. That is, the checksum is used to\ndetermine whether bits within the UDP segment have been altered (for example, by\nnoise in the links or while stored in a router) as it moved from source to destination.\nUDP at the sender side performs the 1s complement of the sum of all the 16-bit\nwords in the segment, with any overflow encountered during the sum being \n202\nCHAPTER 3\n•\nTRANSPORT LAYER\nSource port #\n32 bits\nDest. port #\nLength\nChecksum\nApplication\ndata\n(message)\nFigure 3.7 \u0002 UDP segment structure\n\nwrapped around. This result is put in the checksum field of the UDP segment. Here\nwe give a simple example of the checksum calculation. You can find details about\nefficient implementation of the calculation in RFC 1071 and performance over real\ndata in [Stone 1998; Stone 2000]. As an example, suppose that we have the follow-\ning three 16-bit words:\n0110011001100000\n0101010101010101\n1000111100001100\nThe sum of first two of these 16-bit words is\n0110011001100000\n0101010101010101\n1011101110110101\nAdding the third word to the above sum gives\n1011101110110101\n1000111100001100\n0100101011000010\nNote that this last addition had overflow, which was wrapped around. The 1s com-\nplement is obtained by converting all the 0s to 1s and converting all the 1s to 0s.\nThus the 1s complement of the sum 0100101011000010 is 1011010100111101,\nwhich becomes the checksum. At the receiver, all four 16-bit words are added,\nincluding the checksum. If no errors are introduced into the packet, then clearly the\nsum at the receiver will be 1111111111111111. If one of the bits is a 0, then we know\nthat errors have been introduced into the packet.\nYou may wonder why UDP provides a checksum in the first place, as many link-\nlayer protocols (including the popular Ethernet protocol) also provide error checking.\nThe reason is that there is no guarantee that all the links between source and destination\nprovide error checking; that is, one of the links may use a link-layer protocol that does\nnot provide error checking. Furthermore, even if segments are correctly transferred\nacross a link, it’s possible that bit errors could be introduced when a segment is stored\nin a router’s memory. Given that neither link-by-link reliability nor in-memory error\ndetection is guaranteed, UDP must provide error detection at the transport layer, on an\nend-end basis, if the end-end data transfer service is to provide error detection. This is\nan example of the celebrated end-end principle in system design [Saltzer 1984], which\nstates that since certain functionality (error detection, in this case) must be implemented\non an end-end basis: “functions placed at the lower levels may be redundant or of little\nvalue when compared to the cost of providing them at the higher level.”\nBecause IP is supposed to run over just about any layer-2 protocol, it is useful\nfor the transport layer to provide error checking as a safety measure. Although UDP\n3.3\n•\nCONNECTIONLESS TRANSPORT: UDP\n203"
    },
    {
      "chunk_id": "434a9511-fe36-4fa5-92c9-f7139927f7ee",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.4 Principles of Reliable Data Transfer",
      "original_titles": [
        "3.4 Principles of Reliable Data Transfer"
      ],
      "path": "Chapter 3 Transport Layer > 3.4 Principles of Reliable Data Transfer",
      "start_page": 231,
      "end_page": 232,
      "token_count": 1041,
      "text": "provides error checking, it does not do anything to recover from an error. Some\nimplementations of UDP simply discard the damaged segment; others pass the dam-\naged segment to the application with a warning.\nThat wraps up our discussion of UDP. We will soon see that TCP offers reliable\ndata transfer to its applications as well as other services that UDP doesn’t offer. Natu-\nrally, TCP is also more complex than UDP. Before discussing TCP, however, it will be\nuseful to step back and first discuss the underlying principles of reliable data transfer.\n3.4 Principles of Reliable Data Transfer\nIn this section, we consider the problem of reliable data transfer in a general con-\ntext. This is appropriate since the problem of implementing reliable data transfer\noccurs not only at the transport layer, but also at the link layer and the application\nlayer as well. The general problem is thus of central importance to networking.\nIndeed, if one had to identify a “top-ten” list of fundamentally important problems\nin all of networking, this would be a candidate to lead the list. In the next section\nwe’ll examine TCP and show, in particular, that TCP exploits many of the principles\nthat we are about to describe.\nFigure 3.8 illustrates the framework for our study of reliable data transfer. The\nservice abstraction provided to the upper-layer entities is that of a reliable channel\nthrough which data can be transferred. With a reliable channel, no transferred data\nbits are corrupted (flipped from 0 to 1, or vice versa) or lost, and all are delivered in\nthe order in which they were sent. This is precisely the service model offered by\nTCP to the Internet applications that invoke it.\nIt is the responsibility of a reliable data transfer protocol to implement this\nservice abstraction. This task is made difficult by the fact that the layer below the\nreliable data transfer protocol may be unreliable. For example, TCP is a reliable data\ntransfer protocol that is implemented on top of an unreliable (IP) end-to-end net-\nwork layer. More generally, the layer beneath the two reliably communicating end\npoints might consist of a single physical link (as in the case of a link-level data\ntransfer protocol) or a global internetwork (as in the case of a transport-level proto-\ncol). For our purposes, however, we can view this lower layer simply as an unreli-\nable point-to-point channel.\nIn this section, we will incrementally develop the sender and receiver sides of a\nreliable data transfer protocol, considering increasingly complex models of the under-\nlying channel. For example, we’ll consider what protocol mechanisms are needed when\nthe underlying channel can corrupt bits or lose entire packets. One assumption we’ll\nadopt throughout our discussion here is that packets will be delivered in the order in\nwhich they were sent, with some packets possibly being lost; that is, the underlying\nchannel will not reorder packets. Figure 3.8(b) illustrates the interfaces for our data\ntransfer protocol. The sending side of the data transfer protocol will be invoked from\nabove by a call to rdt_send(). It will pass the data to be delivered to the upper layer\nat the receiving side. (Here rdt stands for reliable data transfer protocol and _send\n204\nCHAPTER 3\n•\nTRANSPORT LAYER\n\nindicates that the sending side of rdt is being called. The first step in developing any\nprotocol is to choose a good name!) On the receiving side, rdt_rcv() will be called\nwhen a packet arrives from the receiving side of the channel. When the rdt protocol\nwants to deliver data to the upper layer, it will do so by calling deliver_data(). In\nthe following we use the terminology “packet” rather than transport-layer “segment.”\nBecause the theory developed in this section applies to computer networks in general\nand not just to the Internet transport layer, the generic term “packet” is perhaps more\nappropriate here.\nIn this section we consider only the case of unidirectional data transfer, that is,\ndata transfer from the sending to the receiving side. The case of reliable bidirectional\n(that is, full-duplex) data transfer is conceptually no more difficult but considerably\nmore tedious to explain. Although we consider only unidirectional data transfer, it is\nimportant to note that the sending and receiving sides of our protocol will nonetheless\nneed to transmit packets in both directions, as indicated in Figure 3.8. We will see\nshortly that, in addition to exchanging packets containing the data to be transferred, the\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n205\nReliable channel \nUnreliable channel \nrdt_send()\nudt_send()\nSending\nprocess \nReceiver\nprocess \ndeliver_data\nApplication\nlayer\nTransport\nlayer\na. Provided service\nNetwork\nlayer\nKey:\nData \nPacket \nb. Service implementation\nReliable data \ntransfer protocol \n(sending side) \nReliable data \ntransfer protocol \n(receiving side) \nrdt_rcv()\nFigure 3.8 \u0002 Reliable data transfer: Service model and service \nimplementation"
    },
    {
      "chunk_id": "93b8ea4b-111a-4174-a406-259f881aea5b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.4.1 Building a Reliable Data Transfer Protocol",
      "original_titles": [
        "3.4.1 Building a Reliable Data Transfer Protocol"
      ],
      "path": "Chapter 3 Transport Layer > 3.4 Principles of Reliable Data Transfer > 3.4.1 Building a Reliable Data Transfer Protocol",
      "start_page": 233,
      "end_page": 241,
      "token_count": 4188,
      "text": "sending and receiving sides of rdt will also need to exchange control packets back and\nforth. Both the send and receive sides of rdt send packets to the other side by a call to\nudt_send() (where udt stands for unreliable data transfer).\n3.4.1 Building a Reliable Data Transfer Protocol\nWe now step through a series of protocols, each one becoming more complex, arriv-\ning at a flawless, reliable data transfer protocol.\nReliable Data Transfer over a Perfectly Reliable Channel: rdt1.0\nWe first consider the simplest case, in which the underlying channel is completely\nreliable. The protocol itself, which we’ll call rdt1.0, is trivial. The finite-state\nmachine (FSM) definitions for the rdt1.0 sender and receiver are shown in\nFigure 3.9. The FSM in Figure 3.9(a) defines the operation of the sender, while the\nFSM in Figure 3.9(b) defines the operation of the receiver. It is important to note\nthat there are separate FSMs for the sender and for the receiver. The sender and\nreceiver FSMs in Figure 3.9 each have just one state. The arrows in the FSM\ndescription indicate the transition of the protocol from one state to another. (Since\neach FSM in Figure 3.9 has just one state, a transition is necessarily from the one\nstate back to itself; we’ll see more complicated state diagrams shortly.) The event\ncausing the transition is shown above the horizontal line labeling the transition, and\n206\nCHAPTER 3\n•\nTRANSPORT LAYER\nWait for\ncall from\nabove\na.  rdt1.0: sending side\nrdt_send(data)\npacket=make_pkt(data)\nudt_send(packet)\nWait for\ncall from\nbelow\nb.  rdt1.0: receiving side\nrdt_rcv(packet)\nextract(packet,data)\ndeliver_data(data)\nFigure 3.9 \u0002 rdt1.0 – A protocol for a completely reliable channel\n\nthe actions taken when the event occurs are shown below the horizontal line. When\nno action is taken on an event, or no event occurs and an action is taken, we’ll use\nthe symbol \u0002 below or above the horizontal, respectively, to explicitly denote the\nlack of an action or event. The initial state of the FSM is indicated by the dashed\narrow. Although the FSMs in Figure 3.9 have but one state, the FSMs we will see\nshortly have multiple states, so it will be important to identify the initial state of\neach FSM.\nThe sending side of rdt simply accepts data from the upper layer via the\nrdt_send(data) event, creates a packet containing the data (via the action\nmake_pkt(data)) and sends the packet into the channel. In practice, the\nrdt_send(data) event would result from a procedure call (for example, to\nrdt_send()) by the upper-layer application.\nOn the receiving side, rdt receives a packet from the underlying channel via\nthe rdt_rcv(packet) event, removes the data from the packet (via the action\nextract (packet, data)) and passes the data up to the upper layer (via the\naction deliver_data(data)). In practice, the rdt_rcv(packet) event\nwould result from a procedure call (for example, to rdt_rcv()) from the lower-\nlayer protocol.\nIn this simple protocol, there is no difference between a unit of data and a\npacket. Also, all packet flow is from the sender to receiver; with a perfectly reliable\nchannel there is no need for the receiver side to provide any feedback to the sender\nsince nothing can go wrong! Note that we have also assumed that the receiver is able\nto receive data as fast as the sender happens to send data. Thus, there is no need for\nthe receiver to ask the sender to slow down!\nReliable Data Transfer over a Channel with Bit Errors: rdt2.0\nA more realistic model of the underlying channel is one in which bits in a packet\nmay be corrupted. Such bit errors typically occur in the physical components of a\nnetwork as a packet is transmitted, propagates, or is buffered. We’ll continue to\nassume for the moment that all transmitted packets are received (although their bits\nmay be corrupted) in the order in which they were sent.\nBefore developing a protocol for reliably communicating over such a channel,\nfirst consider how people might deal with such a situation. Consider how you your-\nself might dictate a long message over the phone. In a typical scenario, the message\ntaker might say “OK” after each sentence has been heard, understood, and recorded.\nIf the message taker hears a garbled sentence, you’re asked to repeat the garbled\nsentence. This message-dictation protocol uses both positive acknowledgments\n(“OK”) and negative acknowledgments (“Please repeat that.”). These control mes-\nsages allow the receiver to let the sender know what has been received correctly, and\nwhat has been received in error and thus requires repeating. In a computer network\nsetting, reliable data transfer protocols based on such retransmission are known as\nARQ (Automatic Repeat reQuest) protocols.\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n207\n\nFundamentally, three additional protocol capabilities are required in ARQ\nprotocols to handle the presence of bit errors:\n•\nError detection. First, a mechanism is needed to allow the receiver to detect\nwhen bit errors have occurred. Recall from the previous section that UDP uses\nthe Internet checksum field for exactly this purpose. In Chapter 5 we’ll exam-\nine error-detection and -correction techniques in greater detail; these tech-\nniques allow the receiver to detect and possibly correct packet bit errors. For\nnow, we need only know that these techniques require that extra bits (beyond\nthe bits of original data to be transferred) be sent from the sender to the\nreceiver; these bits will be gathered into the packet checksum field of the\nrdt2.0 data packet.\n•\nReceiver feedback. Since the sender and receiver are typically executing on differ-\nent end systems, possibly separated by thousands of miles, the only way for the\nsender to learn of the receiver’s view of the world (in this case, whether or not a\npacket was received correctly) is for the receiver to provide explicit feedback to the\nsender. The positive (ACK) and negative (NAK) acknowledgment replies in the\nmessage-dictation scenario are examples of such feedback. Our rdt2.0 protocol\nwill similarly send ACK and NAK packets back from the receiver to the sender. In\nprinciple, these packets need only be one bit long; for example, a 0 value could indi-\ncate a NAK and a value of 1 could indicate an ACK.\n•\nRetransmission. A packet that is received in error at the receiver will be retrans-\nmitted by the sender.\nFigure 3.10 shows the FSM representation of rdt2.0, a data transfer protocol\nemploying error detection, positive acknowledgments, and negative acknowledgments.\nThe send side of rdt2.0 has two states. In the leftmost state, the send-side proto-\ncol is waiting for data to be passed down from the upper layer. When the\nrdt_send(data) event occurs, the sender will create a packet (sndpkt)\ncontaining the data to be sent, along with a packet checksum (for example, as discussed\nin Section 3.3.2 for the case of a UDP segment), and then send the packet via the\nudt_send(sndpkt) operation. In the rightmost state, the sender protocol is wait-\ning for an ACK or a NAK packet from the receiver. If an ACK packet is received (the\nnotation rdt_rcv(rcvpkt) && isACK (rcvpkt) in Figure 3.10 corresponds\nto this event), the sender knows that the most recently transmitted packet has been\nreceived correctly and thus the protocol returns to the state of waiting for data from the\nupper layer. If a NAK is received, the protocol retransmits the last packet and waits for\nan ACK or NAK to be returned by the receiver in response to the retransmitted data\npacket. It is important to note that when the sender is in the wait-for-ACK-or-NAK\nstate, it cannot get more data from the upper layer; that is, the rdt_send() event can\nnot occur; that will happen only after the sender receives an ACK and leaves this state.\nThus, the sender will not send a new piece of data until it is sure that the receiver has\n208\nCHAPTER 3\n•\nTRANSPORT LAYER\n\ncorrectly received the current packet. Because of this behavior, protocols such as\nrdt2.0 are known as stop-and-wait protocols.\nThe receiver-side FSM for rdt2.0 still has a single state. On packet arrival,\nthe receiver replies with either an ACK or a NAK, depending on whether or not the\nreceived packet is corrupted. In Figure 3.10, the notation rdt_rcv(rcvpkt) &&\ncorrupt(rcvpkt) corresponds to the event in which a packet is received and is\nfound to be in error.\nProtocol rdt2.0 may look as if it works but, unfortunately, it has a fatal\nflaw. In particular, we haven’t accounted for the possibility that the ACK or NAK\npacket could be corrupted! (Before proceeding on, you should think about how this\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n209\nWait for\ncall from\nabove\na.  rdt2.0: sending side\nb.  rdt2.0: receiving side\nrdt_rcv(rcvpkt) && corrupt(rcvpkt)\nsndpkt=make_pkt(NAK)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && isNAK(rcvpkt)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && isACK(rcvpkt)\nΛ\nrdt_send(data)\nsndpkt=make_pkt(data,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && notcorrupt(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(ACK)\nudt_send(sndpkt)\nWait for\ncall from\nbelow\nWait for\nACK or\nNAK\nFigure 3.10 \u0002 rdt2.0–A protocol for a channel with bit errors\n\nproblem may be fixed.) Unfortunately, our slight oversight is not as innocuous as it\nmay seem. Minimally, we will need to add checksum bits to ACK/NAK packets in\norder to detect such errors. The more difficult question is how the protocol should\nrecover from errors in ACK or NAK packets. The difficulty here is that if an ACK\nor NAK is corrupted, the sender has no way of knowing whether or not the receiver\nhas correctly received the last piece of transmitted data.\nConsider three possibilities for handling corrupted ACKs or NAKs:\n•\nFor the first possibility, consider what a human might do in the message-\ndictation scenario. If the speaker didn’t understand the “OK” or “Please repeat\nthat” reply from the receiver, the speaker would probably ask, “What did you\nsay?” (thus introducing a new type of sender-to-receiver packet to our protocol).\nThe receiver would then repeat the reply. But what if the speaker’s “What did\nyou say?” is corrupted? The receiver, having no idea whether the garbled sen-\ntence was part of the dictation or a request to repeat the last reply, would proba-\nbly then respond with “What did you say?” And then, of course, that response\nmight be garbled. Clearly, we’re heading down a difficult path.\n•\nA second alternative is to add enough checksum bits to allow the sender not only\nto detect, but also to recover from, bit errors. This solves the immediate problem\nfor a channel that can corrupt packets but not lose them.\n•\nA third approach is for the sender simply to resend the current data packet when\nit receives a garbled ACK or NAK packet. This approach, however, introduces\nduplicate packets into the sender-to-receiver channel. The fundamental diffi-\nculty with duplicate packets is that the receiver doesn’t know whether the ACK\nor NAK it last sent was received correctly at the sender. Thus, it cannot know a\npriori whether an arriving packet contains new data or is a retransmission!\nA simple solution to this new problem (and one adopted in almost all existing\ndata transfer protocols, including TCP) is to add a new field to the data packet and\nhave the sender number its data packets by putting a sequence number into this\nfield. The receiver then need only check this sequence number to determine whether\nor not the received packet is a retransmission. For this simple case of a stop-and-\nwait protocol, a 1-bit sequence number will suffice, since it will allow the receiver\nto know whether the sender is resending the previously transmitted packet (the\nsequence number of the received packet has the same sequence number as the most\nrecently received packet) or a new packet (the sequence number changes, moving\n“forward” in modulo-2 arithmetic). Since we are currently assuming a channel that\ndoes not lose packets, ACK and NAK packets do not themselves need to indicate\nthe sequence number of the packet they are acknowledging. The sender knows that\na received ACK or NAK packet (whether garbled or not) was generated in response\nto its most recently transmitted data packet.\n210\nCHAPTER 3\n•\nTRANSPORT LAYER\n\nFigures 3.11 and 3.12 show the FSM description for rdt2.1, our fixed version\nof rdt2.0. The rdt2.1 sender and receiver FSMs each now have twice as many\nstates as before. This is because the protocol state must now reflect whether the packet\ncurrently being sent (by the sender) or expected (at the receiver) should have a\nsequence number of 0 or 1. Note that the actions in those states where a 0-numbered\npacket is being sent or expected are mirror images of those where a 1-numbered\npacket is being sent or expected; the only differences have to do with the handling of\nthe sequence number.\nProtocol rdt2.1 uses both positive and negative acknowledgments from the\nreceiver to the sender. When an out-of-order packet is received, the receiver sends a\npositive acknowledgment for the packet it has received. When a corrupted packet is\nreceived, the receiver sends a negative acknowledgment. We can accomplish the\nsame effect as a NAK if, instead of sending a NAK, we send an ACK for the last\ncorrectly received packet. A sender that receives two ACKs for the same packet (that\nis, receives duplicate ACKs) knows that the receiver did not correctly receive the\npacket following the packet that is being ACKed twice. Our NAK-free reliable data\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n211\nWait for\ncall 0 from\nabove\nrdt_rcv(rcvpkt)&&\n(corrupt(rcvpkt)||\nisNAK(rcvpkt))\nudt_send(sndpkt)\nrdt_rcv(rcvpkt)&&\n(corrupt(rcvpkt)||\nisNAK(rcvpkt))\nudt_send(sndpkt)\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt)\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt)\nΛ\nΛ\nrdt_send(data)\nsndpkt=make_pkt(0,data,checksum)\nudt_send(sndpkt)\nrdt_send(data)\nsndpkt=make_pkt(1,data,checksum)\nudt_send(sndpkt)\nWait for\nACK or\nNAK 0\nWait for\nACK or\nNAK 1\nWait for\ncall 1 from\nabove\nFigure 3.11 \u0002 rdt2.1 sender\n\ntransfer protocol for a channel with bit errors is rdt2.2, shown in Figures 3.13 and\n3.14. One subtle change between rtdt2.1 and rdt2.2 is that the receiver must\nnow include the sequence number of the packet being acknowledged by an ACK\nmessage (this is done by including the ACK,0 or ACK,1 argument in make_pkt()\nin the receiver FSM), and the sender must now check the sequence number of the\npacket being acknowledged by a received ACK message (this is done by including\nthe 0 or 1 argument in isACK()in the sender FSM).\nReliable Data Transfer over a Lossy Channel with Bit Errors: rdt3.0\nSuppose now that in addition to corrupting bits, the underlying channel can lose\npackets as well, a not-uncommon event in today’s computer networks (including the\nInternet). Two additional concerns must now be addressed by the protocol: how to\ndetect packet loss and what to do when packet loss occurs. The use of checksum-\nming, sequence numbers, ACK packets, and retransmissions—the techniques\nalready developed in rdt2.2—will allow us to answer the latter concern. Han-\ndling the first concern will require adding a new protocol mechanism.\nThere are many possible approaches toward dealing with packet loss (several\nmore of which are explored in the exercises at the end of the chapter). Here, we’ll\nput the burden of detecting and recovering from lost packets on the sender. Suppose\n212\nCHAPTER 3\n•\nTRANSPORT LAYER\nrdt_rcv(rcvpkt)&& notcorrupt\n(rcvpkt)&&has_seq0(rcvpkt)\nsndpkt=make_pkt(ACK,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && corrupt(rcvpkt)\nsndpkt=make_pkt(NAK,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt)\n && corrupt(rcvpkt)\nsndpkt=make_pkt(NAK,checksum)\nudt_send(sndpkt)\nsndpkt=make_pkt(ACK,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && notcorrupt(rcvpkt)\n&& has_seq1(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(ACK,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt)&& notcorrupt(rcvpkt)\n && has_seq0(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(ACK,checksum)\nudt_send(sndpkt)\nWait for\n0 from\nbelow\nWait for\n1 from\nbelow\nrdt_rcv(rcvpkt)&& notcorrupt\n(rcvpkt)&&has_seq1(rcvpkt)\nFigure 3.12 \u0002 rdt2.1 receiver\n\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n213\nthat the sender transmits a data packet and either that packet, or the receiver’s ACK\nof that packet, gets lost. In either case, no reply is forthcoming at the sender from\nthe receiver. If the sender is willing to wait long enough so that it is certain that a\npacket has been lost, it can simply retransmit the data packet. You should convince\nyourself that this protocol does indeed work.\nBut how long must the sender wait to be certain that something has been lost?\nThe sender must clearly wait at least as long as a round-trip delay between the\nsender and receiver (which may include buffering at intermediate routers) plus\nwhatever amount of time is needed to process a packet at the receiver. In many net-\nworks, this worst-case maximum delay is very difficult even to estimate, much less\nknow with certainty. Moreover, the protocol should ideally recover from packet\nloss as soon as possible; waiting for a worst-case delay could mean a long wait\nuntil error recovery is initiated. The approach thus adopted in practice is for the\nsender to judiciously choose a time value such that packet loss is likely, although\nnot guaranteed, to have happened. If an ACK is not received within this time, the\npacket is retransmitted. Note that if a packet experiences a particularly large delay,\nthe sender may retransmit the packet even though neither the data packet nor its\nACK have been lost. This introduces the possibility of duplicate data packets in\nWait for\ncall 0 from\nabove\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nisACK(rcvpkt,1))\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nisACK(rcvpkt,0))\nudt_send(sndpkt)\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt,0)\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt,1)\nrdt_send(data)\nsndpkt=make_pkt(0,data,checksum)\nudt_send(sndpkt)\nrdt_send(data)\nsndpkt=make_pkt(1,data,checksum)\nudt_send(sndpkt)\nWait for\nACK 0\nWait for\nACK 1\nΛ\nΛ\nWait for\ncall 1 from\nabove\nFigure 3.13 \u0002 rdt2.2 sender\n\nthe sender-to-receiver channel. Happily, protocol rdt2.2 already has enough\nfunctionality (that is, sequence numbers) to handle the case of duplicate packets.\nFrom the sender’s viewpoint, retransmission is a panacea. The sender does not\nknow whether a data packet was lost, an ACK was lost, or if the packet or ACK was\nsimply overly delayed. In all cases, the action is the same: retransmit. Implementing\na time-based retransmission mechanism requires a countdown timer that can\ninterrupt the sender after a given amount of time has expired. The sender will thus\nneed to be able to (1) start the timer each time a packet (either a first-time packet or\na retransmission) is sent, (2) respond to a timer interrupt (taking appropriate\nactions), and (3) stop the timer.\nFigure 3.15 shows the sender FSM for rdt3.0, a protocol that reliably transfers\ndata over a channel that can corrupt or lose packets; in the homework problems, you’ll\nbe asked to provide the receiver FSM for rdt3.0. Figure 3.16 shows how the proto-\ncol operates with no lost or delayed packets and how it handles lost data packets. In\nFigure 3.16, time moves forward from the top of the diagram toward the bottom of the\ndiagram; note that a receive time for a packet is necessarily later than the send time\nfor a packet as a result of transmission and propagation delays. In Figures 3.16(b)–(d),\nthe send-side brackets indicate the times at which a timer is set and later times out.\nSeveral of the more subtle aspects of this protocol are explored in the exercises at the\nend of this chapter. Because packet sequence numbers alternate between 0 and 1, pro-\ntocol rdt3.0 is sometimes known as the alternating-bit protocol.\n214\nCHAPTER 3\n•\nTRANSPORT LAYER\nWait for\n0 from\nbelow\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nhas_seq0(rcvpkt))\nsndpkt=make_pkt(ACK,0,che\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nhas_seq1(rcvpkt))\nsndpkt=make_pkt(ACK,1,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && notcorrupt(rcvpkt)\n&& has_seq1(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(ACK,1,checksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) && notcorrupt(rcvpkt)\n&& has_seq0(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(ACK,0,checksum)\nudt_send(sndpkt)\nWait for\n1 from\nbelow\nFigure 3.14 \u0002 rdt2.2 receiver"
    },
    {
      "chunk_id": "dd4e5121-b278-4ac4-9d94-2c7b09149268",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.4.2 Pipelined Reliable Data Transfer Protocols",
      "original_titles": [
        "3.4.2 Pipelined Reliable Data Transfer Protocols"
      ],
      "path": "Chapter 3 Transport Layer > 3.4 Principles of Reliable Data Transfer > 3.4.2 Pipelined Reliable Data Transfer Protocols",
      "start_page": 242,
      "end_page": 244,
      "token_count": 1037,
      "text": "We have now assembled the key elements of a data transfer protocol. Check-\nsums, sequence numbers, timers, and positive and negative acknowledgment pack-\nets each play a crucial and necessary role in the operation of the protocol. We now\nhave a working reliable data transfer protocol!\n3.4.2 Pipelined Reliable Data Transfer Protocols\nProtocol rdt3.0 is a functionally correct protocol, but it is unlikely that anyone would\nbe happy with its performance, particularly in today’s high-speed networks. At the heart\nof rdt3.0’s performance problem is the fact that it is a stop-and-wait protocol.\nTo appreciate the performance impact of this stop-and-wait behavior, consider\nan idealized case of two hosts, one located on the West Coast of the United States\nand the other located on the East Coast, as shown in Figure 3.17. The speed-of-light\nround-trip propagation delay between these two end systems, RTT, is approxi-\nmately 30 milliseconds. Suppose that they are connected by a channel with a trans-\nmission rate, R, of 1 Gbps (109 bits per second). With a packet size, L, of 1,000 bytes\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n215\nWait for\ncall 0 from\nabove\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nisACK(rcvpkt,1))\ntimeout\nudt_send(sndpkt)\nstart_timer\nrdt_rcv(rcvpkt)\nΛ\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nisACK(rcvpkt,0))\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt,0)\nstop_timer\nrdt_rcv(rcvpkt)\n&& notcorrupt(rcvpkt)\n&& isACK(rcvpkt,1)\nstop_timer\ntimeout\nudt_send(sndpkt)\nstart_timer\nrdt_send(data)\nsndpkt=make_pkt(0,data,checksum)\nudt_send(sndpkt)\nstart_timer\nrdt_send(data)\nsndpkt=make_pkt(1,data,checksum)\nudt_send(sndpkt)\nstart_timer\nWait for\nACK 0\nWait for\nACK 1\nΛ\nΛ\nWait for\ncall 1 from\nabove\nrdt_rcv(rcvpkt)\nΛ\nFigure 3.15 \u0002 rdt3.0 sender\nVideoNote\nDeveloping a protocol\nand FSM representation \nfor a simple application-\nlayer protocol\n\n216\nCHAPTER 3\n•\nTRANSPORT LAYER\nrcv pkt0\nsend ACK0\nrcv pkt1\nsend ACK1\nrcv pkt0\nsend ACK0\nSender\nReceiver\na. Operation with no loss\npkt0\nACK0\npkt1\npkt0\nACK1\nACK0\n(loss) X\nb. Lost packet\nrcv pkt0\nsend ACK0\nrcv pkt1\nsend ACK1\nc. Lost ACK\nsend pkt0\nrcv ACK0\nsend pkt1\nrcv ACK1\nsend pkt0\nsend pkt0\nrcv ACK0\nsend pkt1\ntimeout\nresend pkt1\nrcv ACK1\nsend pkt0\nrcv pkt0\nsend ACK0\nrcv pkt1\n(detect\nduplicate)\nsend ACK1\nsend pkt0\nrcv ACK0\nsend pkt1\nrcv pkt0\nsend ACK0\ntimeout\nresend pkt1\nrcv pkt1\nsend ACK1\nd. Premature timeout\nrcv ACK1\nsend pkt0\nrcv ACK1\ndo nothing\nrcv pkt0\nsend ACK0\nrcv pkt 1\n(detect duplicate)\nsend ACK1\nSender\nReceiver\nReceiver\nSender\npkt0\nACK0\npkt1\nACK1\nACK1\nACK0\nACK1\nACK0\npkt1\npkt0\npkt0\npkt1\npkt1\npkt0\nACK1\nACK0\nX (loss)\npkt1\nrcv pkt0\nsend ACK0\nsend pkt0\nrcv ACK0\nsend pkt1\ntimeout\nresend pkt1\nrcv ACK1\nsend pkt0\nrcv pkt0\nsend ACK0\nrcv pkt1\nsend ACK1\nSender\nReceiver\npkt0\nACK0\npkt1\npkt0\nACK1\nACK0\nFigure 3.16 \u0002 Operation of rdt3.0, the alternating-bit protocol\n\n(8,000 bits) per packet, including both header fields and data, the time needed to\nactually transmit the packet into the 1 Gbps link is\ndtrans\nFigure 3.18(a) shows that with our stop-and-wait protocol, if the sender begins\nsending the packet at t = 0, then at t = L/R = 8 microseconds, the last bit enters the\nchannel at the sender side. The packet then makes its 15-msec cross-country jour-\nney, with the last bit of the packet emerging at the receiver at t = RTT/2 + L/R =\n15.008 msec. Assuming for simplicity that ACK packets are extremely small (so that\nwe can ignore their transmission time) and that the receiver can send an ACK as\nsoon as the last bit of a data packet is received, the ACK emerges back at the sender\nat t = RTT + L/R = 30.008 msec. At this point, the sender can now transmit the next\nmessage. Thus, in 30.008 msec, the sender was sending for only 0.008 msec. If we\ndefine the utilization of the sender (or the channel) as the fraction of time the sender\nis actually busy sending bits into the channel, the analysis in Figure 3.18(a) shows\nthat the stop-and-wait protocol has a rather dismal sender utilization, Usender, of\nThat is, the sender was busy only 2.7 hundredths of one percent of the time!\nViewed another way, the sender was able to send only 1,000 bytes in 30.008 mil-\nliseconds, an effective throughput of only 267 kbps—even though a 1 Gbps link was\navailable! Imagine the unhappy network manager who just paid a fortune for a giga-\nbit capacity link but manages to get a throughput of only 267 kilobits per second!\nThis is a graphic example of how network protocols can limit the capabilities\nUsender =\nL>R\nRTT + L>R =\n.008\n30.008 = 0.00027\n= L\nR =\n8000 bits>packet\n109 bits/sec\n= 8 microseconds\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n217\nData packets\nData packet\nACK packets\na. A stop-and-wait protocol in operation\nb. A pipelined protocol in operation\nFigure 3.17 \u0002 Stop-and-wait versus pipelined protocol"
    },
    {
      "chunk_id": "cb972ebe-57d8-410d-8fba-c9a6c8099538",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.4.3 Go-Back-N (GBN)",
      "original_titles": [
        "3.4.3 Go-Back-N (GBN)"
      ],
      "path": "Chapter 3 Transport Layer > 3.4 Principles of Reliable Data Transfer > 3.4.3 Go-Back-N (GBN)",
      "start_page": 245,
      "end_page": 249,
      "token_count": 2160,
      "text": "provided by the underlying network hardware. Also, we have neglected lower-layer\nprotocol-processing times at the sender and receiver, as well as the processing and\nqueuing delays that would occur at any intermediate routers between the sender\nand receiver. Including these effects would serve only to further increase the delay\nand further accentuate the poor performance.\nThe solution to this particular performance problem is simple: Rather than oper-\nate in a stop-and-wait manner, the sender is allowed to send multiple packets with-\nout waiting for acknowledgments, as illustrated in Figure 3.17(b). Figure 3.18(b)\nshows that if the sender is allowed to transmit three packets before having to wait\nfor acknowledgments, the utilization of the sender is essentially tripled. Since the\nmany in-transit sender-to-receiver packets can be visualized as filling a pipeline, this\ntechnique is known as pipelining. Pipelining has the following consequences for\nreliable data transfer protocols:\n•\nThe range of sequence numbers must be increased, since each in-transit packet\n(not counting retransmissions) must have a unique sequence number and there\nmay be multiple, in-transit, unacknowledged packets.\n•\nThe sender and receiver sides of the protocols may have to buffer more than one\npacket. Minimally, the sender will have to buffer packets that have been trans-\nmitted but not yet acknowledged. Buffering of correctly received packets may\nalso be needed at the receiver, as discussed below.\n•\nThe range of sequence numbers needed and the buffering requirements will\ndepend on the manner in which a data transfer protocol responds to lost, cor-\nrupted, and overly delayed packets. Two basic approaches toward pipelined error\nrecovery can be identified: Go-Back-N and selective repeat.\n3.4.3 Go-Back-N (GBN)\nIn a Go-Back-N (GBN) protocol, the sender is allowed to transmit multiple packets\n(when available) without waiting for an acknowledgment, but is constrained to have no\nmore than some maximum allowable number, N, of unacknowledged packets in the\npipeline. We describe the GBN protocol in some detail in this section. But before read-\ning on, you are encouraged to play with the GBN applet (an awesome applet!) at the\ncompanion Web site.\nFigure 3.19 shows the sender’s view of the range of sequence numbers in a GBN\nprotocol. If we define base to be the sequence number of the oldest unacknowledged\npacket and nextseqnum to be the smallest unused sequence number (that is, the\nsequence number of the next packet to be sent), then four intervals in the range of\nsequence numbers can be identified. Sequence numbers in the interval [0,base-1]\ncorrespond to packets that have already been transmitted and acknowledged. The inter-\nval [base,nextseqnum-1] corresponds to packets that have been sent but not yet\nacknowledged. Sequence numbers in the interval [nextseqnum,base+N-1] can\n218\nCHAPTER 3\n•\nTRANSPORT LAYER\n\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n219\nFirst bit of first packet\ntransmitted, t = 0\nLast bit of first packet\ntransmitted, t = L/R\nFirst bit of first packet\ntransmitted, t = 0\nLast bit of first packet\ntransmitted, t = L/R\nACK arrives, send next packet,\nt = RTT + L/R\na. Stop-and-wait operation\nSender\nReceiver\nRTT\nFirst bit of first packet arrives\nLast bit of first packet arrives, send ACK\nFirst bit of first packet arrives\nLast bit of first packet arrives, send ACK\nACK arrives, send next packet,\nt = RTT + L/R\nb. Pipelined operation\nSender\nReceiver\nRTT\nLast bit of 2nd packet arrives, send ACK\nLast bit of 3rd packet arrives, send ACK\nFigure 3.18 \u0002 Stop-and-wait and pipelined sending\n\nbe used for packets that can be sent immediately, should data arrive from the upper\nlayer. Finally, sequence numbers greater than or equal to base+N cannot be used until\nan unacknowledged packet currently in the pipeline (specifically, the packet with\nsequence number base) has been acknowledged.\nAs suggested by Figure 3.19, the range of permissible sequence numbers for\ntransmitted but not yet acknowledged packets can be viewed as a window of size N\nover the range of sequence numbers. As the protocol operates, this window slides\nforward over the sequence number space. For this reason, N is often referred to as\nthe window size and the GBN protocol itself as a sliding-window protocol. You\nmight be wondering why we would even limit the number of outstanding, unac-\nknowledged packets to a value of N in the first place. Why not allow an unlimited\nnumber of such packets? We’ll see in Section 3.5 that flow control is one reason to\nimpose a limit on the sender. We’ll examine another reason to do so in Section 3.7,\nwhen we study TCP congestion control.\nIn practice, a packet’s sequence number is carried in a fixed-length field in the\npacket header. If k is the number of bits in the packet sequence number field, the range\nof sequence numbers is thus [0,2k– 1]. With a finite range of sequence numbers, all\narithmetic involving sequence numbers must then be done using modulo 2k arithmetic.\n(That is, the sequence number space can be thought of as a ring of size 2k, where\nsequence number 2k– 1 is immediately followed by sequence number 0.) Recall that\nrdt3.0 had a 1-bit sequence number and a range of sequence numbers of [0,1]. Sev-\neral of the problems at the end of this chapter explore the consequences of a finite range\nof sequence numbers. We will see in Section 3.5 that TCP has a 32-bit sequence number\nfield, where TCP sequence numbers count bytes in the byte stream rather than packets.\nFigures 3.20 and 3.21 give an extended FSM description of the sender and\nreceiver sides of an ACK-based, NAK-free, GBN protocol. We refer to this FSM\ndescription as an extended FSM because we have added variables (similar to pro-\ngramming-language variables) for base and nextseqnum, and added operations\non these variables and conditional actions involving these variables. Note that the\nextended FSM specification is now beginning to look somewhat like a programming-\nlanguage specification. [Bochman 1984] provides an excellent survey of additional\nextensions to FSM techniques as well as other programming-language-based tech-\nniques for specifying protocols.\n220\nCHAPTER 3\n•\nTRANSPORT LAYER\nbase\nnextseqnum\nWindow size\nN\nKey:\nAlready\nACK’d\nSent, not\nyet ACK’d\nUsable,\nnot yet sent\nNot usable\nFigure 3.19 \u0002 Sender’s view of sequence numbers in Go-Back-N\n\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n221\nrdt_send(data)\nif(nextseqnum<base+N){\n   sndpkt[nextseqnum]=make_pkt(nextseqnum,data,checksum)\n   udt_send(sndpkt[nextseqnum])\n   if(base==nextseqnum)\n      start_timer\n   nextseqnum++\n   }\nelse\n   refuse_data(data)\nΛ\nrdt_rcv(rcvpkt) && notcorrupt(rcvpkt)\nbase=getacknum(rcvpkt)+1\nIf(base==nextseqnum)\n   stop_timer\nelse\n   start_timer\nrdt_rcv(rcvpkt) && corrupt(rcvpkt)\nΛ\nbase=1\nnextseqnum=1\ntimeout\nstart_timer\nudt_send(sndpkt[base])\nudt_send(sndpkt[base+1])\n...\nudt_send(sndpkt[nextseqnum-1])\nWait\nFigure 3.20 \u0002 Extended FSM description of GBN sender\nrdt_rcv(rcvpkt)\n  && notcorrupt(rcvpkt)\n  && hasseqnum(rcvpkt,expectedseqnum)\nextract(rcvpkt,data)\ndeliver_data(data)\nsndpkt=make_pkt(expectedseqnum,ACK,checksum)\nudt_send(sndpkt)\nexpectedseqnum++\nΛ\nexpectedseqnum=1\nsndpkt=make_pkt(0,ACK,checksum)\ndefault\nudt_send(sndpkt)\nWait\nFigure 3.21 \u0002 Extended FSM description of GBN receiver\n\nThe GBN sender must respond to three types of events:\n•\nInvocation from above. When rdt_send() is called from above, the sender\nfirst checks to see if the window is full, that is, whether there are N outstanding,\nunacknowledged packets. If the window is not full, a packet is created and sent,\nand variables are appropriately updated. If the window is full, the sender simply\nreturns the data back to the upper layer, an implicit indication that the window is\nfull. The upper layer would presumably then have to try again later. In a real\nimplementation, the sender would more likely have either buffered (but not\nimmediately sent) this data, or would have a synchronization mechanism (for\nexample, a semaphore or a flag) that would allow the upper layer to call\nrdt_send() only when the window is not full.\n•\nReceipt of an ACK. In our GBN protocol, an acknowledgment for a packet with\nsequence number n will be taken to be a cumulative acknowledgment, indicat-\ning that all packets with a sequence number up to and including n have been cor-\nrectly received at the receiver. We’ll come back to this issue shortly when we\nexamine the receiver side of GBN.\n•\nA timeout event. The protocol’s name, “Go-Back-N,” is derived from the sender’s\nbehavior in the presence of lost or overly delayed packets. As in the stop-and-wait\nprotocol, a timer will again be used to recover from lost data or acknowledgment\npackets. If a timeout occurs, the sender resends all packets that have been previ-\nously sent but that have not yet been acknowledged. Our sender in Figure 3.20 uses\nonly a single timer, which can be thought of as a timer for the oldest transmitted but\nnot yet acknowledged packet. If an ACK is received but there are still additional\ntransmitted but not yet acknowledged packets, the timer is restarted. If there are no\noutstanding, unacknowledged packets, the timer is stopped.\nThe receiver’s actions in GBN are also simple. If a packet with sequence num-\nber n is received correctly and is in order (that is, the data last delivered to the upper\nlayer came from a packet with sequence number n – 1), the receiver sends an ACK\nfor packet n and delivers the data portion of the packet to the upper layer. In all other\ncases, the receiver discards the packet and resends an ACK for the most recently\nreceived in-order packet. Note that since packets are delivered one at a time to the\nupper layer, if packet k has been received and delivered, then all packets with a\nsequence number lower than k have also been delivered. Thus, the use of cumula-\ntive acknowledgments is a natural choice for GBN.\nIn our GBN protocol, the receiver discards out-of-order packets. Although it\nmay seem silly and wasteful to discard a correctly received (but out-of-order)\npacket, there is some justification for doing so. Recall that the receiver must deliver\ndata in order to the upper layer. Suppose now that packet n is expected, but packet\nn + 1 arrives. Because data must be delivered in order, the receiver could buffer\n(save) packet n + 1 and then deliver this packet to the upper layer after it had later\n222\nCHAPTER 3\n•\nTRANSPORT LAYER"
    },
    {
      "chunk_id": "e0b46b8b-50bb-480b-84e8-c2095e8a9973",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.4.4 Selective Repeat (SR)",
      "original_titles": [
        "3.4.4 Selective Repeat (SR)"
      ],
      "path": "Chapter 3 Transport Layer > 3.4 Principles of Reliable Data Transfer > 3.4.4 Selective Repeat (SR)",
      "start_page": 250,
      "end_page": 256,
      "token_count": 3333,
      "text": "received and delivered packet n. However, if packet n is lost, both it and packet\nn + 1 will eventually be retransmitted as a result of the GBN retransmission rule at\nthe sender. Thus, the receiver can simply discard packet n + 1. The advantage of this\napproach is the simplicity of receiver buffering—the receiver need not buffer any\nout-of-order packets. Thus, while the sender must maintain the upper and lower\nbounds of its window and the position of nextseqnum within this window, the\nonly piece of information the receiver need maintain is the sequence number of the\nnext in-order packet. This value is held in the variable expectedseqnum, shown\nin the receiver FSM in Figure 3.21. Of course, the disadvantage of throwing away a\ncorrectly received packet is that the subsequent retransmission of that packet might\nbe lost or garbled and thus even more retransmissions would be required.\nFigure 3.22 shows the operation of the GBN protocol for the case of a window\nsize of four packets. Because of this window size limitation, the sender sends pack-\nets 0 through 3 but then must wait for one or more of these packets to be acknowl-\nedged before proceeding. As each successive ACK (for example, ACK0 and ACK1)\nis received, the window slides forward and the sender can transmit one new packet\n(pkt4 and pkt5, respectively). On the receiver side, packet 2 is lost and thus packets\n3, 4, and 5 are found to be out of order and are discarded.\nBefore closing our discussion of GBN, it is worth noting that an implementa-\ntion of this protocol in a protocol stack would likely have a structure similar to that\nof the extended FSM in Figure 3.20. The implementation would also likely be in the\nform of various procedures that implement the actions to be taken in response to the\nvarious events that can occur. In such event-based programming, the various pro-\ncedures are called (invoked) either by other procedures in the protocol stack, or as\nthe result of an interrupt. In the sender, these events would be (1) a call from the\nupper-layer entity to invoke rdt_send(), (2) a timer interrupt, and (3) a call from\nthe lower layer to invoke rdt_rcv() when a packet arrives. The programming\nexercises at the end of this chapter will give you a chance to actually implement\nthese routines in a simulated, but realistic, network setting.\nWe note here that the GBN protocol incorporates almost all of the techniques\nthat we will encounter when we study the reliable data transfer components of TCP\nin Section 3.5. These techniques include the use of sequence numbers, cumulative\nacknowledgments, checksums, and a timeout/retransmit operation.\n3.4.4 Selective Repeat (SR)\nThe GBN protocol allows the sender to potentially “fill the pipeline” in Figure 3.17\nwith packets, thus avoiding the channel utilization problems we noted with stop-\nand-wait protocols. There are, however, scenarios in which GBN itself suffers from\nperformance problems. In particular, when the window size and bandwidth-delay\nproduct are both large, many packets can be in the pipeline. A single packet error\ncan thus cause GBN to retransmit a large number of packets, many unnecessarily.\nAs the probability of channel errors increases, the pipeline can become filled with\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n223\n\nthese unnecessary retransmissions. Imagine, in our message-dictation scenario, that\nif every time a word was garbled, the surrounding 1,000 words (for example, a win-\ndow size of 1,000 words) had to be repeated. The dictation would be slowed by all\nof the reiterated words.\nAs the name suggests, selective-repeat protocols avoid unnecessary retransmis-\nsions by having the sender retransmit only those packets that it suspects were\nreceived in error (that is, were lost or corrupted) at the receiver. This individual, as-\nneeded, retransmission will require that the receiver individually acknowledge cor-\nrectly received packets. A window size of N will again be used to limit the number\n224\nCHAPTER 3\n•\nTRANSPORT LAYER\nSender\nReceiver\n send pkt0\n send pkt1\n send pkt2\nsend pkt3\n  \n(wait)\n rcv ACK0\nsend pkt4\n rcv ACK1\nsend pkt5\nsend pkt2\nsend pkt3\nsend pkt4\nsend pkt5\npkt2 timeout\nrcv pkt0\nsend ACK0\nrcv pkt1\nsend ACK1\nrcv pkt3, discard\nsend ACK1\nrcv pkt4, discard\nsend ACK1\nrcv pkt5, discard\nsend ACK1\nrcv pkt2, deliver\nsend ACK2\nrcv pkt3, deliver\nsend ACK3\nX\n(loss)\nFigure 3.22 \u0002 Go-Back-N in operation\n\nof outstanding, unacknowledged packets in the pipeline. However, unlike GBN, the\nsender will have already received ACKs for some of the packets in the window.\nFigure 3.23 shows the SR sender’s view of the sequence number space. Figure 3.24\ndetails the various actions taken by the SR sender.\nThe SR receiver will acknowledge a correctly received packet whether or not it\nis in order. Out-of-order packets are buffered until any missing packets (that is,\npackets with lower sequence numbers) are received, at which point a batch of pack-\nets can be delivered in order to the upper layer. Figure 3.25 itemizes the various\nactions taken by the SR receiver. Figure 3.26 shows an example of SR operation in\nthe presence of lost packets. Note that in Figure 3.26, the receiver initially buffers\npackets 3, 4, and 5, and delivers them together with packet 2 to the upper layer when\npacket 2 is finally received.\nIt is important to note that in Step 2 in Figure 3.25, the receiver reacknowledges\n(rather than ignores) already received packets with certain sequence numbers below\nthe current window base. You should convince yourself that this reacknowledgment\nis indeed needed. Given the sender and receiver sequence number spaces in Figure\n3.23, for example, if there is no ACK for packet send_base propagating from the\nreceiver to the sender, the sender will eventually retransmit packet send_base,\neven though it is clear (to us, not the sender!) that the receiver has already received\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n225\nsend_base\nnextseqnum\nWindow size\nN\nKey:\nKey:\nAlready\nACK’d\nSent, not\nyet ACK’d\nUsable,\nnot yet sent\nNot usable\nOut of order\n(buffered) but\nalready ACK’d\nExpected, not\nyet received\nAcceptable\n(within\nwindow)\nNot usable\na. Sender view of sequence numbers\nb. Receiver view of sequence numbers\nrcv_base\nWindow size\nN\nFigure 3.23 \u0002 Selective-repeat (SR) sender and receiver views of\nsequence-number space\n\nthat packet. If the receiver were not to acknowledge this packet, the sender’s win-\ndow would never move forward! This example illustrates an important aspect of SR\nprotocols (and many other protocols as well). The sender and receiver will not\nalways have an identical view of what has been received correctly and what has not.\nFor SR protocols, this means that the sender and receiver windows will not always\ncoincide.\n226\nCHAPTER 3\n•\nTRANSPORT LAYER\n1. Packet with sequence number in [rcv_base, rcv_base+N-1] is cor-\nrectly received. In this case, the received packet falls within the receiver’s win-\ndow and a selective ACK packet is returned to the sender. If the packet was not\npreviously received, it is buffered. If this packet has a sequence number equal to\nthe base of the receive window (rcv_base in Figure 3.22), then this packet,\nand any previously buffered and consecutively numbered (beginning with\nrcv_base) packets are delivered to the upper layer. The receive window is\nthen moved forward by the number of packets delivered to the upper layer. As\nan example, consider Figure 3.26. When a packet with a sequence number of\nrcv_base=2 is received, it and packets 3, 4, and 5 can be delivered to the\nupper layer.\n2. Packet with sequence number in [rcv_base-N, rcv_base-1] is cor-\nrectly received. In this case, an ACK must be generated, even though this is a\npacket that the receiver has previously acknowledged.\n3. Otherwise. Ignore the packet.\nFigure 3.25 \u0002 SR receiver events and actions\n1. Data received from above. When data is received from above, the SR sender\nchecks the next available sequence number for the packet. If the sequence\nnumber is within the sender’s window, the data is packetized and sent; other-\nwise it is either buffered or returned to the upper layer for later transmission, \nas in GBN.\n2. Timeout. Timers are again used to protect against lost packets. However, each\npacket must now have its own logical timer, since only a single packet will \nbe transmitted on timeout. A single hardware timer can be used to mimic the\noperation of multiple logical timers [Varghese 1997].\n3. ACK received. If an ACK is received, the SR sender marks that packet as \nhaving been received, provided it is in the window. If the packet’s sequence\nnumber is equal to send_base, the window base is moved forward to the \nunacknowledged packet with the smallest sequence number. If the window\nmoves and there are untransmitted packets with sequence numbers that now\nfall within the window, these packets are transmitted.\nFigure 3.24 \u0002 SR sender events and actions\n\nThe lack of synchronization between sender and receiver windows has impor-\ntant consequences when we are faced with the reality of a finite range of sequence\nnumbers. Consider what could happen, for example, with a finite range of four packet\nsequence numbers, 0, 1, 2, 3, and a window size of three. Suppose packets 0 through\n2 are transmitted and correctly received and acknowledged at the receiver. At this\npoint, the receiver’s window is over the fourth, fifth, and sixth packets, which have\nsequence numbers 3, 0, and 1, respectively. Now consider two scenarios. In the first\nscenario, shown in Figure 3.27(a), the ACKs for the first three packets are lost and\npkt0 rcvd, delivered, ACK0 sent\n0 1 2 3 4 5 6 7 8 9\npkt1 rcvd, delivered, ACK1 sent\n0 1 2 3 4 5 6 7 8 9\npkt3 rcvd, buffered, ACK3 sent\n0 1 2 3 4 5 6 7 8 9\npkt4 rcvd, buffered, ACK4 sent\n0 1 2 3 4 5 6 7 8 9\npkt5 rcvd; buffered, ACK5 sent\n0 1 2 3 4 5 6 7 8 9\npkt2 rcvd, pkt2,pkt3,pkt4,pkt5\ndelivered, ACK2 sent\n0 1 2 3 4 5 6 7 8 9\npkt0 sent\n0 1 2 3 4 5 6 7 8 9\npkt1 sent\n0 1 2 3 4 5 6 7 8 9\npkt2 sent\n0 1 2 3 4 5 6 7 8 9\npkt3 sent, window full\n0 1 2 3 4 5 6 7 8 9\nACK0 rcvd, pkt4 sent\n0 1 2 3 4 5 6 7 8 9\nACK1 rcvd, pkt5 sent\n0 1 2 3 4 5 6 7 8 9\npkt2 TIMEOUT, pkt2\nresent\n0 1 2 3 4 5 6 7 8 9\nACK3 rcvd, nothing sent\n0 1 2 3 4 5 6 7 8 9\nX\n(loss)\nSender\nReceiver\nFigure 3.26 \u0002 SR operation\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n227\n\n228\nCHAPTER 3\n•\nTRANSPORT LAYER\npkt0\ntimeout\nretransmit pkt0\n0 1 2 3 0 1 2\npkt0\npkt1\npkt2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\nACK0\nACK1\nACK2\nx\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\nSender window\n(after receipt)\na.\nb.\nReceiver window\n(after receipt)\nreceive packet\nwith seq number 0\n0 1 2 3 0 1 2\npkt0\npkt1\npkt2\npkt3\npkt0\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\nACK0\nACK1\nACK2\n0 1 2 3 0 1 2\n0 1 2 3 0 1 2\nSender window\n(after receipt)\nReceiver window\n(after receipt)\nreceive packet\nwith seq number 0\n0 1 2 3 0 1 2\nx\nx\nx\nFigure 3.27 \u0002 SR receiver dilemma with too-large windows: A new packet\nor a retransmission?\n\nthe sender retransmits these packets. The receiver thus next receives a packet with\nsequence number 0—a copy of the first packet sent.\nIn the second scenario, shown in Figure 3.27(b), the ACKs for the first three\npackets are all delivered correctly. The sender thus moves its window forward and\nsends the fourth, fifth, and sixth packets, with sequence numbers 3, 0, and 1, respec-\ntively. The packet with sequence number 3 is lost, but the packet with sequence\nnumber 0 arrives—a packet containing new data.\nNow consider the receiver’s viewpoint in Figure 3.27, which has a figurative\ncurtain between the sender and the receiver, since the receiver cannot “see” the\nactions taken by the sender. All the receiver observes is the sequence of messages it\nreceives from the channel and sends into the channel. As far as it is concerned, the\ntwo scenarios in Figure 3.27 are identical. There is no way of distinguishing the\nretransmission of the first packet from an original transmission of the fifth packet.\nClearly, a window size that is 1 less than the size of the sequence number space\nwon’t work. But how small must the window size be? A problem at the end of the\nchapter asks you to show that the window size must be less than or equal to half the\nsize of the sequence number space for SR protocols.\nAt the companion Web site, you will find an applet that animates the operation\nof the SR protocol. Try performing the same experiments that you did with the GBN\napplet. Do the results agree with what you expect?\nThis completes our discussion of reliable data transfer protocols. We’ve covered\na lot of ground and introduced numerous mechanisms that together provide for reli-\nable data transfer. Table 3.1 summarizes these mechanisms. Now that we have seen all\nof these mechanisms in operation and can see the “big picture,” we encourage you to\nreview this section again to see how these mechanisms were incrementally added to\ncover increasingly complex (and realistic) models of the channel connecting the\nsender and receiver, or to improve the performance of the protocols.\nLet’s conclude our discussion of reliable data transfer protocols by considering\none remaining assumption in our underlying channel model. Recall that we have\nassumed that packets cannot be reordered within the channel between the sender and\nreceiver. This is generally a reasonable assumption when the sender and receiver are\nconnected by a single physical wire. However, when the “channel” connecting the two\nis a network, packet reordering can occur. One manifestation of packet reordering is\nthat old copies of a packet with a sequence or acknowledgment number of x can\nappear, even though neither the sender’s nor the receiver’s window contains x. With\npacket reordering, the channel can be thought of as essentially buffering packets and\nspontaneously emitting these packets at any point in the future. Because sequence\nnumbers may be reused, some care must be taken to guard against such duplicate\npackets. The approach taken in practice is to ensure that a sequence number is not\nreused until the sender is “sure” that any previously sent packets with sequence num-\nber x are no longer in the network. This is done by assuming that a packet cannot\n“live” in the network for longer than some fixed maximum amount of time. A maxi-\nmum packet lifetime of approximately three minutes is assumed in the TCP extensions\n3.4\n•\nPRINCIPLES OF RELIABLE DATA TRANSFER\n229"
    },
    {
      "chunk_id": "de648406-097e-4bff-ac45-c5deccaa314f",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.5 Connection-Oriented Transport: TCP",
      "original_titles": [
        "3.5 Connection-Oriented Transport: TCP"
      ],
      "path": "Chapter 3 Transport Layer > 3.5 Connection-Oriented Transport: TCP",
      "start_page": 257,
      "end_page": 257,
      "token_count": 483,
      "text": "230\nCHAPTER 3\n•\nTRANSPORT LAYER\nTable 3.1 \u0002 Summary of reliable data transfer mechanisms and their use\nMechanism\nUse, Comments\nChecksum\nUsed to detect bit errors in a transmitted packet.\nTimer\nUsed to timeout/retransmit a packet, possibly because the packet (or its ACK) was\nlost within the channel. Because timeouts can occur when a packet is delayed but\nnot lost (premature timeout), or when a packet has been received by the receiver\nbut the receiver-to-sender ACK has been lost, duplicate copies of a packet may be\nreceived by a receiver.\nSequence number\nUsed for sequential numbering of packets of data flowing from sender to receiver.\nGaps in the sequence numbers of received packets allow the receiver to detect a\nlost packet. Packets with duplicate sequence numbers allow the receiver to detect\nduplicate copies of a packet.\nAcknowledgment\nUsed by the receiver to tell the sender that a packet or set of packets has been\nreceived correctly. Acknowledgments will typically carry the sequence number of the\npacket or packets being acknowledged. Acknowledgments may be individual or\ncumulative, depending on the protocol.\nNegative acknowledgment\nUsed by the receiver to tell the sender that a packet has not been received correct-\nly. Negative acknowledgments will typically carry the sequence number of the pack-\net that was not received correctly.\nWindow, pipelining\nThe sender may be restricted to sending only packets with sequence numbers that\nfall within a given range. By allowing multiple packets to be transmitted but not yet\nacknowledged, sender utilization can be increased over a stop-and-wait mode of\noperation. We’ll see shortly that the window size may be set on the basis of the\nreceiver’s ability to receive and buffer messages, or the level of congestion in the\nnetwork, or both.\nfor high-speed networks [RFC 1323]. [Sunshine 1978] describes a method for using\nsequence numbers such that reordering problems can be completely avoided.\n3.5 Connection-Oriented Transport: TCP\nNow that we have covered the underlying principles of reliable data transfer, let’s\nturn to TCP—the Internet’s transport-layer, connection-oriented, reliable transport\nprotocol. In this section, we’ll see that in order to provide reliable data transfer, TCP\nrelies on many of the underlying principles discussed in the previous section,\nincluding error detection, retransmissions, cumulative acknowledgments, timers,"
    },
    {
      "chunk_id": "a4b5404e-3660-47ed-b894-04598cb48f5a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.5.1 The TCP Connection",
      "original_titles": [
        "3.5.1 The TCP Connection"
      ],
      "path": "Chapter 3 Transport Layer > 3.5 Connection-Oriented Transport: TCP > 3.5.1 The TCP Connection",
      "start_page": 258,
      "end_page": 259,
      "token_count": 1244,
      "text": "and header fields for sequence and acknowledgment numbers. TCP is defined in\nRFC 793, RFC 1122, RFC 1323, RFC 2018, and RFC 2581.\n3.5.1 The TCP Connection\nTCP is said to be connection-oriented because before one application process can\nbegin to send data to another, the two processes must first “handshake” with each\nother—that is, they must send some preliminary segments to each other to establish the\nparameters of the ensuing data transfer. As part of TCP connection establishment, both\nsides of the connection will initialize many TCP state variables (many of which will be\ndiscussed in this section and in Section 3.7) associated with the TCP connection.\nThe TCP “connection” is not an end-to-end TDM or FDM circuit as in a circuit-\nswitched network. Nor is it a virtual circuit (see Chapter 1), as the connection state\nresides entirely in the two end systems. Because the TCP protocol runs only in the\nend systems and not in the intermediate network elements (routers and link-layer\nswitches), the intermediate network elements do not maintain TCP connection state.\nVINTON CERF, ROBERT KAHN, AND TCP/IP\nIn the early 1970s, packet-switched networks began to proliferate, with the\nARPAnet—the precursor of the Internet—being just one of many networks. Each of\nthese networks had its own protocol. Two researchers, Vinton Cerf and Robert Kahn,\nrecognized the importance of interconnecting these networks and invented a cross-\nnetwork protocol called TCP/IP, which stands for Transmission Control\nProtocol/Internet Protocol. Although Cerf and Kahn began by seeing the protocol as\na single entity, it was later split into its two parts, TCP and IP, which operated sepa-\nrately. Cerf and Kahn published a paper on TCP/IP in May 1974 in IEEE\nTransactions on Communications Technology [Cerf 1974].\nThe TCP/IP protocol, which is the bread and butter of today’s Internet, was devised\nbefore PCs, workstations, smartphones, and tablets, before the proliferation of Ethernet,\ncable, and DSL, WiFi, and other access network technologies, and before the Web,\nsocial media, and streaming video. Cerf and Kahn saw the need for a networking pro-\ntocol that, on the one hand, provides broad support for yet-to-be-defined applications\nand, on the other hand, allows arbitrary hosts and link-layer protocols to interoperate.\nIn 2004, Cerf and Kahn received the ACM’s Turing Award, considered the\n“Nobel Prize of Computing” for “pioneering work on internetworking, including the\ndesign and implementation of the Internet’s basic communications protocols, TCP/IP,\nand for inspired leadership in networking.”\nCASE HISTORY\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n231\n\n232\nCHAPTER 3\n•\nTRANSPORT LAYER\nIn fact, the intermediate routers are completely oblivious to TCP connections; they\nsee datagrams, not connections.\nA TCP connection provides a full-duplex service: If there is a TCP connection\nbetween Process A on one host and Process B on another host, then application-\nlayer data can flow from Process A to Process B at the same time as application-\nlayer data flows from Process B to Process A. A TCP connection is also always\npoint-to-point, that is, between a single sender and a single receiver. So-called\n“multicasting” (see Section 4.7)—the transfer of data from one sender to many\nreceivers in a single send operation—is not possible with TCP. With TCP, two hosts\nare company and three are a crowd!\nLet’s now take a look at how a TCP connection is established. Suppose a\nprocess running in one host wants to initiate a connection with another process in\nanother host. Recall that the process that is initiating the connection is called the\nclient process, while the other process is called the server process. The client appli-\ncation process first informs the client transport layer that it wants to establish a\nconnection to a process in the server. Recall from Section 2.7.2, a Python client pro-\ngram does this by issuing the command\nclientSocket.connect((serverName,serverPort))\nwhere serverName is the name of the server and serverPort identifies the\nprocess on the server. TCP in the client then proceeds to establish a TCP connection\nwith TCP in the server. At the end of this section we discuss in some detail the con-\nnection-establishment procedure. For now it suffices to know that the client first sends\na special TCP segment; the server responds with a second special TCP segment; and\nfinally the client responds again with a third special segment. The first two segments\ncarry no payload, that is, no application-layer data; the third of these segments may\ncarry a payload. Because three segments are sent between the two hosts, this connec-\ntion-establishment procedure is often referred to as a three-way handshake.\nOnce a TCP connection is established, the two application processes can send\ndata to each other. Let’s consider the sending of data from the client process to the\nserver process. The client process passes a stream of data through the socket (the\ndoor of the process), as described in Section 2.7. Once the data passes through\nthe door, the data is in the hands of TCP running in the client. As shown in Figure\n3.28, TCP directs this data to the connection’s send buffer, which is one of the\nbuffers that is set aside during the initial three-way handshake. From time to time,\nTCP will grab chunks of data from the send buffer and pass the data to the network\nlayer. Interestingly, the TCP specification [RFC 793] is very laid back about speci-\nfying when TCP should actually send buffered data, stating that TCP should “send\nthat data in segments at its own convenience.” The maximum amount of data that\ncan be grabbed and placed in a segment is limited by the maximum segment size\n(MSS). The MSS is typically set by first determining the length of the largest \nlink-layer frame that can be sent by the local sending host (the so-called maximum"
    },
    {
      "chunk_id": "08ba9cd2-b562-4909-b081-6e6ef9e377ff",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.5.2 TCP Segment Structure",
      "original_titles": [
        "3.5.2 TCP Segment Structure"
      ],
      "path": "Chapter 3 Transport Layer > 3.5 Connection-Oriented Transport: TCP > 3.5.2 TCP Segment Structure",
      "start_page": 260,
      "end_page": 264,
      "token_count": 2935,
      "text": "3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n233\ntransmission unit, MTU), and then setting the MSS to ensure that a TCP segment\n(when encapsulated in an IP datagram) plus the TCP/IP header length (typically 40\nbytes) will fit into a single link-layer frame. Both Ethernet and PPP link-layer proto-\ncols have an MSS of 1,500 bytes. Approaches have also been proposed for discov-\nering the path MTU—the largest link-layer frame that can be sent on all links from\nsource to destination [RFC 1191]—and setting the MSS based on the path MTU\nvalue. Note that the MSS is the maximum amount of application-layer data in the\nsegment, not the maximum size of the TCP segment including headers. (This termi-\nnology is confusing, but we have to live with it, as it is well entrenched.)\nTCP pairs each chunk of client data with a TCP header, thereby forming TCP\nsegments. The segments are passed down to the network layer, where they are sepa-\nrately encapsulated within network-layer IP datagrams. The IP datagrams are then\nsent into the network. When TCP receives a segment at the other end, the segment’s\ndata is placed in the TCP connection’s receive buffer, as shown in Figure 3.28. The\napplication reads the stream of data from this buffer. Each side of the connection has\nits own send buffer and its own receive buffer. (You can see the online flow-control\napplet at http://www.awl.com/kurose-ross, which provides an animation of the send\nand receive buffers.)\nWe see from this discussion that a TCP connection consists of buffers, vari-\nables, and a socket connection to a process in one host, and another set of buffers,\nvariables, and a socket connection to a process in another host. As mentioned ear-\nlier, no buffers or variables are allocated to the connection in the network elements\n(routers, switches, and repeaters) between the hosts.\n3.5.2 TCP Segment Structure\nHaving taken a brief look at the TCP connection, let’s examine the TCP segment\nstructure. The TCP segment consists of header fields and a data field. The data\nfield contains a chunk of application data. As mentioned above, the MSS limits the\nProcess\nwrites data\nProcess\nreads data\nTCP\nsend\nbuffer\nSocket\nTCP\nreceive\nbuffer\nSocket\nSegment\nSegment\nFigure 3.28 \u0002 TCP send and receive buffers\n\n234\nCHAPTER 3\n•\nTRANSPORT LAYER\nmaximum size of a segment’s data field. When TCP sends a large file, such as an\nimage as part of a Web page, it typically breaks the file into chunks of size MSS\n(except for the last chunk, which will often be less than the MSS). Interactive appli-\ncations, however, often transmit data chunks that are smaller than the MSS; for\nexample, with remote login applications like Telnet, the data field in the TCP seg-\nment is often only one byte. Because the TCP header is typically 20 bytes (12 bytes\nmore than the UDP header), segments sent by Telnet may be only 21 bytes in length.\nFigure 3.29 shows the structure of the TCP segment. As with UDP, the header\nincludes\nsource and destination port numbers, which are used for\nmultiplexing/demultiplexing data from/to upper-layer applications. Also, as with\nUDP, the header includes a checksum field. A TCP segment header also contains\nthe following fields:\n•\nThe 32-bit sequence number field and the 32-bit acknowledgment number\nfield are used by the TCP sender and receiver in implementing a reliable data\ntransfer service, as discussed below.\n•\nThe 16-bit receive window field is used for flow control. We will see shortly that\nit is used to indicate the number of bytes that a receiver is willing to accept.\n•\nThe 4-bit header length field specifies the length of the TCP header in 32-bit\nwords. The TCP header can be of variable length due to the TCP options field.\nSource port #\nInternet checksum\nHeader\nlength\nUnused\nURG\nACK\nPSH\nRST\nSYN\nFIN\n32 bits\nDest port #\nReceive window\nUrgent data pointer\nSequence number\nAcknowledgment number\nOptions\nData\nFigure 3.29 \u0002 TCP segment structure\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n235\n(Typically, the options field is empty, so that the length of the typical TCP header\nis 20 bytes.)\n•\nThe optional and variable-length options field is used when a sender and\nreceiver negotiate the maximum segment size (MSS) or as a window scaling fac-\ntor for use in high-speed networks. A time-stamping option is also defined. See\nRFC 854 and RFC 1323 for additional details.\n•\nThe flag field contains 6 bits. The ACK bit is used to indicate that the value car-\nried in the acknowledgment field is valid; that is, the segment contains an\nacknowledgment for a segment that has been successfully received. The RST,\nSYN, and FIN bits are used for connection setup and teardown, as we will dis-\ncuss at the end of this section. Setting the PSH bit indicates that the receiver\nshould pass the data to the upper layer immediately. Finally, the URG bit is used\nto indicate that there is data in this segment that the sending-side upper-layer\nentity has marked as “urgent.” The location of the last byte of this urgent data is\nindicated by the 16-bit urgent data pointer field. TCP must inform the receiv-\ning-side upper-layer entity when urgent data exists and pass it a pointer to the\nend of the urgent data. (In practice, the PSH, URG, and the urgent data pointer\nare not used. However, we mention these fields for completeness.)\nSequence Numbers and Acknowledgment Numbers\nTwo of the most important fields in the TCP segment header are the sequence number\nfield and the acknowledgment number field. These fields are a critical part of TCP’s\nreliable data transfer service. But before discussing how these fields are used to provide\nreliable data transfer, let us first explain what exactly TCP puts in these fields.\nTCP views data as an unstructured, but ordered, stream of bytes. TCP’s use of\nsequence numbers reflects this view in that sequence numbers are over the stream of\ntransmitted bytes and not over the series of transmitted segments. The sequence\nnumber for a segment is therefore the byte-stream number of the first byte in the\nsegment. Let’s look at an example. Suppose that a process in Host A wants to send a\nstream of data to a process in Host B over a TCP connection. The TCP in Host A will\nimplicitly number each byte in the data stream. Suppose that the data stream consists\nof a file consisting of 500,000 bytes, that the MSS is 1,000 bytes, and that the first\nbyte of the data stream is numbered 0. As shown in Figure 3.30, TCP constructs 500\nsegments out of the data stream. The first segment gets assigned sequence number 0,\nthe second segment gets assigned sequence number 1,000, the third segment gets\nassigned sequence number 2,000, and so on. Each sequence number is inserted in the\nsequence number field in the header of the appropriate TCP segment.\nNow let’s consider acknowledgment numbers. These are a little trickier than\nsequence numbers. Recall that TCP is full-duplex, so that Host A may be receiving\ndata from Host B while it sends data to Host B (as part of the same TCP connection).\nEach of the segments that arrive from Host B has a sequence number for the data\n\n236\nCHAPTER 3\n•\nTRANSPORT LAYER\nflowing from B to A. The acknowledgment number that Host A puts in its segment\nis the sequence number of the next byte Host A is expecting from Host B. It is good\nto look at a few examples to understand what is going on here. Suppose that Host A\nhas received all bytes numbered 0 through 535 from B and suppose that it is about\nto send a segment to Host B. Host A is waiting for byte 536 and all the subsequent\nbytes in Host B’s data stream. So Host A puts 536 in the acknowledgment number\nfield of the segment it sends to B.\nAs another example, suppose that Host A has received one segment from Host\nB containing bytes 0 through 535 and another segment containing bytes 900 through\n1,000. For some reason Host A has not yet received bytes 536 through 899. In this\nexample, Host A is still waiting for byte 536 (and beyond) in order to re-create B’s\ndata stream. Thus, A’s next segment to B will contain 536 in the acknowledgment\nnumber field. Because TCP only acknowledges bytes up to the first missing byte in\nthe stream, TCP is said to provide cumulative acknowledgments.\nThis last example also brings up an important but subtle issue. Host A received\nthe third segment (bytes 900 through 1,000) before receiving the second segment\n(bytes 536 through 899). Thus, the third segment arrived out of order. The subtle\nissue is: What does a host do when it receives out-of-order segments in a TCP con-\nnection? Interestingly, the TCP RFCs do not impose any rules here and leave the\ndecision up to the people programming a TCP implementation. There are basically\ntwo choices: either (1) the receiver immediately discards out-of-order segments\n(which, as we discussed earlier, can simplify receiver design), or (2) the receiver\nkeeps the out-of-order bytes and waits for the missing bytes to fill in the gaps.\nClearly, the latter choice is more efficient in terms of network bandwidth, and is the\napproach taken in practice.\nIn Figure 3.30, we assumed that the initial sequence number was zero. In truth,\nboth sides of a TCP connection randomly choose an initial sequence number. This is\ndone to minimize the possibility that a segment that is still present in the network\nfrom an earlier, already-terminated connection between two hosts is mistaken for a\nvalid segment in a later connection between these same two hosts (which also hap-\npen to be using the same port numbers as the old connection) [Sunshine 1978].\n0\n1\n1,000\n1,999\n499,999\nFile\nData for 1st segment\nData for 2nd segment\nFigure 3.30 \u0002 Dividing file data into TCP segments\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n237\nTelnet: A Case Study for Sequence and Acknowledgment Numbers\nTelnet, defined in RFC 854, is a popular application-layer protocol used for\nremote login. It runs over TCP and is designed to work between any pair of hosts.\nUnlike the bulk data transfer applications discussed in Chapter 2, Telnet is an\ninteractive application. We discuss a Telnet example here, as it nicely illustrates\nTCP sequence and acknowledgment numbers. We note that many users now \nprefer to use the SSH protocol rather than Telnet, since data sent in a Telnet con-\nnection (including passwords!) is not encrypted, making Telnet vulnerable to\neavesdropping attacks (as discussed in Section 8.7).\nSuppose Host A initiates a Telnet session with Host B. Because Host A initiates\nthe session, it is labeled the client, and Host B is labeled the server. Each character\ntyped by the user (at the client) will be sent to the remote host; the remote host will\nsend back a copy of each character, which will be displayed on the Telnet user’s\nscreen. This “echo back” is used to ensure that characters seen by the Telnet user\nhave already been received and processed at the remote site. Each character thus\ntraverses the network twice between the time the user hits the key and the time the\ncharacter is displayed on the user’s monitor.\nNow suppose the user types a single letter, ‘C,’and then grabs a coffee. Let’s exam-\nine the TCP segments that are sent between the client and server. As shown in Figure\n3.31, we suppose the starting sequence numbers are 42 and 79 for the client and server,\nrespectively. Recall that the sequence number of a segment is the sequence number of\nthe first byte in the data field. Thus, the first segment sent from the client will have\nsequence number 42; the first segment sent from the server will have sequence number\n79. Recall that the acknowledgment number is the sequence number of the next byte of\ndata that the host is waiting for. After the TCP connection is established but before any\ndata is sent, the client is waiting for byte 79 and the server is waiting for byte 42.\nAs shown in Figure 3.31, three segments are sent. The first segment is sent from\nthe client to the server, containing the 1-byte ASCII representation of the letter ‘C’\nin its data field. This first segment also has 42 in its sequence number field, as we\njust described. Also, because the client has not yet received any data from the server,\nthis first segment will have 79 in its acknowledgment number field.\nThe second segment is sent from the server to the client. It serves a dual pur-\npose. First it provides an acknowledgment of the data the server has received. By\nputting 43 in the acknowledgment field, the server is telling the client that it has suc-\ncessfully received everything up through byte 42 and is now waiting for bytes 43\nonward. The second purpose of this segment is to echo back the letter ‘C.’ Thus, the\nsecond segment has the ASCII representation of ‘C’ in its data field. This second\nsegment has the sequence number 79, the initial sequence number of the server-to-\nclient data flow of this TCP connection, as this is the very first byte of data that the\nserver is sending. Note that the acknowledgment for client-to-server data is carried\nin a segment carrying server-to-client data; this acknowledgment is said to be\npiggybacked on the server-to-client data segment."
    },
    {
      "chunk_id": "c106d992-f9b8-4a9f-8e84-3229f1b1b276",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.5.3 Round-Trip Time Estimation and Timeout",
      "original_titles": [
        "3.5.3 Round-Trip Time Estimation and Timeout"
      ],
      "path": "Chapter 3 Transport Layer > 3.5 Connection-Oriented Transport: TCP > 3.5.3 Round-Trip Time Estimation and Timeout",
      "start_page": 265,
      "end_page": 268,
      "token_count": 1822,
      "text": "238\nCHAPTER 3\n•\nTRANSPORT LAYER\nThe third segment is sent from the client to the server. Its sole purpose is to\nacknowledge the data it has received from the server. (Recall that the second seg-\nment contained data—the letter ‘C’—from the server to the client.) This segment\nhas an empty data field (that is, the acknowledgment is not being piggybacked with\nany client-to-server data). The segment has 80 in the acknowledgment number field\nbecause the client has received the stream of bytes up through byte sequence num-\nber 79 and it is now waiting for bytes 80 onward. You might think it odd that this\nsegment also has a sequence number since the segment contains no data. But\nbecause TCP has a sequence number field, the segment needs to have some\nsequence number.\n3.5.3 Round-Trip Time Estimation and Timeout\nTCP, like our rdt protocol in Section 3.4, uses a timeout/retransmit mechanism to\nrecover from lost segments. Although this is conceptually simple, many subtle\nissues arise when we implement a timeout/retransmit mechanism in an actual proto-\ncol such as TCP. Perhaps the most obvious question is the length of the timeout\nTime\nTime\nHost A\nHost B\nUser types\n'C'\nSeq=42, ACK=79, data='C'\nSeq=79, ACK=43, data='C'\nSeq=43, ACK=80\nHost ACKs\nreceipt of 'C',\nechoes back 'C'\nHost ACKs\nreceipt of\nechoed 'C'\nFigure 3.31 \u0002 Sequence and acknowledgment numbers for a simple \nTelnet application over TCP\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n239\nintervals. Clearly, the timeout should be larger than the connection’s round-trip time\n(RTT), that is, the time from when a segment is sent until it is acknowledged. Other-\nwise, unnecessary retransmissions would be sent. But how much larger? How\nshould the RTT be estimated in the first place? Should a timer be associated with\neach and every unacknowledged segment? So many questions! Our discussion in\nthis section is based on the TCP work in [Jacobson 1988] and the current IETF rec-\nommendations for managing TCP timers [RFC 6298].\nEstimating the Round-Trip Time\nLet’s begin our study of TCP timer management by considering how TCP estimates\nthe round-trip time between sender and receiver. This is accomplished as follows.\nThe sample RTT, denoted SampleRTT, for a segment is the amount of time\nbetween when the segment is sent (that is, passed to IP) and when an acknowledg-\nment for the segment is received. Instead of measuring a SampleRTT for every\ntransmitted segment, most TCP implementations take only one SampleRTT meas-\nurement at a time. That is, at any point in time, the SampleRTT is being estimated\nfor only one of the transmitted but currently unacknowledged segments, leading to a\nnew value of SampleRTT approximately once every RTT. Also, TCP never com-\nputes a SampleRTT for a segment that has been retransmitted; it only measures\nSampleRTT for segments that have been transmitted once [Karn 1987]. (A prob-\nlem at the end of the chapter asks you to consider why.)\nObviously, the SampleRTT values will fluctuate from segment to segment due\nto congestion in the routers and to the varying load on the end systems. Because of\nthis fluctuation, any given SampleRTT value may be atypical. In order to estimate\na typical RTT, it is therefore natural to take some sort of average of the Sam-\npleRTT values. TCP maintains an average, called EstimatedRTT, of the Sam-\npleRTT\nvalues. Upon obtaining a new SampleRTT, TCP\nupdates\nEstimatedRTT according to the following formula:\nEstimatedRTT = (1 – \u0003) • EstimatedRTT + \u0003 • SampleRTT\nThe formula above is written in the form of a programming-language statement—\nthe new value of EstimatedRTT is a weighted combination of the previous value\nof EstimatedRTT and the new value for SampleRTT. The recommended value\nof \u0003 is \u0003 = 0.125 (that is, 1/8) [RFC 6298], in which case the formula above\nbecomes:\nEstimatedRTT = 0.875 • EstimatedRTT + 0.125 • SampleRTT\nNote that EstimatedRTT is a weighted average of the SampleRTT values.\nAs discussed in a homework problem at the end of this chapter, this weighted aver-\nage puts more weight on recent samples than on old samples. This is natural, as the\n\n240\nCHAPTER 3\n•\nTRANSPORT LAYER\nmore recent samples better reflect the current congestion in the network. In statis-\ntics, such an average is called an exponential weighted moving average (EWMA).\nThe word “exponential” appears in EWMA because the weight of a given Sam-\npleRTT decays exponentially fast as the updates proceed. In the homework prob-\nlems you will be asked to derive the exponential term in EstimatedRTT.\nFigure 3.32 shows the SampleRTT values and EstimatedRTT for a value of \u0003\n= 1/8 for a TCP connection between gaia.cs.umass.edu (in Amherst, Massachu-\nsetts) to fantasia.eurecom.fr (in the south of France). Clearly, the variations in\nthe SampleRTT are smoothed out in the computation of the EstimatedRTT.\nIn addition to having an estimate of the RTT, it is also valuable to have a\nmeasure of the variability of the RTT. [RFC 6298] defines the RTT variation,\nDevRTT, as an estimate of how much SampleRTT typically deviates from\nEstimatedRTT:\nDevRTT = (1 – \u0004) • DevRTT + \u0004•| SampleRTT – EstimatedRTT |\nNote that DevRTT is an EWMA of the difference between SampleRTT and\nEstimatedRTT. If the SampleRTT values have little fluctuation, then DevRTT\nwill be small; on the other hand, if there is a lot of fluctuation, DevRTT will be\nlarge. The recommended value of β is 0.25.\nTCP provides reliable data transfer by using positive acknowledgments and timers in much\nthe same way that we studied in Section 3.4. TCP acknowledges data that has been\nreceived correctly, and it then retransmits segments when segments or their corresponding\nacknowledgments are thought to be lost or corrupted. Certain versions of TCP also have an\nimplicit NAK mechanism—with TCP’s fast retransmit mechanism, the receipt of three dupli-\ncate ACKs for a given segment serves as an implicit NAK for the following segment, trig-\ngering retransmission of that segment before timeout. TCP uses sequences of numbers to\nallow the receiver to identify lost or duplicate segments. Just as in the case of our reliable\ndata transfer protocol, rdt3.0, TCP cannot itself tell for certain if a segment, or its\nACK, is lost, corrupted, or overly delayed. At the sender, TCP’s response will be the same:\nretransmit the segment in question.\nTCP also uses pipelining, allowing the sender to have multiple transmitted but yet-to-be-\nacknowledged segments outstanding at any given time. We saw earlier that pipelining\ncan greatly improve a session’s throughput when the ratio of the segment size to round-\ntrip delay is small. The specific number of outstanding, unacknowledged segments that a\nsender can have is determined by TCP’s flow-control and congestion-control mechanisms.\nTCP flow control is discussed at the end of this section; TCP congestion control is dis-\ncussed in Section 3.7. For the time being, we must simply be aware that the TCP sender\nuses pipelining.\nPRINCIPLES IN PRACTICE\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n241\nSetting and Managing the Retransmission Timeout Interval\nGiven values of EstimatedRTT and DevRTT, what value should be used for\nTCP’s timeout interval? Clearly, the interval should be greater than or equal to\nEstimatedRTT, or unnecessary retransmissions would be sent. But the timeout\ninterval should not be too much larger than EstimatedRTT; otherwise, when a seg-\nment is lost, TCP would not quickly retransmit the segment, leading to large data trans-\nfer delays. It is therefore desirable to set the timeout equal to the EstimatedRTT plus\nsome margin. The margin should be large when there is a lot of fluctuation in the\nSampleRTT values; it should be small when there is little fluctuation. The value of\nDevRTT should thus come into play here. All of these considerations are taken into\naccount in TCP’s method for determining the retransmission timeout interval:\nTimeoutInterval = EstimatedRTT + 4 • DevRTT\nAn initial TimeoutInterval value of 1 second is recommended [RFC 6298].\nAlso, when a timeout occurs, the value of TimeoutInterval is doubled to avoid\na premature timeout occurring for a subsequent segment that will soon be acknowl-\nedged. However, as soon as a segment is received and EstimatedRTT is updated,\nthe TimeoutInterval is again computed using the formula above.\nRTT (milliseconds)\n150\n200\n250\n300\n350\n100\n1\n8\n15\n22\n29\n36\n43\n50\nTime (seconds)\nSample RTT\n57\n64\n71\n78\n85\n92\n99\n106\nEstimated RTT\nFigure 3.32 \u0002 RTT samples and RTT estimates"
    },
    {
      "chunk_id": "7c8c298d-50dc-4344-9268-3c85803a3fda",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.5.4 Reliable Data Transfer",
      "original_titles": [
        "3.5.4 Reliable Data Transfer"
      ],
      "path": "Chapter 3 Transport Layer > 3.5 Connection-Oriented Transport: TCP > 3.5.4 Reliable Data Transfer",
      "start_page": 269,
      "end_page": 276,
      "token_count": 3270,
      "text": "242\nCHAPTER 3\n•\nTRANSPORT LAYER\n3.5.4 Reliable Data Transfer\nRecall that the Internet’s network-layer service (IP service) is unreliable. IP does\nnot guarantee datagram delivery, does not guarantee in-order delivery of data-\ngrams, and does not guarantee the integrity of the data in the datagrams. With IP\nservice, datagrams can overflow router buffers and never reach their destination,\ndatagrams can arrive out of order, and bits in the datagram can get corrupted\n(flipped from 0 to 1 and vice versa). Because transport-layer segments are carried\nacross the network by IP datagrams, transport-layer segments can suffer from these\nproblems as well.\nTCP creates a reliable data transfer service on top of IP’s unreliable best-\neffort service. TCP’s reliable data transfer service ensures that the data stream that a\nprocess reads out of its TCP receive buffer is uncorrupted, without gaps, without\nduplication, and in sequence; that is, the byte stream is exactly the same byte stream\nthat was sent by the end system on the other side of the connection. How TCP pro-\nvides a reliable data transfer involves many of the principles that we studied in\nSection 3.4.\nIn our earlier development of reliable data transfer techniques, it was conceptu-\nally easiest to assume that an individual timer is associated with each transmitted\nbut not yet acknowledged segment. While this is great in theory, timer management\ncan require considerable overhead. Thus, the recommended TCP timer management\nprocedures [RFC 6298] use only a single retransmission timer, even if there are mul-\ntiple transmitted but not yet acknowledged segments. The TCP protocol described\nin this section follows this single-timer recommendation.\nWe will discuss how TCP provides reliable data transfer in two incremental\nsteps. We first present a highly simplified description of a TCP sender that uses only\ntimeouts to recover from lost segments; we then present a more complete descrip-\ntion that uses duplicate acknowledgments in addition to timeouts. In the ensuing dis-\ncussion, we suppose that data is being sent in only one direction, from Host A to\nHost B, and that Host A is sending a large file.\nFigure 3.33 presents a highly simplified description of a TCP sender. We see\nthat there are three major events related to data transmission and retransmission in\nthe TCP sender: data received from application above; timer timeout; and ACK\nreceipt. Upon the occurrence of the first major event, TCP receives data from the\napplication, encapsulates the data in a segment, and passes the segment to IP. Note\nthat each segment includes a sequence number that is the byte-stream number of\nthe first data byte in the segment, as described in Section 3.5.2. Also note that if the\ntimer is already not running for some other segment, TCP starts the timer when the\nsegment is passed to IP. (It is helpful to think of the timer as being associated with\nthe oldest unacknowledged segment.) The expiration interval for this timer is the\nTimeoutInterval, which is calculated from EstimatedRTT and DevRTT,\nas described in Section 3.5.3.\n\nFigure 3.33 \u0002 Simplified TCP sender\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n243\nThe second major event is the timeout. TCP responds to the timeout event by\nretransmitting the segment that caused the timeout. TCP then restarts the timer.\nThe third major event that must be handled by the TCP sender is the arrival of an\nacknowledgment segment (ACK) from the receiver (more specifically, a segment con-\ntaining a valid ACK field value). On the occurrence of this event, TCP compares the\nACK value y with its variable SendBase. The TCP state variable SendBase is the\nsequence number of the oldest unacknowledged byte. (Thus SendBase–1 is the\nsequence number of the last byte that is known to have been received correctly and in\norder at the receiver.) As indicated earlier, TCP uses cumulative acknowledgments, so\nthat y acknowledges the receipt of all bytes before byte number y. If y > SendBase,\n/* Assume sender is not constrained by TCP flow or congestion control, that data from above is less\nthan MSS in size, and that data transfer is in one direction only. */\nNextSeqNum=InitialSeqNumber\nSendBase=InitialSeqNumber\nloop (forever) {\nswitch(event)\nevent: data received from application above\ncreate TCP segment with sequence number NextSeqNum\nif (timer currently not running)\nstart timer\npass segment to IP\nNextSeqNum=NextSeqNum+length(data)\nbreak;\nevent: timer timeout\nretransmit not-yet-acknowledged segment with\nsmallest sequence number\nstart timer\nbreak;\nevent: ACK received, with ACK field value of y\nif (y > SendBase) {\nSendBase=y\nif (there are currently any not-yet-acknowledged segments)\nstart timer\n}\nbreak;\n} /* end of loop forever */\n\n244\nCHAPTER 3\n•\nTRANSPORT LAYER\nthen the ACK is acknowledging one or more previously unacknowledged segments.\nThus the sender updates its SendBase variable; it also restarts the timer if there cur-\nrently are any not-yet-acknowledged segments.\nA Few Interesting Scenarios\nWe have just described a highly simplified version of how TCP provides reliable\ndata transfer. But even this highly simplified version has many subtleties. To get a\ngood feeling for how this protocol works, let’s now walk through a few simple\nscenarios. Figure 3.34 depicts the first scenario, in which Host A sends one seg-\nment to Host B. Suppose that this segment has sequence number 92 and contains 8\nbytes of data. After sending this segment, Host A waits for a segment from B with\nacknowledgment number 100. Although the segment from A is received at B, the\nacknowledgment from B to A gets lost. In this case, the timeout event occurs, and\nHost A retransmits the same segment. Of course, when Host B receives the\nretransmission, it observes from the sequence number that the segment contains\ndata that has already been received. Thus, TCP in Host B will discard the bytes in\nthe retransmitted segment.\nTime\nTime\nHost A\nHost B\nTimeout\nSeq=92, 8 bytes data\nSeq=92, 8 bytes data\nACK=100\nACK=100\nX\n(loss)\nFigure 3.34 \u0002 Retransmission due to a lost acknowledgment\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n245\nIn a second scenario, shown in Figure 3.35, Host A sends two segments back to\nback. The first segment has sequence number 92 and 8 bytes of data, and the second\nsegment has sequence number 100 and 20 bytes of data. Suppose that both segments\narrive intact at B, and B sends two separate acknowledgments for each of these seg-\nments. The first of these acknowledgments has acknowledgment number 100; the\nsecond has acknowledgment number 120. Suppose now that neither of the acknowl-\nedgments arrives at Host A before the timeout. When the timeout event occurs, Host\nA resends the first segment with sequence number 92 and restarts the timer. As long\nas the ACK for the second segment arrives before the new timeout, the second seg-\nment will not be retransmitted.\nIn a third and final scenario, suppose Host A sends the two segments, exactly as\nin the second example. The acknowledgment of the first segment is lost in the\nnetwork, but just before the timeout event, Host A receives an acknowledgment with\nacknowledgment number 120. Host A therefore knows that Host B has received\neverything up through byte 119; so Host A does not resend either of the two\nsegments. This scenario is illustrated in Figure 3.36.\nTime\nTime\nHost A\nHost B\nseq=92 timeout interval\nSeq=92, 8 bytes data\nSeq=100, 20 bytes data\nACK=100\nACK=120\nACK=120\nseq=92 timeout interval\nSeq=92, 8 bytes data\nFigure 3.35 \u0002 Segment 100 not retransmitted\n\n246\nCHAPTER 3\n•\nTRANSPORT LAYER\nDoubling the Timeout Interval\nWe now discuss a few modifications that most TCP implementations employ. The\nfirst concerns the length of the timeout interval after a timer expiration. In this mod-\nification, whenever the timeout event occurs, TCP retransmits the not-yet-\nacknowledged segment with the smallest sequence number, as described above. But\neach time TCP retransmits, it sets the next timeout interval to twice the previous\nvalue, rather than deriving it from the last EstimatedRTT and DevRTT (as\ndescribed in Section 3.5.3). For example, suppose TimeoutInterval associated\nwith the oldest not yet acknowledged segment is .75 sec when the timer first expires.\nTCP will then retransmit this segment and set the new expiration time to 1.5 sec. If\nthe timer expires again 1.5 sec later, TCP will again retransmit this segment, now\nsetting the expiration time to 3.0 sec. Thus the intervals grow exponentially after\neach retransmission. However, whenever the timer is started after either of the two\nother events (that is, data received from application above, and ACK received), the\nTime\nTime\nHost A\nHost B\nSeq=92 timeout interval\nSeq=92, 8 bytes data\nSeq=100,  20 bytes data\nACK=100\nACK=120\nX\n(loss)\nFigure 3.36 \u0002 A cumulative acknowledgment avoids retransmission of the\nfirst segment\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n247\nTimeoutInterval is derived from the most recent values of EstimatedRTT\nand DevRTT.\nThis modification provides a limited form of congestion control. (More com-\nprehensive forms of TCP congestion control will be studied in Section 3.7.) The\ntimer expiration is most likely caused by congestion in the network, that is, too\nmany packets arriving at one (or more) router queues in the path between the source\nand destination, causing packets to be dropped and/or long queuing delays. In times\nof congestion, if the sources continue to retransmit packets persistently, the conges-\ntion may get worse. Instead, TCP acts more politely, with each sender retransmitting\nafter longer and longer intervals. We will see that a similar idea is used by Ethernet\nwhen we study CSMA/CD in Chapter 5.\nFast Retransmit\nOne of the problems with timeout-triggered retransmissions is that the timeout\nperiod can be relatively long. When a segment is lost, this long timeout period\nforces the sender to delay resending the lost packet, thereby increasing the end-to-\nend delay. Fortunately, the sender can often detect packet loss well before the time-\nout event occurs by noting so-called duplicate ACKs. A duplicate ACK is an ACK\nthat reacknowledges a segment for which the sender has already received an earlier\nacknowledgment. To understand the sender’s response to a duplicate ACK, we must\nlook at why the receiver sends a duplicate ACK in the first place. Table 3.2 summa-\nrizes the TCP receiver’s ACK generation policy [RFC 5681]. When a TCP receiver\nreceives a segment with a sequence number that is larger than the next, expected,\nin-order sequence number, it detects a gap in the data stream—that is, a missing seg-\nment. This gap could be the result of lost or reordered segments within the network.\nEvent\nTCP Receiver Action\nArrival of in-order segment with expected sequence number. All\nDelayed ACK. Wait up to 500 msec for arrival of another in-order seg-\ndata up to expected sequence number already acknowledged.\nment. If next in-order segment does not arrive in this interval, send an ACK.\nArrival of in-order segment with expected sequence number. One\nImmediately send single cumulative ACK, ACKing both in-order segments.\nother in-order segment waiting for ACK transmission.\nArrival of out-of-order segment with higher-than-expected sequence\nImmediately send duplicate ACK, indicating sequence number of next\nnumber. Gap detected.\nexpected byte (which is the lower end of the gap).\nArrival of segment that partially or completely fills in gap in \nImmediately send ACK, provided that segment starts at the lower end\nreceived data.\nof gap.\nTable 3.2 \u0002 TCP ACK Generation Recommendation [RFC 5681]\n\n248\nCHAPTER 3\n•\nTRANSPORT LAYER\nSince TCP does not use negative acknowledgments, the receiver cannot send an\nexplicit negative acknowledgment back to the sender. Instead, it simply reacknowl-\nedges (that is, generates a duplicate ACK for) the last in-order byte of data it has\nreceived. (Note that Table 3.2 allows for the case that the receiver does not discard\nout-of-order segments.)\nBecause a sender often sends a large number of segments back to back, if one seg-\nment is lost, there will likely be many back-to-back duplicate ACKs. If the TCP sender\nreceives three duplicate ACKs for the same data, it takes this as an indication that the\nsegment following the segment that has been ACKed three times has been lost. (In the\nhomework problems, we consider the question of why the sender waits for three dupli-\ncate ACKs, rather than just a single duplicate ACK.) In the case that three duplicate\nACKs are received, the TCP sender performs a fast retransmit [RFC 5681], retrans-\nmitting the missing segment before that segment’s timer expires. This is shown in\nFigure 3.37, where the second segment is lost, then retransmitted before its timer\nexpires. For TCP with fast retransmit, the following code snippet replaces the ACK\nreceived event in Figure 3.33:\nevent: ACK received, with ACK field value of y\nif (y > SendBase) {\nSendBase=y\nif (there are currently any not yet\nacknowledged segments)\nstart timer\n}\nelse { /* a duplicate ACK for already ACKed\nsegment */\nincrement number of duplicate ACKs\nreceived for y\nif (number of duplicate ACKS received\nfor y==3)\n/* TCP fast retransmit */\nresend segment with sequence number y\n}\nbreak;\nWe noted earlier that many subtle issues arise when a timeout/retransmit mech-\nanism is implemented in an actual protocol such as TCP. The procedures above,\nwhich have evolved as a result of more than 20 years of experience with TCP timers,\nshould convince you that this is indeed the case!\nGo-Back-N or Selective Repeat?\nLet us close our study of TCP’s error-recovery mechanism by considering the follow-\ning question: Is TCP a GBN or an SR protocol? Recall that TCP acknowledgments are\ncumulative and correctly received but out-of-order segments are not individually\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n249\nACKed by the receiver. Consequently, as shown in Figure 3.33 (see also Figure 3.19),\nthe TCP sender need only maintain the smallest sequence number of a transmitted but\nunacknowledged byte (SendBase) and the sequence number of the next byte to be\nsent (NextSeqNum). In this sense, TCP looks a lot like a GBN-style protocol. But\nthere are some striking differences between TCP and Go-Back-N. Many TCP imple-\nmentations will buffer correctly received but out-of-order segments [Stevens 1994].\nConsider also what happens when the sender sends a sequence of segments 1, 2, . . . ,\nN, and all of the segments arrive in order without error at the receiver. Further suppose\nthat the acknowledgment for packet n < N gets lost, but the remaining N – 1 acknowl-\nedgments arrive at the sender before their respective timeouts. In this example, GBN\nwould retransmit not only packet n, but also all of the subsequent packets n + 1, n + 2,\n. . . , N. TCP, on the other hand, would retransmit at most one segment, namely, seg-\nment n. Moreover, TCP would not even retransmit segment n if the acknowledgment\nfor segment n + 1 arrived before the timeout for segment n.\nHost A\nHost B\nseq=100, 20 bytes of data\nTimeout\nTime\nTime\nX\nseq=100, 20 bytes of data\nseq=92, 8 bytes of data\nseq=120, 15 bytes of data\nseq=135, 6 bytes of data\nseq=141, 16 bytes of data\nack=100\nack=100\nack=100\nack=100\nFigure 3.37 \u0002 Fast retransmit: retransmitting the missing segment before\nthe segment’s timer expires"
    },
    {
      "chunk_id": "257d01c7-5c7a-4c6d-a1ae-073a6475965d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.5.5 Flow Control",
      "original_titles": [
        "3.5.5 Flow Control"
      ],
      "path": "Chapter 3 Transport Layer > 3.5 Connection-Oriented Transport: TCP > 3.5.5 Flow Control",
      "start_page": 277,
      "end_page": 278,
      "token_count": 1001,
      "text": "A proposed modification to TCP, the so-called selective acknowledgment\n[RFC 2018], allows a TCP receiver to acknowledge out-of-order segments selec-\ntively rather than just cumulatively acknowledging the last correctly received, in-\norder segment. When combined with selective retransmission—skipping the\nretransmission of segments that have already been selectively acknowledged by the\nreceiver—TCP looks a lot like our generic SR protocol. Thus, TCP’s error-recovery\nmechanism is probably best categorized as a hybrid of GBN and SR protocols.\n3.5.5 Flow Control\nRecall that the hosts on each side of a TCP connection set aside a receive buffer for\nthe connection. When the TCP connection receives bytes that are correct and in\nsequence, it places the data in the receive buffer. The associated application process\nwill read data from this buffer, but not necessarily at the instant the data arrives.\nIndeed, the receiving application may be busy with some other task and may not\neven attempt to read the data until long after it has arrived. If the application is rela-\ntively slow at reading the data, the sender can very easily overflow the connection’s\nreceive buffer by sending too much data too quickly.\nTCP provides a flow-control service to its applications to eliminate the possibility\nof the sender overflowing the receiver’s buffer. Flow control is thus a speed-matching\nservice—matching the rate at which the sender is sending against the rate at which the\nreceiving application is reading. As noted earlier, a TCP sender can also be throttled\ndue to congestion within the IP network; this form of sender control is referred to as\ncongestion control, a topic we will explore in detail in Sections 3.6 and 3.7. Even\nthough the actions taken by flow and congestion control are similar (the throttling of\nthe sender), they are obviously taken for very different reasons. Unfortunately, many\nauthors use the terms interchangeably, and the savvy reader would be wise to distin-\nguish between them. Let’s now discuss how TCP provides its flow-control service. In\norder to see the forest for the trees, we suppose throughout this section that the TCP\nimplementation is such that the TCP receiver discards out-of-order segments.\nTCP provides flow control by having the sender maintain a variable called the\nreceive window. Informally, the receive window is used to give the sender an idea of\nhow much free buffer space is available at the receiver. Because TCP is full-duplex, the\nsender at each side of the connection maintains a distinct receive window. Let’s investi-\ngate the receive window in the context of a file transfer. Suppose that Host A is sending\na large file to Host B over a TCP connection. Host B allocates a receive buffer to this\nconnection; denote its size by RcvBuffer. From time to time, the application process\nin Host B reads from the buffer. Define the following variables:\n•\nLastByteRead: the number of the last byte in the data stream read from the\nbuffer by the application process in B\n•\nLastByteRcvd: the number of the last byte in the data stream that has arrived\nfrom the network and has been placed in the receive buffer at B\n250\nCHAPTER 3\n•\nTRANSPORT LAYER\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n251\nBecause TCP is not permitted to overflow the allocated buffer, we must have\nLastByteRcvd – LastByteRead \u0005 RcvBuffer\nThe receive window, denoted rwnd is set to the amount of spare room in the buffer:\nrwnd = RcvBuffer – [LastByteRcvd – LastByteRead]\nBecause the spare room changes with time, rwnd is dynamic. The variable rwnd is\nillustrated in Figure 3.38.\nHow does the connection use the variable rwnd to provide the flow-control\nservice? Host B tells Host A how much spare room it has in the connection buffer\nby placing its current value of rwnd in the receive window field of every segment it\nsends to A. Initially, Host B sets rwnd = RcvBuffer. Note that to pull this off,\nHost B must keep track of several connection-specific variables.\nHost A in turn keeps track of two variables, LastByteSent and Last-\nByteAcked, which have obvious meanings. Note that the difference between these\ntwo variables, LastByteSent – LastByteAcked, is the amount of unac-\nknowledged data that A has sent into the connection. By keeping the amount of\nunacknowledged data less than the value of rwnd, Host A is assured that it is not\noverflowing the receive buffer at Host B. Thus, Host A makes sure throughout the\nconnection’s life that\nLastByteSent – LastByteAcked \u0005 rwnd\nApplication\nprocess\nData\nfrom IP\nTCP data\nin buffer\nrwnd\nRcvBuffer\nSpare room\nFigure 3.38 \u0002 The receive window (rwnd) and the receive buffer\n(RcvBuffer)"
    },
    {
      "chunk_id": "faa16bc4-f8cd-473d-99c9-41f8a31f28eb",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.5.6 TCP Connection Management",
      "original_titles": [
        "3.5.6 TCP Connection Management"
      ],
      "path": "Chapter 3 Transport Layer > 3.5 Connection-Oriented Transport: TCP > 3.5.6 TCP Connection Management",
      "start_page": 279,
      "end_page": 285,
      "token_count": 3590,
      "text": "There is one minor technical problem with this scheme. To see this, suppose\nHost B’s receive buffer becomes full so that rwnd = 0. After advertising rwnd = 0\nto Host A, also suppose that B has nothing to send to A. Now consider what hap-\npens. As the application process at B empties the buffer, TCP does not send new seg-\nments with new rwnd values to Host A; indeed, TCP sends a segment to Host A\nonly if it has data to send or if it has an acknowledgment to send. Therefore, Host A\nis never informed that some space has opened up in Host B’s receive buffer—Host\nA is blocked and can transmit no more data! To solve this problem, the TCP specifi-\ncation requires Host A to continue to send segments with one data byte when B’s\nreceive window is zero. These segments will be acknowledged by the receiver.\nEventually the buffer will begin to empty and the acknowledgments will contain a\nnonzero rwnd value.\nThe online site at http://www.awl.com/kurose-ross for this book provides an\ninteractive Java applet that illustrates the operation of the TCP receive window.\nHaving described TCP’s flow-control service, we briefly mention here that UDP\ndoes not provide flow control. To understand the issue, consider sending a series of\nUDP segments from a process on Host A to a process on Host B. For a typical UDP\nimplementation, UDP will append the segments in a finite-sized buffer that “precedes”\nthe corresponding socket (that is, the door to the process). The process reads one entire\nsegment at a time from the buffer. If the process does not read the segments fast\nenough from the buffer, the buffer will overflow and segments will get dropped.\n3.5.6 TCP Connection Management\nIn this subsection we take a closer look at how a TCP connection is established and\ntorn down. Although this topic may not seem particularly thrilling, it is important\nbecause TCP connection establishment can significantly add to perceived delays\n(for example, when surfing the Web). Furthermore, many of the most common net-\nwork attacks—including the incredibly popular SYN flood attack—exploit vulnera-\nbilities in TCP connection management. Let’s first take a look at how a TCP\nconnection is established. Suppose a process running in one host (client) wants to\ninitiate a connection with another process in another host (server). The client appli-\ncation process first informs the client TCP that it wants to establish a connection to\na process in the server. The TCP in the client then proceeds to establish a TCP con-\nnection with the TCP in the server in the following manner:\n•\nStep 1. The client-side TCP first sends a special TCP segment to the server-side\nTCP. This special segment contains no application-layer data. But one of the flag\nbits in the segment’s header (see Figure 3.29), the SYN bit, is set to 1. For this\nreason, this special segment is referred to as a SYN segment. In addition, the \nclient randomly chooses an initial sequence number (client_isn) and puts\nthis number in the sequence number field of the initial TCP SYN segment. This\nsegment is encapsulated within an IP datagram and sent to the server. There has\n252\nCHAPTER 3\n•\nTRANSPORT LAYER\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n253\nbeen considerable interest in properly randomizing the choice of the\nclient_isn in order to avoid certain security attacks [CERT 2001–09].\n•\nStep 2. Once the IP datagram containing the TCP SYN segment arrives at the\nserver host (assuming it does arrive!), the server extracts the TCP SYN segment\nfrom the datagram, allocates the TCP buffers and variables to the connection, and\nsends a connection-granted segment to the client TCP. (We’ll see in Chapter 8 that\nthe allocation of these buffers and variables before completing the third step of the\nthree-way handshake makes TCP vulnerable to a denial-of-service attack known\nas SYN flooding.) This connection-granted segment also contains no application-\nlayer data. However, it does contain three important pieces of information in the\nsegment header. First, the SYN bit is set to 1. Second, the acknowledgment field\nof the TCP segment header is set to client_isn+1. Finally, the server\nchooses its own initial sequence number (server_isn) and puts this value in\nthe sequence number field of the TCP segment header. This connection-granted\nsegment is saying, in effect, “I received your SYN packet to start a connection\nwith your initial sequence number, client_isn. I agree to establish this con-\nnection. My own initial sequence number is server_isn.” The connection-\ngranted segment is referred to as a SYNACK segment.\n•\nStep 3. Upon receiving the SYNACK segment, the client also allocates buffers\nand variables to the connection. The client host then sends the server yet another\nsegment; this last segment acknowledges the server’s connection-granted seg-\nment (the client does so by putting the value server_isn+1 in the acknowl-\nedgment field of the TCP segment header). The SYN bit is set to zero, since the\nconnection is established. This third stage of the three-way handshake may carry\nclient-to-server data in the segment payload.\nOnce these three steps have been completed, the client and server hosts can send\nsegments containing data to each other. In each of these future segments, the SYN bit\nwill be set to zero. Note that in order to establish the connection, three packets are sent\nbetween the two hosts, as illustrated in Figure 3.39. For this reason, this connection-\nestablishment procedure is often referred to as a three-way handshake. Several\naspects of the TCP three-way handshake are explored in the homework problems\n(Why are initial sequence numbers needed? Why is a three-way handshake, as\nopposed to a two-way handshake, needed?). It’s interesting to note that a rock climber\nand a belayer (who is stationed below the rock climber and whose job it is to handle\nthe climber’s safety rope) use a three-way-handshake communication protocol that is\nidentical to TCP’s to ensure that both sides are ready before the climber begins ascent.\nAll good things must come to an end, and the same is true with a TCP connec-\ntion. Either of the two processes participating in a TCP connection can end the con-\nnection. When a connection ends, the “resources” (that is, the buffers and variables)\nin the hosts are deallocated. As an example, suppose the client decides to close the\nconnection, as shown in Figure 3.40. The client application process issues a close\n\n254\nCHAPTER 3\n•\nTRANSPORT LAYER\ncommand. This causes the client TCP to send a special TCP segment to the server\nprocess. This special segment has a flag bit in the segment’s header, the FIN bit\n(see Figure 3.29), set to 1. When the server receives this segment, it sends the client\nan acknowledgment segment in return. The server then sends its own shutdown\nsegment, which has the FIN bit set to 1. Finally, the client acknowledges the\nserver’s shutdown segment. At this point, all the resources in the two hosts are now\ndeallocated.\nDuring the life of a TCP connection, the TCP protocol running in each host\nmakes transitions through various TCP states. Figure 3.41 illustrates a typical\nsequence of TCP states that are visited by the client TCP. The client TCP begins in\nthe CLOSED state. The application on the client side initiates a new TCP connec-\ntion (by creating a Socket object in our Java examples as in the Python examples\nfrom Chapter 2). This causes TCP in the client to send a SYN segment to TCP in the\nserver. After having sent the SYN segment, the client TCP enters the SYN_SENT\nstate. While in the SYN_SENT state, the client TCP waits for a segment from the\nserver TCP that includes an acknowledgment for the client’s previous segment and\nhas the SYN bit set to 1. Having received such a segment, the client TCP enters the\nESTABLISHED state. While in the ESTABLISHED state, the TCP client can send\nand receive TCP segments containing payload (that is, application-generated) data.\nTime\nTime\nClient host\nConnection\nrequest\nConnection\ngranted\nServer host\nSYN=1, seq=client_isn\nSYN=1, seq=server_isn,\nack=client_isn+1\nSYN=0, seq=client_isn+1,\nack=server_isn+1\nACK\nFigure 3.39 \u0002 TCP three-way handshake: segment exchange\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n255\nSuppose that the client application decides it wants to close the connection.\n(Note that the server could also choose to close the connection.) This causes the\nclient TCP to send a TCP segment with the FIN bit set to 1 and to enter the\nFIN_WAIT_1 state. While in the FIN_WAIT_1 state, the client TCP waits for a TCP\nsegment from the server with an acknowledgment. When it receives this segment,\nthe client TCP enters the FIN_WAIT_2 state. While in the FIN_WAIT_2 state, the\nclient waits for another segment from the server with the FIN bit set to 1; after\nreceiving this segment, the client TCP acknowledges the server’s segment and\nenters the TIME_WAIT state. The TIME_WAIT state lets the TCP client resend the\nfinal acknowledgment in case the ACK is lost. The time spent in the TIME_WAIT\nstate is implementation-dependent, but typical values are 30 seconds, 1 minute, and\n2 minutes. After the wait, the connection formally closes and all resources on the\nclient side (including port numbers) are released.\nFigure 3.42 illustrates the series of states typically visited by the server-side\nTCP, assuming the client begins connection teardown. The transitions are self-\nexplanatory. In these two state-transition diagrams, we have only shown how a TCP\nconnection is normally established and shut down. We have not described what\nTime\nTime\nClient\nClose\nClose\nServer\nFIN\nACK\nACK\nFIN\nClosed\nTimed wait\nFigure 3.40 \u0002 Closing a TCP connection\n\n256\nCHAPTER 3\n•\nTRANSPORT LAYER\nCLOSED\nSYN_SENT\nESTABLISHED\nFIN_WAIT_1\nFIN_WAIT_2\nTIME_WAIT\nSend SYN\nSend FIN\nReceive ACK, \nsend nothing\nWait 30 seconds\nReceive FIN, \nsend ACK\nReceive SYN & ACK, \nsend ACK\nClient application\ninitiates a TCP connection\nClient application\ninitiates close connection\nFigure 3.41 \u0002 A typical sequence of TCP states visited by a client TCP\nCLOSED\nLISTEN\nSYN_RCVD\nESTABLISHED\nCLOSE_WAIT\nLAST_ACK\nReceive FIN,\nsend ACK\nReceive ACK, \nsend nothing\nSend FIN\nReceive SYN \nsend SYN & ACK\nServer application\ncreates a listen socket\nReceive ACK, \nsend nothing\nFigure 3.42 \u0002 A typical sequence of TCP states visited by a server-side TCP\n\n3.5\n•\nCONNECTION-ORIENTED TRANSPORT: TCP\n257\nTHE SYN FLOOD ATTACK\nWe’ve seen in our discussion of TCP’s three-way handshake that a server allocates\nand initializes connection variables and buffers in response to a received SYN. The\nserver then sends a SYNACK in response, and awaits an ACK segment from the\nclient. If the client does not send an ACK to complete the third step of this 3-way\nhandshake, eventually (often after a minute or more) the server will terminate the half-\nopen connection and reclaim the allocated resources. \nThis TCP connection management protocol sets the stage for a classic Denial of\nService (DoS) attack known as the SYN flood attack. In this attack, the attacker(s)\nsend a large number of TCP SYN segments, without completing the third handshake\nstep. With this deluge of SYN segments, the server’s connection resources become\nexhausted as they are allocated (but never used!) for half-open connections; legiti-\nmate clients are then denied service. Such SYN flooding attacks were among the first\ndocumented DoS attacks [CERT SYN 1996]. Fortunately, an effective defense known\nas SYN cookies [RFC 4987] are now deployed in most major operating systems.\nSYN cookies work as follows:\no  When the server receives a SYN segment, it does not know if the segment is com-\ning from a legitimate user or is part of a SYN flood attack. So, instead of creating\na half-open TCP connection for this SYN, the server creates an initial TCP\nsequence number that is a complicated function (hash function) of source and des-\ntination IP addresses and port numbers of the SYN segment, as well as a secret\nnumber only known to the server. This carefully crafted initial sequence number is\nthe so-called “cookie.” The server then sends the client a SYNACK packet with this\nspecial initial sequence number. Importantly, the server does not remember the\ncookie or any other state information corresponding to the SYN.\no  A legitimate client will return an ACK segment. When the server receives this\nACK, it must verify that the ACK corresponds to some SYN sent earlier. But how is\nthis done if the server maintains no memory about SYN segments? As you may\nhave guessed, it is done with the cookie. Recall that for a legitimate ACK, the\nvalue in the acknowledgment field is equal to the initial sequence number in the\nSYNACK (the cookie value in this case) plus one (see Figure 3.39). The server can\nthen run the same hash function using the source and destination IP address and\nport numbers in the SYNACK (which are the same as in the original SYN) and the\nsecret number. If the result of the function plus one is the same as the acknowledg-\nment (cookie) value in the client’s SYNACK, the server concludes that the ACK cor-\nresponds to an earlier SYN segment and is hence valid. The server then creates a\nfully open connection along with a socket.\no  On the other hand, if the client does not return an ACK segment, then the original\nSYN has done no harm at the server, since the server hasn’t yet allocated any\nresources in response to the original bogus SYN.\nFOCUS ON SECURITY\n\n258\nCHAPTER 3\n•\nTRANSPORT LAYER\nhappens in certain pathological scenarios, for example, when both sides of a con-\nnection want to initiate or shut down at the same time. If you are interested in learn-\ning about this and other advanced issues concerning TCP, you are encouraged to see\nStevens’ comprehensive book [Stevens 1994].\nOur discussion above has assumed that both the client and server are prepared\nto communicate, i.e., that the server is listening on the port to which the client sends\nits SYN segment. Let’s consider what happens when a host receives a TCP segment\nwhose port numbers or source IP address do not match with any of the ongoing\nsockets in the host. For example, suppose a host receives a TCP SYN packet with\ndestination port 80, but the host is not accepting connections on port 80 (that is, it is\nnot running a Web server on port 80). Then the host will send a special reset seg-\nment to the source. This TCP segment has the RST flag bit (see Section 3.5.2) set to\n1. Thus, when a host sends a reset segment, it is telling the source “I don’t have a\nsocket for that segment. Please do not resend the segment.” When a host receives a\nUDP packet whose destination port number doesn’t match with an ongoing UDP\nsocket, the host sends a special ICMP datagram, as discussed in Chapter 4.\nNow that we have a good understanding of TCP connection management, let’s\nrevisit the nmap port-scanning tool and examine more closely how it works. To explore\na specific TCP port, say port 6789, on a target host, nmap will send a TCP SYN seg-\nment with destination port 6789 to that host. There are three possible outcomes:\n•\nThe source host receives a TCP SYNACK segment from the target host. Since this\nmeans that an application is running with TCP port 6789 on the target post, nmap\nreturns “open.”\n•\nThe source host receives a TCP RST segment from the target host. This means that\nthe SYN segment reached the target host, but the target host is not running an appli-\ncation with TCP port 6789. But the attacker at least knows that the segments des-\ntined to the host at port 6789 are not blocked by any firewall on the path between\nsource and target hosts. (Firewalls are discussed in Chapter 8.)\n•\nThe source receives nothing. This likely means that the SYN segment was blocked\nby an intervening firewall and never reached the target host.\nNmap is a powerful tool, which can “case the joint” not only for open TCP ports,\nbut also for open UDP ports, for firewalls and their configurations, and even for the ver-\nsions of applications and operating systems. Most of this is done by manipulating TCP\nconnection-management segments [Skoudis 2006].  You can download nmap from\nwww.nmap.org.\nThis completes our introduction to error control and flow control in TCP. In\nSection 3.7 we’ll return to TCP and look at TCP congestion control in some depth.\nBefore doing so, however, we first step back and examine congestion-control issues\nin a broader context."
    },
    {
      "chunk_id": "d4a1b94f-64e1-45b8-9084-3cf68d7a2a81",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.6 Principles of Congestion Control",
      "original_titles": [
        "3.6 Principles of Congestion Control"
      ],
      "path": "Chapter 3 Transport Layer > 3.6 Principles of Congestion Control",
      "start_page": 286,
      "end_page": 291,
      "token_count": 2628,
      "text": "3.6\n•\nPRINCIPLES OF CONGESTION CONTROL\n259\n3.6 Principles of Congestion Control\nIn the previous sections, we examined both the general principles and specific\nTCP mechanisms used to provide for a reliable data transfer service in the face of\npacket loss. We mentioned earlier that, in practice, such loss typically results from\nthe overflowing of router buffers as the network becomes congested. Packet\nretransmission thus treats a symptom of network congestion (the loss of a specific\ntransport-layer segment) but does not treat the cause of network congestion—too\nmany sources attempting to send data at too high a rate. To treat the cause of net-\nwork congestion, mechanisms are needed to throttle senders in the face of network\ncongestion.\nIn this section, we consider the problem of congestion control in a general con-\ntext, seeking to understand why congestion is a bad thing, how network congestion\nis manifested in the performance received by upper-layer applications, and various\napproaches that can be taken to avoid, or react to, network congestion. This more\ngeneral study of congestion control is appropriate since, as with reliable data trans-\nfer, it is high on our “top-ten” list of fundamentally important problems in network-\ning. We conclude this section with a discussion of congestion control in the\navailable bit-rate (ABR) service in asynchronous transfer mode (ATM)\nnetworks. The following section contains a detailed study of TCP’s congestion-\ncontrol algorithm.\n3.6.1 The Causes and the Costs of Congestion\nLet’s begin our general study of congestion control by examining three increasingly\ncomplex scenarios in which congestion occurs. In each case, we’ll look at why con-\ngestion occurs in the first place and at the cost of congestion (in terms of resources\nnot fully utilized and poor performance received by the end systems). We’ll not (yet)\nfocus on how to react to, or avoid, congestion but rather focus on the simpler issue\nof understanding what happens as hosts increase their transmission rate and the net-\nwork becomes congested.\nScenario 1: Two Senders, a Router with Infinite Buffers\nWe begin by considering perhaps the simplest congestion scenario possible: Two\nhosts (A and B) each have a connection that shares a single hop between source and\ndestination, as shown in Figure 3.43.\nLet’s assume that the application in Host A is sending data into the connection\n(for example, passing data to the transport-level protocol via a socket) at an aver-\nage rate of \u0006in bytes/sec. These data are original in the sense that each unit of data\nis sent into the socket only once. The underlying transport-level protocol is a\n\n260\nCHAPTER 3\n•\nTRANSPORT LAYER\nsimple one. Data is encapsulated and sent; no error recovery (for example, retrans-\nmission), flow control, or congestion control is performed. Ignoring the additional\noverhead due to adding transport- and lower-layer header information, the rate at\nwhich Host A offers traffic to the router in this first scenario is thus \u0006in bytes/sec.\nHost B operates in a similar manner, and we assume for simplicity that it too is\nsending at a rate of \u0006in bytes/sec. Packets from Hosts A and B pass through a\nrouter and over a shared outgoing link of capacity R. The router has buffers that\nallow it to store incoming packets when the packet-arrival rate exceeds the outgo-\ning link’s capacity. In this first scenario, we assume that the router has an infinite\namount of buffer space.\nFigure 3.44 plots the performance of Host A’s connection under this first\nscenario. The left graph plots the per-connection throughput (number of bytes per\nsecond at the receiver) as a function of the connection-sending rate. For a sending\nrate between 0 and R/2, the throughput at the receiver equals the sender’s sending\nrate—everything sent by the sender is received at the receiver with a finite delay.\nWhen the sending rate is above R/2, however, the throughput is only R/2. This upper\nlimit on throughput is a consequence of the sharing of link capacity between two\nconnections. The link simply cannot deliver packets to a receiver at a steady-state\nrate that exceeds R/2. No matter how high Hosts A and B set their sending rates,\nthey will each never see a throughput higher than R/2.\nAchieving a per-connection throughput of R/2 might actually appear to be a\ngood thing, because the link is fully utilized in delivering packets to their destina-\ntions. The right-hand graph in Figure 3.44, however, shows the consequence of\noperating near link capacity. As the sending rate approaches R/2 (from the left), the\naverage delay becomes larger and larger. When the sending rate exceeds R/2, the\nHost B\nUnlimited shared\noutput link buffers\nλin: original data\nHost A\nHost D\nHost C\nλout\nFigure 3.43 \u0002 Congestion scenario 1: Two connections sharing a single\nhop with infinite buffers\n\n3.6\n•\nPRINCIPLES OF CONGESTION CONTROL\n261\naverage number of queued packets in the router is unbounded, and the average delay\nbetween source and destination becomes infinite (assuming that the connections\noperate at these sending rates for an infinite period of time and there is an infinite\namount of buffering available). Thus, while operating at an aggregate throughput of\nnear R may be ideal from a throughput standpoint, it is far from ideal from a delay\nstandpoint. Even in this (extremely) idealized scenario, we’ve already found one\ncost of a congested network—large queuing delays are experienced as the packet-\narrival rate nears the link capacity.\nScenario 2: Two Senders and a Router with Finite Buffers\nLet us now slightly modify scenario 1 in the following two ways (see Figure 3.45).\nFirst, the amount of router buffering is assumed to be finite. A consequence of this\nreal-world assumption is that packets will be dropped when arriving to an already-\nfull buffer. Second, we assume that each connection is reliable. If a packet contain-\ning a transport-level segment is dropped at the router, the sender will eventually\nretransmit it. Because packets can be retransmitted, we must now be more careful\nwith our use of the term sending rate. Specifically, let us again denote the rate at\nwhich the application sends original data into the socket by \u0006in bytes/sec. The rate at\nwhich the transport layer sends segments (containing original data and retransmit-\nted data) into the network will be denoted \u0006\u0007in bytes/sec. \u0006\u0007in is sometimes referred to\nas the offered load to the network.\nThe performance realized under scenario 2 will now depend strongly on\nhow retransmission is performed. First, consider the unrealistic case that Host A is\nable to somehow (magically!) determine whether or not a buffer is free in the router\nand thus sends a packet only when a buffer is free. In this case, no loss would occur,\nR/2\nR/2\nDelay\nR/2\nλin\nλin\nλout\na.\nb.\nFigure 3.44 \u0002 Congestion scenario 1: Throughput and delay as a function\nof host sending rate\n\n262\nCHAPTER 3\n•\nTRANSPORT LAYER\n\u0006in would be equal to \u0006\u0007in, and the throughput of the connection would be equal to\n\u0006in. This case is shown in Figure 3.46(a). From a throughput standpoint, perform-\nance is ideal—everything that is sent is received. Note that the average host sending\nrate cannot exceed R/2 under this scenario, since packet loss is assumed never\nto occur.\nConsider next the slightly more realistic case that the sender retransmits only\nwhen a packet is known for certain to be lost. (Again, this assumption is a bit of a\nstretch. However, it is possible that the sending host might set its timeout large\nenough to be virtually assured that a packet that has not been acknowledged has\nbeen lost.) In this case, the performance might look something like that shown in\nFigure 3.46(b). To appreciate what is happening here, consider the case that the\noffered load, \u0006\u0007in (the rate of original data transmission plus retransmissions), equals\nR/2. According to Figure 3.46(b), at this value of the offered load, the rate at which\ndata are delivered to the receiver application is R/3. Thus, out of the 0.5R units of\ndata transmitted, 0.333R bytes/sec (on average) are original data and 0.166R bytes/\nsec (on average) are retransmitted data. We see here another cost of a congested net-\nwork—the sender must perform retransmissions in order to compensate for dropped\n(lost) packets due to buffer overflow.\nFinally, let us consider the case that the sender may time out prematurely and\nretransmit a packet that has been delayed in the queue but not yet lost. In this case,\nboth the original data packet and the retransmission may reach the receiver. Of\nFinite shared output\nlink buffers\nHost B\nHost A\nHost D\nHost C\nλout\nλin: original data\nλ’in: original data, plus\nretransmitted data\nFigure 3.45 \u0002 Scenario 2: Two hosts (with retransmissions) and a router\nwith finite buffers\n\n3.6\n•\nPRINCIPLES OF CONGESTION CONTROL\n263\ncourse, the receiver needs but one copy of this packet and will discard the retrans-\nmission. In this case, the work done by the router in forwarding the retransmitted\ncopy of the original packet was wasted, as the receiver will have already received\nthe original copy of this packet. The router would have better used the link trans-\nmission capacity to send a different packet instead. Here then is yet another cost of\na congested network—unneeded retransmissions by the sender in the face of large\ndelays may cause a router to use its link bandwidth to forward unneeded copies of a\npacket. Figure 3.46 (c) shows the throughput versus offered load when each packet\nis assumed to be forwarded (on average) twice by the router. Since each packet is\nforwarded twice, the throughput will have an asymptotic value of R/4 as the offered\nload approaches R/2.\nScenario 3: Four Senders, Routers with Finite Buffers, and \nMultihop Paths\nIn our final congestion scenario, four hosts transmit packets, each over overlap-\nping two-hop paths, as shown in Figure 3.47. We again assume that each host\nuses a timeout/retransmission mechanism to implement a reliable data transfer\nservice, that all hosts have the same value of \u0006in, and that all router links have\ncapacity R bytes/sec.\nLet’s consider the connection from Host A to Host C, passing through routers\nR1 and R2. The A–C connection shares router R1 with the D–B connection and\nshares router R2 with the B–D connection. For extremely small values of \u0006in, buffer\noverflows are rare (as in congestion scenarios 1 and 2), and the throughput approxi-\nmately equals the offered load. For slightly larger values of \u0006in, the corresponding\nthroughput is also larger, since more original data is being transmitted into the\nR/2\nR/2\nR/2\nλout\na.\nb.\nR/2\nλout\nR/3\nR/2\nR/2\nλout\nR/4\nc.\nλ’in\nλ’in\nλ’in\nFigure 3.46 \u0002 Scenario 2 performance with finite buffers\n\n264\nCHAPTER 3\n•\nTRANSPORT LAYER\nnetwork and delivered to the destination, and overflows are still rare. Thus, for small\nvalues of \u0006in, an increase in \u0006in results in an increase in \u0006out.\nHaving considered the case of extremely low traffic, let’s next examine the\ncase that \u0006in (and hence \u0006\u0007in) is extremely large. Consider router R2. The A–C\ntraffic arriving to router R2 (which arrives at R2 after being forwarded from R1)\ncan have an arrival rate at R2 that is at most R, the capacity of the link from R1\nto R2, regardless of the value of \u0006in. If \u0006\u0007in is extremely large for all connections\n(including the B–D connection), then the arrival rate of B–D traffic at R2 can be\nmuch larger than that of the A–C traffic. Because the A–C and B–D traffic must\ncompete at router R2 for the limited amount of buffer space, the amount of A–C\ntraffic that successfully gets through R2 (that is, is not lost due to buffer over-\nflow) becomes smaller and smaller as the offered load from B–D gets larger and\nlarger. In the limit, as the offered load approaches infinity, an empty buffer at R2\nHost B\nHost A\nR1\nR4\nR2\nR3\nHost C\nHost D\nFinite shared output\nlink buffers\nλin: original data\nλ’in : original\ndata, plus\nretransmitted\ndata\nλout\nFigure 3.47 \u0002 Four senders, routers with finite buffers, and multihop paths"
    },
    {
      "chunk_id": "b0725d4e-bf08-4d60-a4a6-90e9ef20052e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.6.1 The Causes and the Costs of Congestion",
      "original_titles": [
        "3.6.1 The Causes and the Costs of Congestion"
      ],
      "path": "Chapter 3 Transport Layer > 3.6 Principles of Congestion Control > 3.6.1 The Causes and the Costs of Congestion",
      "start_page": 286,
      "end_page": 291,
      "token_count": 2628,
      "text": "3.6\n•\nPRINCIPLES OF CONGESTION CONTROL\n259\n3.6 Principles of Congestion Control\nIn the previous sections, we examined both the general principles and specific\nTCP mechanisms used to provide for a reliable data transfer service in the face of\npacket loss. We mentioned earlier that, in practice, such loss typically results from\nthe overflowing of router buffers as the network becomes congested. Packet\nretransmission thus treats a symptom of network congestion (the loss of a specific\ntransport-layer segment) but does not treat the cause of network congestion—too\nmany sources attempting to send data at too high a rate. To treat the cause of net-\nwork congestion, mechanisms are needed to throttle senders in the face of network\ncongestion.\nIn this section, we consider the problem of congestion control in a general con-\ntext, seeking to understand why congestion is a bad thing, how network congestion\nis manifested in the performance received by upper-layer applications, and various\napproaches that can be taken to avoid, or react to, network congestion. This more\ngeneral study of congestion control is appropriate since, as with reliable data trans-\nfer, it is high on our “top-ten” list of fundamentally important problems in network-\ning. We conclude this section with a discussion of congestion control in the\navailable bit-rate (ABR) service in asynchronous transfer mode (ATM)\nnetworks. The following section contains a detailed study of TCP’s congestion-\ncontrol algorithm.\n3.6.1 The Causes and the Costs of Congestion\nLet’s begin our general study of congestion control by examining three increasingly\ncomplex scenarios in which congestion occurs. In each case, we’ll look at why con-\ngestion occurs in the first place and at the cost of congestion (in terms of resources\nnot fully utilized and poor performance received by the end systems). We’ll not (yet)\nfocus on how to react to, or avoid, congestion but rather focus on the simpler issue\nof understanding what happens as hosts increase their transmission rate and the net-\nwork becomes congested.\nScenario 1: Two Senders, a Router with Infinite Buffers\nWe begin by considering perhaps the simplest congestion scenario possible: Two\nhosts (A and B) each have a connection that shares a single hop between source and\ndestination, as shown in Figure 3.43.\nLet’s assume that the application in Host A is sending data into the connection\n(for example, passing data to the transport-level protocol via a socket) at an aver-\nage rate of \u0006in bytes/sec. These data are original in the sense that each unit of data\nis sent into the socket only once. The underlying transport-level protocol is a\n\n260\nCHAPTER 3\n•\nTRANSPORT LAYER\nsimple one. Data is encapsulated and sent; no error recovery (for example, retrans-\nmission), flow control, or congestion control is performed. Ignoring the additional\noverhead due to adding transport- and lower-layer header information, the rate at\nwhich Host A offers traffic to the router in this first scenario is thus \u0006in bytes/sec.\nHost B operates in a similar manner, and we assume for simplicity that it too is\nsending at a rate of \u0006in bytes/sec. Packets from Hosts A and B pass through a\nrouter and over a shared outgoing link of capacity R. The router has buffers that\nallow it to store incoming packets when the packet-arrival rate exceeds the outgo-\ning link’s capacity. In this first scenario, we assume that the router has an infinite\namount of buffer space.\nFigure 3.44 plots the performance of Host A’s connection under this first\nscenario. The left graph plots the per-connection throughput (number of bytes per\nsecond at the receiver) as a function of the connection-sending rate. For a sending\nrate between 0 and R/2, the throughput at the receiver equals the sender’s sending\nrate—everything sent by the sender is received at the receiver with a finite delay.\nWhen the sending rate is above R/2, however, the throughput is only R/2. This upper\nlimit on throughput is a consequence of the sharing of link capacity between two\nconnections. The link simply cannot deliver packets to a receiver at a steady-state\nrate that exceeds R/2. No matter how high Hosts A and B set their sending rates,\nthey will each never see a throughput higher than R/2.\nAchieving a per-connection throughput of R/2 might actually appear to be a\ngood thing, because the link is fully utilized in delivering packets to their destina-\ntions. The right-hand graph in Figure 3.44, however, shows the consequence of\noperating near link capacity. As the sending rate approaches R/2 (from the left), the\naverage delay becomes larger and larger. When the sending rate exceeds R/2, the\nHost B\nUnlimited shared\noutput link buffers\nλin: original data\nHost A\nHost D\nHost C\nλout\nFigure 3.43 \u0002 Congestion scenario 1: Two connections sharing a single\nhop with infinite buffers\n\n3.6\n•\nPRINCIPLES OF CONGESTION CONTROL\n261\naverage number of queued packets in the router is unbounded, and the average delay\nbetween source and destination becomes infinite (assuming that the connections\noperate at these sending rates for an infinite period of time and there is an infinite\namount of buffering available). Thus, while operating at an aggregate throughput of\nnear R may be ideal from a throughput standpoint, it is far from ideal from a delay\nstandpoint. Even in this (extremely) idealized scenario, we’ve already found one\ncost of a congested network—large queuing delays are experienced as the packet-\narrival rate nears the link capacity.\nScenario 2: Two Senders and a Router with Finite Buffers\nLet us now slightly modify scenario 1 in the following two ways (see Figure 3.45).\nFirst, the amount of router buffering is assumed to be finite. A consequence of this\nreal-world assumption is that packets will be dropped when arriving to an already-\nfull buffer. Second, we assume that each connection is reliable. If a packet contain-\ning a transport-level segment is dropped at the router, the sender will eventually\nretransmit it. Because packets can be retransmitted, we must now be more careful\nwith our use of the term sending rate. Specifically, let us again denote the rate at\nwhich the application sends original data into the socket by \u0006in bytes/sec. The rate at\nwhich the transport layer sends segments (containing original data and retransmit-\nted data) into the network will be denoted \u0006\u0007in bytes/sec. \u0006\u0007in is sometimes referred to\nas the offered load to the network.\nThe performance realized under scenario 2 will now depend strongly on\nhow retransmission is performed. First, consider the unrealistic case that Host A is\nable to somehow (magically!) determine whether or not a buffer is free in the router\nand thus sends a packet only when a buffer is free. In this case, no loss would occur,\nR/2\nR/2\nDelay\nR/2\nλin\nλin\nλout\na.\nb.\nFigure 3.44 \u0002 Congestion scenario 1: Throughput and delay as a function\nof host sending rate\n\n262\nCHAPTER 3\n•\nTRANSPORT LAYER\n\u0006in would be equal to \u0006\u0007in, and the throughput of the connection would be equal to\n\u0006in. This case is shown in Figure 3.46(a). From a throughput standpoint, perform-\nance is ideal—everything that is sent is received. Note that the average host sending\nrate cannot exceed R/2 under this scenario, since packet loss is assumed never\nto occur.\nConsider next the slightly more realistic case that the sender retransmits only\nwhen a packet is known for certain to be lost. (Again, this assumption is a bit of a\nstretch. However, it is possible that the sending host might set its timeout large\nenough to be virtually assured that a packet that has not been acknowledged has\nbeen lost.) In this case, the performance might look something like that shown in\nFigure 3.46(b). To appreciate what is happening here, consider the case that the\noffered load, \u0006\u0007in (the rate of original data transmission plus retransmissions), equals\nR/2. According to Figure 3.46(b), at this value of the offered load, the rate at which\ndata are delivered to the receiver application is R/3. Thus, out of the 0.5R units of\ndata transmitted, 0.333R bytes/sec (on average) are original data and 0.166R bytes/\nsec (on average) are retransmitted data. We see here another cost of a congested net-\nwork—the sender must perform retransmissions in order to compensate for dropped\n(lost) packets due to buffer overflow.\nFinally, let us consider the case that the sender may time out prematurely and\nretransmit a packet that has been delayed in the queue but not yet lost. In this case,\nboth the original data packet and the retransmission may reach the receiver. Of\nFinite shared output\nlink buffers\nHost B\nHost A\nHost D\nHost C\nλout\nλin: original data\nλ’in: original data, plus\nretransmitted data\nFigure 3.45 \u0002 Scenario 2: Two hosts (with retransmissions) and a router\nwith finite buffers\n\n3.6\n•\nPRINCIPLES OF CONGESTION CONTROL\n263\ncourse, the receiver needs but one copy of this packet and will discard the retrans-\nmission. In this case, the work done by the router in forwarding the retransmitted\ncopy of the original packet was wasted, as the receiver will have already received\nthe original copy of this packet. The router would have better used the link trans-\nmission capacity to send a different packet instead. Here then is yet another cost of\na congested network—unneeded retransmissions by the sender in the face of large\ndelays may cause a router to use its link bandwidth to forward unneeded copies of a\npacket. Figure 3.46 (c) shows the throughput versus offered load when each packet\nis assumed to be forwarded (on average) twice by the router. Since each packet is\nforwarded twice, the throughput will have an asymptotic value of R/4 as the offered\nload approaches R/2.\nScenario 3: Four Senders, Routers with Finite Buffers, and \nMultihop Paths\nIn our final congestion scenario, four hosts transmit packets, each over overlap-\nping two-hop paths, as shown in Figure 3.47. We again assume that each host\nuses a timeout/retransmission mechanism to implement a reliable data transfer\nservice, that all hosts have the same value of \u0006in, and that all router links have\ncapacity R bytes/sec.\nLet’s consider the connection from Host A to Host C, passing through routers\nR1 and R2. The A–C connection shares router R1 with the D–B connection and\nshares router R2 with the B–D connection. For extremely small values of \u0006in, buffer\noverflows are rare (as in congestion scenarios 1 and 2), and the throughput approxi-\nmately equals the offered load. For slightly larger values of \u0006in, the corresponding\nthroughput is also larger, since more original data is being transmitted into the\nR/2\nR/2\nR/2\nλout\na.\nb.\nR/2\nλout\nR/3\nR/2\nR/2\nλout\nR/4\nc.\nλ’in\nλ’in\nλ’in\nFigure 3.46 \u0002 Scenario 2 performance with finite buffers\n\n264\nCHAPTER 3\n•\nTRANSPORT LAYER\nnetwork and delivered to the destination, and overflows are still rare. Thus, for small\nvalues of \u0006in, an increase in \u0006in results in an increase in \u0006out.\nHaving considered the case of extremely low traffic, let’s next examine the\ncase that \u0006in (and hence \u0006\u0007in) is extremely large. Consider router R2. The A–C\ntraffic arriving to router R2 (which arrives at R2 after being forwarded from R1)\ncan have an arrival rate at R2 that is at most R, the capacity of the link from R1\nto R2, regardless of the value of \u0006in. If \u0006\u0007in is extremely large for all connections\n(including the B–D connection), then the arrival rate of B–D traffic at R2 can be\nmuch larger than that of the A–C traffic. Because the A–C and B–D traffic must\ncompete at router R2 for the limited amount of buffer space, the amount of A–C\ntraffic that successfully gets through R2 (that is, is not lost due to buffer over-\nflow) becomes smaller and smaller as the offered load from B–D gets larger and\nlarger. In the limit, as the offered load approaches infinity, an empty buffer at R2\nHost B\nHost A\nR1\nR4\nR2\nR3\nHost C\nHost D\nFinite shared output\nlink buffers\nλin: original data\nλ’in : original\ndata, plus\nretransmitted\ndata\nλout\nFigure 3.47 \u0002 Four senders, routers with finite buffers, and multihop paths"
    },
    {
      "chunk_id": "e3727e7b-b000-4f28-9b6a-467aa6a04b31",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.6.2 Approaches to Congestion Control",
      "original_titles": [
        "3.6.2 Approaches to Congestion Control"
      ],
      "path": "Chapter 3 Transport Layer > 3.6 Principles of Congestion Control > 3.6.2 Approaches to Congestion Control",
      "start_page": 292,
      "end_page": 292,
      "token_count": 413,
      "text": "3.6\n•\nPRINCIPLES OF CONGESTION CONTROL\n265\nis immediately filled by a B–D packet, and the throughput of the A–C connection\nat R2 goes to zero. This, in turn, implies that the A–C end-to-end throughput goes\nto zero in the limit of heavy traffic. These considerations give rise to the offered\nload versus throughput tradeoff shown in Figure 3.48.\nThe reason for the eventual decrease in throughput with increasing offered\nload is evident when one considers the amount of wasted work done by the net-\nwork. In the high-traffic scenario outlined above, whenever a packet is dropped\nat a second-hop router, the work done by the first-hop router in forwarding a\npacket to the second-hop router ends up being “wasted.” The network would\nhave been equally well off (more accurately, equally bad off) if the first router\nhad simply discarded that packet and remained idle. More to the point, the trans-\nmission capacity used at the first router to forward the packet to the second router\ncould have been much more profitably used to transmit a different packet. (For\nexample, when selecting a packet for transmission, it might be better for a router\nto give priority to packets that have already traversed some number of upstream\nrouters.) So here we see yet another cost of dropping a packet due to conges-\ntion—when a packet is dropped along a path, the transmission capacity that was\nused at each of the upstream links to forward that packet to the point at which it\nis dropped ends up having been wasted.\n3.6.2 Approaches to Congestion Control\nIn Section 3.7, we’ll examine TCP’s specific approach to congestion control in great\ndetail. Here, we identify the two broad approaches to congestion control that are\ntaken in practice and discuss specific network architectures and congestion-control\nprotocols embodying these approaches.\nR/2\nλout\nλ’in\nFigure 3.48 \u0002 Scenario 3 performance with finite buffers and multihop\npaths"
    },
    {
      "chunk_id": "35716d76-17f2-4c85-a292-3f459d1e290a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.6.3 Network-Assisted Congestion-Control Example: ATM ABR Congestion Control",
      "original_titles": [
        "3.6.3 Network-Assisted Congestion-Control Example: ATM ABR Congestion Control"
      ],
      "path": "Chapter 3 Transport Layer > 3.6 Principles of Congestion Control > 3.6.3 Network-Assisted Congestion-Control Example: ATM ABR Congestion Control",
      "start_page": 293,
      "end_page": 295,
      "token_count": 1461,
      "text": "266\nCHAPTER 3\n•\nTRANSPORT LAYER\nAt the broadest level, we can distinguish among congestion-control approaches\nby whether the network layer provides any explicit assistance to the transport layer\nfor congestion-control purposes:\n•\nEnd-to-end congestion control. In an end-to-end approach to congestion control,\nthe network layer provides no explicit support to the transport layer for congestion-\ncontrol purposes. Even the presence of congestion in the network must be inferred\nby the end systems based only on observed network behavior (for example, packet\nloss and delay). We will see in Section 3.7 that TCP must necessarily take this end-\nto-end approach toward congestion control, since the IP layer provides no feedback\nto the end systems regarding network congestion. TCP segment loss (as indicated\nby a timeout or a triple duplicate acknowledgment) is taken as an indication of net-\nwork congestion and TCP decreases its window size accordingly. We will also see\na more recent proposal for TCP congestion control that uses increasing round-trip\ndelay values as indicators of increased network congestion.\n•\nNetwork-assisted congestion control. With network-assisted congestion control,\nnetwork-layer components (that is, routers) provide explicit feedback to the\nsender regarding the congestion state in the network. This feedback may be as\nsimple as a single bit indicating congestion at a link. This approach was taken in\nthe early IBM SNA [Schwartz 1982] and DEC DECnet [Jain 1989; Ramakrish-\nnan 1990] architectures, was recently proposed for TCP/IP networks [Floyd TCP\n1994; RFC 3168], and is used in ATM available bit-rate (ABR) congestion con-\ntrol as well, as discussed below. More sophisticated network feedback is also pos-\nsible. For example, one form of ATM ABR congestion control that we will study\nshortly allows a router to inform the sender explicitly of the transmission rate it\n(the router) can support on an outgoing link. The XCP protocol [Katabi 2002] pro-\nvides router-computed feedback to each source, carried in the packet header,\nregarding how that source should increase or decrease its transmission rate.\nFor network-assisted congestion control, congestion information is typically fed\nback from the network to the sender in one of two ways, as shown in Figure 3.49.\nDirect feedback may be sent from a network router to the sender. This form of notifi-\ncation typically takes the form of a choke packet (essentially saying, “I’m con-\ngested!”). The second form of notification occurs when a router marks/updates a field\nin a packet flowing from sender to receiver to indicate congestion. Upon receipt of a\nmarked packet, the receiver then notifies the sender of the congestion indication.\nNote that this latter form of notification takes at least a full round-trip time.\n3.6.3 Network-Assisted Congestion-Control Example:\nATM ABR Congestion Control\nWe conclude this section with a brief case study of the congestion-control algorithm\nin ATM ABR—a protocol that takes a network-assisted approach toward congestion\ncontrol. We stress that our goal here is not to describe aspects of the ATM architecture\n\nin great detail, but rather to illustrate a protocol that takes a markedly different\napproach toward congestion control from that of the Internet’s TCP protocol. Indeed,\nwe only present below those few aspects of the ATM architecture that are needed to\nunderstand ABR congestion control.\nFundamentally ATM takes a virtual-circuit (VC) oriented approach toward\npacket switching. Recall from our discussion in Chapter 1, this means that each\nswitch on the source-to-destination path will maintain state about the source-to-\ndestination VC. This per-VC state allows a switch to track the behavior of indi-\nvidual senders (e.g., tracking their average transmission rate) and to take\nsource-specific congestion-control actions (such as explicitly signaling to the\nsender to reduce its rate when the switch becomes congested). This per-VC state\nat network switches makes ATM ideally suited to perform network-assisted con-\ngestion control.\nABR has been designed as an elastic data transfer service in a manner reminis-\ncent of TCP. When the network is underloaded, ABR service should be able to take\nadvantage of the spare available bandwidth; when the network is congested, ABR\nservice should throttle its transmission rate to some predetermined minimum trans-\nmission rate. A detailed tutorial on ATM ABR congestion control and traffic man-\nagement is provided in [Jain 1996].\nFigure 3.50 shows the framework for ATM ABR congestion control. In our\ndiscussion we adopt ATM terminology (for example, using the term switch rather\nthan router, and the term cell rather than packet). With ATM ABR service, data\ncells are transmitted from a source to a destination through a series of intermedi-\nate switches. Interspersed with the data cells are resource-management cells\n3.6\n•\nPRINCIPLES OF CONGESTION CONTROL\n267\nHost A\nNetwork feedback via receiver\nDirect network\nfeedback\nHost B\nFigure 3.49 \u0002 Two feedback pathways for network-indicated congestion\ninformation\n\n268\nCHAPTER 3\n•\nTRANSPORT LAYER\n(RM cells); these RM cells can be used to convey congestion-related information\namong the hosts and switches. When an RM cell arrives at a destination, it will\nbe turned around and sent back to the sender (possibly after the destination has\nmodified the contents of the RM cell). It is also possible for a switch to generate\nan RM cell itself and send this RM cell directly to a source. RM cells can thus be\nused to provide both direct network feedback and network feedback via the\nreceiver, as shown in Figure 3.50.\nATM ABR congestion control is a rate-based approach. That is, the sender\nexplicitly computes a maximum rate at which it can send and regulates itself accord-\ningly. ABR provides three mechanisms for signaling congestion-related information\nfrom the switches to the receiver:\n•\nEFCI bit. Each data cell contains an explicit forward congestion indication\n(EFCI) bit. A congested network switch can set the EFCI bit in a data cell to\n1 to signal congestion to the destination host. The destination must check the\nEFCI bit in all received data cells. When an RM cell arrives at the destination,\nif the most recently received data cell had the EFCI bit set to 1, then the desti-\nnation sets the congestion indication bit (the CI bit) of the RM cell to 1 and\nsends the RM cell back to the sender. Using the EFCI in data cells and the CI\nbit in RM cells, a sender can thus be notified about congestion at a network\nswitch.\n•\nCI and NI bits. As noted above, sender-to-receiver RM cells are interspersed\nwith data cells. The rate of RM cell interspersion is a tunable parameter, with\nthe default value being one RM cell every 32 data cells. These RM cells have a\ncongestion indication (CI) bit and a no increase (NI) bit that can be set by a\nSource\nDestination\nSwitch\nSwitch\nKey:\nRM cells\nData cells\nFigure 3.50 \u0002 Congestion-control framework for ATM ABR service"
    },
    {
      "chunk_id": "cc8124dd-08c1-4c1a-83d5-c083e1de1e93",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.7 TCP Congestion Control",
      "original_titles": [
        "3.7 TCP Congestion Control"
      ],
      "path": "Chapter 3 Transport Layer > 3.7 TCP Congestion Control",
      "start_page": 296,
      "end_page": 305,
      "token_count": 5267,
      "text": "congested network switch. Specifically, a switch can set the NI bit in a passing\nRM cell to 1 under mild congestion and can set the CI bit to 1 under severe\ncongestion conditions. When a destination host receives an RM cell, it will\nsend the RM cell back to the sender with its CI and NI bits intact (except that\nCI may be set to 1 by the destination as a result of the EFCI mechanism\ndescribed above).\n•\nER setting. Each RM cell also contains a 2-byte explicit rate (ER) field. A con-\ngested switch may lower the value contained in the ER field in a passing RM\ncell. In this manner, the ER field will be set to the minimum supportable rate of\nall switches on the source-to-destination path.\nAn ATM ABR source adjusts the rate at which it can send cells as a function of\nthe CI, NI, and ER values in a returned RM cell. The rules for making this rate\nadjustment are rather complicated and a bit tedious. The interested reader is referred\nto [Jain 1996] for details.\n3.7 TCP Congestion Control\nIn this section we return to our study of TCP. As we learned in Section 3.5, TCP pro-\nvides a reliable transport service between two processes running on different hosts.\nAnother key component of TCP is its congestion-control mechanism. As indicated\nin the previous section, TCP must use end-to-end congestion control rather than net-\nwork-assisted congestion control, since the IP layer provides no explicit feedback to\nthe end systems regarding network congestion.\nThe approach taken by TCP is to have each sender limit the rate at which it\nsends traffic into its connection as a function of perceived network congestion. If a\nTCP sender perceives that there is little congestion on the path between itself and\nthe destination, then the TCP sender increases its send rate; if the sender perceives\nthat there is congestion along the path, then the sender reduces its send rate. But this\napproach raises three questions. First, how does a TCP sender limit the rate at which\nit sends traffic into its connection? Second, how does a TCP sender perceive that\nthere is congestion on the path between itself and the destination? And third, what\nalgorithm should the sender use to change its send rate as a function of perceived\nend-to-end congestion?\nLet’s first examine how a TCP sender limits the rate at which it sends traffic\ninto its connection. In Section 3.5 we saw that each side of a TCP connection consists\nof a receive buffer, a send buffer, and several variables (LastByteRead, rwnd,\nand so on). The TCP congestion-control mechanism operating at the sender keeps\ntrack of an additional variable, the congestion window. The congestion window,\ndenoted cwnd, imposes a constraint on the rate at which a TCP sender can send traffic\n3.7\n•\nTCP CONGESTION CONTROL\n269\n\n270\nCHAPTER 3\n•\nTRANSPORT LAYER\ninto the network. Specifically, the amount of unacknowledged data at a sender may\nnot exceed the minimum of cwnd and rwnd, that is:\nLastByteSent – LastByteAcked \u0005 min{cwnd, rwnd}\nIn order to focus on congestion control (as opposed to flow control), let us hence-\nforth assume that the TCP receive buffer is so large that the receive-window con-\nstraint can be ignored; thus, the amount of unacknowledged data at the sender is\nsolely limited by cwnd. We will also assume that the sender always has data to\nsend, i.e., that all segments in the congestion window are sent.\nThe constraint above limits the amount of unacknowledged data at the sender\nand therefore indirectly limits the sender’s send rate. To see this, consider a connec-\ntion for which loss and packet transmission delays are negligible. Then, roughly, at\nthe beginning of every RTT, the constraint permits the sender to send cwnd bytes of\ndata into the connection; at the end of the RTT the sender receives acknowledg-\nments for the data. Thus the sender’s send rate is roughly cwnd/RTT bytes/sec. By\nadjusting the value of cwnd, the sender can therefore adjust the rate at which it\nsends data into its connection.\nLet’s next consider how a TCP sender perceives that there is congestion on\nthe path between itself and the destination. Let us define a “loss event” at a TCP\nsender as the occurrence of either a timeout or the receipt of three duplicate\nACKs from the receiver. (Recall our discussion in Section 3.5.4 of the timeout\nevent in Figure 3.33 and the subsequent modification to include fast retransmit\non receipt of three duplicate ACKs.) When there is excessive congestion, then\none (or more) router buffers along the path overflows, causing a datagram (con-\ntaining a TCP segment) to be dropped. The dropped datagram, in turn, results in\na loss event at the sender—either a timeout or the receipt of three duplicate\nACKs—which is taken by the sender to be an indication of congestion on the\nsender-to-receiver path.\nHaving considered how congestion is detected, let’s next consider the more\noptimistic case when the network is congestion-free, that is, when a loss event\ndoesn’t occur. In this case, acknowledgments for previously unacknowledged\nsegments will be received at the TCP sender. As we’ll see, TCP will take the\narrival of these acknowledgments as an indication that all is well—that segments\nbeing transmitted into the network are being successfully delivered to the\ndestination—and will use acknowledgments to increase its congestion window\nsize (and hence its transmission rate). Note that if acknowledgments arrive at a\nrelatively slow rate (e.g., if the end-end path has high delay or contains a \nlow-bandwidth link), then the congestion window will be increased at a relatively\nslow rate. On the other hand, if acknowledgments arrive at a high rate, then the\ncongestion window will be increased more quickly. Because TCP uses\n\nacknowledgments to trigger (or clock) its increase in congestion window size,\nTCP is said to be self-clocking.\nGiven the mechanism of adjusting the value of cwnd to control the sending rate,\nthe critical question remains: How should a TCP sender determine the rate at which\nit should send? If TCP senders collectively send too fast, they can congest the net-\nwork, leading to the type of congestion collapse that we saw in Figure 3.48. Indeed,\nthe version of TCP that we’ll study shortly was developed in response to observed\nInternet congestion collapse [Jacobson 1988] under earlier versions of TCP. How-\never, if TCP senders are too cautious and send too slowly, they could under utilize\nthe bandwidth in the network; that is, the TCP senders could send at a higher rate\nwithout congesting the network. How then do the TCP senders determine their send-\ning rates such that they don’t congest the network but at the same time make use of\nall the available bandwidth? Are TCP senders explicitly coordinated, or is there a\ndistributed approach in which the TCP senders can set their sending rates based only\non local information? TCP answers these questions using the following guiding\nprinciples:\n•\nA lost segment implies congestion, and hence, the TCP sender’s rate should be\ndecreased when a segment is lost. Recall from our discussion in Section 3.5.4,\nthat a timeout event or the receipt of four acknowledgments for a given seg-\nment (one original ACK and then three duplicate ACKs) is interpreted as an\nimplicit “loss event” indication of the segment following the quadruply ACKed\nsegment, triggering a retransmission of the lost segment. From a congestion-\ncontrol standpoint, the question is how the TCP sender should decrease its con-\ngestion window size, and hence its sending rate, in response to this inferred\nloss event.\n•\nAn acknowledged segment indicates that the network is delivering the sender’s\nsegments to the receiver, and hence, the sender’s rate can be increased when an\nACK arrives for a previously unacknowledged segment. The arrival of acknowl-\nedgments is taken as an implicit indication that all is well—segments are being\nsuccessfully delivered from sender to receiver, and the network is thus not con-\ngested. The congestion window size can thus be increased.\n•\nBandwidth probing. Given ACKs indicating a congestion-free source-to-destination\npath and loss events indicating a congested path, TCP’s strategy for adjusting its\ntransmission rate is to increase its rate in response to arriving ACKs until a loss\nevent occurs, at which point, the transmission rate is decreased. The TCP sender\nthus increases its transmission rate to probe for the rate that at which congestion\nonset begins, backs off from that rate, and then to begins probing again to see if\nthe congestion onset rate has changed. The TCP sender’s behavior is perhaps anal-\nogous to the child who requests (and gets) more and more goodies until finally\nhe/she is finally told “No!”, backs off a bit, but then begins making requests\n3.7\n•\nTCP CONGESTION CONTROL\n271\n\n272\nCHAPTER 3\n•\nTRANSPORT LAYER\nagain shortly afterwards. Note that there is no explicit signaling of congestion\nstate by the network—ACKs and loss events serve as implicit signals—and that\neach TCP sender acts on local information asynchronously from other TCP\nsenders.\nGiven this overview of TCP congestion control, we’re now in a position to consider\nthe details of the celebrated TCP congestion-control algorithm, which was first\ndescribed in [Jacobson 1988] and is standardized in [RFC 5681]. The algorithm has\nthree major components: (1) slow start, (2) congestion avoidance, and (3) fast recov-\nery. Slow start and congestion avoidance are mandatory components of TCP, differ-\ning in how they increase the size of cwnd in response to received ACKs. We’ll see\nshortly that slow start increases the size of cwnd more rapidly (despite its name!)\nthan congestion avoidance. Fast recovery is recommended, but not required, for\nTCP senders.\nSlow Start\nWhen a TCP connection begins, the value of cwnd is typically initialized to a\nsmall value of 1 MSS [RFC 3390], resulting in an initial sending rate of roughly\nMSS/RTT. For example, if MSS = 500 bytes and RTT = 200 msec, the resulting\ninitial sending rate is only about 20 kbps. Since the available bandwidth to the\nTCP sender may be much larger than MSS/RTT, the TCP sender would like to\nfind the amount of available bandwidth quickly. Thus, in the slow-start state, the\nvalue of cwnd begins at 1 MSS and increases by 1 MSS every time a transmitted\nsegment is first acknowledged. In the example of Figure 3.51, TCP sends the first\nsegment into the network and waits for an acknowledgment. When this acknowl-\nedgment arrives, the TCP sender increases the congestion window by one MSS\nand sends out two maximum-sized segments. These segments are then acknowl-\nedged, with the sender increasing the congestion window by 1 MSS for each of\nthe acknowledged segments, giving a congestion window of 4 MSS, and so on.\nThis process results in a doubling of the sending rate every RTT. Thus, the TCP\nsend rate starts slow but grows exponentially during the slow start phase.\nBut when should this exponential growth end? Slow start provides several\nanswers to this question. First, if there is a loss event (i.e., congestion) indicated\nby a timeout, the TCP sender sets the value of cwnd to 1 and begins the slow\nstart process anew. It also sets the value of a second state variable, ssthresh\n(shorthand for “slow start threshold”) to cwnd/2—half of the value of the con-\ngestion window value when congestion was detected. The second way in which\nslow start may end is directly tied to the value of ssthresh. Since ssthresh\nis half the value of cwnd when congestion was last detected, it might be a bit\nreckless to keep doubling cwnd when it reaches or surpasses the value of\nssthresh. Thus, when the value of cwnd equals ssthresh, slow start ends\nand TCP transitions into congestion avoidance mode. As we’ll see, TCP increases\n\n3.7\n•\nTCP CONGESTION CONTROL\n273\nTCP SPLITTING: OPTIMIZING THE PERFORMANCE OF CLOUD SERVICES\nFor cloud services such as search, e-mail, and social networks, it is desirable to provide a\nhigh-level of responsiveness, ideally giving users the illusion that the services are running\nwithin their own end systems (including their smartphones). This can be a major challenge,\nas users are often located far away from the data centers that are responsible for serving\nthe dynamic content associated with the cloud services. Indeed, if the end system is far\nfrom a data center, then the RTT will be large, potentially leading to poor response time\nperformance due to TCP slow start. \nAs a case study, consider the delay in receiving a response for a search query.\nTypically, the server requires three TCP windows during slow start to deliver the\nresponse [Pathak 2010]. Thus the time from when an end system initiates a TCP con-\nnection until the time when it receives the last packet of the response is roughly 4\nRTT\n(one RTT to set up the TCP connection plus three RTTs for the three windows of data)\nplus the processing time in the data center. These RTT delays can lead to a noticeable\ndelay in returning search results for a significant fraction of queries. Moreover, there\ncan be significant packet loss in access networks, leading to TCP retransmissions and\neven larger delays.\nOne way to mitigate this problem and improve user-perceived performance is to (1)\ndeploy front-end servers closer to the users, and (2) utilize TCP splitting by breaking the\nTCP connection at the front-end server. With TCP splitting, the client establishes a TCP con-\nnection to the nearby front-end, and the front-end maintains a persistent TCP connection to\nthe data center with a very large TCP congestion window [Tariq 2008, Pathak 2010,\nChen 2011]. With this approach, the response time roughly becomes 4\nRTTFE\nRTTBE\nprocessing time, where RTTFE is the round-trip time between client and front-end server, and\nRTTBE is the round-trip time between the front-end server and the data center (back-end server).\nIf the front-end server is close to client, then this response time approximately becomes\nRTT plus processing time, since RTTFE is negligibly small and RTTBE is approximately RTT. In\nsummary, TCP splitting can reduce the networking delay roughly from 4\nRTT to RTT, signifi-\ncantly improving user-perceived performance, particularly for users who are far from the\nnearest data center. TCP splitting also helps reduce TCP retransmission delays caused by\nlosses in access networks. Today, Google and Akamai make extensive use of their CDN\nservers in access networks (see Section 7.2) to perform TCP splitting for the cloud services\nthey support [Chen 2011].\n\b\n+\n+\n\b\n\b\nPRINCIPLES IN PRACTICE\ncwnd more cautiously when in congestion-avoidance mode. The final way in\nwhich slow start can end is if three duplicate ACKs are detected, in which case\nTCP performs a fast retransmit (see Section 3.5.4) and enters the fast recovery\nstate, as discussed below. TCP’s behavior in slow start is summarized in the FSM\n\n274\nCHAPTER 3\n•\nTRANSPORT LAYER\ndescription of TCP congestion control in Figure 3.52. The slow-start algorithm\ntraces it roots to [Jacobson 1988]; an approach similar to slow start was also pro-\nposed independently in [Jain 1986].\nCongestion Avoidance\nOn entry to the congestion-avoidance state, the value of cwnd is approximately half\nits value when congestion was last encountered—congestion could be just around\nthe corner! Thus, rather than doubling the value of cwnd every RTT, TCP adopts a\nmore conservative approach and increases the value of cwnd by just a single MSS\nevery RTT [RFC 5681]. This can be accomplished in several ways. A common\napproach is for the TCP sender to increase cwnd by MSS bytes (MSS/cwnd) when-\never a new acknowledgment arrives. For example, if MSS is 1,460 bytes and cwnd\nis 14,600 bytes, then 10 segments are being sent within an RTT. Each arriving ACK\n(assuming one ACK per segment) increases the congestion window size by 1/10\nHost A\nHost B\none segment\ntwo segments\nfour segments\nRTT\nTime\nTime\nFigure 3.51 \u0002 TCP slow start\n\n3.7\n•\nTCP CONGESTION CONTROL\n275\nMSS, and thus, the value of the congestion window will have increased by one MSS\nafter ACKs when all 10 segments have been received.\nBut when should congestion avoidance’s linear increase (of 1 MSS per RTT)\nend? TCP’s congestion-avoidance algorithm behaves the same when a timeout\noccurs. As in the case of slow start: The value of cwnd is set to 1 MSS, and the\nvalue of ssthresh is updated to half the value of cwnd when the loss event\noccurred. Recall, however, that a loss event also can be triggered by a triple dupli-\ncate ACK event. In this case, the network is continuing to deliver segments from\nsender to receiver (as indicated by the receipt of duplicate ACKs). So TCP’s behav-\nior to this type of loss event should be less drastic than with a timeout-indicated loss:\nTCP halves the value of cwnd (adding in 3 MSS for good measure to account for\nSlow\nstart\nduplicate ACK\ndupACKcount++\nduplicate ACK\ndupACKcount++\ntimeout\nssthresh=cwnd/2\ncwnd=1 MSS\ndupACKcount=0\ncwnd=1 MSS\nssthresh=64 KB\ndupACKcount=0\ntimeout\nssthresh=cwnd/2\ncwnd=1\ndupACKcount=0\ntimeout\nssthresh=cwnd/2\ncwnd=1 MSS\ndupACKcount=0\ncwnd ≥ssthresh\nCongestion\navoidance\nFast\nrecovery\nnew ACK\ncwnd=cwnd+MSS •(MSS/cwnd)\ndupACKcount=0\ntransmit new segment(s), as allowed\nnew ACK\ncwnd=cwnd+MSS\ndupACKcount=0\ntransmit new segment(s), as allowed\nretransmit missing segment\nretransmit missing segment\ndupACKcount==3\nssthresh=cwnd/2\ncwnd=ssthresh+3•MSS\nretransmit missing segment\nduplicate ACK\ncwnd=cwnd+MSS\ntransmit new segment(s), as allowed\ndupACKcount==3\nssthresh=cwnd/2\ncwnd=ssthresh+3•MSS\nretransmit missing segment\nretransmit missing segment\nnew ACK\ncwnd=ssthresh\ndupACKcount=0\nΛ\nΛ\nFigure 3.52 \u0002 FSM description of TCP congestion control\n\n276\nCHAPTER 3\n•\nTRANSPORT LAYER\nthe triple duplicate ACKs received) and records the value of ssthresh to be half\nthe value of cwnd when the triple duplicate ACKs were received. The fast-recovery\nstate is then entered. \nFast Recovery\nIn fast recovery, the value of cwnd is increased by 1 MSS for every duplicate ACK\nreceived for the missing segment that caused TCP to enter the fast-recovery state.\nEventually, when an ACK arrives for the missing segment, TCP enters the\ncongestion-avoidance state after deflating cwnd. If a timeout event occurs, fast\nrecovery transitions to the slow-start state after performing the same actions as in\nslow start and congestion avoidance: The value of cwnd is set to 1 MSS, and the\nvalue of ssthresh is set to half the value of cwnd when the loss event occurred.\nFast recovery is a recommended, but not required, component of TCP [RFC\n5681]. It is interesting that an early version of TCP, known as TCP Tahoe, uncondi-\ntionally cut its congestion window to 1 MSS and entered the slow-start phase after\neither a timeout-indicated or triple-duplicate-ACK-indicated loss event. The newer\nversion of TCP, TCP Reno, incorporated fast recovery. \nFigure 3.53 illustrates the evolution of TCP’s congestion window for both Reno\nand Tahoe. In this figure, the threshold is initially equal to 8 MSS. For the first eight\ntransmission rounds, Tahoe and Reno take identical actions. The congestion window\nclimbs exponentially fast during slow start and hits the threshold at the fourth round\nof transmission. The congestion window then climbs linearly until a triple duplicate-\nACK event occurs, just after transmission round 8. Note that the congestion window\nis 12 • MSS when this loss event occurs. The value of ssthresh is then set to \n0\n1\n0\n2\n3\n4\n5\n6\n7\n8\nTransmission round\nTCP Tahoe\nssthresh\nssthresh\nCongestion window\n(in segments)\n9\n10 11 12 13 14 15\n2\n4\n6\n8\n10\n12\n14\n16\nTCP Reno\nFigure 3.53 \u0002 Evolution of TCP’s congestion window (Tahoe and Reno)\nVideoNote\nExamining the \nbehavior of TCP\n\n3.7\n•\nTCP CONGESTION CONTROL\n277\n0.5 • cwnd = 6 • MSS. Under TCP Reno, the congestion window is set to cwnd =\n6 • MSS and then grows linearly. Under TCP Tahoe, the congestion window is set to\n1 MSS and grows exponentially until it reaches the value of ssthresh, at which\npoint it grows linearly. \nFigure 3.52 presents the complete FSM description of TCP’s congestion-\ncontrol algorithms—slow start, congestion avoidance, and fast recovery. The figure\nalso indicates where transmission of new segments or retransmitted segments can\noccur. Although it is important to distinguish between TCP error control/retransmis-\nsion and TCP congestion control, it’s also important to appreciate how these two\naspects of TCP are inextricably linked.\nTCP Congestion Control: Retrospective\nHaving delved into the details of slow start, congestion avoidance, and fast recov-\nery, it’s worthwhile to now step back and view the forest from the trees. Ignoring the\ninitial slow-start period when a connection begins and assuming that losses are indi-\ncated by triple duplicate ACKs rather than timeouts, TCP’s congestion control con-\nsists of linear (additive) increase in cwnd of 1 MSS per RTT and then a halving\n(multiplicative decrease) of cwnd on a triple duplicate-ACK event. For this reason,\nTCP congestion control is often referred to as an additive-increase, multiplicative-\ndecrease (AIMD) form of congestion control. AIMD congestion control gives rise\nto the “saw tooth” behavior shown in Figure 3.54, which also nicely illustrates our\nearlier intuition of TCP “probing” for bandwidth—TCP linearly increases its con-\ngestion window size (and hence its transmission rate) until a triple duplicate-ACK\nevent occurs. It then decreases its congestion window size by a factor of two but\nthen again begins increasing it linearly, probing to see if there is additional available\nbandwidth.\n24 K\n16 K\n8 K\nTime\nCongestion window\nFigure 3.54 \u0002 Additive-increase, multiplicative-decrease congestion control\n\n278\nCHAPTER 3\n•\nTRANSPORT LAYER\nAs noted previously, many TCP implementations use the Reno algorithm [Padhye\n2001]. Many variations of the Reno algorithm have been proposed [RFC 3782; RFC\n2018]. The TCP Vegas algorithm [Brakmo 1995; Ahn 1995] attempts to avoid conges-\ntion while maintaining good throughput. The basic idea of Vegas is to (1) detect con-\ngestion in the routers between source and destination before packet loss occurs, and (2)\nlower the rate linearly when this imminent packet loss is detected. Imminent packet loss\nis predicted by observing the RTT. The longer the RTT of the packets, the greater the\ncongestion in the routers. Linux supports a number of congestion-control algorithms\n(including TCP Reno and TCP Vegas) and allows a system administrator to configure\nwhich version of TCP will be used. The default version of TCP in Linux version 2.6.18\nwas set to CUBIC [Ha 2008], a version of TCP developed for high-bandwidth applica-\ntions. For a recent survey of the many flavors of TCP, see [Afanasyev 2010].\nTCP’s AIMD algorithm was developed based on a tremendous amount of engi-\nneering insight and experimentation with congestion control in operational net-\nworks. Ten years after TCP’s development, theoretical analyses showed that TCP’s\ncongestion-control algorithm serves as a distributed asynchronous-optimization\nalgorithm that results in several important aspects of user and network performance\nbeing simultaneously optimized [Kelly 1998]. A rich theory of congestion control\nhas since been developed [Srikant 2004].\nMacroscopic Description of TCP Throughput\nGiven the saw-toothed behavior of TCP, it’s natural to consider what the average\nthroughput (that is, the average rate) of a long-lived TCP connection might be. In this\nanalysis we’ll ignore the slow-start phases that occur after timeout events. (These\nphases are typically very short, since the sender grows out of the phase exponentially\nfast.) During a particular round-trip interval, the rate at which TCP sends data is a\nfunction of the congestion window and the current RTT. When the window size is w\nbytes and the current round-trip time is RTT seconds, then TCP’s transmission rate is\nroughly w/RTT. TCP then probes for additional bandwidth by increasing w by 1 MSS\neach RTT until a loss event occurs. Denote by W the value of w when a loss event\noccurs. Assuming that RTT and W are approximately constant over the duration of\nthe connection, the TCP transmission rate ranges from W/(2 · RTT) to W/RTT.\nThese assumptions lead to a highly simplified macroscopic model for the\nsteady-state behavior of TCP. The network drops a packet from the connection when\nthe rate increases to W/RTT; the rate is then cut in half and then increases by\nMSS/RTT every RTT until it again reaches W/RTT. This process repeats itself over\nand over again. Because TCP’s throughput (that is, rate) increases linearly between\nthe two extreme values, we have\naverage throughput of a connection = 0.75 \b W\nRTT"
    },
    {
      "chunk_id": "0a3ca0c4-b1f3-4477-8e29-97ab8395c54f",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.7.1 Fairness",
      "original_titles": [
        "3.7.1 Fairness"
      ],
      "path": "Chapter 3 Transport Layer > 3.7 TCP Congestion Control > 3.7.1 Fairness",
      "start_page": 306,
      "end_page": 309,
      "token_count": 2130,
      "text": "3.7\n•\nTCP CONGESTION CONTROL\n279\nUsing this highly idealized model for the steady-state dynamics of TCP, we can\nalso derive an interesting expression that relates a connection’s loss rate to its avail-\nable bandwidth [Mahdavi 1997]. This derivation is outlined in the homework prob-\nlems. A more sophisticated model that has been found empirically to agree with\nmeasured data is [Padhye 2000].\nTCP Over High-Bandwidth Paths\nIt is important to realize that TCP congestion control has evolved over the years and\nindeed continues to evolve. For a summary of  current TCP variants and discussion\nof TCP evolution, see [Floyd 2001, RFC 5681,  Afanasyev 2010]. What was good\nfor the Internet when the bulk of the TCP connections carried SMTP, FTP, and Tel-\nnet traffic is not necessarily good for today’s HTTP-dominated Internet or for a\nfuture Internet with services that are still undreamed of.\nThe need for continued evolution of TCP can be illustrated by considering the\nhigh-speed TCP connections that are needed for grid- and cloud-computing applica-\ntions. For example, consider a TCP connection with 1,500-byte segments and a 100\nms RTT, and suppose we want to send data through this connection at 10 Gbps.\nFollowing [RFC 3649], we note that using the TCP throughput formula above, in\norder to achieve a 10 Gbps throughput, the average congestion window size would\nneed to be 83,333 segments. That’s a lot of segments, leading us to be rather con-\ncerned that one of these 83,333 in-flight segments might be lost. What would happen\nin the case of a loss? Or, put another way, what fraction of the transmitted segments\ncould be lost that would allow the TCP congestion-control algorithm specified in Fig-\nure 3.52 still to achieve the desired 10 Gbps rate? In the homework questions for this\nchapter, you are led through the derivation of a formula relating the throughput of a\nTCP connection as a function of the loss rate (L), the round-trip time (RTT), and the\nmaximum segment size (MSS):\nUsing this formula, we can see that in order to achieve a throughput of 10 Gbps,\ntoday’s TCP congestion-control algorithm can only tolerate a segment loss probabil-\nity of 2 · 10–10 (or equivalently, one loss event for every 5,000,000,000 segments)—\na very low rate. This observation has led a number of researchers to investigate new\nversions of TCP that are specifically designed for such high-speed environments;\nsee [Jin 2004; RFC 3649; Kelly 2003; Ha 2008] for discussions of these efforts.\n3.7.1 Fairness\nConsider K TCP connections, each with a different end-to-end path, but all passing\nthrough a bottleneck link with transmission rate R bps. (By bottleneck link, we mean\naverage throughput of a connection = 1.22 \b MSS\nRTT 2L\n\n280\nCHAPTER 3\n•\nTRANSPORT LAYER\nthat for each connection, all the other links along the connection’s path are not con-\ngested and have abundant transmission capacity as compared with the transmission\ncapacity of the bottleneck link.) Suppose each connection is transferring a large file\nand there is no UDP traffic passing through the bottleneck link. A congestion-con-\ntrol mechanism is said to be fair if the average transmission rate of each connection\nis approximately R/K; that is, each connection gets an equal share of the link band-\nwidth.\nIs TCP’s AIMD algorithm fair, particularly given that different TCP connec-\ntions may start at different times and thus may have different window sizes at a\ngiven point in time? [Chiu 1989] provides an elegant and intuitive explanation of\nwhy TCP congestion control converges to provide an equal share of a bottleneck\nlink’s bandwidth among competing TCP connections.\nLet’s consider the simple case of two TCP connections sharing a single link\nwith transmission rate R, as shown in Figure 3.55. Assume that the two connections\nhave the same MSS and RTT (so that if they have the same congestion window size,\nthen they have the same throughput), that they have a large amount of data to send,\nand that no other TCP connections or UDP datagrams traverse this shared link. Also,\nignore the slow-start phase of TCP and assume the TCP connections are operating\nin CA mode (AIMD) at all times.\nFigure 3.56 plots the throughput realized by the two TCP connections. If TCP is\nto share the link bandwidth equally between the two connections, then the realized\nthroughput should fall along the 45-degree arrow (equal bandwidth share) emanat-\ning from the origin. Ideally, the sum of the two throughputs should equal R. (Cer-\ntainly, each connection receiving an equal, but zero, share of the link capacity is not\na desirable situation!) So the goal should be to have the achieved throughputs fall\nsomewhere near the intersection of the equal bandwidth share line and the full band-\nwidth utilization line in Figure 3.56.\nSuppose that the TCP window sizes are such that at a given point in time, con-\nnections 1 and 2 realize throughputs indicated by point A in Figure 3.56. Because\nthe amount of link bandwidth jointly consumed by the two connections is less than\nTCP connection 2\nTCP connection 1\nBottleneck\nrouter capacity R\nFigure 3.55 \u0002 Two TCP connections sharing a single bottleneck link\n\n3.7\n•\nTCP CONGESTION CONTROL\n281\nR, no loss will occur, and both connections will increase their window by 1 MSS\nper RTT as a result of TCP’s congestion-avoidance algorithm. Thus, the joint\nthroughput of the two connections proceeds along a 45-degree line (equal increase\nfor both connections) starting from point A. Eventually, the link bandwidth jointly\nconsumed by the two connections will be greater than R, and eventually packet loss\nwill occur. Suppose that connections 1 and 2 experience packet loss when they\nrealize throughputs indicated by point B. Connections 1 and 2 then decrease their\nwindows by a factor of two. The resulting throughputs realized are thus at point C,\nhalfway along a vector starting at B and ending at the origin. Because the joint\nbandwidth use is less than R at point C, the two connections again increase their\nthroughputs along a 45-degree line starting from C. Eventually, loss will again\noccur, for example, at point D, and the two connections again decrease their win-\ndow sizes by a factor of two, and so on. You should convince yourself that the\nbandwidth realized by the two connections eventually fluctuates along the equal\nbandwidth share line. You should also convince yourself that the two connections\nwill converge to this behavior regardless of where they are in the two-dimensional\nspace! Although a number of idealized assumptions lie behind this scenario, it still\nprovides an intuitive feel for why TCP results in an equal sharing of bandwidth\namong connections.\nIn our idealized scenario, we assumed that only TCP connections traverse \nthe bottleneck link, that the connections have the same RTT value, and that only a\nR\nR\nEqual\nbandwidth\nshare\nConnection 1 throughput\nConnection 2 throughput\nD\nB\nC\nA\nFull bandwidth\nutilization line\nFigure 3.56 \u0002 Throughput realized by TCP connections 1 and 2\n\n282\nCHAPTER 3\n•\nTRANSPORT LAYER\nsingle TCP connection is associated with a host-destination pair. In practice, these\nconditions are typically not met, and client-server applications can thus obtain very\nunequal portions of link bandwidth. In particular, it has been shown that when mul-\ntiple connections share a common bottleneck, those sessions with a smaller RTT are\nable to grab the available bandwidth at that link more quickly as it becomes free\n(that is, open their congestion windows faster) and thus will enjoy higher through-\nput than those connections with larger RTTs [Lakshman 1997].\nFairness and UDP\nWe have just seen how TCP congestion control regulates an application’s transmis-\nsion rate via the congestion window mechanism. Many multimedia applications,\nsuch as Internet phone and video conferencing, often do not run over TCP for this\nvery reason—they do not want their transmission rate throttled, even if the network\nis very congested. Instead, these applications prefer to run over UDP, which does\nnot have built-in congestion control. When running over UDP, applications can\npump their audio and video into the network at a constant rate and occasionally lose\npackets, rather than reduce their rates to “fair” levels at times of congestion and not\nlose any packets. From the perspective of TCP, the multimedia applications running\nover UDP are not being fair—they do not cooperate with the other connections nor\nadjust their transmission rates appropriately. Because TCP congestion control will\ndecrease its transmission rate in the face of increasing congestion (loss), while UDP\nsources need not, it is possible for UDP sources to crowd out TCP traffic. An area of\nresearch today is thus the development of congestion-control mechanisms for the\nInternet that prevent UDP traffic from bringing the Internet’s throughput to a grind-\ning halt [Floyd 1999; Floyd 2000; Kohler 2006].\nFairness and Parallel TCP Connections\nBut even if we could force UDP traffic to behave fairly, the fairness problem would\nstill not be completely solved. This is because there is nothing to stop a TCP-based\napplication from using multiple parallel connections. For example, Web browsers\noften use multiple parallel TCP connections to transfer the multiple objects within\na Web page. (The exact number of multiple connections is configurable in most\nbrowsers.) When an application uses multiple parallel connections, it gets a larger\nfraction of the bandwidth in a congested link. As an example, consider a link of rate\nR supporting nine ongoing client-server applications, with each of the applications\nusing one TCP connection. If a new application comes along and also uses one\nTCP connection, then each application gets approximately the same transmission\nrate of R/10. But if this new application instead uses 11 parallel TCP connections,\nthen the new application gets an unfair allocation of more than R/2. Because \nWeb traffic is so pervasive in the Internet, multiple parallel connections are not\nuncommon."
    },
    {
      "chunk_id": "4169d286-bf04-4016-ba80-3d0f1febf781",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "3.8 Summary",
      "original_titles": [
        "3.8 Summary"
      ],
      "path": "Chapter 3 Transport Layer > 3.8 Summary",
      "start_page": 310,
      "end_page": 311,
      "token_count": 1305,
      "text": "3.8\n•\nSUMMARY\n283\n3.8 Summary\nWe began this chapter by studying the services that a transport-layer protocol can\nprovide to network applications. At one extreme, the transport-layer protocol can be\nvery simple and offer a no-frills service to applications, providing only a multiplex-\ning/demultiplexing function for communicating processes. The Internet’s UDP pro-\ntocol is an example of such a no-frills transport-layer protocol. At the other extreme,\na transport-layer protocol can provide a variety of guarantees to applications, such\nas reliable delivery of data, delay guarantees, and bandwidth guarantees. Neverthe-\nless, the services that a transport protocol can provide are often constrained by the\nservice model of the underlying network-layer protocol. If the network-layer proto-\ncol cannot provide delay or bandwidth guarantees to transport-layer segments, then\nthe transport-layer protocol cannot provide delay or bandwidth guarantees for the\nmessages sent between processes.\nWe learned in Section 3.4 that a transport-layer protocol can provide reliable\ndata transfer even if the underlying network layer is unreliable. We saw that provid-\ning reliable data transfer has many subtle points, but that the task can be accom-\nplished by carefully combining acknowledgments, timers, retransmissions, and\nsequence numbers.\nAlthough we covered reliable data transfer in this chapter, we should keep in\nmind that reliable data transfer can be provided by link-, network-, transport-, or\napplication-layer protocols. Any of the upper four layers of the protocol stack can\nimplement acknowledgments, timers, retransmissions, and sequence numbers and\nprovide reliable data transfer to the layer above. In fact, over the years, engineers\nand computer scientists have independently designed and implemented link-, net-\nwork-, transport-, and application-layer protocols that provide reliable data transfer\n(although many of these protocols have quietly disappeared).\nIn Section 3.5, we took a close look at TCP, the Internet’s connection-oriented\nand reliable transport-layer protocol. We learned that TCP is complex, involving\nconnection management, flow control, and round-trip time estimation, as well as\nreliable data transfer. In fact, TCP is actually more complex than our description—\nwe intentionally did not discuss a variety of TCP patches, fixes, and improvements\nthat are widely implemented in various versions of TCP. All of this complexity,\nhowever, is hidden from the network application. If a client on one host wants to\nsend data reliably to a server on another host, it simply opens a TCP socket to the\nserver and pumps data into that socket. The client-server application is blissfully\nunaware of TCP’s complexity.\nIn Section 3.6, we examined congestion control from a broad perspective, and in\nSection 3.7, we showed how TCP implements congestion control. We learned that\ncongestion control is imperative for the well-being of the network. Without conges-\ntion control, a network can easily become gridlocked, with little or no data being trans-\nported end-to-end. In Section 3.7 we learned that TCP implements an end-to-end\n\n284\nCHAPTER 3\n•\nTRANSPORT LAYER\ncongestion-control mechanism that additively increases its transmission rate when the\nTCP connection’s path is judged to be congestion-free, and multiplicatively decreases\nits transmission rate when loss occurs. This mechanism also strives to give each TCP\nconnection passing through a congested link an equal share of the link bandwidth. We\nalso examined in some depth the impact of TCP connection establishment and slow\nstart on latency. We observed that in many important scenarios, connection establish-\nment and slow start significantly contribute to end-to-end delay. We emphasize once\nmore that while TCP congestion control has evolved over the years, it remains an area\nof intensive research and will likely continue to evolve in the upcoming years.\nOur discussion of specific Internet transport protocols in this chapter has\nfocused on UDP and TCP—the two “work horses” of the Internet transport layer.\nHowever, two decades of experience with these two protocols has identified\ncircumstances in which neither is ideally suited. Researchers have thus been\nbusy developing additional transport-layer protocols, several of which are now\nIETF proposed standards.\nThe Datagram Congestion Control Protocol (DCCP) [RFC 4340] provides a low-\noverhead, message-oriented, UDP-like unreliable service, but with an application-\nselected form of congestion control that is compatible with TCP. If reliable or\nsemi-reliable data transfer is needed by an application, then this would be performed\nwithin the application itself, perhaps using the mechanisms we have studied in Section\n3.4. DCCP is envisioned for use in applications such as streaming media (see Chapter 7)\nthat can exploit the tradeoff between timeliness and reliability of data delivery, but that\nwant to be responsive to network congestion.\nThe Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is a\nreliable, message-oriented protocol that allows several different application-level\n“streams” to be multiplexed through a single SCTP connection (an approach known as\n“multi-streaming”). From a reliability standpoint, the different streams within the con-\nnection are handled separately, so that packet loss in one stream does not affect the\ndelivery of data in other streams. SCTP also allows data to be transferred over two out-\ngoing paths when a host is connected to two or more networks, optional delivery of out-\nof-order data, and a number of other features. SCTP’s flow- and congestion-control\nalgorithms are essentially the same as in TCP.\nThe TCP-Friendly Rate Control (TFRC) protocol [RFC 5348] is a congestion-\ncontrol protocol rather than a full-fledged transport-layer protocol. It specifies a\ncongestion-control mechanism that could be used in anther transport protocol such as\nDCCP (indeed one of the two application-selectable protocols available in DCCP is\nTFRC). The goal of TFRC is to smooth out the “saw tooth” behavior (see Figure 3.54)\nin TCP congestion control, while maintaining a long-term sending rate that is “reason-\nably” close to that of TCP. With a smoother sending rate than TCP, TFRC is well-suited\nfor multimedia applications such as IP telephony or streaming media where such a\nsmooth rate is important. TFRC is an “equation-based” protocol that uses the measured\npacket loss rate as input to an equation [Padhye 2000] that estimates what TCP’s\nthroughput would be if a TCP session experiences that loss rate. This rate is then taken\nas TFRC’s target sending rate."
    },
    {
      "chunk_id": "eded42e5-847e-4dcf-87ae-d84ea9986fee",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Homework Problems and Questions",
      "original_titles": [
        "Homework Problems and Questions"
      ],
      "path": "Chapter 3 Transport Layer > Homework Problems and Questions",
      "start_page": 312,
      "end_page": 326,
      "token_count": 8424,
      "text": "HOMEWORK PROBLEMS AND QUESTIONS\n285\nOnly the future will tell whether DCCP, SCTP, or TFRC will see widespread\ndeployment. While these protocols clearly provide enhanced capabilities over TCP and\nUDP, TCP and UDP have proven themselves “good enough” over the years. Whether\n“better” wins out over “good enough” will depend on a complex mix of technical,\nsocial, and business considerations.\nIn Chapter 1, we said that a computer network can be partitioned into the\n“network edge” and the “network core.” The network edge covers everything that\nhappens in the end systems. Having now covered the application layer and the\ntransport layer, our discussion of the network edge is complete. It is time to\nexplore the network core! This journey begins in the next chapter, where we’ll\nstudy the network layer, and continues into Chapter 5, where we’ll study the \nlink layer.\nHomework Problems and Questions\nChapter 3 Review Questions\nSECTIONS 3.1–3.3\nR1. Suppose the network layer provides the following service. The network layer\nin the source host accepts a segment of maximum size 1,200 bytes and a des-\ntination host address from the transport layer. The network layer then guaran-\ntees to deliver the segment to the transport layer at the destination host.\nSuppose many network application processes can be running at the\ndestination host.\na. Design the simplest possible transport-layer protocol that will get applica-\ntion data to the desired process at the destination host. Assume the operat-\ning system in the destination host has assigned a 4-byte port number to\neach running application process.\nb. Modify this protocol so that it provides a “return address” to the destina-\ntion process.\nc. In your protocols, does the transport layer “have to do anything” in the\ncore of the computer network?\nR2. Consider a planet where everyone belongs to a family of six, every family\nlives in its own house, each house has a unique address, and each person in a\ngiven house has a unique name. Suppose this planet has a mail service that\ndelivers letters from source house to destination house. The mail service\nrequires that (1) the letter be in an envelope, and that (2) the address of the\ndestination house (and nothing more) be clearly written on the envelope. Sup-\npose each family has a delegate family member who collects and distributes\nletters for the other family members. The letters do not necessarily provide\nany indication of the recipients of the letters.\n\n286\nCHAPTER 3\n•\nTRANSPORT LAYER\na. Using the solution to Problem R1 above as inspiration, describe a protocol\nthat the delegates can use to deliver letters from a sending family member\nto a receiving family member.\nb. In your protocol, does the mail service ever have to open the envelope and\nexamine the letter in order to provide its service?\nR3. Consider a TCP connection between Host A and Host B. Suppose that the\nTCP segments traveling from Host A to Host B have source port number x\nand destination port number y. What are the source and destination port num-\nbers for the segments traveling from Host B to Host A?\nR4. Describe why an application developer might choose to run an application\nover UDP rather than TCP.\nR5. Why is it that voice and video traffic is often sent over TCP rather than UDP\nin today’s Internet? (Hint: The answer we are looking for has nothing to do\nwith TCP’s congestion-control mechanism.)\nR6. Is it possible for an application to enjoy reliable data transfer even when the\napplication runs over UDP? If so, how?\nR7. Suppose a process in Host C has a UDP socket with port number 6789. Sup-\npose both Host A and Host B each send a UDP segment to Host C with desti-\nnation port number 6789. Will both of these segments be directed to the same\nsocket at Host C? If so, how will the process at Host C know that these two\nsegments originated from two different hosts?\nR8. Suppose that a Web server runs in Host C on port 80. Suppose this Web\nserver uses persistent connections, and is currently receiving requests from\ntwo different Hosts, A and B. Are all of the requests being sent through the\nsame socket at Host C? If they are being passed through different sockets, do\nboth of the sockets have port 80? Discuss and explain.\nSECTION 3.4\nR9. In our rdt protocols, why did we need to introduce sequence numbers?\nR10. In our rdt protocols, why did we need to introduce timers?\nR11. Suppose that the roundtrip delay between sender and receiver is constant and\nknown to the sender. Would a timer still be necessary in protocol rdt 3.0,\nassuming that packets can be lost? Explain.\nR12. Visit the Go-Back-N Java applet at the companion Web site.\na. Have the source send five packets, and then pause the animation before\nany of the five packets reach the destination. Then kill the first packet and\nresume the animation. Describe what happens.\nb. Repeat the experiment, but now let the first packet reach the destination\nand kill the first acknowledgment. Describe again what happens.\nc. Finally, try sending six packets. What happens?\n\nHOMEWORK PROBLEMS AND QUESTIONS\n287\nR13. Repeat R12, but now with the Selective Repeat Java applet. How are Selec-\ntive Repeat and Go-Back-N different?\nSECTION 3.5\nR14. True or false?\na. Host A is sending Host B a large file over a TCP connection. Assume\nHost B has no data to send Host A. Host B will not send acknowledg-\nments to Host A because Host B cannot piggyback the acknowledgments\non data.\nb. The size of the TCP rwnd never changes throughout the duration of the\nconnection.\nc. Suppose Host A is sending Host B a large file over a TCP connection. The\nnumber of unacknowledged bytes that A sends cannot exceed the size of\nthe receive buffer.\nd. Suppose Host A is sending a large file to Host B over a TCP connection. If\nthe sequence number for a segment of this connection is m, then the\nsequence number for the subsequent segment will necessarily be m + 1.\ne. The TCP segment has a field in its header for rwnd.\nf. Suppose that the last SampleRTT in a TCP connection is equal to 1 sec.\nThe current value of TimeoutInterval for the connection will neces-\nsarily be ≥1 sec.\ng. Suppose Host A sends one segment with sequence number 38 and 4 bytes\nof data over a TCP connection to Host B. In this same segment the\nacknowledgment number is necessarily 42.\nR15. Suppose Host A sends two TCP segments back to back to Host B over a TCP\nconnection. The first segment has sequence number 90; the second has\nsequence number 110.\na. How much data is in the first segment?\nb. Suppose that the first segment is lost but the second segment arrives at B.\nIn the acknowledgment that Host B sends to Host A, what will be the\nacknowledgment number?\nR16. Consider the Telnet example discussed in Section 3.5. A few seconds after the\nuser types the letter ‘C,’ the user types the letter ‘R.’After typing the letter\n‘R,’ how many segments are sent, and what is put in the sequence number\nand acknowledgment fields of the segments?\nSECTION 3.7\nR17. Suppose two TCP connections are present over some bottleneck link of rate R\nbps. Both connections have a huge file to send (in the same direction over the\n\n288\nCHAPTER 3\n•\nTRANSPORT LAYER\nbottleneck link). The transmissions of the files start at the same time. What\ntransmission rate would TCP like to give to each of the connections?\nR18. True or false? Consider congestion control in TCP. When the timer expires at\nthe sender, the value of ssthresh is set to one half of its previous value.\nR19. In the discussion of TCP splitting in the sidebar in Section 7.2, it was\nclaimed that the response time with TCP splitting is approximately\nJustify this claim.\nProblems\nP1. Suppose Client A initiates a Telnet session with Server S. At about the same\ntime, Client B also initiates a Telnet session with Server S. Provide possible\nsource and destination port numbers for\na. The segments sent from A to S.\nb. The segments sent from B to S.\nc. The segments sent from S to A.\nd. The segments sent from S to B.\ne. If A and B are different hosts, is it possible that the source port number in\nthe segments from A to S is the same as that from B to S?\nf. How about if they are the same host?\nP2. Consider Figure 3.5. What are the source and destination port values in the seg-\nments flowing from the server back to the clients’processes? What are the IP\naddresses in the network-layer datagrams carrying the transport-layer segments?\nP3. UDP and TCP use 1s complement for their checksums. Suppose you have the\nfollowing three 8-bit bytes: 01010011, 01100110, 01110100. What is the 1s\ncomplement of the sum of these 8-bit bytes? (Note that although UDP and\nTCP use 16-bit words in computing the checksum, for this problem you are\nbeing asked to consider 8-bit sums.) Show all work. Why is it that UDP takes\nthe 1s complement of the sum; that is, why not just use the sum? With the 1s\ncomplement scheme, how does the receiver detect errors? Is it possible that a\n1-bit error will go undetected? How about a 2-bit error?\nP4. a. Suppose you have the following 2 bytes: 01011100 and 01100101. What is\nthe 1s complement of the sum of these 2 bytes?\nb. Suppose you have the following 2 bytes: 11011010 and 01100101. What is\nthe 1s complement of the sum of these 2 bytes?\nc. For the bytes in part (a), give an example where one bit is flipped in each\nof the 2 bytes and yet the 1s complement doesn’t change.\n4 \b RTTFE + RTTBE + processing time.\n\nPROBLEMS\n289\nP5. Suppose that the UDP receiver computes the Internet checksum for the received\nUDP segment and finds that it matches the value carried in the checksum field.\nCan the receiver be absolutely certain that no bit errors have occurred? Explain.\nP6. Consider our motivation for correcting protocol rdt2.1. Show that the\nreceiver, shown in Figure 3.57, when operating with the sender shown in Fig-\nure 3.11, can lead the sender and receiver to enter into a deadlock state, where\neach is waiting for an event that will never occur.\nP7. In protocol rdt3.0, the ACK packets flowing from the receiver to the\nsender do not have sequence numbers (although they do have an ACK field\nthat contains the sequence number of the packet they are acknowledging).\nWhy is it that our ACK packets do not require sequence numbers?\nP8. Draw the FSM for the receiver side of protocol rdt3.0.\nP9. Give a trace of the operation of protocol rdt3.0 when data packets and\nacknowledgment packets are garbled. Your trace should be similar to that\nused in Figure 3.16.\nP10. Consider a channel that can lose packets but has a maximum delay that is\nknown. Modify protocol rdt2.1 to include sender timeout and retransmit.\nInformally argue why your protocol can communicate correctly over this\nchannel.\nWait for\n0 from\nbelow\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nhas_seq0(rcvpkt)))\ncompute chksum\nmake_pkt(sndpkt,NAK,chksum)\nudt_send(sndpkt)\nrdt_rcv(rcvpkt) &&\n(corrupt(rcvpkt)||\nhas_seq1(rcvpkt)))\ncompute chksum\nmake_pkt(sndpkt,NAK,chksum)\nudt_send(sndpkt)\nrdt_rvc(rcvpkt) && notcorrupt(rcvpkt)\n&& has_seq1(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\ncompute chksum\nmake_pkt(sendpkt,ACK,chksum)\nudt_send(sndpkt)\nrdt_rvc(rcvpkt) && notcorrupt(rcvpkt)\n&& has_seq0(rcvpkt)\nextract(rcvpkt,data)\ndeliver_data(data)\ncompute chksum\nmake_pkt(sendpkt,ACK,chksum)\nudt_send(sndpkt)\nWait for\n1 from\nbelow\nFigure 3.57 \u0002 An incorrect receiver for protocol rdt 2.1\n\n290\nCHAPTER 3\n•\nTRANSPORT LAYER\nP11. Consider the rdt2.2 receiver in Figure 3.14, and the creation of a new packet\nin the self-transition (i.e., the transition from the state back to itself) in the Wait-\nfor-0-from-below and the Wait-for-1-from-below states: sndpkt=make_\npkt(ACK,0,checksum) and sndpkt=make_pkt(ACK,0,\nchecksum). Would the protocol work correctly if this action were removed\nfrom the self-transition in the Wait-for-1-from-below state? Justify your\nanswer. What if this event were removed from the self-transition in the Wait-\nfor-0-from-below state? [Hint: In this latter case, consider what would hap-\npen if the first sender-to-receiver packet were corrupted.]\nP12. The sender side of rdt3.0 simply ignores (that is, takes no action on) \nall received packets that are either in error or have the wrong value in the\nacknum field of an acknowledgment packet. Suppose that in such circum-\nstances, rdt3.0 were simply to retransmit the current data packet. Would\nthe protocol still work? (Hint: Consider what would happen if there were\nonly bit errors; there are no packet losses but premature timeouts can occur.\nConsider how many times the nth packet is sent, in the limit as n\napproaches infinity.)\nP13. Consider the rdt 3.0 protocol. Draw a diagram showing that if the net-\nwork connection between the sender and receiver can reorder messages\n(that is, that two messages propagating in the medium between the sender\nand receiver can be reordered), then the alternating-bit protocol will not\nwork correctly (make sure you clearly identify the sense in which it will\nnot work correctly). Your diagram should have the sender on the left and\nthe receiver on the right, with the time axis running down the page, show-\ning data (D) and acknowledgment (A) message exchange. Make sure you\nindicate the sequence number associated with any data or acknowledgment\nsegment.\nP14. Consider a reliable data transfer protocol that uses only negative acknowledg-\nments. Suppose the sender sends data only infrequently. Would a NAK-only\nprotocol be preferable to a protocol that uses ACKs? Why? Now suppose the\nsender has a lot of data to send and the end-to-end connection experiences\nfew losses. In this second case, would a NAK-only protocol be preferable to a\nprotocol that uses ACKs? Why?\nP15. Consider the cross-country example shown in Figure 3.17. How big would\nthe window size have to be for the channel utilization to be greater than 98\npercent? Suppose that the size of a packet is 1,500 bytes, including both\nheader fields and data.\nP16. Suppose an application uses rdt 3.0 as its transport layer protocol. As the\nstop-and-wait protocol has very low channel utilization (shown in the cross-\ncountry example), the designers of this application let the receiver keep send-\ning back a number (more than two) of alternating ACK 0 and ACK 1 even if\n\nPROBLEMS\n291\nthe corresponding data have not arrived at the receiver. Would this applica-\ntion design increase the channel utilization? Why? Are there any potential\nproblems with this approach? Explain. \nP17. Consider two network entities, A and B, which are connected by a perfect bi-\ndirectional channel (i.e., any message sent will be received correctly; the\nchannel will not corrupt, lose, or re-order packets). A and B are to deliver\ndata messages to each other in an alternating manner: First, A must deliver a\nmessage to B, then B must deliver a message to A, then A must deliver a mes-\nsage to B and so on. If an entity is in a state where it should not attempt to\ndeliver a message to the other side, and there is an event like\nrdt_send(data) call from above that attempts to pass data down for\ntransmission to the other side, this call from above can simply be ignored\nwith a call to rdt_unable_to_send(data), which informs the higher\nlayer that it is currently not able to send data. [Note: This simplifying\nassumption is made so you don’t have to worry about buffering data.]\nDraw a FSM specification for this protocol (one FSM for A, and one FSM for\nB!). Note that you do not have to worry about a reliability mechanism here;\nthe main point of this question is to create a FSM specification that reflects\nthe synchronized behavior of the two entities. You should use the following\nevents and actions that have the same meaning as protocol rdt1.0 in \nFigure 3.9: rdt_send(data), packet = make_pkt(data),\nudt_send(packet), rdt_rcv(packet), extract\n(packet,data), deliver_data(data). Make sure your protocol\nreflects the strict alternation of sending between A and B. Also, make sure to\nindicate the initial states for A and B in your FSM descriptions.\nP18. In the generic SR protocol that we studied in Section 3.4.4, the sender trans-\nmits a message as soon as it is available (if it is in the window) without wait-\ning for an acknowledgment. Suppose now that we want an SR protocol that\nsends messages two at a time. That is, the sender will send a pair of messages\nand will send the next pair of messages only when it knows that both mes-\nsages in the first pair have been received correctly.\nSuppose that the channel may lose messages but will not corrupt or reorder\nmessages. Design an error-control protocol for the unidirectional reliable\ntransfer of messages. Give an FSM description of the sender and receiver.\nDescribe the format of the packets sent between sender and receiver, and vice\nversa. If you use any procedure calls other than those in Section 3.4 (for\nexample, udt_send(), start_timer(), rdt_rcv(), and so on),\nclearly state their actions. Give an example (a timeline trace of sender and\nreceiver) showing how your protocol recovers from a lost packet.\nP19. Consider a scenario in which Host A wants to simultaneously send packets to\nHosts B and C. A is connected to B and C via a broadcast channel—a packet\n\n292\nCHAPTER 3\n•\nTRANSPORT LAYER\nsent by A is carried by the channel to both B and C. Suppose that the broad-\ncast channel connecting A, B, and C can independently lose and corrupt\npackets (and so, for example, a packet sent from A might be correctly\nreceived by B, but not by C). Design a stop-and-wait-like error-control proto-\ncol for reliably transferring packets from A to B and C, such that A will not\nget new data from the upper layer until it knows that both B and C have cor-\nrectly received the current packet. Give FSM descriptions of A and C. (Hint:\nThe FSM for B should be essentially the same as for C.) Also, give a descrip-\ntion of the packet format(s) used.\nP20. Consider a scenario in which Host A and Host B want to send messages to\nHost C. Hosts A and C are connected by a channel that can lose and corrupt\n(but not reorder) messages. Hosts B and C are connected by another chan-\nnel (independent of the channel connecting A and C) with the same proper-\nties. The transport layer at Host C should alternate in delivering messages\nfrom A and B to the layer above (that is, it should first deliver the data from\na packet from A, then the data from a packet from B, and so on). Design a\nstop-and-wait-like error-control protocol for reliably transferring packets\nfrom A and B to C, with alternating delivery at C as described above. Give\nFSM descriptions of A and C. (Hint: The FSM for B should be essentially\nthe same as for A.) Also, give a description of the packet format(s) used.\nP21. Suppose we have two network entities, A and B. B has a supply of data mes-\nsages that will be sent to A according to the following conventions. When A\ngets a request from the layer above to get the next data (D) message from B,\nA must send a request (R) message to B on the A-to-B channel. Only when B\nreceives an R message can it send a data (D) message back to A on the B-to-\nA channel. A should deliver exactly one copy of each D message to the layer\nabove. R messages can be lost (but not corrupted) in the A-to-B channel; D\nmessages, once sent, are always delivered correctly. The delay along both\nchannels is unknown and variable.\nDesign (give an FSM description of) a protocol that incorporates the appro-\npriate mechanisms to compensate for the loss-prone A-to-B channel and\nimplements message passing to the layer above at entity A, as discussed\nabove. Use only those mechanisms that are absolutely necessary.\nP22. Consider the GBN protocol with a sender window size of 4 and a sequence\nnumber range of 1,024. Suppose that at time t, the next in-order packet that the\nreceiver is expecting has a sequence number of k. Assume that the medium\ndoes not reorder messages. Answer the following questions:\na. What are the possible sets of sequence numbers inside the sender’s win-\ndow at time t? Justify your answer.\nb. What are all possible values of the ACK field in all possible messages cur-\nrently propagating back to the sender at time t? Justify your answer.\n\nPROBLEMS\n293\nP23. Consider the GBN and SR protocols. Suppose the sequence number space is\nof size k. What is the largest allowable sender window that will avoid the\noccurrence of problems such as that in Figure 3.27 for each of these protocols?\nP24. Answer true or false to the following questions and briefly justify your\nanswer:\na. With the SR protocol, it is possible for the sender to receive an ACK for a\npacket that falls outside of its current window.\nb. With GBN, it is possible for the sender to receive an ACK for a packet that\nfalls outside of its current window.\nc. The alternating-bit protocol is the same as the SR protocol with a sender\nand receiver window size of 1.\nd. The alternating-bit protocol is the same as the GBN protocol with a sender\nand receiver window size of 1.\nP25. We have said that an application may choose UDP for a transport protocol\nbecause UDP offers finer application control (than TCP) of what data is sent\nin a segment and when.\na. Why does an application have more control of what data is sent in a segment?\nb. Why does an application have more control on when the segment is sent?\nP26. Consider transferring an enormous file of L bytes from Host A to Host B.\nAssume an MSS of 536 bytes.\na. What is the maximum value of L such that TCP sequence numbers are not\nexhausted? Recall that the TCP sequence number field has 4 bytes.\nb. For the L you obtain in (a), find how long it takes to transmit the file.\nAssume that a total of 66 bytes of transport, network, and data-link header\nare added to each segment before the resulting packet is sent out over a\n155 Mbps link. Ignore flow control and congestion control so A can pump\nout the segments back to back and continuously.\nP27. Host A and B are communicating over a TCP connection, and Host B has\nalready received from A all bytes up through byte 126. Suppose Host A then\nsends two segments to Host B back-to-back. The first and second segments\ncontain 80 and 40 bytes of data, respectively. In the first segment, the\nsequence number is 127, the source port number is 302, and the destination\nport number is 80. Host B sends an acknowledgment whenever it receives a\nsegment from Host A.\na. In the second segment sent from Host A to B, what are the sequence num-\nber, source port number, and destination port number?\nb. If the first segment arrives before the second segment, in the acknowledg-\nment of the first arriving segment, what is the acknowledgment number,\nthe source port number, and the destination port number?\n\n294\nCHAPTER 3\n•\nTRANSPORT LAYER\nc. If the second segment arrives before the first segment, in the acknowl-\nedgment of the first arriving segment, what is the acknowledgment\nnumber?\nd. Suppose the two segments sent by A arrive in order at B. The first acknowl-\nedgment is lost and the second acknowledgment arrives after the first time-\nout interval. Draw a timing diagram, showing these segments and all other\nsegments and acknowledgments sent. (Assume there is no additional packet\nloss.) For each segment in your figure, provide the sequence number and\nthe number of bytes of data; for each acknowledgment that you add, pro-\nvide the acknowledgment number.\nP28. Host A and B are directly connected with a 100 Mbps link. There is one TCP\nconnection between the two hosts, and Host A is sending to Host B an enor-\nmous file over this connection. Host A can send its application data into its TCP\nsocket at a rate as high as 120 Mbps but Host B can read out of its TCP receive\nbuffer at a maximum rate of 50 Mbps. Describe the effect of TCP flow control.\nP29. SYN cookies were discussed in Section 3.5.6.\na. Why is it necessary for the server to use a special initial sequence number\nin the SYNACK?\nb. Suppose an attacker knows that a target host uses SYN cookies. Can the\nattacker create half-open or fully open connections by simply sending an\nACK packet to the target? Why or why not?\nc. Suppose an attacker collects a large amount of initial sequence numbers sent\nby the server. Can the attacker cause the server to create many fully open\nconnections by sending ACKs with those initial sequence numbers? Why?\nP30. Consider the network shown in Scenario 2 in Section 3.6.1. Suppose both\nsending hosts A and B have some fixed timeout values. \na. Argue that increasing the size of the finite buffer of the router might possi-\nbly decrease the throughput (\u0006out).\nb. Now suppose both hosts dynamically adjust their timeout values (like\nwhat TCP does) based on the buffering delay at the router. Would increas-\ning the buffer size help to increase the throughput? Why?\nP31. Suppose that the five measured SampleRTT values (see Section 3.5.3) are\n106 ms, 120 ms, 140 ms, 90 ms, and 115 ms. Compute the EstimatedRTT\nafter each of these SampleRTT values is obtained, using a value of α = 0.125\nand assuming that the value of EstimatedRTT was 100 ms just before the\nfirst of these five samples were obtained. Compute also the DevRTT after\neach sample is obtained, assuming a value of β = 0.25 and assuming the\nvalue of DevRTT was 5 ms just before the first of these five samples was\nobtained. Last, compute the TCP TimeoutInterval after each of these\nsamples is obtained.\n\nPROBLEMS\n295\nP32. Consider the TCP procedure for estimating RTT. Suppose that \u0003 = 0.1. Let\nSampleRTT1 be the most recent sample RTT, let SampleRTT2 be the next\nmost recent sample RTT, and so on.\na. For a given TCP connection, suppose four acknowledgments have been\nreturned with corresponding sample RTTs: SampleRTT4, SampleRTT3,\nSampleRTT2, and SampleRTT1. Express EstimatedRTT in terms of\nthe four sample RTTs.\nb. Generalize your formula for n sample RTTs.\nc. For the formula in part (b) let n approach infinity. Comment on why this\naveraging procedure is called an exponential moving average.\nP33. In Section 3.5.3, we discussed TCP’s estimation of RTT. Why do you think\nTCP avoids measuring the SampleRTT for retransmitted segments?\nP34. What is the relationship between the variable SendBase in Section 3.5.4\nand the variable LastByteRcvd in Section 3.5.5?\nP35. What is the relationship between the variable LastByteRcvd in Section\n3.5.5 and the variable y in Section 3.5.4?\nP36. In Section 3.5.4, we saw that TCP waits until it has received three \nduplicate ACKs before performing a fast retransmit. Why do you think the\nTCP designers chose not to perform a fast retransmit after the first duplicate\nACK for a segment is received?\nP37. Compare GBN, SR, and TCP (no delayed ACK). Assume that the timeout\nvalues for all three protocols are sufficiently long such that 5 consecutive data\nsegments and their corresponding ACKs can be received (if not lost in the\nchannel) by the receiving host (Host B) and the sending host (Host A) respec-\ntively. Suppose Host A sends 5 data segments to Host B, and the 2nd segment\n(sent from A) is lost. In the end, all 5 data segments have been correctly\nreceived by Host B. \na. How many segments has Host A sent in total and how many ACKs has\nHost B sent in total? What are their sequence numbers? Answer this ques-\ntion for all three protocols.\nb. If the timeout values for all three protocol are much longer than 5 RTT,\nthen which protocol successfully delivers all five data segments in shortest\ntime interval? \nP38. In our description of TCP in Figure 3.53, the value of the threshold,\nssthresh, is set as ssthresh=cwnd/2 in several places and\nssthresh value is referred to as being set to half the window size when a\nloss event occurred. Must the rate at which the sender is sending when the\nloss event occurred be approximately equal to cwnd segments per RTT?\nExplain your answer. If your answer is no, can you suggest a different\nmanner in which ssthresh should be set?\n\n296\nCHAPTER 3\n•\nTRANSPORT LAYER\nP39. Consider Figure 3.46(b). If \u0006\u0007in increases beyond R/2, can \u0006out increase\nbeyond R/3? Explain. Now consider Figure 3.46(c). If \u0006\u0007in increases\nbeyond R/2, can \u0006out increase beyond R/4 under the assumption that a\npacket will be forwarded twice on average from the router to the receiver?\nExplain.\nP40. Consider Figure 3.58. Assuming TCP Reno is the protocol experiencing the\nbehavior shown above, answer the following questions. In all cases, you\nshould provide a short discussion justifying your answer.\na. Identify the intervals of time when TCP slow start is operating.\nb. Identify the intervals of time when TCP congestion avoidance is \noperating.\nc. After the 16th transmission round, is segment loss detected by a triple\nduplicate ACK or by a timeout?\nd. After the 22nd transmission round, is segment loss detected by a triple\nduplicate ACK or by a timeout?\ne. What is the initial value of ssthresh at the first transmission round?\nf. What is the value of ssthresh at the 18th transmission round?\ng. What is the value of ssthresh at the 24th transmission round?\nh. During what transmission round is the 70th segment sent?\ni. Assuming a packet loss is detected after the 26th round by the receipt of a\ntriple duplicate ACK, what will be the values of the congestion window\nsize and of ssthresh?\n0\n0\n2\n4\n6\n8\n10 12\nTransmission round\n14 16 18 20 22 24 26\n5\n10\n15\n20\n25\nCongestion window size (segments)\n30\n35\n40\n45\nFigure 3.58 \u0002 TCP window size as a function of time\nVideoNote\nExamining the \nbehavior of TCP\n\nPROBLEMS\n297\nj. Suppose TCP Tahoe is used (instead of TCP Reno), and assume that triple\nduplicate ACKs are received at the 16th round. What are the ssthresh\nand the congestion window size at the 19th round?\nk. Again suppose TCP Tahoe is used, and there is a timeout event at 22nd\nround. How many packets have been sent out from 17th round till 22nd\nround, inclusive?\nP41. Refer to Figure 3.56, which illustrates the convergence of TCP’s AIMD \nalgorithm. Suppose that instead of a multiplicative decrease, TCP decreased\nthe window size by a constant amount. Would the resulting AIAD algorithm\nconverge to an equal share algorithm? Justify your answer using a diagram\nsimilar to Figure 3.56.\nP42. In Section 3.5.4, we discussed the doubling of the timeout interval after a\ntimeout event. This mechanism is a form of congestion control. Why does\nTCP need a window-based congestion-control mechanism (as studied in\nSection 3.7) in addition to this doubling-timeout-interval mechanism?\nP43. Host A is sending an enormous file to Host B over a TCP connection. \nOver this connection there is never any packet loss and the timers never\nexpire. Denote the transmission rate of the link connecting Host A to the\nInternet by R bps. Suppose that the process in Host A is capable of sending\ndata into its TCP socket at a rate S bps, where S = 10 · R. Further suppose \nthat the TCP receive buffer is large enough to hold the entire file, and the\nsend buffer can hold only one percent of the file. What would prevent the\nprocess in Host A from continuously passing data to its TCP socket at rate S\nbps? TCP flow control? TCP congestion control? Or something else?\nElaborate.\nP44. Consider sending a large file from a host to another over a TCP connection\nthat has no loss.\na. Suppose TCP uses AIMD for its congestion control without slow start.\nAssuming cwnd increases by 1 MSS every time a batch of ACKs is received\nand assuming approximately constant round-trip times, how long does it take\nfor cwnd increase from 6 MSS to 12 MSS (assuming no loss events)?\nb. What is the average throughout (in terms of MSS and RTT) for this con-\nnection up through time = 6 RTT?\nP45. Recall the macroscopic description of TCP throughput. In the period of time\nfrom when the connection’s rate varies from W/(2 · RTT) to W/RTT, only one\npacket is lost (at the very end of the period).\na. Show that the loss rate (fraction of packets lost) is equal to\nL = loss rate =\n1\n3\n8 W2 + 3\n4 W\n\n298\nCHAPTER 3\n•\nTRANSPORT LAYER\nb. Use the result above to show that if a connection has loss rate L, then its\naverage rate is approximately given by\nP46. Consider that only a single TCP (Reno) connection uses one 10Mbps link\nwhich does not buffer any data. Suppose that this link is the only congested\nlink between the sending and receiving hosts. Assume that the TCP sender\nhas a huge file to send to the receiver, and the receiver’s receive buffer is\nmuch larger than the congestion window. We also make the following\nassumptions: each TCP segment size is 1,500 bytes; the two-way propagation\ndelay of this connection is 150 msec; and this TCP connection is always in\ncongestion avoidance phase, that is, ignore slow start. \na. What is the maximum window size (in segments) that this TCP connection\ncan achieve?\nb. What is the average window size (in segments) and average throughput (in\nbps) of this TCP connection?\nc. How long would it take for this TCP connection to reach its maximum\nwindow again after recovering from a packet loss?\nP47. Consider the scenario described in the previous problem. Suppose that the\n10Mbps link can buffer a finite number of segments. Argue that in order for\nthe link to always be busy sending data, we would like to choose a buffer size\nthat is at least the product of the link speed C and the two-way propagation\ndelay between the sender and the receiver.\nP48. Repeat Problem 43, but replacing the 10 Mbps link with a 10 Gbps link. Note\nthat in your answer to part c, you will realize that it takes a very long time for\nthe congestion window size to reach its maximum window size after recover-\ning from a packet loss. Sketch a solution to solve this problem.\nP49. Let T (measured by RTT) denote the time interval that a TCP connection\ntakes to increase its congestion window size from W/2 to W, where W is the\nmaximum congestion window size. Argue that T is a function of TCP’s aver-\nage throughput.\nP50. Consider a simplified TCP’s AIMD algorithm where the congestion window\nsize is measured in number of segments, not in bytes. In additive increase, the\ncongestion window size increases by one segment in each RTT. In multiplica-\ntive decrease, the congestion window size decreases by half (if the result is\nnot an integer, round down to the nearest integer). Suppose that two TCP\nconnections, C1 and C2, share a single congested link of speed 30 segments\nper second. Assume that both C1 and C2 are in the congestion avoidance\n\u0002 1.22 \b MSS\nRTT 2L\n\nPROBLEMS\n299\nphase. Connection C1’s RTT is 50 msec and connection C2’s RTT is \n100 msec. Assume that when the data rate in the link exceeds the link’s\nspeed, all TCP connections experience data segment loss. \na. If both C1 and C2 at time t0 have a congestion window of 10 segments,\nwhat are their congestion window sizes after 1000 msec? \nb. In the long run, will these two connections get the same share of the band-\nwidth of the congested link? Explain. \nP51. Consider the network described in the previous problem. Now suppose that\nthe two TCP connections, C1 and C2, have the same RTT of 100 msec. Sup-\npose that at time t0, C1’s congestion window size is 15 segments but C2’s\ncongestion window size is 10 segments. \na. What are their congestion window sizes after 2200msec?\nb. In the long run, will these two connections get about the same share of the\nbandwidth of the congested link? \nc. We say that two connections are synchronized, if both connections reach\ntheir maximum window sizes at the same time and reach their minimum\nwindow sizes at the same time. In the long run, will these two connections\nget synchronized eventually? If so, what are their maximum window sizes?\nd. Will this synchronization help to improve the utilization of the shared\nlink? Why? Sketch some idea to break this synchronization.\nP52. Consider a modification to TCP’s congestion control algorithm. Instead of\nadditive increase, we can use multiplicative increase. A TCP sender increases\nits window size by a small positive constant a (0 < a < 1) whenever it\nreceives a valid ACK. Find the functional relationship between loss rate L\nand maximum congestion window W. Argue that for this modified TCP,\nregardless of TCP’s average throughput, a TCP connection always spends the\nsame amount of time to increase its congestion window size from W/2 to W.\nP53. In our discussion of TCP futures in Section 3.7, we noted that to achieve a\nthroughput of 10 Gbps, TCP could only tolerate a segment loss probability of\n2 · 10-10 (or equivalently, one loss event for every 5,000,000,000 segments).\nShow the derivation for the values of 2 · 10-10 (1 out of 5,000,000) for the\nRTT and MSS values given in Section 3.7. If TCP needed to support a 100\nGbps connection, what would the tolerable loss be?\nP54. In our discussion of TCP congestion control in Section 3.7, we implicitly\nassumed that the TCP sender always had data to send. Consider now the case\nthat the TCP sender sends a large amount of data and then goes idle (since it\nhas no more data to send) at t1. TCP remains idle for a relatively long period of\ntime and then wants to send more data at t2. What are the advantages and dis-\nadvantages of having TCP use the cwnd and ssthresh values from t1 when\nstarting to send data at t2? What alternative would you recommend? Why?"
    },
    {
      "chunk_id": "e8f4b6f0-643a-4c14-8bc6-466191688b93",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Programming Assignments",
      "original_titles": [
        "Programming Assignments"
      ],
      "path": "Chapter 3 Transport Layer > Programming Assignments",
      "start_page": 327,
      "end_page": 327,
      "token_count": 507,
      "text": "300\nCHAPTER 3\n•\nTRANSPORT LAYER\nP55. In this problem we investigate whether either UDP or TCP provides a degree\nof end-point authentication.\na. Consider a server that receives a request within a UDP packet and\nresponds to that request within a UDP packet (for example, as done by a\nDNS server). If a client with IP address X spoofs its address with address\nY, where will the server send its response?\nb. Suppose a server receives a SYN with IP source address Y, and after\nresponding with a SYNACK, receives an ACK with IP source address Y\nwith the correct acknowledgment number. Assuming the server chooses a\nrandom initial sequence number and there is no “man-in-the-middle,” can\nthe server be certain that the client is indeed at Y (and not at some other\naddress X that is spoofing Y)?\nP56. In this problem, we consider the delay introduced by the TCP slow-start\nphase. Consider a client and a Web server directly connected by one link of\nrate R. Suppose the client wants to retrieve an object whose size is exactly\nequal to 15 S, where S is the maximum segment size (MSS). Denote the\nround-trip time between client and server as RTT (assumed to be constant).\nIgnoring protocol headers, determine the time to retrieve the object (including\nTCP connection establishment) when\na. 4 S/R > S/R + RTT > 2S/R\nb. S/R + RTT > 4 S/R\nc. S/R > RTT.\nProgramming Assignments\nImplementing a Reliable Transport Protocol\nIn this laboratory programming assignment, you will be writing the sending and\nreceiving transport-level code for implementing a simple reliable data transfer pro-\ntocol. There are two versions of this lab, the alternating-bit-protocol version and the\nGBN version. This lab should be fun—your implementation will differ very little\nfrom what would be required in a real-world situation.\nSince you probably don’t have standalone machines (with an OS that you can\nmodify), your code will have to execute in a simulated hardware/software envi-\nronment. However, the programming interface provided to your routines—the\ncode that would call your entities from above and from below—is very close to\nwhat is done in an actual UNIX environment. (Indeed, the software interfaces\ndescribed in this programming assignment are much more realistic than the infi-\nnite loop senders and receivers that many texts describe.) Stopping and starting"
    },
    {
      "chunk_id": "ea1ffd60-0d30-4684-b915-2eda88eafc75",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Wireshark Labs: TCP, UDP",
      "original_titles": [
        "Wireshark Labs: TCP, UDP"
      ],
      "path": "Chapter 3 Transport Layer > Wireshark Labs: TCP, UDP",
      "start_page": 328,
      "end_page": 328,
      "token_count": 366,
      "text": "timers are also simulated, and timer interrupts will cause your timer handling rou-\ntine to be activated.\nThe full lab assignment, as well as code you will need to compile with your\nown code, are available at this book’s Web site: http://www.awl.com/kurose-ross.\nWireshark Lab: Exploring TCP\nIn this lab, you’ll use your Web browser to access a file from a Web server. As in\nearlier Wireshark labs, you’ll use Wireshark to capture the packets arriving at your\ncomputer. Unlike earlier labs, you’ll also be able to download a Wireshark-readable\npacket trace from the Web server from which you downloaded the file. In this server\ntrace, you’ll find the packets that were generated by your own access of the Web\nserver. You’ll analyze the client- and server-side traces to explore aspects of TCP. In\nparticular, you’ll evaluate the performance of the TCP connection between your\ncomputer and the Web server. You’ll trace TCP’s window behavior, and infer packet\nloss, retransmission, flow control and congestion control behavior, and estimated\nroundtrip time.\nAs is the case with all Wireshark labs, the full description of this lab is avail-\nable at this book’s Web site, http://www.awl.com/kurose-ross.\nWireshark Lab: Exploring UDP\nIn this short lab, you’ll do a packet capture and analysis of your favorite application\nthat uses UDP (for example, DNS or a multimedia application such as Skype). As we\nlearned in Section 3.3, UDP is a simple, no-frills transport protocol. In this lab, you’ll\ninvestigate the header fields in the UDP segment as well as the checksum calculation.\nAs is the case with all Wireshark labs, the full description of this lab is available\nat this book’s Web site, http://www.awl.com/kurose-ross.\nWIRESHARK LAB: EXPLORING UDP\n301"
    },
    {
      "chunk_id": "1088b507-8462-4b04-9bb5-e7c0e8416694",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Interview: Van Jacobson",
      "original_titles": [
        "Interview: Van Jacobson"
      ],
      "path": "Chapter 3 Transport Layer > Interview: Van Jacobson",
      "start_page": 329,
      "end_page": 331,
      "token_count": 834,
      "text": "302\nPlease describe one or two of the most exciting projects you have worked on during your\ncareer. What were the biggest challenges?\nSchool teaches us lots of ways to find answers. In every interesting problem I’ve worked\non, the challenge has been finding the right question. When Mike Karels and I started look-\ning at TCP congestion, we spent months staring at protocol and packet traces asking “Why\nis it failing?”. One day in Mike’s office, one of us said “The reason I can’t figure out why it\nfails is because I don’t understand how it ever worked to begin with.” That turned out to be\nthe right question and it forced us to figure out the “ack clocking” that makes TCP work.\nAfter that, the rest was easy.\nMore generally, where do you see the future of networking and the Internet?\nFor most people, the Web is the Internet. Networking geeks smile politely since we know\nthe Web is an application running over the Internet but what if they’re right? The Internet is\nabout enabling conversations between pairs of hosts. The Web is about distributed informa-\ntion production and consumption. “Information propagation” is a very general view of com-\nmunication of which “pairwise conversation” is a tiny subset. We need to move into the\nlarger tent. Networking today deals with broadcast media (radios, PONs, etc.) by pretending\nit’s a point-to-point wire. That’s massively inefficient. Terabits-per-second of data are being\nexchanged all over the World via thumb drives or smart phones but we don’t know how to\ntreat that as “networking”. ISPs are busily setting up caches and CDNs to scalably distribute\nvideo and audio. Caching is a necessary part of the solution but there's no part of today's\nnetworking—from Information, Queuing or Traffic Theory down to the Internet protocol\nspecs—that tells us how to engineer and deploy it. I think and hope that over the next few\nyears, networking will evolve to embrace the much larger vision of communication that\nunderlies the Web.\nVan Jacobson\nVan Jacobson is a Research Fellow at PARC. Prior to that, he was\nco-founder and Chief Scientist of Packet Design. Before that, he was\nChief Scientist at Cisco. Before joining Cisco, he was head of the\nNetwork Research Group at Lawrence Berkeley National Laboratory\nand taught at UC Berkeley and Stanford. Van received the ACM\nSIGCOMM Award in 2001 for outstanding lifetime contribution to\nthe field of communication networks and the IEEE Kobayashi Award\nin 2002 for “contributing to the understanding of network congestion\nand developing congestion control mechanisms that enabled the suc-\ncessful scaling of the Internet”. He was elected to the U.S. National\nAcademy of Engineering in 2004.\nAN INTERVIEW WITH...\n\n303\nWhat people inspired you professionally?\nWhen I was in grad school, Richard Feynman visited and gave a colloquium. He talked\nabout a piece of Quantum theory that I’d been struggling with all semester and his explana-\ntion was so simple and lucid that what had been incomprehensible gibberish to me became\nobvious and inevitable. That ability to see and convey the simplicity that underlies our com-\nplex world seems to me a rare and wonderful gift. \nWhat are your recommendations for students who want careers in computer science and\nnetworking?\nIt’s a wonderful field—computers and networking have probably had more impact on socie-\nty than any invention since the book. Networking is fundamentally about connecting stuff,\nand studying it helps you make intellectual connections: Ant foraging & Bee dances demon-\nstrate protocol design better than RFCs, traffic jams or people leaving a packed stadium are\nthe essence of congestion, and students finding flights back to school in a post-Thanksgiving\nblizzard are the core of dynamic routing. If you’re interested in lots of stuff and want to\nhave an impact, it’s hard to imagine a better field.\n\nThis page intentionally left blank"
    },
    {
      "chunk_id": "ca770f50-425e-4a94-8b0e-73cd206ad20a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Chapter 4 The Network Layer",
      "original_titles": [
        "Chapter 4 The Network Layer"
      ],
      "path": "Chapter 4 The Network Layer",
      "start_page": 332,
      "end_page": 332,
      "token_count": 322,
      "text": "CHAPTER 4\nThe Network\nLayer\n305\nWe learned in the previous chapter that the transport layer provides various forms of\nprocess-to-process communication by relying on the network layer’s host-to-host\ncommunication service. We also learned that the transport layer does so without any\nknowledge about how the network layer actually implements this service. So per-\nhaps you’re now wondering, what’s under the hood of the host-to-host communica-\ntion service, what makes it tick?\nIn this chapter, we’ll learn exactly how the network layer implements the host-\nto-host communication service. We’ll see that unlike the transport and application\nlayers, there is a piece of the network layer in each and every host and router in the\nnetwork. Because of this, network-layer protocols are among the most challenging\n(and therefore among the most interesting!) in the protocol stack.\nThe network layer is also one of the most complex layers in the protocol stack,\nand so we’ll have a lot of ground to cover here. We’ll begin our study with an\noverview of the network layer and the services it can provide. We’ll then examine\ntwo broad approaches towards structuring network-layer packet delivery—the data-\ngram and the virtual-circuit model—and see the fundamental role that addressing\nplays in delivering a packet to its destination host.\nIn this chapter, we’ll make an important distinction between the forwarding\nand routing functions of the network layer. Forwarding involves the transfer of a\npacket from an incoming link to an outgoing link within a single router. Routing"
    },
    {
      "chunk_id": "76efeca0-c1ca-4b97-9172-7be5e0e7d103",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.1 Introduction",
      "original_titles": [
        "4.1 Introduction"
      ],
      "path": "Chapter 4 The Network Layer > 4.1 Introduction",
      "start_page": 333,
      "end_page": 334,
      "token_count": 666,
      "text": "involves all of a network’s routers, whose collective interactions via routing proto-\ncols determine the paths that packets take on their trips from source to destination\nnode. This will be an important distinction to keep in mind as you progress through\nthis chapter.\nIn order to deepen our understanding of packet forwarding, we’ll look “inside”\na router—at its hardware architecture and organization. We’ll then look at packet\nforwarding in the Internet, along with the celebrated Internet Protocol (IP). We’ll\ninvestigate network-layer addressing and the IPv4 datagram format. We’ll then\nexplore network address translation (NAT), datagram fragmentation, the Internet\nControl Message Protocol (ICMP), and IPv6.\nWe’ll then turn our attention to the network layer’s routing function. We’ll see\nthat the job of a routing algorithm is to determine good paths (equivalently, routes)\nfrom senders to receivers. We’ll first study the theory of routing algorithms, concen-\ntrating on the two most prevalent classes of algorithms: link-state and distance-\nvector algorithms. Since the complexity of routing algorithms grows considerably\nas the number of network routers increases, hierarchical routing approaches will\nalso be of interest. We’ll then see how theory is put into practice when we cover the\nInternet’s intra-autonomous system routing protocols (RIP, OSPF, and IS-IS) and its\ninter-autonomous system routing protocol, BGP. We’ll close this chapter with a dis-\ncussion of broadcast and multicast routing.\nIn summary, this chapter has three major parts. The first part, Sections 4.1 and\n4.2, covers network-layer functions and services. The second part, Sections 4.3 and\n4.4, covers forwarding. Finally, the third part, Sections 4.5 through 4.7, covers\nrouting.\n4.1 Introduction\nFigure 4.1 shows a simple network with two hosts, H1 and H2, and several routers\non the path between H1 and H2. Suppose that H1 is sending information to H2, and\nconsider the role of the network layer in these hosts and in the intervening routers.\nThe network layer in H1 takes segments from the transport layer in H1, encapsu-\nlates each segment into a datagram (that is, a network-layer packet), and then sends\nthe datagrams to its nearby router, R1. At the receiving host, H2, the network layer\nreceives the datagrams from its nearby router R2, extracts the transport-layer seg-\nments, and delivers the segments up to the transport layer at H2. The primary role of\nthe routers is to forward datagrams from input links to output links. Note that the\nrouters in Figure 4.1 are shown with a truncated protocol stack, that is, with no\nupper layers above the network layer, because (except for control purposes) routers\ndo not run application- and transport-layer protocols such as those we examined in\nChapters 2 and 3.\n306\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\n4.1\n•\nINTRODUCTION\n307\nMobile Network\nRouter R1\nRouter R2\nNational or\nGlobal ISP\nLocal or\nRegional ISP\nEnterprise Network\nHome Network\nEnd system H1\nData link\nPhysical\nApplication\nTransport\nNetwork\nEnd system H2\nData link\nPhysical\nApplication\nTransport\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nNetwork\nData link\nPhysical\nNetwork\nFigure 4.1 \u0002 The network layer"
    },
    {
      "chunk_id": "65d0fc73-7c52-4362-9b6b-5e19e86b87fa",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.1.1 Forwarding and Routing",
      "original_titles": [
        "4.1.1 Forwarding and Routing"
      ],
      "path": "Chapter 4 The Network Layer > 4.1 Introduction > 4.1.1 Forwarding and Routing",
      "start_page": 335,
      "end_page": 336,
      "token_count": 1006,
      "text": "4.1.1 Forwarding and Routing\nThe role of the network layer is thus deceptively simple—to move packets from a\nsending host to a receiving host. To do so, two important network-layer functions\ncan be identified:\n•\nForwarding. When a packet arrives at a router’s input link, the router must move\nthe packet to the appropriate output link. For example, a packet arriving from\nHost H1 to Router R1 must be forwarded to the next router on a path to H2. In\nSection 4.3, we’ll look inside a router and examine how a packet is actually for-\nwarded from an input link to an output link within a router.\n•\nRouting. The network layer must determine the route or path taken by packets as\nthey flow from a sender to a receiver. The algorithms that calculate these paths\nare referred to as routing algorithms. A routing algorithm would determine, for\nexample, the path along which packets flow from H1 to H2.\nThe terms forwarding and routing are often used interchangeably by authors dis-\ncussing the network layer. We’ll use these terms much more precisely in this book.\nForwarding refers to the router-local action of transferring a packet from an input link\ninterface to the appropriate output link interface. Routing refers to the network-wide\nprocess that determines the end-to-end paths that packets take from source to destina-\ntion. Using a driving analogy, consider the trip from Pennsylvania to Florida under-\ntaken by our traveler back in Section 1.3.1. During this trip, our driver passes through\nmany interchanges en route to Florida. We can think of forwarding as the process of\ngetting through a single interchange: A car enters the interchange from one road and\ndetermines which road it should take to leave the interchange. We can think of routing\nas the process of planning the trip from Pennsylvania to Florida: Before embarking on\nthe trip, the driver has consulted a map and chosen one of many paths possible, with\neach path consisting of a series of road segments connected at interchanges.\nEvery router has a forwarding table. A router forwards a packet by examin-\ning the value of a field in the arriving packet’s header, and then using this header\nvalue to index into the router’s forwarding table. The value stored in the forward-\ning table entry for that header indicates the router’s outgoing link interface to\nwhich that packet is to be forwarded. Depending on the network-layer protocol,\nthe header value could be the destination address of the packet or an indication of\nthe connection to which the packet belongs. Figure 4.2 provides an example. In\nFigure 4.2, a packet with a header field value of 0111 arrives to a router. The\nrouter indexes into its forwarding table and determines that the output link\ninterface for this packet is interface 2. The router then internally forwards the\npacket to interface 2. In Section 4.3, we’ll look inside a router and examine the\nforwarding function in much greater detail.\nYou might now be wondering how the forwarding tables in the routers are con-\nfigured. This is a crucial issue, one that exposes the important interplay between\n308\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nrouting and forwarding. As shown in Figure 4.2, the routing algorithm determines\nthe values that are inserted into the routers’ forwarding tables. The routing algorithm\nmay be centralized (e.g., with an algorithm executing on a central site and down-\nloading routing information to each of the routers) or decentralized (i.e., with a\npiece of the distributed routing algorithm running in each router). In either case, a\nrouter receives routing protocol messages, which are used to configure its forward-\ning table. The distinct and different purposes of the forwarding and routing func-\ntions can be further illustrated by considering the hypothetical (and unrealistic, but\ntechnically feasible) case of a network in which all forwarding tables are configured\ndirectly by human network operators physically present at the routers. In this case,\nno routing protocols would be required! Of course, the human operators would need\nto interact with each other to ensure that the forwarding tables were configured in\nsuch a way that packets reached their intended destinations. It’s also likely that\nhuman configuration would be more error-prone and much slower to respond to\nchanges in the network topology than a routing protocol. We’re thus fortunate that\nall networks have both a forwarding and a routing function!\n4.1\n•\nINTRODUCTION\n309\nValue in arriving\npacket’s header\n1\n2\n3\nRouting algorithm\nLocal forwarding table\nheader value\n0100\n0101\n0111\n1001\n0111\n3\n2\n2\n1\noutput link\nFigure 4.2 \u0002 Routing algorithms determine values in forwarding tables"
    },
    {
      "chunk_id": "af801c3f-bb9b-4f84-9832-9a36685a4904",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.1.2 Network Service Models",
      "original_titles": [
        "4.1.2 Network Service Models"
      ],
      "path": "Chapter 4 The Network Layer > 4.1 Introduction > 4.1.2 Network Service Models",
      "start_page": 337,
      "end_page": 339,
      "token_count": 1769,
      "text": "While we’re on the topic of terminology, it’s worth mentioning two other terms\nthat are often used interchangeably, but that we will use more carefully. We’ll reserve\nthe term packet switch to mean a general packet-switching device that transfers a\npacket from input link interface to output link interface, according to the value in a field\nin the header of the packet. Some packet switches, called link-layer switches (exam-\nined in Chapter 5), base their forwarding decision on values in the fields of the link-\nlayer frame; switches are thus referred to as link-layer (layer 2) devices. Other packet\nswitches, called routers, base their forwarding decision on the value in the network-\nlayer field. Routers are  thus network-layer (layer 3) devices, but must also implement\nlayer 2 protocols as well, since layer 3 devices require the services of layer 2 to imple-\nment their (layer 3) functionality. (To fully appreciate this important distinction, you\nmight want to review Section 1.5.2, where we discuss network-layer datagrams and\nlink-layer frames and their relationship.) To confuse matters, marketing literature often\nrefers to “layer 3 switches” for routers with Ethernet interfaces, but these are really\nlayer 3 devices. Since our focus in this chapter is on the network layer, we use the term\nrouter in place of packet switch. We’ll even use the term router when talking about\npacket switches in virtual-circuit networks (soon to be discussed).\nConnection Setup\nWe just said that the network layer has two important functions, forwarding and rout-\ning. But we’ll soon see that in some computer networks there is actually a third impor-\ntant network-layer function, namely, connection setup. Recall from our study of TCP\nthat a three-way handshake is required before data can flow from sender to receiver.\nThis allows the sender and receiver to set up the needed state information (for example,\nsequence number and initial flow-control window size). In an analogous manner, some\nnetwork-layer architectures—for example, ATM, frame relay, and MPLS (which we\nwill study in Section 5.8)––require the routers along the chosen path from source to\ndestination to handshake with each other in order to set up state before network-layer\ndata packets within a given source-to-destination connection can begin to flow. In the\nnetwork layer, this process is referred to as connection setup. We’ll examine connec-\ntion setup in Section 4.2.\n4.1.2 Network Service Models\nBefore delving into the network layer, let’s take the broader view and consider the dif-\nferent types of service that might be offered by the network layer. When the transport\nlayer at a sending host transmits a packet into the network (that is, passes it down to\nthe network layer at the sending host), can the transport layer rely on the network layer\nto deliver the packet to the destination? When multiple packets are sent, will they be\ndelivered to the transport layer in the receiving host in the order in which they were\nsent? Will the amount of time between the sending of two sequential packet transmis-\nsions be the same as the amount of time between their reception? Will the network\n310\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nprovide any feedback about congestion in the network? What is the abstract view\n(properties) of the channel connecting the transport layer in the sending and receiving\nhosts? The answers to these questions and others are determined by the service model\nprovided by the network layer. The network service model defines the characteristics\nof end-to-end transport of packets between sending and receiving end systems.\nLet’s now consider some possible services that the network layer could provide.\nIn the sending host, when the transport layer passes a packet to the network layer,\nspecific services that could be provided by the network layer include:\n•\nGuaranteed delivery. This service guarantees that the packet will eventually\narrive at its destination.\n•\nGuaranteed delivery with bounded delay. This service not only guarantees deliv-\nery of the packet, but delivery within a specified host-to-host delay bound (for\nexample, within 100 msec).\nFurthermore, the following services could be provided to a flow of packets between\na given source and destination:\n•\nIn-order packet delivery. This service guarantees that packets arrive at the desti-\nnation in the order that they were sent.\n•\nGuaranteed minimal bandwidth. This network-layer service emulates the behavior\nof a transmission link of a specified bit rate (for example, 1 Mbps) between send-\ning and receiving hosts. As long as the sending host transmits bits (as part of pack-\nets) at a rate below the specified bit rate, then no packet is lost and each packet\narrives within a prespecified host-to-host delay (for example, within 40 msec).\n•\nGuaranteed maximum jitter. This service guarantees that the amount of time\nbetween the transmission of two successive packets at the sender is equal to the\namount of time between their receipt at the destination (or that this spacing\nchanges by no more than some specified value).\n•\nSecurity services. Using a secret session key known only by a source and desti-\nnation host, the network layer in the source host could encrypt the payloads of\nall datagrams being sent to the destination host. The network layer in the\ndestination host would then be responsible for decrypting the payloads. With\nsuch a service, confidentiality would be provided to all transport-layer segments\n(TCP and UDP) between the source and destination hosts. In addition to confi-\ndentiality, the network layer could provide data integrity and source authentica-\ntion services.\nThis is only a partial list of services that a network layer could provide—there are\ncountless variations possible.\nThe Internet’s network layer provides a single service, known as best-effort\nservice. From Table 4.1, it might appear that best-effort service is a euphemism for\n4.1\n•\nINTRODUCTION\n311\n\nno service at all. With best-effort service, timing between packets is not guaranteed\nto be preserved, packets are not guaranteed to be received in the order in which they\nwere sent, nor is the eventual delivery of transmitted packets guaranteed. Given this\ndefinition, a network that delivered no packets to the destination would satisfy the\ndefinition of best-effort delivery service. As we’ll discuss shortly, however, there\nare sound reasons for such a minimalist network-layer service model.\nOther network architectures have defined and implemented service models that\ngo beyond the Internet’s best-effort service. For example, the ATM network archi-\ntecture [MFA Forum 2012, Black 1995] provides for multiple service models, mean-\ning that different connections can be provided with different classes of service\nwithin the same network. A discussion of how an ATM network provides such serv-\nices is well beyond the scope of this book; our aim here is only to note that alterna-\ntives do exist to the Internet’s best-effort model. Two of the more important ATM\nservice models are constant bit rate and available bit rate service:\n•\nConstant bit rate (CBR) ATM network service. This was the first ATM service\nmodel to be standardized, reflecting early interest by the telephone companies in\nATM and the suitability of CBR service for carrying real-time, constant bit rate\naudio and video traffic. The goal of CBR service is conceptually simple—to pro-\nvide a flow of packets (known as cells in ATM terminology) with a virtual pipe\nwhose properties are the same as if a dedicated fixed-bandwidth transmission\nlink existed between sending and receiving hosts. With CBR service, a flow of\nATM cells is carried across the network in such a way that a cell’s end-to-end\ndelay, the variability in a cell’s end-to-end delay (that is, the jitter), and the fraction\nof cells that are lost or delivered late are all guaranteed to be less than specified\nvalues. These values are agreed upon by the sending host and the ATM network\nwhen the CBR connection is first established.\n312\nCHAPTER 4\n•\nTHE NETWORK LAYER\nTable 4.1 \u0002 Internet, ATM CBR, and ATM ABR service models\nNetwork\nArchitecture\nService\nModel\nBandwidth\nGuarantee\nNo-Loss\nGuarantee\nOrdering\nTiming\nInternet\nBest Effort\nNone\nNone\nAny order\npossible\nNot\nmaintained\nATM\nCBR\nGuaranteed\nconstant rate\nYes\nIn order\nMaintained\nATM\nABR\nGuaranteed\nminimum\nNone\nIn order\nNot\nmaintained\nCongestion\nIndication\nNone\nCongestion\nwill not occur\nCongestion\nindication\nprovided"
    },
    {
      "chunk_id": "6fe70fa3-e0be-4174-ac97-6481fee6a465",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.2 Virtual Circuit and Datagram Networks",
      "original_titles": [
        "4.2 Virtual Circuit and Datagram Networks"
      ],
      "path": "Chapter 4 The Network Layer > 4.2 Virtual Circuit and Datagram Networks",
      "start_page": 340,
      "end_page": 340,
      "token_count": 549,
      "text": "•\nAvailable bit rate (ABR) ATM network service. With the Internet offering so-\ncalled best-effort service, ATM’s ABR might best be characterized as being a\nslightly-better-than-best-effort service. As with the Internet service model,\ncells may be lost under ABR service. Unlike in the Internet, however, cells\ncannot be reordered (although they may be lost), and a minimum cell transmis-\nsion rate (MCR) is guaranteed to a connection using ABR service. If the net-\nwork has enough free resources at a given time, a sender may also be able to\nsend cells successfully at a higher rate than the MCR. Additionally, as we saw\nin Section 3.6, ATM ABR service can provide feedback to the sender (in terms\nof a congestion notification bit, or an explicit rate at which to send) that con-\ntrols how the sender adjusts its rate between the MCR and an allowable peak\ncell rate.\n4.2 Virtual Circuit and Datagram Networks\nRecall from Chapter 3 that a transport layer can offer applications connectionless\nservice or connection-oriented service between two processes. For example, the Inter-\nnet’s transport layer provides each application a choice between two services: UDP, a\nconnectionless service; or TCP, a connection-oriented service. In a similar manner, a\nnetwork layer can provide connectionless service or connection service between two\nhosts. Network-layer connection and connectionless services in many ways parallel\ntransport-layer connection-oriented and connectionless services. For example, a net-\nwork-layer connection service begins with handshaking between the source and desti-\nnation hosts; and a network-layer connectionless service does not have any\nhandshaking preliminaries.\nAlthough the network-layer connection and connectionless services have some\nparallels with transport-layer connection-oriented and connectionless services, there\nare crucial differences:\n•\nIn the network layer, these services are host-to-host services provided by the net-\nwork layer for the transport layer. In the transport layer these services are process-\nto-process services provided by the transport layer for the application layer.\n•\nIn all major computer network architectures to date (Internet, ATM, frame relay,\nand so on), the network layer provides either a host-to-host connectionless serv-\nice or a host-to-host connection service, but not both. Computer networks that\nprovide only a connection service at the network layer are called virtual-circuit\n(VC) networks; computer networks that provide only a connectionless service\nat the network layer are called datagram networks.\n•\nThe implementations of connection-oriented service in the transport layer and\nthe connection service in the network layer are fundamentally different. We saw\nin the previous chapter that the transport-layer connection-oriented service is\n4.2\n•\nVIRTUAL CIRCUIT AND DATAGRAM NETWORKS\n313"
    },
    {
      "chunk_id": "0b28a21c-5ad2-45dd-9bf4-b62e0c896cca",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.2.1 Virtual-Circuit Networks",
      "original_titles": [
        "4.2.1 Virtual-Circuit Networks"
      ],
      "path": "Chapter 4 The Network Layer > 4.2 Virtual Circuit and Datagram Networks > 4.2.1 Virtual-Circuit Networks",
      "start_page": 341,
      "end_page": 343,
      "token_count": 1505,
      "text": "implemented at the edge of the network in the end systems; we’ll see shortly that\nthe network-layer connection service is implemented in the routers in the net-\nwork core as well as in the end systems.\nVirtual-circuit and datagram networks are two fundamental classes of computer net-\nworks. They use very different information in making their forwarding decisions.\nLet’s now take a closer look at their implementations.\n4.2.1 Virtual-Circuit Networks\nWhile the Internet is a datagram network, many alternative network architectures—\nincluding those of ATM and frame relay—are virtual-circuit networks and, there-\nfore, use connections at the network layer. These network-layer connections are\ncalled virtual circuits (VCs). Let’s now consider how a VC service can be imple-\nmented in a computer network.\nA VC consists of (1) a path (that is, a series of links and routers) between the\nsource and destination hosts, (2) VC numbers, one number for each link along the\npath, and (3) entries in the forwarding table in each router along the path. A packet\nbelonging to a virtual circuit will carry a VC number in its header. Because a virtual\ncircuit may have a different VC number on each link, each intervening router must\nreplace the VC number of each traversing packet with a new VC number. The new\nVC number is obtained from the forwarding table.\nTo illustrate the concept, consider the network shown in Figure 4.3. The numbers\nnext to the links of R1 in Figure 4.3 are the link interface numbers. Suppose now that\nHost A requests that the network establish a VC between itself and Host B. Suppose\nalso that the network chooses the path A-R1-R2-B and assigns VC numbers 12, 22,\nand 32 to the three links in this path for this virtual circuit. In this case, when a packet\nin this VC leaves Host A, the value in the VC number field in the packet header is 12;\nwhen it leaves R1, the value is 22; and when it leaves R2, the value is 32.\nHow does the router determine the replacement VC number for a packet tra-\nversing the router? For a VC network, each router’s forwarding table includes VC\n314\nCHAPTER 4\n•\nTHE NETWORK LAYER\nR1\nR2\nA\nB\n1\n2\n3\n1\n2\n3\nR3\nR4\nFigure 4.3 \u0002 A simple virtual circuit network\n\n4.2\n•\nVIRTUAL CIRCUIT AND DATAGRAM NETWORKS\n315\nnumber translation; for example, the forwarding table in R1 might look something\nlike this:\nWhenever a new VC is established across a router, an entry is added to the forward-\ning table. Similarly, whenever a VC terminates, the appropriate entries in each table\nalong its path are removed.\nYou might be wondering why a packet doesn’t just keep the same VC number\non each of the links along its route. The answer is twofold. First, replacing the num-\nber from link to link reduces the length of the VC field in the packet header. Second,\nand more importantly, VC setup is considerably simplified by permitting a different\nVC number at each link along the path of the VC. Specifically, with multiple VC\nnumbers, each link in the path can choose a VC number independently of the VC\nnumbers chosen at other links along the path. If a common VC number were required\nfor all links along the path, the routers would have to exchange and process a sub-\nstantial number of messages to agree on a common VC number (e.g., one that is not\nbeing used by any other existing VC at these routers) to be used for a connection.\nIn a VC network, the network’s routers must maintain connection state infor-\nmation for the ongoing connections. Specifically, each time a new connection is\nestablished across a router, a new connection entry must be added to the router’s for-\nwarding table; and each time a connection is released, an entry must be removed\nfrom the table. Note that even if there is no VC-number translation, it is still neces-\nsary to maintain connection state information that associates VC numbers with out-\nput interface numbers. The issue of whether or not a router maintains connection\nstate information for each ongoing connection is a crucial one—one that we’ll return\nto repeatedly in this book.\nThere are three identifiable phases in a virtual circuit:\n•\nVC setup. During the setup phase, the sending transport layer contacts the net-\nwork layer, specifies the receiver’s address, and waits for the network to set up\nthe VC. The network layer determines the path between sender and receiver, that\nis, the series of links and routers through which all packets of the VC will travel.\nThe network layer also determines the VC number for each link along the path.\nFinally, the network layer adds an entry in the forwarding table in each router\nIncoming Interface\nIncoming VC #\nOutgoing Interface\nOutgoing VC #\n1\n12\n2\n22\n2\n63\n1\n18\n3\n7\n2\n17\n1\n97\n3\n87\n...\n...\n...\n...\n\nalong the path. During VC setup, the network layer may also reserve resources\n(for example, bandwidth) along the path of the VC.\n•\nData transfer. As shown in Figure 4.4, once the VC has been established, pack-\nets can begin to flow along the VC.\n•\nVC teardown. This is initiated when the sender (or receiver) informs the network\nlayer of its desire to terminate the VC. The network layer will then typically\ninform the end system on the other side of the network of the call termination\nand update the forwarding tables in each of the packet routers on the path to indi-\ncate that the VC no longer exists.\nThere is a subtle but important distinction between VC setup at the network\nlayer and connection setup at the transport layer (for example, the TCP three-way\nhandshake we studied in Chapter 3). Connection setup at the transport layer\ninvolves only the two end systems. During transport-layer connection setup, the\ntwo end systems alone determine the parameters (for example, initial sequence\nnumber and flow-control window size) of their transport-layer connection.\nAlthough the two end systems are aware of the transport-layer connection, the\nrouters within the network are completely oblivious to it. On the other hand, with\na VC network layer, routers along the path between the two end systems are\ninvolved in VC setup, and each router is fully aware of all the VCs passing\nthrough it.\nThe messages that the end systems send into the network to initiate or terminate a\nVC, and the messages passed between the routers to set up the VC (that is, to modify\nconnection state in router tables) are known as signaling messages, and the protocols\n316\nCHAPTER 4\n•\nTHE NETWORK LAYER\nTransport\nData link\nPhysical\nApplication\nNetwork\nTransport\nData link\nPhysical\nApplication\nNetwork\n1. Initiate call\n2. Incoming call\n5. Data flow\n    begins\n6. Receive\n    data\n4. Call connected\n3. Accept call\nFigure 4.4 \u0002 Virtual-circuit setup"
    },
    {
      "chunk_id": "fbf03898-6549-4ebc-888d-c3eb7d9aacdb",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.2.2 Datagram Networks",
      "original_titles": [
        "4.2.2 Datagram Networks"
      ],
      "path": "Chapter 4 The Network Layer > 4.2 Virtual Circuit and Datagram Networks > 4.2.2 Datagram Networks",
      "start_page": 344,
      "end_page": 345,
      "token_count": 789,
      "text": "used to exchange these messages are often referred to as signaling protocols. VC setup\nis shown pictorially in Figure 4.4. We’ll not cover VC signaling protocols in this book;\nsee [Black 1997] for a general discussion of signaling in connection-oriented networks\nand [ITU-T Q.2931 1995] for the specification of ATM’s Q.2931 signaling protocol.\n4.2.2 Datagram Networks\nIn a datagram network, each time an end system wants to send a packet, it stamps\nthe packet with the address of the destination end system and then pops the packet\ninto the network. As shown in Figure 4.5, there is no VC setup and routers do not\nmaintain any VC state information (because there are no VCs!).\nAs a packet is transmitted from source to destination, it passes through a series\nof routers. Each of these routers uses the packet’s destination address to forward the\npacket. Specifically, each router has a forwarding table that maps destination\naddresses to link interfaces; when a packet arrives at the router, the router uses the\npacket’s destination address to look up the appropriate output link interface in the\nforwarding table. The router then intentionally forwards the packet to that output\nlink interface.\nTo get some further insight into the lookup operation, let’s look at a specific\nexample. Suppose that all destination addresses are 32 bits (which just happens to\nbe the length of the destination address in an IP datagram). A brute-force implemen-\ntation of the forwarding table would have one entry for every possible destination\naddress. Since there are more than 4 billion possible addresses, this option is totally\nout of the question.\n4.2\n•\nVIRTUAL CIRCUIT AND DATAGRAM NETWORKS\n317\nTransport\n1. Send\n    data\n2. Receive\n    data\nData link\nPhysical\nApplication\nNetwork\nTransport\nData link\nPhysical\nApplication\nNetwork\nFigure 4.5 \u0002 Datagram network\n\nNow let’s further suppose that our router has four links, numbered 0 through 3,\nand that packets are to be forwarded to the link interfaces as follows:\nDestination Address Range\nLink Interface\n11001000 00010111 00010000 00000000\nthrough\n0\n11001000 00010111 00010111 11111111\n11001000 00010111 00011000 00000000\nthrough\n1\n11001000 00010111 00011000 11111111\n11001000 00010111 00011001 00000000\nthrough\n2\n11001000 00010111 00011111 11111111\notherwise\n3\nClearly, for this example, it is not necessary to have 4 billion entries in the router’s\nforwarding table. We could, for example, have the following forwarding table with\njust four entries:\nPrefix Match\nLink Interface\n11001000 00010111 00010\n0\n11001000 00010111 00011000\n1\n11001000 00010111 00011\n2\notherwise\n3\nWith this style of forwarding table, the router matches a prefix of the packet’s desti-\nnation address with the entries in the table; if there’s a match, the router forwards\nthe packet to a link associated with the match. For example, suppose the packet’s\ndestination address is 11001000 00010111 00010110 10100001; because the 21-bit\nprefix of this address matches the first entry in the table, the router forwards the\npacket to link interface 0. If a prefix doesn’t match any of the first three entries, then\nthe router forwards the packet to interface 3. Although this sounds simple enough,\nthere’s an important subtlety here. You may have noticed that it is possible for a des-\ntination address to match more than one entry. For example, the first 24 bits of the\naddress 11001000 00010111 00011000 10101010 match the second entry in the\ntable, and the first 21 bits of the address match the third entry in the table. When\nthere are multiple matches, the router uses the longest prefix matching rule; that\nis, it finds the longest matching entry in the table and forwards the packet to the link\n318\nCHAPTER 4\n•\nTHE NETWORK LAYER"
    },
    {
      "chunk_id": "6e174af1-20c7-434b-ae99-feb5d3f17a51",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.2.3 Origins of VC and Datagram Networks",
      "original_titles": [
        "4.2.3 Origins of VC and Datagram Networks"
      ],
      "path": "Chapter 4 The Network Layer > 4.2 Virtual Circuit and Datagram Networks > 4.2.3 Origins of VC and Datagram Networks",
      "start_page": 346,
      "end_page": 346,
      "token_count": 624,
      "text": "interface associated with the longest prefix match. We’ll see exactly why this\nlongest prefix-matching rule is used when we study Internet addressing in more\ndetail in Section 4.4.\nAlthough routers in datagram networks maintain no connection state informa-\ntion, they nevertheless maintain forwarding state information in their forwarding\ntables. However, the time scale at which this forwarding state information changes\nis relatively slow. Indeed, in a datagram network the forwarding tables are modified\nby the routing algorithms, which typically update a forwarding table every one-to-\nfive minutes or so. In a VC network, a forwarding table in a router is modified\nwhenever a new connection is set up through the router or whenever an existing\nconnection through the router is torn down. This could easily happen at a microsec-\nond timescale in a backbone, tier-1 router.\nBecause forwarding tables in datagram networks can be modified at any time, a\nseries of packets sent from one end system to another may follow different paths\nthrough the network and may arrive out of order. [Paxson 1997] and [Jaiswal 2003]\npresent interesting measurement studies of packet reordering and other phenomena\nin the public Internet.\n4.2.3 Origins of VC and Datagram Networks\nThe evolution of datagram and VC networks reflects their origins. The notion of a\nvirtual circuit as a central organizing principle has its roots in the telephony world,\nwhich uses real circuits. With call setup and per-call state being maintained at the\nrouters within the network, a VC network is arguably more complex than a data-\ngram network (although see [Molinero-Fernandez 2002] for an interesting compari-\nson of the complexity of circuit- versus packet-switched networks). This, too, is in\nkeeping with its telephony heritage. Telephone networks, by necessity, had their\ncomplexity within the network, since they were connecting dumb end-system\ndevices such as rotary telephones. (For those too young to know, a rotary phone is\nan analog telephone with no buttons—only a dial.)\nThe Internet as a datagram network, on the other hand, grew out of the need to\nconnect computers together. Given more sophisticated end-system devices, the\nInternet architects chose to make the network-layer service model as simple as pos-\nsible. As we have already seen in Chapters 2 and 3, additional functionality (for\nexample, in-order delivery, reliable data transfer, congestion control, and DNS name\nresolution) is then implemented at a higher layer, in the end systems. This inverts\nthe model of the telephone network, with some interesting consequences:\n•\nSince the resulting Internet network-layer service model makes minimal (no!)\nservice guarantees, it imposes minimal requirements on the network layer. This\nmakes it easier to interconnect networks that use very different link-layer tech-\nnologies (for example, satellite, Ethernet, fiber, or radio) that have very different\ntransmission rates and loss characteristics. We will address the interconnection\nof IP networks in detail in Section 4.4.\n4.2\n•\nVIRTUAL CIRCUIT AND DATAGRAM NETWORKS\n319"
    },
    {
      "chunk_id": "c0101557-ae93-4422-b1d5-2e4b042c368a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.3 What’s Inside a Router?",
      "original_titles": [
        "4.3 What’s Inside a Router?"
      ],
      "path": "Chapter 4 The Network Layer > 4.3 What’s Inside a Router?",
      "start_page": 347,
      "end_page": 348,
      "token_count": 959,
      "text": "•\nAs we saw in Chapter 2, applications such as e-mail, the Web, and even some\nnetwork infrastructure services such as the DNS are implemented in hosts\n(servers) at the network edge. The ability to add a new service simply by attach-\ning a host to the network and defining a new application-layer protocol (such as\nHTTP) has allowed new Internet applications such as the Web to be deployed in\na remarkably short period of time.\n4.3\nWhat’s Inside a Router?\nNow that we’ve overviewed the network layer’s services and functions, let’s turn\nour attention to its forwarding function—the actual transfer of packets from a\nrouter’s incoming links to the appropriate outgoing links at that router. We\nalready took a brief look at a few aspects of forwarding in Section 4.2, namely,\naddressing and longest prefix matching. We mention here in passing that the terms\nforwarding and switching are often used interchangeably by computer-networking\nresearchers and practitioners; we’ll use both terms interchangeably in this\ntextbook as well.\nA high-level view of a generic router architecture is shown in Figure 4.6. Four\nrouter components can be identified:\n•\nInput ports. An input port performs several key functions. It performs the\nphysical layer function of terminating an incoming physical link at a router;\nthis is shown in the leftmost box of the input port and the rightmost box of the\noutput port in Figure 4.6. An input port also performs link-layer functions\nneeded to interoperate with the link layer at the other side of the incoming\nlink; this is represented by the middle boxes in the input and output ports. Per-\nhaps most crucially, the lookup function is also performed at the input port;\nthis will occur in the rightmost box of the input port. It is here that the for-\nwarding table is consulted to determine the router output port to which an\narriving packet will be forwarded via the switching fabric. Control packets\n(for example, packets carrying routing protocol information) are forwarded\nfrom an input port to the routing processor. Note that the term port here—\nreferring to the physical input and output router interfaces—is distinctly \ndifferent from the software ports associated with network applications and\nsockets discussed in Chapters 2 and 3.\n•\nSwitching fabric. The switching fabric connects the router’s input ports to its \noutput ports. This switching fabric is completely contained within the router—\na network inside of a network router!\n•\nOutput ports. An output port stores packets received from the switching fabric\nand transmits these packets on the outgoing link by performing the necessary\nlink-layer and physical-layer functions. When a link is bidirectional (that is,\n320\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\ncarries traffic in both directions), an output port will typically be paired with the\ninput port for that link on the same line card (a printed circuit board containing\none or more input ports, which is connected to the switching fabric).\n•\nRouting processor. The routing processor executes the routing protocols (which\nwe’ll study in Section 4.6), maintains routing tables and attached link state infor-\nmation, and computes the forwarding table for the router. It also performs the\nnetwork management functions that we’ll study in Chapter 9.\nRecall that in Section 4.1.1 we distinguished between a router’s forwarding and\nrouting functions. A router’s input ports, output ports, and switching fabric\ntogether implement the forwarding function and are almost always implemented\nin hardware, as shown in Figure 4.6. These forwarding functions are sometimes\ncollectively referred to as the router forwarding plane. To appreciate why a\nhardware implementation is needed, consider that with a 10 Gbps input link and a\n64-byte IP datagram, the input port has only 51.2 ns to process the datagram\nbefore another datagram may arrive. If N ports are combined on a line card (as is\noften done in practice), the datagram-processing pipeline must operate N times\nfaster—far too fast for software implementation. Forwarding plane hardware can\nbe implemented either using a router vendor’s own hardware designs, or con-\nstructed using purchased merchant-silicon chips (e.g., as sold by companies such\nas Intel and Broadcom).\nWhile the forwarding plane operates at the nanosecond time scale, a router’s\ncontrol functions—executing the routing protocols, responding to attached links that\n4.3\n•\nWHAT’S INSIDE A ROUTER?\n321\nInput port\nOutput port\nInput port\nOutput port\nRouting\nprocessor\nRouting, management\ncontrol plane (software)\nForwarding\ndata plane (hardware)\nSwitch\nfabric\nFigure 4.6 \u0002 Router architecture"
    },
    {
      "chunk_id": "1cc2d26e-6b8d-4388-bfa5-894b94e3c3db",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.3.1 Input Processing",
      "original_titles": [
        "4.3.1 Input Processing"
      ],
      "path": "Chapter 4 The Network Layer > 4.3 What’s Inside a Router? > 4.3.1 Input Processing",
      "start_page": 349,
      "end_page": 350,
      "token_count": 1220,
      "text": "go up or down, and performing management functions such as those we’ll study in\nChapter 9—operate at the millisecond or second timescale. These router control\nplane functions are usually implemented in software and execute on the routing\nprocessor (typically a traditional CPU).\nBefore delving into the details of a router’s control and data plane, let’s return to\nour analogy of Section 4.1.1, where packet forwarding was compared to cars entering\nand leaving an interchange. Let’s suppose that the interchange is a roundabout, and that\nbefore a car enters the roundabout, a bit of processing is required—the car stops at an\nentry station and indicates its final destination (not at the local roundabout, but the ulti-\nmate destination of its journey). An attendant at the entry station looks up the final des-\ntination, determines the roundabout exit that leads to that final destination, and tells the\ndriver which roundabout exit to take. The car enters the roundabout (which may be\nfilled with other cars entering from other input roads and heading to other roundabout\nexits) and eventually leaves at the prescribed roundabout exit ramp, where it may\nencounter other cars leaving the roundabout at that exit.\nWe can recognize the principal router components in Figure 4.6 in this anal-\nogy—the entry road and entry station correspond to the input port (with a lookup\nfunction to determine to local outgoing port); the roundabout corresponds to the\nswitch fabric; and the roundabout exit road corresponds to the output port. With\nthis analogy, it’s instructive to consider where bottlenecks might occur. What hap-\npens if cars arrive blazingly fast (for example, the roundabout is in Germany or\nItaly!) but the station attendant is slow? How fast must the attendant work to ensure\nthere’s no backup on an entry road? Even with a blazingly fast attendant, what hap-\npens if cars traverse the roundabout slowly—can backups still occur? And what\nhappens if most of the entering cars all want to leave the roundabout at the same\nexit ramp—can backups occur at the exit ramp or elsewhere? How should the\nroundabout operate if we want to assign priorities to different cars, or block certain\ncars from entering the roundabout in the first place? These are all analogous to crit-\nical questions faced by router and switch designers.\nIn the following subsections, we’ll look at router functions in more detail. [Iyer\n2008, Chao 2001; Chuang 2005; Turner 1988; McKeown 1997a; Partridge 1998]\nprovide a discussion of specific router architectures. For concreteness, the ensuing\ndiscussion assumes a datagram network in which forwarding decisions are based\non the packet’s destination address (rather than a VC number in a virtual-circuit\nnetwork). However, the concepts and techniques are quite similar for a virtual-\ncircuit network.\n4.3.1 Input Processing\nA more detailed view of input processing is given in Figure 4.7. As discussed above,\nthe input port’s line termination function and link-layer processing implement the\nphysical and link layers for that individual input link. The lookup performed in the\ninput port is central to the router’s operation—it is here that the router uses the for-\nwarding table to look up the output port to which an arriving packet will be \n322\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nforwarded via the switching fabric. The forwarding table is computed and updated\nby the routing processor, with a shadow copy typically stored at each input port. The\nforwarding table is copied from the routing processor to the line cards over a sepa-\nrate bus (e.g., a PCI bus) indicated by the dashed line from the routing processor to\nthe input line cards in Figure 4.6. With a shadow copy, forwarding decisions can be\nmade locally, at each input port, without invoking the centralized routing processor\non a per-packet basis and thus avoiding a centralized processing bottleneck.\nGiven the existence of a forwarding table, lookup is conceptually simple—we just\nsearch through the forwarding table looking for the longest prefix match, as described\n4.3\n•\nWHAT’S INSIDE A ROUTER?\n323\nLine\ntermination\nData link\nprocessing\n(protocol,\ndecapsulation)\nLookup, fowarding,\nqueuing\nSwitch\nfabric\nFigure 4.7 \u0002 Input port processing\nCISCO SYSTEMS: DOMINATING THE NETWORK CORE\nAs of this writing 2012, Cisco employs more than 65,000 people. How did this\ngorilla of a networking company come to be? It all started in 1984 in the living room\nof a Silicon Valley apartment.\nLen Bosak and his wife Sandy Lerner were working at Stanford University when they\nhad the idea to build and sell Internet routers to research and academic institutions, the\nprimary adopters of the Internet at that time. Sandy Lerner came up with the name Cisco\n(an abbreviation for San Francisco), and she also designed the company’s bridge logo.\nCorporate headquarters was their living room, and they financed the project with credit\ncards and moonlighting consulting jobs. At the end of 1986, Cisco’s revenues reached\n$250,000 a month. At the end of 1987, Cisco succeeded in attracting venture capital—\n$2 million from Sequoia Capital in exchange for one-third of the company. Over the next\nfew years, Cisco continued to grow and grab more and more market share. At the same\ntime, relations between Bosak/Lerner and Cisco management became strained. Cisco\nwent public in 1990; in the same year Lerner and Bosak left the company.\nOver the years, Cisco has expanded well beyond the router market, selling security,\nwireless caching, Ethernet switch, datacenter infrastructure, video conferencing, and\nvoice-over IP products and services. However, Cisco is facing increased international\ncompetition, including from Huawei, a rapidly growing Chinese network-gear compa-\nny. Other sources of competition for Cisco in the router and switched Ethernet space\ninclude Alcatel-Lucent and Juniper.\nCASE HISTORY"
    },
    {
      "chunk_id": "afd8c4ad-1b58-4b48-9fe5-f1092421533f",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.3.2 Switching",
      "original_titles": [
        "4.3.2 Switching"
      ],
      "path": "Chapter 4 The Network Layer > 4.3 What’s Inside a Router? > 4.3.2 Switching",
      "start_page": 351,
      "end_page": 352,
      "token_count": 1092,
      "text": "in Section 4.2.2. But at Gigabit transmission rates, this lookup must be performed in\nnanoseconds (recall our earlier example of a 10 Gbps link and a 64-byte IP datagram).\nThus, not only must lookup be performed in hardware, but techniques beyond a simple\nlinear search through a large table are needed; surveys of fast lookup algorithms can be\nfound in [Gupta 2001, Ruiz-Sanchez 2001]. Special attention must also be paid to mem-\nory access times, resulting in designs with embedded on-chip DRAM and faster SRAM\n(used as a DRAM cache) memories. Ternary Content Address Memories (TCAMs) are\nalso often used for lookup. With a TCAM, a 32-bit IP address is presented to the mem-\nory, which returns the content of the forwarding table entry for that address in essen-\ntially constant time. The Cisco 8500 has a 64K CAM for each input port.\nOnce a packet’s output port has been determined via the lookup, the packet can\nbe sent into the switching fabric. In some designs, a packet may be temporarily\nblocked from entering the switching fabric if packets from other input ports are cur-\nrently using the fabric. A blocked packet will be queued at the input port and then\nscheduled to cross the fabric at a later point in time. We’ll take a closer look at the\nblocking, queuing, and scheduling of packets (at both input ports and output ports)\nin Section 4.3.4. Although “lookup” is arguably the most important action in input\nport processing, many other actions must be taken: (1) physical- and link-layer pro-\ncessing must occur, as discussed above; (2) the packet’s version number, checksum\nand time-to-live field—all of which we’ll study in Section 4.4.1—must be checked\nand the latter two fields rewritten;  and (3) counters used for network management\n(such as the number of IP datagrams received) must be updated.\nLet’s close our discussion of input port processing by noting that the input port\nsteps of looking up an IP address (“match”) then sending the packet into the switching\nfabric (“action”) is a specific case of a more general “match plus action” abstraction\nthat is performed in many networked devices, not just routers. In link-layer switches\n(covered in Chapter 5), link-layer destination addresses are looked up and several\nactions may be taken in addition to sending the frame into the switching fabric towards\nthe output port. In firewalls (covered in Chapter 8)—devices that filter out selected\nincoming packets—an incoming packet whose header matches a given criteria (e.g., a\ncombination of source/destination IP addresses and transport-layer port numbers) may\nbe prevented from being forwarded (action). In a network address translator (NAT, cov-\nered in Section 4.4), an incoming packet whose transport-layer port number matches a\ngiven value will have its port number rewritten before forwarding (action). Thus, the\n“match plus action” abstraction is both powerful and prevalent in network devices.\n4.3.2 Switching\nThe switching fabric is at the very heart of a router, as it is through this fabric that\nthe packets are actually switched (that is, forwarded) from an input port to an output\nport. Switching can be accomplished in a number of ways, as shown in Figure 4.8:\n•\nSwitching via memory. The simplest, earliest routers were traditional computers,\nwith switching between input and output ports being done under direct control of\n324\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nthe CPU (routing processor). Input and output ports functioned as traditional I/O\ndevices in a traditional operating system. An input port with an arriving packet\nfirst signaled the routing processor via an interrupt. The packet was then copied\nfrom the input port into processor memory. The routing processor then extracted\nthe destination address from the header, looked up the appropriate output port in\nthe forwarding table, and copied the packet to the output port’s buffers. In this\nscenario, if the memory bandwidth is such that B packets per second can be writ-\nten into, or read from, memory, then the overall forwarding throughput (the total\nrate at which packets are transferred from input ports to output ports) must be\nless than B/2. Note also that two packets cannot be forwarded at the same time,\neven if they have different destination ports, since only one memory read/write\nover the shared system bus can be done at a time.\nMany modern routers switch via memory. A major difference from early routers,\nhowever, is that the lookup of the destination address and the storing of the packet\ninto the appropriate memory location are performed by processing on the input\nline cards. In some ways, routers that switch via memory look very much like\nshared-memory multiprocessors, with the processing on a line card switching\n(writing) packets into the memory of the appropriate output port. Cisco’s Catalyst\n8500 series switches [Cisco 8500 2012] forward packets via a shared memory.\n4.3\n•\nWHAT’S INSIDE A ROUTER?\n325\nMemory\nA\nB\nC\nX\nY\nZ\nMemory\nKey:\nInput port\nOutput port\nA\nX\nY\nZ\nB\nC\nCrossbar\nA\nB\nC\nX\nY\nZ\nBus\nFigure 4.8 \u0002 Three switching techniques"
    },
    {
      "chunk_id": "124abf5b-0857-4375-8eeb-0aab07968d9e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.3.3 Output Processing",
      "original_titles": [
        "4.3.3 Output Processing"
      ],
      "path": "Chapter 4 The Network Layer > 4.3 What’s Inside a Router? > 4.3.3 Output Processing",
      "start_page": 353,
      "end_page": 353,
      "token_count": 717,
      "text": "•\nSwitching via a bus. In this approach, an input port transfers a packet directly to the\noutput port over a shared bus, without intervention by the routing processor. This is\ntypically done by having the input port pre-pend a switch-internal label (header) to\nthe packet indicating the local output port to which this packet is being transferred\nand transmitting the packet onto the bus. The packet is received by all output ports,\nbut only the port that matches the label will keep the packet. The label is then\nremoved at the output port, as this label is only used within the switch to cross the\nbus. If multiple packets arrive to the router at the same time, each at a different input\nport, all but one must wait since only one packet can cross the bus at a time. Because\nevery packet must cross the single bus, the switching speed of the router is limited\nto the bus speed; in our roundabout analogy, this is as if the roundabout could only\ncontain one car at a time. Nonetheless, switching via a bus is often sufficient for\nrouters that operate in small local area and enterprise networks. The Cisco 5600\n[Cisco Switches 2012] switches packets over a 32 Gbps backplane bus.\n•\nSwitching via an interconnection network. One way to overcome the bandwidth\nlimitation of a single, shared bus is to use a more sophisticated interconnection net-\nwork, such as those that have been used in the past to interconnect processors in a\nmultiprocessor computer architecture. A crossbar switch is an interconnection net-\nwork consisting of 2N buses that connect N input ports to N output ports, as shown\nin Figure 4.8. Each vertical bus intersects each horizontal bus at a crosspoint, which\ncan be opened or closed at any time by the switch fabric controller (whose logic is\npart of the switching fabric itself). When a packet arrives from port A and needs to\nbe forwarded to port Y, the switch controller closes the crosspoint at the intersection\nof busses A and Y, and port A then sends the packet onto its bus, which is picked up\n(only) by bus Y. Note that a packet from port B can be forwarded to port X at the\nsame time, since the A-to-Y and B-to-X packets use different input and output\nbusses. Thus, unlike the previous two switching approaches, crossbar networks are\ncapable of forwarding multiple packets in parallel. However, if two packets from\ntwo different input ports are destined to the same output port, then one will have to\nwait at the input, since only one packet can be sent over any given bus at a time.\nMore sophisticated interconnection networks use multiple stages of switching\nelements to allow packets from different input ports to proceed towards the same\noutput port at the same time through the switching fabric. See [Tobagi 1990] for\na survey of switch architectures. Cisco 12000 family switches [Cisco 12000\n2012] use an interconnection network.\n4.3.3 Output Processing\nOutput port processing, shown in Figure 4.9, takes packets that have been stored in\nthe output port’s memory and transmits them over the output link. This includes\nselecting and de-queueing packets for transmission, and performing the needed link-\nlayer and physical-layer transmission functions.\n326\nCHAPTER 4\n•\nTHE NETWORK LAYER"
    },
    {
      "chunk_id": "6b1b7782-479e-405e-bb18-a08f3273e4b6",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.3.4 Where Does Queuing Occur?",
      "original_titles": [
        "4.3.4 Where Does Queuing Occur?"
      ],
      "path": "Chapter 4 The Network Layer > 4.3 What’s Inside a Router? > 4.3.4 Where Does Queuing Occur?",
      "start_page": 354,
      "end_page": 357,
      "token_count": 2096,
      "text": "4.3.4 Where Does Queueing Occur?\nIf we consider input and output port functionality and the configurations shown in \nFigure 4.8, it’s clear that packet queues may form at both the input ports and the out-\nput ports, just as we identified cases where cars may wait at the inputs and outputs of\nthe traffic intersection in our roundabout analogy. The location and extent of queueing\n(either at the input port queues or the output port queues) will depend on the traffic\nload, the relative speed of the switching fabric, and the line speed. Let’s now consider\nthese queues in a bit more detail, since as these queues grow large, the router’s mem-\nory can eventually be exhausted and packet loss will occur when no memory is avail-\nable to store arriving packets. Recall that in our earlier discussions, we said that\npackets were “lost within the network” or “dropped at a router.” It is here, at these\nqueues within a router, where such packets are actually dropped and lost.\nSuppose that the input and output line speeds (transmission rates) all have an\nidentical transmission rate of Rline packets per second, and that there are N input\nports and N output ports. To further simplify the discussion, let’s assume that all\npackets have the same fixed length, and the packets arrive to input ports in a syn-\nchronous manner. That is, the time to send a packet on any link is equal to the time\nto receive a packet on any link, and during such an interval of time, either zero or\none packet can arrive on an input link. Define the switching fabric transfer rate\nRswitch as the rate at which packets can be moved from input port to output port. If\nRswitch is N times faster than Rline, then only negligible queuing will occur at the\ninput ports. This is because even in the worst case, where all N input lines are\nreceiving packets, and all packets are to be forwarded to the same output port, each\nbatch of N packets (one packet per input port) can be cleared through the switch fab-\nric before the next batch arrives.\nBut what can happen at the output ports? Let’s suppose that Rswitch is still N\ntimes faster than Rline. Once again, packets arriving at each of the N input ports \nare destined to the same output port. In this case, in the time it takes to send a single\npacket onto the outgoing link, N new packets will arrive at this output port. Since\nthe output port can transmit only a single packet in a unit of time (the packet trans-\nmission time), the N arriving packets will have to queue (wait) for transmission over\nthe outgoing link. Then N more packets can possibly arrive in the time it takes to\n4.3\n•\nWHAT’S INSIDE A ROUTER?\n327\nLine\ntermination\nData link\nprocessing\n(protocol,\nencapsulation)\nQueuing (buffer\nmanagement)\nSwitch\nfabric\nFigure 4.9 \u0002 Output port processing\n\ntransmit just one of the N packets that had just previously been queued. And so on.\nEventually, the number of queued packets can grow large enough to exhaust avail-\nable memory at the output port, in which case packets are dropped.\nOutput port queuing is illustrated in Figure 4.10. At time t, a packet has arrived at\neach of the incoming input ports, each destined for the uppermost outgoing port.\nAssuming identical line speeds and a switch operating at three times the line speed,\none time unit later (that is, in the time needed to receive or send a packet), all three\noriginal packets have been transferred to the outgoing port and are queued awaiting\ntransmission. In the next time unit, one of these three packets will have been transmit-\nted over the outgoing link. In our example, two new packets have arrived at the incom-\ning side of the switch; one of these packets is destined for this uppermost output port.\nGiven that router buffers are needed to absorb the fluctuations in traffic load, the\nnatural question to ask is how much buffering is required. For many years, the rule of\nthumb [RFC 3439] for buffer sizing was that the amount of buffering (B) should be\nequal to an average round-trip time (RTT, say 250 msec) times the link capacity (C).\nThis result is based on an analysis of the queueing dynamics of a relatively small num-\nber of TCP flows [Villamizar 1994]. Thus, a 10 Gbps link with an RTT of 250 msec\nwould need an amount of buffering equal to B = RTT · C = 2.5 Gbits of buffers. Recent\n328\nCHAPTER 4\n•\nTHE NETWORK LAYER\nSwitch\nfabric\nOutput port contention at time t\nOne packet time later\nSwitch\nfabric\nFigure 4.10 \u0002 Output port queuing\n\ntheoretical and experimental efforts [Appenzeller 2004], however, suggest that when\nthere are a large number of TCP flows (N) passing through a link, the amount of buffer-\ning needed is B = RTT \u0002 C/√N\n—. With a large number of flows typically passing through\nlarge backbone router links (see, e.g., [Fraleigh 2003]), the value of N can be large, with\nthe decrease in needed buffer size becoming quite significant. [Appenzellar 2004; Wis-\nchik 2005; Beheshti 2008] provide very readable discussions of the buffer sizing prob-\nlem from a theoretical, implementation, and operational standpoint.\nA consequence of output port queuing is that a packet scheduler at the output\nport must choose one packet among those queued for transmission. This selection\nmight be done on a simple basis, such as first-come-first-served (FCFS) scheduling,\nor a more sophisticated scheduling discipline such as weighted fair queuing (WFQ),\nwhich shares the outgoing link fairly among the different end-to-end connections\nthat have packets queued for transmission. Packet scheduling plays a crucial role in\nproviding quality-of-service guarantees. We’ll thus cover packet scheduling exten-\nsively in Chapter 7. A discussion of output port packet scheduling disciplines is\n[Cisco Queue 2012].\nSimilarly, if there is not enough memory to buffer an incoming packet, a decision\nmust be made to either drop the arriving packet (a policy known as drop-tail) or\nremove one or more already-queued packets to make room for the newly arrived\npacket. In some cases, it may be advantageous to drop (or mark the header of) a packet\nbefore the buffer is full in order to provide a congestion signal to the sender. A number\nof packet-dropping and -marking policies (which collectively have become known as\nactive queue management (AQM) algorithms) have been proposed and analyzed\n[Labrador 1999, Hollot 2002]. One of the most widely studied and implemented AQM\nalgorithms is the Random Early Detection (RED) algorithm. Under RED, a\nweighted average is maintained for the length of the output queue. If the average\nqueue length is less than a minimum threshold, minth, when a packet arrives, the\npacket is admitted to the queue. Conversely, if the queue is full or the average queue\nlength is greater than a maximum threshold, maxth, when a packet arrives, the packet\nis marked or dropped. Finally, if the packet arrives to find an average queue length in\nthe interval [minth, maxth], the packet is marked or dropped with a probability that is\ntypically some function of the average queue length, minth, and maxth. A number of\nprobabilistic marking/dropping functions have been proposed, and various versions of\nRED have been analytically modeled, simulated, and/or implemented. [Christiansen\n2001] and [Floyd 2012] provide overviews and pointers to additional reading.\n4.3\n•\nWHAT’S INSIDE A ROUTER?\n329\nIf the switch fabric is not fast enough (relative to the input line speeds) to transfer\nall arriving packets through the fabric without delay, then packet queuing can also\noccur at the input ports, as packets must join input port queues to wait their turn to be\ntransferred through the switching fabric to the output port. To illustrate an important\nconsequence of this queuing, consider a crossbar switching fabric and suppose that \n(1) all link speeds are identical, (2) that one packet can be transferred from any one\ninput port to a given output port in the same amount of time it takes for a packet to be\nreceived on an input link, and (3) packets are moved from a given input queue to their\n\ndesired output queue in an FCFS manner. Multiple packets can be transferred in paral-\nlel, as long as their output ports are different. However, if two packets at the front of\ntwo input queues are destined for the same output queue, then one of the packets will\nbe blocked and must wait at the input queue—the switching fabric can transfer only\none packet to a given output port at a time.\nFigure 4.11 shows an example in which two packets (darkly shaded) at the front\nof their input queues are destined for the same upper-right output port. Suppose that\nthe switch fabric chooses to transfer the packet from the front of the upper-left\nqueue. In this case, the darkly shaded packet in the lower-left queue must wait. But\nnot only must this darkly shaded packet wait, so too must the lightly shaded packet\nthat is queued behind that packet in the lower-left queue, even though there is no\ncontention for the middle-right output port (the destination for the lightly shaded\npacket). This phenomenon is known as head-of-the-line (HOL) blocking in an\n330\nCHAPTER 4\n•\nTHE NETWORK LAYER\nSwitch\nfabric\nOutput port contention at time t —\none dark packet can be transferred\nLight blue packet experiences HOL blocking\nSwitch\nfabric\nKey:\ndestined for upper output \nport\ndestined for middle output \nport\ndestined for lower output \nport\nFigure 4.11 \u0002 HOL blocking at an input queued switch"
    },
    {
      "chunk_id": "5459e937-8b12-41d1-aae4-ac9b1a3e0d02",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.3.5 The Routing Control Plane",
      "original_titles": [
        "4.3.5 The Routing Control Plane"
      ],
      "path": "Chapter 4 The Network Layer > 4.3 What’s Inside a Router? > 4.3.5 The Routing Control Plane",
      "start_page": 358,
      "end_page": 358,
      "token_count": 562,
      "text": "input-queued switch—a queued packet in an input queue must wait for transfer\nthrough the fabric (even though its output port is free) because it is blocked by\nanother packet at the head of the line. [Karol 1987] shows that due to HOL block-\ning, the input queue will grow to unbounded length (informally, this is equivalent to\nsaying that significant packet loss will occur) under certain assumptions as soon as\nthe packet arrival rate on the input links reaches only 58 percent of their capacity. A\nnumber of solutions to HOL blocking are discussed in [McKeown 1997b].\n4.3.5 The Routing Control Plane\nIn our discussion thus far and in Figure 4.6, we’ve implicitly assumed that the rout-\ning control plane fully resides and executes in a routing processor within the router.\nThe network-wide routing control plane is thus decentralized—with different pieces\n(e.g., of a routing algorithm) executing at different routers and interacting by send-\ning control messages to each other. Indeed, today’s Internet routers and the routing\nalgorithms we’ll study in Section 4.6 operate in exactly this manner. Additionally,\nrouter and switch vendors bundle their hardware data plane and software control\nplane together into closed (but inter-operable) platforms in a vertically integrated\nproduct.\nRecently, a number of researchers [Caesar 2005a, Casado 2009, McKeown\n2008] have begun exploring new router control plane architectures in which part of\nthe control plane is implemented in the routers (e.g., local measurement/reporting of\nlink state, forwarding table installation and maintenance) along with the data plane,\nand part of the control plane can be implemented externally to the router (e.g., in a\ncentralized server, which could perform route calculation). A well-defined API dic-\ntates how these two parts interact and communicate with each other. These\nresearchers argue that separating the software control plane from the hardware data\nplane (with a minimal router-resident control plane) can simplify routing by replac-\ning distributed routing calculation with centralized routing calculation, and enable\nnetwork innovation by allowing different customized control planes to operate over\nfast hardware data planes.\n4.4 The Internet Protocol (IP): Forwarding and\nAddressing in the Internet\nOur discussion of network-layer addressing and forwarding thus far has been\nwithout reference to any specific computer network. In this section, we’ll turn our\nattention to how addressing and forwarding are done in the Internet. We’ll see that\nInternet addressing and forwarding are important components of the Internet\nProtocol (IP). There are two versions of IP in use today. We’ll first examine the\nwidely deployed IP protocol version 4, which is usually referred to simply as IPv4\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n331"
    },
    {
      "chunk_id": "378e4a89-7498-471c-aff1-745fbf259f05",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.4 The Internet Protocol (IP): Forwarding and Addressing in the Internet",
      "original_titles": [
        "4.4 The Internet Protocol (IP): Forwarding and Addressing in the Internet"
      ],
      "path": "Chapter 4 The Network Layer > 4.4 The Internet Protocol (IP): Forwarding and Addressing in the Internet",
      "start_page": 358,
      "end_page": 358,
      "token_count": 562,
      "text": "input-queued switch—a queued packet in an input queue must wait for transfer\nthrough the fabric (even though its output port is free) because it is blocked by\nanother packet at the head of the line. [Karol 1987] shows that due to HOL block-\ning, the input queue will grow to unbounded length (informally, this is equivalent to\nsaying that significant packet loss will occur) under certain assumptions as soon as\nthe packet arrival rate on the input links reaches only 58 percent of their capacity. A\nnumber of solutions to HOL blocking are discussed in [McKeown 1997b].\n4.3.5 The Routing Control Plane\nIn our discussion thus far and in Figure 4.6, we’ve implicitly assumed that the rout-\ning control plane fully resides and executes in a routing processor within the router.\nThe network-wide routing control plane is thus decentralized—with different pieces\n(e.g., of a routing algorithm) executing at different routers and interacting by send-\ning control messages to each other. Indeed, today’s Internet routers and the routing\nalgorithms we’ll study in Section 4.6 operate in exactly this manner. Additionally,\nrouter and switch vendors bundle their hardware data plane and software control\nplane together into closed (but inter-operable) platforms in a vertically integrated\nproduct.\nRecently, a number of researchers [Caesar 2005a, Casado 2009, McKeown\n2008] have begun exploring new router control plane architectures in which part of\nthe control plane is implemented in the routers (e.g., local measurement/reporting of\nlink state, forwarding table installation and maintenance) along with the data plane,\nand part of the control plane can be implemented externally to the router (e.g., in a\ncentralized server, which could perform route calculation). A well-defined API dic-\ntates how these two parts interact and communicate with each other. These\nresearchers argue that separating the software control plane from the hardware data\nplane (with a minimal router-resident control plane) can simplify routing by replac-\ning distributed routing calculation with centralized routing calculation, and enable\nnetwork innovation by allowing different customized control planes to operate over\nfast hardware data planes.\n4.4 The Internet Protocol (IP): Forwarding and\nAddressing in the Internet\nOur discussion of network-layer addressing and forwarding thus far has been\nwithout reference to any specific computer network. In this section, we’ll turn our\nattention to how addressing and forwarding are done in the Internet. We’ll see that\nInternet addressing and forwarding are important components of the Internet\nProtocol (IP). There are two versions of IP in use today. We’ll first examine the\nwidely deployed IP protocol version 4, which is usually referred to simply as IPv4\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n331"
    },
    {
      "chunk_id": "becca4b9-36c3-4406-96ae-605ea57a7bf8",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.4.1 Datagram Format",
      "original_titles": [
        "4.4.1 Datagram Format"
      ],
      "path": "Chapter 4 The Network Layer > 4.4 The Internet Protocol (IP): Forwarding and Addressing in the Internet > 4.4.1 Datagram Format",
      "start_page": 359,
      "end_page": 364,
      "token_count": 3238,
      "text": "[RFC 791]. We’ll examine IP version 6 [RFC 2460; RFC 4291], which has been\nproposed to replace IPv4, at the end of this section.\nBut before beginning our foray into IP, let’s take a step back and consider the\ncomponents that make up the Internet’s network layer. As shown in Figure 4.12,\nthe Internet’s network layer has three major components. The first component is\nthe IP protocol, the topic of this section. The second major component is the rout-\ning component, which determines the path a datagram follows from source to des-\ntination. We mentioned earlier that routing protocols compute the forwarding\ntables that are used to forward packets through the network. We’ll study the \nInternet’s routing protocols in Section 4.6. The final component of the network\nlayer is a facility to report errors in datagrams and respond to requests for certain\nnetwork-layer information. We’ll cover the Internet’s network-layer error- and\ninformation-reporting protocol, the Internet Control Message Protocol (ICMP), in\nSection 4.4.3.\n4.4.1 Datagram Format\nRecall that a network-layer packet is referred to as a datagram. We begin our study\nof IP with an overview of the syntax and semantics of the IPv4 datagram. You\nmight be thinking that nothing could be drier than the syntax and semantics of a\npacket’s bits. Nevertheless, the datagram plays a central role in the Internet—every\nnetworking student and professional needs to see it, absorb it, and master it. The\n332\nCHAPTER 4\n•\nTHE NETWORK LAYER\nRouting protocols\n• path selection\n• RIP, OSPF, BGP\nIP protocol\n• addressing conventions\n• datagram format\n• packet handling\n   conventions\nICMP protocol\n• error reporting\n• router “signaling”\nForwarding\ntable\nTransport layer: TCP, UDP\nLink layer\nPhysical layer\nNetwork layer\nFigure 4.12 \u0002 A look inside the Internet’s network layer\n\nIPv4 datagram format is shown in Figure 4.13. The key fields in the IPv4 datagram\nare the following:\n•\nVersion number. These 4 bits specify the IP protocol version of the datagram.\nBy looking at the version number, the router can determine how to interpret\nthe remainder of the IP datagram. Different versions of IP use different data-\ngram formats. The datagram format for the current version of IP, IPv4, is\nshown in Figure 4.13. The datagram format for the new version of IP (IPv6) is\ndiscussed at the end of this section.\n•\nHeader length. Because an IPv4 datagram can contain a variable number of\noptions (which are included in the IPv4 datagram header), these 4 bits are needed\nto determine where in the IP datagram the data actually begins. Most IP data-\ngrams do not contain options, so the typical IP datagram has a 20-byte header.\n•\nType of service. The type of service (TOS) bits were included in the IPv4 header\nto allow different types of IP datagrams (for example, datagrams particularly\nrequiring low delay, high throughput, or reliability) to be distinguished from each\nother. For example, it might be useful to distinguish real-time datagrams (such as\nthose used by an IP telephony application) from non-real-time traffic (for exam-\nple, FTP). The specific level of service to be provided is a policy issue deter-\nmined by the router’s administrator. We’ll explore the topic of differentiated\nservice in Chapter 7.\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n333\nVersion\nType of service\nHeader\nlength\nUpper-layer\nprotocol\n16-bit Identifier\nTime-to-live\n13-bit Fragmentation offset\nFlags\nDatagram length (bytes)\nHeader checksum\n32 bits\n32-bit Source IP address\n32-bit Destination IP address\nOptions (if any)\nData\nFigure 4.13 \u0002 IPv4 datagram format\n\n•\nDatagram length. This is the total length of the IP datagram (header plus data),\nmeasured in bytes. Since this field is 16 bits long, the theoretical maximum size\nof the IP datagram is 65,535 bytes. However, datagrams are rarely larger than\n1,500 bytes.\n•\nIdentifier, flags, fragmentation offset. These three fields have to do with so-called\nIP fragmentation, a topic we will consider in depth shortly. Interestingly, the new\nversion of IP, IPv6, does not allow for fragmentation at routers.\n•\nTime-to-live. The time-to-live (TTL) field is included to ensure that datagrams\ndo not circulate forever (due to, for example, a long-lived routing loop) in the\nnetwork. This field is decremented by one each time the datagram is processed\nby a router. If the TTL field reaches 0, the datagram must be dropped.\n•\nProtocol. This field is used only when an IP datagram reaches its final destina-\ntion. The value of this field indicates the specific transport-layer protocol to\nwhich the data portion of this IP datagram should be passed. For example, a\nvalue of 6 indicates that the data portion is passed to TCP, while a value of 17\nindicates that the data is passed to UDP. For a list of all possible values, see\n[IANA Protocol Numbers 2012]. Note that the protocol number in the IP data-\ngram has a role that is analogous to the role of the port number field in the transport-\nlayer segment. The protocol number is the glue that binds the network and transport\nlayers together, whereas the port number is the glue that binds the transport and\napplication layers together. We’ll see in Chapter 5 that the link-layer frame also\nhas a special field that binds the link layer to the network layer.\n•\nHeader checksum. The header checksum aids a router in detecting bit errors in a\nreceived IP datagram. The header checksum is computed by treating each 2 bytes\nin the header as a number and summing these numbers using 1s complement\narithmetic. As discussed in Section 3.3, the 1s complement of this sum, known\nas the Internet checksum, is stored in the checksum field. A router computes the\nheader checksum for each received IP datagram and detects an error condition if\nthe checksum carried in the datagram header does not equal the computed check-\nsum. Routers typically discard datagrams for which an error has been detected.\nNote that the checksum must be recomputed and stored again at each router, as\nthe TTL field, and possibly the options field as well, may change. An interesting\ndiscussion of fast algorithms for computing the Internet checksum is [RFC\n1071]. A question often asked at this point is, why does TCP/IP perform error\nchecking at both the transport and network layers? There are several reasons for\nthis repetition. First, note that only the IP header is checksummed at the IP layer,\nwhile the TCP/UDP checksum is computed over the entire TCP/UDP segment.\nSecond, TCP/UDP and IP do not necessarily both have to belong to the same pro-\ntocol stack. TCP can, in principle, run over a different protocol (for example,\nATM) and IP can carry data that will not be passed to TCP/UDP.\n•\nSource and destination IP addresses. When a source creates a datagram, it inserts\nits IP address into the source IP address field and inserts the address of the\n334\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nultimate destination into the destination IP address field. Often the source host\ndetermines the destination address via a DNS lookup, as discussed in Chapter 2.\nWe’ll discuss IP addressing in detail in Section 4.4.2.\n•\nOptions. The options fields allow an IP header to be extended. Header options\nwere meant to be used rarely—hence the decision to save overhead by not\nincluding the information in options fields in every datagram header. However,\nthe mere existence of options does complicate matters—since datagram headers\ncan be of variable length, one cannot determine a priori where the data field will\nstart. Also, since some datagrams may require options processing and others may\nnot, the amount of time needed to process an IP datagram at a router can vary\ngreatly. These considerations become particularly important for IP processing in\nhigh-performance routers and hosts. For these reasons and others, IP options\nwere dropped in the IPv6 header, as discussed in Section 4.4.4.\n•\nData (payload). Finally, we come to the last and most important field—the rai-\nson d’être for the datagram in the first place! In most circumstances, the data\nfield of the IP datagram contains the transport-layer segment (TCP or UDP) to\nbe delivered to the destination. However, the data field can carry other types of\ndata, such as ICMP messages (discussed in Section 4.4.3).\nNote that an IP datagram has a total of 20 bytes of header (assuming no options). If\nthe datagram carries a TCP segment, then each (nonfragmented) datagram carries a\ntotal of 40 bytes of header (20 bytes of IP header plus 20 bytes of TCP header) along\nwith the application-layer message.\nIP Datagram Fragmentation\nWe’ll see in Chapter 5 that not all link-layer protocols can carry network-layer pack-\nets of the same size. Some protocols can carry big datagrams, whereas other proto-\ncols can carry only little packets. For example, Ethernet frames can carry up to 1,500\nbytes of data, whereas frames for some wide-area links can carry no more than 576\nbytes. The maximum amount of data that a link-layer frame can carry is called the\nmaximum transmission unit (MTU). Because each IP datagram is encapsulated\nwithin the link-layer frame for transport from one router to the next router, the MTU\nof the link-layer protocol places a hard limit on the length of an IP datagram. Having\na hard limit on the size of an IP datagram is not much of a problem. What is a prob-\nlem is that each of the links along the route between sender and destination can use\ndifferent link-layer protocols, and each of these protocols can have different MTUs.\nTo understand the forwarding issue better, imagine that you are a router that\ninterconnects several links, each running different link-layer protocols with differ-\nent MTUs. Suppose you receive an IP datagram from one link. You check your for-\nwarding table to determine the outgoing link, and this outgoing link has an MTU\nthat is smaller than the length of the IP datagram. Time to panic—how are you going\nto squeeze this oversized IP datagram into the payload field of the link-layer frame?\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n335\n\nThe solution is to fragment the data in the IP datagram into two or more smaller IP\ndatagrams, encapsulate each of these smaller IP datagrams in a separate link-layer\nframe; and send these frames over the outgoing link. Each of these smaller data-\ngrams is referred to as a fragment.\nFragments need to be reassembled before they reach the transport layer at the des-\ntination. Indeed, both TCP and UDP are expecting to receive complete, unfragmented\nsegments from the network layer. The designers of IPv4 felt that reassembling data-\ngrams in the routers would introduce significant complication into the protocol and\nput a damper on router performance. (If you were a router, would you want to be\nreassembling fragments on top of everything else you had to do?) Sticking to the prin-\nciple of keeping the network core simple, the designers of IPv4 decided to put the job\nof datagram reassembly in the end systems rather than in network routers.\nWhen a destination host receives a series of datagrams from the same source, it\nneeds to determine whether any of these datagrams are fragments of some original,\nlarger datagram. If some datagrams are fragments, it must further determine when it\nhas received the last fragment and how the fragments it has received should be\npieced back together to form the original datagram. To allow the destination host to\nperform these reassembly tasks, the designers of IP (version 4) put identification,\nflag, and fragmentation offset fields in the IP datagram header. When a datagram is\ncreated, the sending host stamps the datagram with an identification number as well\nas source and destination addresses. Typically, the sending host increments the iden-\ntification number for each datagram it sends. When a router needs to fragment a\ndatagram, each resulting datagram (that is, fragment) is stamped with the source\naddress, destination address, and identification number of the original datagram.\nWhen the destination receives a series of datagrams from the same sending host, it\ncan examine the identification numbers of the datagrams to determine which of the\ndatagrams are actually fragments of the same larger datagram. Because IP is an\nunreliable service, one or more of the fragments may never arrive at the destination.\nFor this reason, in order for the destination host to be absolutely sure it has received\nthe last fragment of the original datagram, the last fragment has a flag bit set to 0,\nwhereas all the other fragments have this flag bit set to 1. Also, in order for the des-\ntination host to determine whether a fragment is missing (and also to be able to\nreassemble the fragments in their proper order), the offset field is used to specify\nwhere the fragment fits within the original IP datagram.\nFigure 4.14 illustrates an example. A datagram of 4,000 bytes (20 bytes of IP\nheader plus 3,980 bytes of IP payload) arrives at a router and must be forwarded to\na link with an MTU of 1,500 bytes. This implies that the 3,980 data bytes in the\noriginal datagram must be allocated to three separate fragments (each of which is\nalso an IP datagram). Suppose that the original datagram is stamped with an identi-\nfication number of 777. The characteristics of the three fragments are shown in\nTable 4.2. The values in Table 4.2 reflect the requirement that the amount of origi-\nnal payload data in all but the last fragment be a multiple of 8 bytes, and that the off-\nset value be specified in units of 8-byte chunks.\n336\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n337\nFragmentation:\nIn: one large datagram (4,000 bytes)\nOut: 3 smaller datagrams\nReassembly:\nIn: 3 smaller datagrams\nOut: one large datagram (4,000 bytes)\nLink MTU: 1,500 bytes\nFigure 4.14 \u0002 IP fragmentation and reassembly\nTable 4.2 \u0002 IP fragments\nFragment\nBytes\nID\nOffset\nFlag\n1st fragment\n1,480 bytes in\nthe data field of\nthe IP datagram\nidentification \u0003 777\noffset \u0003 0 (meaning the data\nshould be inserted beginning\nat byte 0)\n2nd fragment\n1,480 bytes\nof data\nidentification \u0003 777\noffset \u0003 185 (meaning the data\nshould be inserted beginning at byte\n1,480. Note that 185 · 8 \u0003 1,480)\n3rd fragment\n1,020 bytes\n(\u0003 3,980–1,480–1,480)\nof data\nidentification \u0003 777\noffset \u0003 370 (meaning the data\nshould be inserted beginning at byte\n2,960. Note that 370 · 8 \u0003 2,960)\nflag \u0003 1 (meaning\nthere is more)\nflag \u0003 1 (meaning\nthere is more)\nflag \u0003 0 (meaning this\nis the last fragment)\nAt the destination, the payload of the datagram is passed to the transport layer\nonly after the IP layer has fully reconstructed the original IP datagram. If one or\nmore of the fragments does not arrive at the destination, the incomplete datagram is\ndiscarded and not passed to the transport layer. But, as we learned in the previous"
    },
    {
      "chunk_id": "32ce97d8-8a01-422a-bebb-49355ac4ab54",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.4.2 IPv4 Addressing",
      "original_titles": [
        "4.4.2 IPv4 Addressing"
      ],
      "path": "Chapter 4 The Network Layer > 4.4 The Internet Protocol (IP): Forwarding and Addressing in the Internet > 4.4.2 IPv4 Addressing",
      "start_page": 365,
      "end_page": 379,
      "token_count": 7640,
      "text": "chapter, if TCP is being used at the transport layer, then TCP will recover from this\nloss by having the source retransmit the data in the original datagram.\nWe have just learned that IP fragmentation plays an important role in gluing\ntogether the many disparate link-layer technologies. But fragmentation also has its\ncosts. First, it complicates routers and end systems, which need to be designed to\naccommodate datagram fragmentation and reassembly. Second, fragmentation can\nbe used to create lethal DoS attacks, whereby the attacker sends a series of bizarre\nand unexpected fragments. A classic example is the Jolt2 attack, where the attacker\nsends a stream of small fragments to the target host, none of which has an offset of\nzero. The target can collapse as it attempts to rebuild datagrams out of the degener-\nate packets. Another class of exploits sends overlapping IP fragments, that is, frag-\nments whose offset values are set so that the fragments do not align properly.\nVulnerable operating systems, not knowing what to do with overlapping fragments,\ncan crash [Skoudis 2006]. As we’ll see at the end of this section, a new version of\nthe IP protocol, IPv6, does away with fragmentation altogether, thereby streamlin-\ning IP packet processing and making IP less vulnerable to attack.\nAt this book’s Web site, we provide a Java applet that generates fragments. You\nprovide the incoming datagram size, the MTU, and the incoming datagram identifi-\ncation. The applet automatically generates the fragments for you. See http://\nwww.awl.com/kurose-ross.\n4.4.2 IPv4 Addressing\nWe now turn our attention to IPv4 addressing. Although you may be thinking that\naddressing must be a straightforward topic, hopefully by the end of this chapter\nyou’ll be convinced that Internet addressing is not only a juicy, subtle, and interest-\ning topic but also one that is of central importance to the Internet. Excellent treat-\nments of IPv4 addressing are [3Com Addressing 2012] and the first chapter in\n[Stewart 1999].\nBefore discussing IP addressing, however, we’ll need to say a few words about\nhow hosts and routers are connected into the network. A host typically has only a\nsingle link into the network; when IP in the host wants to send a datagram, it does\nso over this link. The boundary between the host and the physical link is called an\ninterface. Now consider a router and its interfaces. Because a router’s job is to\nreceive a datagram on one link and forward the datagram on some other link, a\nrouter necessarily has two or more links to which it is connected. The boundary\nbetween the router and any one of its links is also called an interface. A router thus\nhas multiple interfaces, one for each of its links. Because every host and router is\ncapable of sending and receiving IP datagrams, IP requires each host and router\ninterface to have its own IP address. Thus, an IP address is technically associated\nwith an interface, rather than with the host or router containing that interface.\nEach IP address is 32 bits long (equivalently, 4 bytes), and there are thus a total\nof 232 possible IP addresses. By approximating 210 by 103, it is easy to see that there\n338\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nare about 4 billion possible IP addresses. These addresses are typically written in\nso-called dotted-decimal notation, in which each byte of the address is written in\nits decimal form and is separated by a period (dot) from other bytes in the address.\nFor example, consider the IP address 193.32.216.9. The 193 is the decimal equiv-\nalent of the first 8 bits of the address; the 32 is the decimal equivalent of the second\n8 bits of the address, and so on. Thus, the address 193.32.216.9 in binary notation is\n11000001 00100000 11011000 00001001\nEach interface on every host and router in the global Internet must have an IP\naddress that is globally unique (except for interfaces behind NATs, as discussed at\nthe end of this section). These addresses cannot be chosen in a willy-nilly manner,\nhowever. A portion of an interface’s IP address will be determined by the subnet to\nwhich it is connected.\nFigure 4.15 provides an example of IP addressing and interfaces. In this figure,\none router (with three interfaces) is used to interconnect seven hosts. Take a close look\nat the IP addresses assigned to the host and router interfaces, as there are several things\nto notice. The three hosts in the upper-left portion of Figure 4.15, and the router inter-\nface to which they are connected, all have an IP address of the form 223.1.1.xxx. That\nis, they all have the same leftmost 24 bits in their IP address. The four interfaces are\nalso interconnected to each other by a network that contains no routers. This network\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n339\n223.1.1.1\n223.1.2.1\n223.1.2.2\n223.1.1.2\n223.1.1.4\n223.1.2.9\n223.1.3.27\n223.1.1.3\n223.1.3.1\n223.1.3.2\nFigure 4.15 \u0002 Interface addresses and subnets\n\ncould be interconnected by an Ethernet LAN, in which case the interfaces would be\ninterconnected by an Ethernet switch (as we’ll discuss in Chapter 5), or by a wireless\naccess point (as we’ll discuss in Chapter 6). We’ll represent this routerless network\nconnecting these hosts as a cloud for now, and dive into the internals of such networks\nin Chapters 5 and 6.\nIn IP terms, this network interconnecting three host interfaces and one router\ninterface forms a subnet [RFC 950]. (A subnet is also called an IP network or\nsimply a network in the Internet literature.) IP addressing assigns an address to this\nsubnet: 223.1.1.0/24, where the /24 notation, sometimes known as a subnet mask,\nindicates that the leftmost 24 bits of the 32-bit quantity define the subnet\naddress. The subnet 223.1.1.0/24 thus consists of the three host interfaces\n(223.1.1.1, 223.1.1.2, and 223.1.1.3) and one router interface (223.1.1.4). Any addi-\ntional hosts attached to the 223.1.1.0/24 subnet would be required to have an\naddress of the form 223.1.1.xxx. There are two additional subnets shown in Figure\n4.15: the 223.1.2.0/24 network and the 223.1.3.0/24 subnet. Figure 4.16 illustrates\nthe three IP subnets present in Figure 4.15.\nThe IP definition of a subnet is not restricted to Ethernet segments that connect\nmultiple hosts to a router interface. To get some insight here, consider Figure 4.17,\nwhich shows three routers that are interconnected with each other by point-to-point\nlinks. Each router has three interfaces, one for each point-to-point link and one for\nthe broadcast link that directly connects the router to a pair of hosts. What subnets\nare present here? Three subnets, 223.1.1.0/24, 223.1.2.0/24, and 223.1.3.0/24, are\nsimilar to the subnets we encountered in Figure 4.15. But note that there are three\n340\nCHAPTER 4\n•\nTHE NETWORK LAYER\n223.1.1.0/23\n223.1.2.0/23\n223.1.3.0/23\nFigure 4.16 \u0002 Subnet addresses\n\nadditional subnets in this example as well: one subnet, 223.1.9.0/24, for the interfaces\nthat connect routers R1 and R2; another subnet, 223.1.8.0/24, for the interfaces that\nconnect routers R2 and R3; and a third subnet, 223.1.7.0/24, for the interfaces that\nconnect routers R3 and R1. For a general interconnected system of routers and hosts,\nwe can use the following recipe to define the subnets in the system:\nTo determine the subnets, detach each interface from its host or router, creating\nislands of isolated networks, with interfaces terminating the end points of the\nisolated networks. Each of these isolated networks is called a subnet.\nIf we apply this procedure to the interconnected system in Figure 4.17, we get six\nislands or subnets.\nFrom the discussion above, it’s clear that an organization (such as a company\nor academic institution) with multiple Ethernet segments and point-to-point links\nwill have multiple subnets, with all of the devices on a given subnet having the same\nsubnet address. In principle, the different subnets could have quite different subnet\naddresses. In practice, however, their subnet addresses often have much in common.\nTo understand why, let’s next turn our attention to how addressing is handled in the\nglobal Internet.\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n341\n223.1.8.1\n223.1.8.0\n223.1.9.1\n223.1.7.1\n223.1.2.6\n223.1.2.1\n223.1.2.2\n223.1.3.1\n223.1.3.2\n223.1.1.3\n223.1.7.0\n223.1.9.2\n223.1.3.27\n223.1.1.1\n223.1.1.4\nR1\nR2\nR3\nFigure 4.17 \u0002 Three routers interconnecting six subnets\n\nThe Internet’s address assignment strategy is known as Classless Interdomain\nRouting (CIDR—pronounced cider) [RFC 4632]. CIDR generalizes the notion of\nsubnet addressing. As with subnet addressing, the 32-bit IP address is divided into\ntwo parts and again has the dotted-decimal form a.b.c.d/x, where x indicates the\nnumber of bits in the first part of the address.\nThe x most significant bits of an address of the form a.b.c.d/x constitute the\nnetwork portion of the IP address, and are often referred to as the prefix (or net-\nwork prefix) of the address. An organization is typically assigned a block of con-\ntiguous addresses, that is, a range of addresses with a common prefix (see the\nPrinciples in Practice sidebar). In this case, the IP addresses of devices within the\norganization will share the common prefix. When we cover the Internet’s BGP\n342\nCHAPTER 4\n•\nTHE NETWORK LAYER\nThis example of an ISP that connects eight organizations to the Internet nicely illustrates\nhow carefully allocated CIDRized addresses facilitate routing. Suppose, as shown in Figure\n4.18, that the ISP (which we’ll call Fly-By-Night-ISP) advertises to the outside world that it\nshould be sent any datagrams whose first 20 address bits match 200.23.16.0/20. The\nrest of the world need not know that within the address block 200.23.16.0/20 there are\nin fact eight other organizations, each with its own subnets. This ability to use a single pre-\nfix to advertise multiple networks is often referred to as address aggregation (also\nroute aggregation or route summarization).\nAddress aggregation works extremely well when addresses are allocated in blocks to\nISPs and then from ISPs to client organizations. But what happens when addresses are\nnot allocated in such a hierarchical manner? What would happen, for example, if Fly-By-\nNight-ISP acquires ISPs-R-Us and then has Organization 1 connect to the Internet through\nits subsidiary ISPs-R-Us? As shown in Figure 4.18, the subsidiary ISPs-R-Us owns the\naddress block 199.31.0.0/16, but Organization 1’s IP addresses are unfortunately out-\nside of this address block. What should be done here? Certainly, Organization 1 could\nrenumber all of its routers and hosts to have addresses within the ISPs-R-Us address\nblock. But this is a costly solution, and Organization 1 might well be reassigned to\nanother subsidiary in the future. The solution typically adopted is for Organization 1\nto keep its IP addresses in 200.23.18.0/23. In this case, as shown in Figure 4.19, \nFly-By-Night-ISP continues to advertise the address block 200.23.16.0/20 and ISPs-R-Us\ncontinues to advertise 199.31.0.0/16. However, ISPs-R-Us now also advertises the block\nof addresses for Organization 1, 200.23.18.0/23. When other routers in the larger\nInternet see the address blocks 200.23.16.0/20 (from Fly-By-Night-ISP) and\n200.23.18.0/23 (from ISPs-R-Us) and want to route to an address in the block\n200.23.18.0/23, they will use longest prefix matching (see Section 4.2.2), and route\ntoward ISPs-R-Us, as it advertises the longest (most specific) address prefix that matches\nthe destination address.\nPRINCIPLES IN PRACTICE\n\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n343\nOrganization 0\n200.23.16.0/23\nOrganization 1\nFly-By-Night-ISP\n“Send me anything\n  with addresses\n  beginning\n  200.23.16.0/20”\nISPs-R-Us\n200.23.18.0/23\nOrganization 2\n200.23.20.0/23\nOrganization 7\n200.23.30.0/23\nInternet\n“Send me anything\n  with addresses\n  beginning\n  199.31.0.0/16”\nFigure 4.18 \u0002 Hierarchical addressing and route aggregation\nOrganization 0\n200.23.16.0/23\nOrganization 2\nFly-By-Night-ISP\n“Send me anything\n  with addresses\n  beginning\n  200.23.16.0/20”\nISPs-R-Us\n200.23.20.0/23\nOrganization 7\n200.23.30.0/23\nOrganization 1\n200.23.18.0/23\nInternet\n“Send me anything\n  with addresses\n  beginning\n  199.31.0.0/16 or\n  200.23.18.0/23”\nFigure 4.19 \u0002 ISPs-R-Us has a more specific route to Organization 1\n\nrouting protocol in Section 4.6, we’ll see that only these x leading prefix bits are\nconsidered by routers outside the organization’s network. That is, when a router\noutside the organization forwards a datagram whose destination address is inside\nthe organization, only the leading x bits of the address need be considered. This\nconsiderably reduces the size of the forwarding table in these routers, since a sin-\ngle entry of the form a.b.c.d/x will be sufficient to forward packets to any destina-\ntion within the organization.\nThe remaining 32-x bits of an address can be thought of as distinguishing\namong the devices within the organization, all of which have the same network pre-\nfix. These are the bits that will be considered when forwarding packets at routers\nwithin the organization. These lower-order bits may (or may not) have an additional\nsubnetting structure, such as that discussed above. For example, suppose the first 21\nbits of the CIDRized address a.b.c.d/21 specify the organization’s network prefix\nand are common to the IP addresses of all devices in that organization. The remain-\ning 11 bits then identify the specific hosts in the organization. The organization’s\ninternal structure might be such that these 11 rightmost bits are used for subnetting\nwithin the organization, as discussed above. For example, a.b.c.d/24 might refer to a\nspecific subnet within the organization.\nBefore CIDR was adopted, the network portions of an IP address were con-\nstrained to be 8, 16, or 24 bits in length, an addressing scheme known as classful\naddressing, since subnets with 8-, 16-, and 24-bit subnet addresses were known as\nclass A, B, and C networks, respectively. The requirement that the subnet portion of\nan IP address be exactly 1, 2, or 3 bytes long turned out to be problematic for sup-\nporting the rapidly growing number of organizations with small and medium-sized\nsubnets. A class C (/24) subnet could accommodate only up to 28 – 2 = 254 hosts\n(two of the 28 = 256 addresses are reserved for special use)—too small for many\norganizations. However, a class B (/16) subnet, which supports up to 65,634 hosts,\nwas too large. Under classful addressing, an organization with, say, 2,000 hosts was\ntypically allocated a class B (/16) subnet address. This led to a rapid depletion of the\nclass B address space and poor utilization of the assigned address space. For exam-\nple, the organization that used a class B address for its 2,000 hosts was allocated\nenough of the address space for up to 65,534 interfaces—leaving more than 63,000\naddresses that could not be used by other organizations.\nWe would be remiss if we did not mention yet another type of IP address, the IP\nbroadcast address 255.255.255.255. When a host sends a datagram with destination\naddress 255.255.255.255, the message is delivered to all hosts on the same subnet.\nRouters optionally forward the message into neighboring subnets as well (although\nthey usually don’t).\nHaving now studied IP addressing in detail, we need to know how hosts and\nsubnets get their addresses in the first place. Let’s begin by looking at how an\norganization gets a block of addresses for its devices, and then look at how a device\n(such as a host) is assigned an address from within the organization’s block of\naddresses.\n344\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nObtaining a Block of Addresses\nIn order to obtain a block of IP addresses for use within an organization’s subnet, a\nnetwork administrator might first contact its ISP, which would provide addresses\nfrom a larger block of addresses that had already been allocated to the ISP. For\nexample, the ISP may itself have been allocated the address block 200.23.16.0/20.\nThe ISP, in turn, could divide its address block into eight equal-sized contiguous\naddress blocks and give one of these address blocks out to each of up to eight organ-\nizations that are supported by this ISP, as shown below. (We have underlined the\nsubnet part of these addresses for your convenience.)\nISP’s block\n200.23.16.0/20\n11001000  00010111  00010000  00000000\nOrganization 0\n200.23.16.0/23\n11001000  00010111  00010000  00000000\nOrganization 1\n200.23.18.0/23\n11001000  00010111  00010010  00000000\nOrganization 2\n200.23.20.0/23\n11001000  00010111  00010100  00000000\n. . .\n. . .\n. . .\nOrganization 7\n200.23.30.0/23\n11001000  00010111  00011110  00000000\nWhile obtaining a set of addresses from an ISP is one way to get a block of\naddresses, it is not the only way. Clearly, there must also be a way for the ISP itself\nto get a block of addresses. Is there a global authority that has ultimate responsibility\nfor managing the IP address space and allocating address blocks to ISPs and other\norganizations? Indeed there is! IP addresses are managed under the authority of the\nInternet Corporation for Assigned Names and Numbers (ICANN) [ICANN 2012],\nbased on guidelines set forth in [RFC 2050]. The role of the nonprofit ICANN organ-\nization [NTIA 1998] is not only to allocate IP addresses, but also to manage the DNS\nroot servers. It also has the very contentious job of assigning domain names and\nresolving domain name disputes. The ICANN allocates addresses to regional Inter-\nnet registries (for example, ARIN, RIPE, APNIC, and LACNIC, which together\nform the Address Supporting Organization of ICANN [ASO-ICANN 2012]), and\nhandle the allocation/management of addresses within their regions.\nObtaining a Host Address: the Dynamic Host Configuration Protocol\nOnce an organization has obtained a block of addresses, it can assign individual IP\naddresses to the host and router interfaces in its organization. A system administra-\ntor will typically manually configure the IP addresses into the router (often\nremotely, with a network management tool). Host addresses can also be configured\nmanually, but more often this task is now done using the Dynamic Host Configu-\nration Protocol (DHCP) [RFC 2131]. DHCP allows a host to obtain (be allocated)\nan IP address automatically. A network administrator can configure DHCP so that a\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n345\n\ngiven host receives the same IP address each time it connects to the network, or a\nhost may be assigned a temporary IP address that will be different each time the\nhost connects to the network. In addition to host IP address assignment, DHCP also\nallows a host to learn additional information, such as its subnet mask, the address of\nits first-hop router (often called the default gateway), and the address of its local\nDNS server.\nBecause of DHCP’s ability to automate the network-related aspects of connect-\ning a host into a network, it is often referred to as a plug-and-play protocol. This\ncapability makes it very attractive to the network administrator who would other-\nwise have to perform these tasks manually! DHCP is also enjoying widespread use\nin residential Internet access networks and in wireless LANs, where hosts join and\nleave the network frequently. Consider, for example, the student who carries a lap-\ntop from a dormitory room to a library to a classroom. It is likely that in each loca-\ntion, the student will be connecting into a new subnet and hence will need a new IP\naddress at each location. DHCP is ideally suited to this situation, as there are many\nusers coming and going, and addresses are needed for only a limited amount of time.\nDHCP is similarly useful in residential ISP access networks. Consider, for example,\na residential ISP that has 2,000 customers, but no more than 400 customers are ever\nonline at the same time. In this case, rather than needing a block of 2,048 addresses,\na DHCP server that assigns addresses dynamically needs only a block of 512\naddresses (for example, a block of the form a.b.c.d/23). As the hosts join and leave,\nthe DHCP server needs to update its list of available IP addresses. Each time a host\njoins, the DHCP server allocates an arbitrary address from its current pool of avail-\nable addresses; each time a host leaves, its address is returned to the pool.\nDHCP is a client-server protocol. A client is typically a newly arriving host\nwanting to obtain network configuration information, including an IP address for\nitself. In the simplest case, each subnet (in the addressing sense of Figure 4.17) will\nhave a DHCP server. If no server is present on the subnet, a DHCP relay agent (typ-\nically a router) that knows the address of a DHCP server for that network is needed.\nFigure 4.20 shows a DHCP server attached to subnet 223.1.2/24, with the router\nserving as the relay agent for arriving clients attached to subnets 223.1.1/24 and\n223.1.3/24. In our discussion below, we’ll assume that a DHCP server is available\non the subnet.\nFor a newly arriving host, the DHCP protocol is a four-step process, as shown\nin Figure 4.21 for the network setting shown in Figure 4.20. In this figure, yiaddr\n(as in “your Internet address”) indicates the address being allocated to the newly\narriving client. The four steps are:\n•\nDHCP server discovery. The first task of a newly arriving host is to find a DHCP\nserver with which to interact. This is done using a DHCP discover message,\nwhich a client sends within a UDP packet to port 67. The UDP packet is encap-\nsulated in an IP datagram. But to whom should this datagram be sent? The host\ndoesn’t even know the IP address of the network to which it is attaching, much\n346\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nless the address of a DHCP server for this network. Given this, the DHCP client\ncreates an IP datagram containing its DHCP discover message along with the\nbroadcast destination IP address of 255.255.255.255 and a “this host” source IP\naddress of 0.0.0.0. The DHCP client passes the IP datagram to the link layer,\nwhich then broadcasts this frame to all nodes attached to the subnet (we will\ncover the details of link-layer broadcasting in Section 5.4).\n•\nDHCP server offer(s). A DHCP server receiving a DHCP discover message\nresponds to the client with a DHCP offer message that is broadcast to all nodes\non the subnet, again using the IP broadcast address of 255.255.255.255. (You\nmight want to think about why this server reply must also be broadcast). Since\nseveral DHCP servers can be present on the subnet, the client may find itself in\nthe enviable position of being able to choose from among several offers. Each\nserver offer message contains the transaction ID of the received discover mes-\nsage, the proposed IP address for the client, the network mask, and an IP address\nlease time—the amount of time for which the IP address will be valid. It is com-\nmon for the server to set the lease time to several hours or days [Droms 2002].\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n347\n223.1.1.1\n223.1.1.2\n223.1.1.4\n223.1.2.9\n223.1.3.27\n223.1.1.3\n223.1.3.1\n223.1.3.2\n223.1.2.1\n223.1.2.5\n223.1.2.2\nArriving\nDHCP\nclient\nDHCP\nserver\nFigure 4.20 \u0002 DHCP client-server scenario\n\n•\nDHCP request. The newly arriving client will choose from among one or more\nserver offers and respond to its selected offer with a DHCP request message,\nechoing back the configuration parameters.\n•\nDHCP ACK. The server responds to the DHCP request message with a DHCP\nACK message, confirming the requested parameters.\nOnce the client receives the DHCP ACK, the interaction is complete and the\nclient can use the DHCP-allocated IP address for the lease duration. Since a client\n348\nCHAPTER 4\n•\nTHE NETWORK LAYER\nDHCP server:\n223.1.2.5\nArriving client\nDHCP discover\nTime\nTime\nsrc: 0.0.0.0, 68\ndest: 255.255.255.255,67\nDHCPDISCOVER\nyiaddr: 0.0.0.0\ntransaction ID: 654\nsrc: 223.1.2.5, 67\ndest: 255.255.255.255,68\nDHCPOFFER\nyiaddrr: 223.1.2.4\ntransaction ID: 654\nDHCP server ID: 223.1.2.5\nLifetime: 3600 secs\nDHCP offer\nsrc: 223.1.2.5, 67\ndest: 255.255.255.255,68\nDHCPACK\nyiaddrr: 223.1.2.4\ntransaction ID: 655\nDHCP server ID: 223.1.2.5\nLifetime: 3600 secs\nDHCP ACK\nsrc: 0.0.0.0, 68\ndest: 255.255.255.255, 67\nDHCPREQUEST\nyiaddrr: 223.1.2.4\ntransaction ID: 655\nDHCP server ID: 223.1.2.5\nLifetime: 3600 secs\nDHCP request\nFigure 4.21 \u0002 DHCP client-server interaction\n\nmay want to use its address beyond the lease’s expiration, DHCP also provides a\nmechanism that allows a client to renew its lease on an IP address.\nThe value of DHCP’s plug-and-play capability is clear, considering the fact that\nthe alternative is to manually configure a host’s IP address. Consider the student\nwho moves from classroom to library to dorm room with a laptop, joins a new sub-\nnet, and thus obtains a new IP address at each location. It is unimaginable that a sys-\ntem administrator would have to reconfigure laptops at each location, and few\nstudents (except those taking a computer networking class!) would have the expert-\nise to configure their laptops manually. From a mobility aspect, however, DHCP\ndoes have shortcomings. Since a new IP address is obtained from DHCP each time\na node connects to a new subnet, a TCP connection to a remote application cannot\nbe maintained as a mobile node moves between subnets. In Chapter 6, we will\nexamine mobile IP—a recent extension to the IP infrastructure that allows a mobile\nnode to use a single permanent address as it moves between subnets. Additional\ndetails about DHCP can be found in [Droms 2002] and [dhc 2012]. An open source\nreference implementation of DHCP is available from the Internet Systems Consor-\ntium [ISC 2012].\nNetwork Address Translation (NAT)\nGiven our discussion about Internet addresses and the IPv4 datagram format, we’re\nnow well aware that every IP-capable device needs an IP address. With the prolifer-\nation of small office, home office (SOHO) subnets, this would seem to imply that\nwhenever a SOHO wants to install a LAN to connect multiple machines, a range of\naddresses would need to be allocated by the ISP to cover all of the SOHO’s\nmachines. If the subnet grew bigger (for example, the kids at home have not only\ntheir own computers, but have smartphones and networked Game Boys as well), a\nlarger block of addresses would have to be allocated. But what if the ISP had already\nallocated the contiguous portions of the SOHO network’s current address range?\nAnd what typical homeowner wants (or should need) to know how to manage IP\naddresses in the first place? Fortunately, there is a simpler approach to address allo-\ncation that has found increasingly widespread use in such scenarios: network\naddress translation (NAT) [RFC 2663; RFC 3022; Zhang 2007].\nFigure 4.22 shows the operation of a NAT-enabled router. The NAT-enabled\nrouter, residing in the home, has an interface that is part of the home network on the\nright of Figure 4.22. Addressing within the home network is exactly as we have seen\nabove—all four interfaces in the home network have the same subnet address of\n10.0.0/24. The address space 10.0.0.0/8 is one of three portions of the IP address\nspace that is reserved in [RFC 1918] for a private network or a realm with private\naddresses, such as the home network in Figure 4.22. A realm with private addresses\nrefers to a network whose addresses only have meaning to devices within that\nnetwork. To see why this is important, consider the fact that there are hundreds of\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n349\n\nthousands of home networks, many using the same address space, 10.0.0.0/24.\nDevices within a given home network can send packets to each other using\n10.0.0.0/24 addressing. However, packets forwarded beyond the home network into\nthe larger global Internet clearly cannot use these addresses (as either a source or a\ndestination address) because there are hundreds of thousands of networks using this\nblock of addresses. That is, the 10.0.0.0/24 addresses can only have meaning within\nthe given home network. But if private addresses only have meaning within a given\nnetwork, how is addressing handled when packets are sent to or received from the\nglobal Internet, where addresses are necessarily unique? The answer lies in under-\nstanding NAT.\nThe NAT-enabled router does not look like a router to the outside world. Instead\nthe NAT router behaves to the outside world as a single device with a single IP\naddress. In Figure 4.22, all traffic leaving the home router for the larger Internet has\na source IP address of 138.76.29.7, and all traffic entering the home router must\nhave a destination address of 138.76.29.7. In essence, the NAT-enabled router is hid-\ning the details of the home network from the outside world. (As an aside, you might\nwonder where the home network computers get their addresses and where the router\ngets its single IP address. Often, the answer is the same—DHCP! The router gets its\naddress from the ISP’s DHCP server, and the router runs a DHCP server to provide\naddresses to computers within the NAT-DHCP-router-controlled home network’s\naddress space.)\n350\nCHAPTER 4\n•\nTHE NETWORK LAYER\n3\n2\n10.0.0.1\n138.76.29.7\n10.0.0.4\n10.0.0.2\n10.0.0.3\nNAT translation table\nWAN side\n138.76.29.7, 5001\nLAN side\n10.0.0.1, 3345\n. . .\n. . .\nS = 138.76.29.7, 5001\nD = 128.119.40.186, 80 \n1\n4\nS = 128.119.40.186, 80\nD = 138.76.29.7, 5001\nS = 128.119.40.186, 80\nD = 10.0.0.1, 3345 \nS = 10.0.0.1, 3345\nD = 128.119.40.186, 80\nFigure 4.22 \u0002 Network address translation\n\nIf all datagrams arriving at the NAT router from the WAN have the same desti-\nnation IP address (specifically, that of the WAN-side interface of the NAT router),\nthen how does the router know the internal host to which it should forward a given\ndatagram? The trick is to use a NAT translation table at the NAT router, and to\ninclude port numbers as well as IP addresses in the table entries.\nConsider the example in Figure 4.22. Suppose a user sitting in a home network\nbehind host 10.0.0.1 requests a Web page on some Web server (port 80) with IP\naddress 128.119.40.186. The host 10.0.0.1 assigns the (arbitrary) source port num-\nber 3345 and sends the datagram into the LAN. The NAT router receives the data-\ngram, generates a new source port number 5001 for the datagram, replaces the\nsource IP address with its WAN-side IP address 138.76.29.7, and replaces the origi-\nnal source port number 3345 with the new source port number 5001. When generat-\ning a new source port number, the NAT router can select any source port number\nthat is not currently in the NAT translation table. (Note that because a port number\nfield is 16 bits long, the NAT protocol can support over 60,000 simultaneous con-\nnections with a single WAN-side IP address for the router!) NAT in the router also\nadds an entry to its NAT translation table. The Web server, blissfully unaware that\nthe arriving datagram containing the HTTP request has been manipulated by the\nNAT router, responds with a datagram whose destination address is the IP address\nof the NAT router, and whose destination port number is 5001. When this datagram\narrives at the NAT router, the router indexes the NAT translation table using the des-\ntination IP address and destination port number to obtain the appropriate IP address\n(10.0.0.1) and destination port number (3345) for the browser in the home network.\nThe router then rewrites the datagram’s destination address and destination port\nnumber, and forwards the datagram into the home network.\nNAT has enjoyed widespread deployment in recent years. But we should\nmention that many purists in the IETF community loudly object to NAT. First,\nthey argue, port numbers are meant to be used for addressing processes, not for\naddressing hosts. (This violation can indeed cause problems for servers running\non the home network, since, as we have seen in Chapter 2, server processes wait\nfor incoming requests at well-known port numbers.) Second, they argue, routers\nare supposed to process packets only up to layer 3. Third, they argue, the NAT\nprotocol violates the so-called end-to-end argument; that is, hosts should be talk-\ning directly with each other, without interfering nodes modifying IP addresses and\nport numbers. And fourth, they argue, we should use IPv6 (see Section 4.4.4) to\nsolve the shortage of IP addresses, rather than recklessly patching up the problem\nwith a stopgap solution like NAT. But like it or not, NAT has become an important\ncomponent of the Internet.\nYet another major problem with NAT is that it interferes with P2P applications,\nincluding P2P file-sharing applications and P2P Voice-over-IP applications. Recall\nfrom Chapter 2 that in a P2P application, any participating Peer A should be able to\ninitiate a TCP connection to any other participating Peer B. The essence of the\nproblem is that if Peer B is behind a NAT, it cannot act as a server and accept TCP\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n351\n\nconnections. As we’ll see in the homework problems, this NAT problem can be cir-\ncumvented if Peer A is not behind a NAT. In this case, Peer A can first contact Peer\nB through an intermediate Peer C, which is not behind a NAT and to which B has\nestablished an ongoing TCP connection. Peer A can then ask Peer B, via Peer C, to\ninitiate a TCP connection directly back to Peer A. Once the direct P2P TCP connec-\ntion is established between Peers A and B, the two peers can exchange messages or\nfiles. This hack, called connection reversal, is actually used by many P2P applica-\ntions for NAT traversal. If both Peer A and Peer B are behind their own NATs, the\nsituation is a bit trickier but can be handled using application relays, as we saw with\nSkype relays in Chapter 2.\nUPnP\nNAT traversal is increasingly provided by Universal Plug and Play (UPnP), which is\na protocol that allows a host to discover and configure a nearby NAT [UPnP Forum\n2012]. UPnP requires that both the host and the NAT be UPnP compatible. With\nUPnP, an application running in a host can request a NAT mapping between its\n(private IP address, private port number) and the (public IP address, public port\nnumber) for some requested public port number. If the NAT accepts the request and\ncreates the mapping, then nodes from the outside can initiate TCP connections to\n(public IP address, public port number). Furthermore, UPnP lets the application\nknow the value of (public IP address, public port number), so that the application\ncan advertise it to the outside world.\nAs an example, suppose your host, behind a UPnP-enabled NAT, has private\naddress 10.0.0.1 and is running BitTorrent on port 3345. Also suppose that the\npublic IP address of the NAT is 138.76.29.7. Your BitTorrent application naturally\nwants to be able to accept connections from other hosts, so that it can trade chunks\nwith them. To this end, the BitTorrent application in your host asks the NAT to cre-\nate a “hole” that maps (10.0.0.1, 3345) to (138.76.29.7, 5001). (The public port\nnumber 5001 is chosen by the application.) The BitTorrent application in your host\ncould also advertise to its tracker that it is available at (138.76.29.7, 5001). In this\nmanner, an external host running BitTorrent can contact the tracker and learn that\nyour BitTorrent application is running at (138.76.29.7, 5001). The external host\ncan send a TCP SYN packet to (138.76.29.7, 5001). When the NAT receives the\nSYN packet, it will change the destination IP address and port number in the\npacket to (10.0.0.1, 3345) and forward the packet through the NAT.\nIn summary, UPnP allows external hosts to initiate communication sessions\nto NATed hosts, using either TCP or UDP. NATs have long been a nemesis \nfor P2P applications; UPnP, providing an effective and robust NAT traversal\nsolution, may be their savior. Our discussion of NAT and UPnP here has been\nnecessarily brief. For more detailed discussions of NAT see [Huston 2004, Cisco\nNAT 2012].\n352\nCHAPTER 4\n•\nTHE NETWORK LAYER"
    },
    {
      "chunk_id": "b1bf862b-5a9a-4b35-90b9-e5b24cea5fd9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.4.3 Internet Control Message Protocol (ICMP)",
      "original_titles": [
        "4.4.3 Internet Control Message Protocol (ICMP)"
      ],
      "path": "Chapter 4 The Network Layer > 4.4 The Internet Protocol (IP): Forwarding and Addressing in the Internet > 4.4.3 Internet Control Message Protocol (ICMP)",
      "start_page": 380,
      "end_page": 382,
      "token_count": 1773,
      "text": "4.4.3 Internet Control Message Protocol (ICMP)\nRecall that the network layer of the Internet has three main components: the IP pro-\ntocol, discussed in the previous section; the Internet routing protocols (including\nRIP, OSPF, and BGP), which are covered in Section 4.6; and ICMP, which is the\nsubject of this section.\nICMP, specified in [RFC 792], is used by hosts and routers to communicate net-\nwork-layer information to each other. The most typical use of ICMP is for error\nreporting. For example, when running a Telnet, FTP, or HTTP session, you may\nhave encountered an error message such as “Destination network unreachable.” This\nmessage had its origins in ICMP. At some point, an IP router was unable to find a\npath to the host specified in your Telnet, FTP, or HTTP application. That router cre-\nated and sent a type-3 ICMP message to your host indicating the error.\nICMP is often considered part of IP but architecturally it lies just above IP, as\nICMP messages are carried inside IP datagrams. That is, ICMP messages are carried\nas IP payload, just as TCP or UDP segments are carried as IP payload. Similarly,\nwhen a host receives an IP datagram with ICMP specified as the upper-layer proto-\ncol, it demultiplexes the datagram’s contents to ICMP, just as it would demultiplex a\ndatagram’s content to TCP or UDP.\nICMP messages have a type and a code field, and contain the header and the\nfirst 8 bytes of the IP datagram that caused the ICMP message to be generated in the\nfirst place (so that the sender can determine the datagram that caused the error).\nSelected ICMP message types are shown in Figure 4.23. Note that ICMP messages\nare used not only for signaling error conditions.\nThe well-known ping program sends an ICMP type 8 code 0 message to the\nspecified host. The destination host, seeing the echo request, sends back a type 0\ncode 0 ICMP echo reply. Most TCP/IP implementations support the ping server\ndirectly in the operating system; that is, the server is not a process. Chapter 11 of\n[Stevens 1990] provides the source code for the ping client program. Note that the\nclient program needs to be able to instruct the operating system to generate an ICMP\nmessage of type 8 code 0.\nAnother interesting ICMP message is the source quench message. This message\nis seldom used in practice. Its original purpose was to perform congestion control—\nto allow a congested router to send an ICMP source quench message to a host to\nforce that host to reduce its transmission rate. We have seen in Chapter 3 that TCP\nhas its own congestion-control mechanism that operates at the transport layer, with-\nout the use of network-layer feedback such as the ICMP source quench message.\nIn Chapter 1 we introduced the Traceroute program, which allows us to trace a\nroute from a host to any other host in the world. Interestingly, Traceroute is imple-\nmented with ICMP messages. To determine the names and addresses of the routers\nbetween source and destination, Traceroute in the source sends a series of ordinary\nIP datagrams to the destination. Each of these datagrams carries a UDP segment\nwith an unlikely UDP port number. The first of these datagrams has a TTL of 1, the\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n353\n\nsecond of 2, the third of 3, and so on. The source also starts timers for each of the\ndatagrams. When the nth datagram arrives at the nth router, the nth router observes\nthat the TTL of the datagram has just expired. According to the rules of the IP proto-\ncol, the router discards the datagram and sends an ICMP warning message to the\nsource (type 11 code 0). This warning message includes the name of the router and\nits IP address. When this ICMP message arrives back at the source, the source\nobtains the round-trip time from the timer and the name and IP address of the nth\nrouter from the ICMP message.\nHow does a Traceroute source know when to stop sending UDP segments?\nRecall that the source increments the TTL field for each datagram it sends. Thus,\none of the datagrams will eventually make it all the way to the destination host.\nBecause this datagram contains a UDP segment with an unlikely port number, the\ndestination host sends a port unreachable ICMP message (type 3 code 3) back to the\nsource. When the source host receives this particular ICMP message, it knows it\ndoes not need to send additional probe packets. (The standard Traceroute program\nactually sends sets of three packets with the same TTL; thus the Traceroute output\nprovides three results for each TTL.)\n354\nCHAPTER 4\n•\nTHE NETWORK LAYER\nICMP Type\nCode\nDescription\n0\n0\necho reply (to ping)\n3\n0\ndestination network unreachable\n3\n1\ndestination host unreachable\n3\n2\ndestination protocol unreachable\n3\n3\ndestination port unreachable\n3\n6\ndestination network unknown\n3\n7\ndestination host unknown\n4\n0\nsource quench (congestion control)\n8\n0\necho request\n9\n0\nrouter advertisement\n10\n0\nrouter discovery\n11\n0\nTTL expired\n12\n0\nIP header bad\nFigure 4.23 \u0002 ICMP message types\n\nIn this manner, the source host learns the number and the identities of routers\nthat lie between it and the destination host and the round-trip time between the two\nhosts. Note that the Traceroute client program must be able to instruct the operating\nsystem to generate UDP datagrams with specific TTL values and must also be able to\nbe notified by its operating system when ICMP messages arrive. Now that you under-\nstand how Traceroute works, you may want to go back and play with it some more.\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n355\nINSPECTING DATAGRAMS: FIREWALLS AND INTRUSION DETECTION\nSYSTEMS\nSuppose you are assigned the task of administering a home, departmental, university, or\ncorporate network. Attackers, knowing the IP address range of your network, can easily\nsend IP datagrams to addresses in your range. These datagrams can do all kinds of\ndevious things, including mapping your network with ping sweeps and port scans,\ncrashing vulnerable hosts with malformed packets, flooding servers with a deluge of\nICMP packets, and infecting hosts by including malware in the packets. As the network\nadministrator, what are you going to do about all those bad guys out there, each capa-\nble of sending malicious packets into your network? Two popular defense mechanisms\nto malicious packet attacks are firewalls and intrusion detection systems (IDSs).\nAs a network administrator, you may first try installing a firewall between your\nnetwork and the Internet. (Most access routers today have firewall capability.)\nFirewalls inspect the datagram and segment header fields, denying suspicious data-\ngrams entry into the internal network. For example, a firewall may be configured to\nblock all ICMP echo request packets, thereby preventing an attacker from doing a\ntraditional ping sweep across your IP address range. Firewalls can also block pack-\nets based on source and destination IP addresses and port numbers. Additionally,\nfirewalls can be configured to track TCP connections, granting entry only to data-\ngrams that belong to approved connections.\nAdditional protection can be provided with an IDS. An IDS, typically situated at the\nnetwork boundary, performs “deep packet inspection,” examining not only header\nfields but also the payloads in the datagram (including application-layer data). An IDS\nhas a database of packet signatures that are known to be part of attacks. This data-\nbase is automatically updated as new attacks are discovered. As packets pass through\nthe IDS, the IDS attempts to match header fields and payloads to the signatures in its\nsignature database. If such a match is found, an alert is created. An intrusion preven-\ntion system (IPS) is similar to an IDS, except that it actually blocks packets in addition to\ncreating alerts. In Chapter 8, we’ll explore firewalls and IDSs in more detail.\nCan firewalls and IDSs fully shield your network from all attacks? The answer is\nclearly no, as attackers continually find new attacks for which signatures are not yet\navailable. But firewalls and traditional signature-based IDSs are useful in protecting\nyour network from known attacks.\nFOCUS ON SECURITY"
    },
    {
      "chunk_id": "4c02977f-7d5a-42fb-938a-a70a02209cad",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.4.4 IPv6",
      "original_titles": [
        "4.4.4 IPv6"
      ],
      "path": "Chapter 4 The Network Layer > 4.4 The Internet Protocol (IP): Forwarding and Addressing in the Internet > 4.4.4 IPv6",
      "start_page": 383,
      "end_page": 388,
      "token_count": 3276,
      "text": "4.4.4 IPv6\nIn the early 1990s, the Internet Engineering Task Force began an effort to develop a\nsuccessor to the IPv4 protocol. A prime motivation for this effort was the realization\nthat the 32-bit IP address space was beginning to be used up, with new subnets and\nIP nodes being attached to the Internet (and being allocated unique IP addresses) at\na breathtaking rate. To respond to this need for a large IP address space, a new IP\nprotocol, IPv6, was developed. The designers of IPv6 also took this opportunity to\ntweak and augment other aspects of IPv4, based on the accumulated operational\nexperience with IPv4.\nThe point in time when IPv4 addresses would be completely allocated (and\nhence no new networks could attach to the Internet) was the subject of considerable\ndebate. The estimates of the two leaders of the IETF’s Address Lifetime Expecta-\ntions working group were that addresses would become exhausted in 2008 and 2018,\nrespectively [Solensky 1996]. In February 2011, IANA allocated out the last remain-\ning pool of unassigned IPv4 addresses to a regional registry. While these registries\nstill have available IPv4 addresses within their pool, once these addresses are\nexhausted, there are no more available address blocks that can be allocated from a\ncentral pool [Huston 2011a]. Although the mid-1990s estimates of IPv4 address\ndepletion suggested that a considerable amount of time might be left until the IPv4\naddress space was exhausted, it was realized that considerable time would be needed\nto deploy a new technology on such an extensive scale, and so the Next Generation\nIP (IPng) effort [Bradner 1996; RFC 1752] was begun. The result of this effort was\nthe specification of IP version 6 (IPv6) [RFC 2460] which we’ll discuss below. (An\noften-asked question is what happened to IPv5? It was initially envisioned that the\nST-2 protocol would become IPv5, but ST-2 was later dropped.) Excellent sources of\ninformation about IPv6 are [Huitema 1998, IPv6 2012].\nIPv6 Datagram Format\nThe format of the IPv6 datagram is shown in Figure 4.24. The most important\nchanges introduced in IPv6 are evident in the datagram format:\n•\nExpanded addressing capabilities. IPv6 increases the size of the IP address\nfrom 32 to 128 bits. This ensures that the world won’t run out of IP addresses.\nNow, every grain of sand on the planet can be IP-addressable. In addition to\nunicast and multicast addresses, IPv6 has introduced a new type of address,\ncalled an anycast address, which allows a datagram to be delivered to any\none of a group of hosts. (This feature could be used, for example, to send an\nHTTP GET to the nearest of a number of mirror sites that contain a given \ndocument.)\n•\nA streamlined 40-byte header. As discussed below, a number of IPv4 fields have\nbeen dropped or made optional. The resulting 40-byte fixed-length header allows\n356\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nfor faster processing of the IP datagram. A new encoding of options allows for\nmore flexible options processing.\n•\nFlow labeling and priority. IPv6 has an elusive definition of a flow. RFC 1752\nand RFC 2460 state that this allows “labeling of packets belonging to particular\nflows for which the sender requests special handling, such as a nondefault quality\nof service or real-time service.” For example, audio and video transmission might\nlikely be treated as a flow. On the other hand, the more traditional applications,\nsuch as file transfer and e-mail, might not be treated as flows. It is possible that the\ntraffic carried by a high-priority user (for example, someone paying for better serv-\nice for their traffic) might also be treated as a flow. What is clear, however, is that\nthe designers of IPv6 foresee the eventual need to be able to differentiate among\nthe flows, even if the exact meaning of a flow has not yet been determined. The\nIPv6 header also has an 8-bit traffic class field. This field, like the TOS field in\nIPv4, can be used to give priority to certain datagrams within a flow, or it can be\nused to give priority to datagrams from certain applications (for example, ICMP)\nover datagrams from other applications (for example, network news).\nAs noted above, a comparison of Figure 4.24 with Figure 4.13 reveals the sim-\npler, more streamlined structure of the IPv6 datagram. The following fields are\ndefined in IPv6:\n•\nVersion. This 4-bit field identifies the IP version number. Not surprisingly, IPv6\ncarries a value of 6 in this field. Note that putting a 4 in this field does not create\na valid IPv4 datagram. (If it did, life would be a lot simpler—see the discussion\nbelow regarding the transition from IPv4 to IPv6.)\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n357\nVersion\nTraffic class\nPayload length\nNext hdr\nHop limit\nFlow label\n32 bits\nSource address\n(128 bits)\nDestination address\n(128 bits)\nData\nFigure 4.24 \u0002 IPv6 datagram format\n\n•\nTraffic class. This 8-bit field is similar in spirit to the TOS field we saw in IPv4.\n•\nFlow label. As discussed above, this 20-bit field is used to identify a flow of\ndatagrams.\n•\nPayload length. This 16-bit value is treated as an unsigned integer giving the\nnumber of bytes in the IPv6 datagram following the fixed-length, 40-byte data-\ngram header.\n•\nNext header. This field identifies the protocol to which the contents (data field)\nof this datagram will be delivered (for example, to TCP or UDP). The field uses\nthe same values as the protocol field in the IPv4 header.\n•\nHop limit. The contents of this field are decremented by one by each router that\nforwards the datagram. If the hop limit count reaches zero, the datagram is\ndiscarded.\n•\nSource and destination addresses. The various formats of the IPv6 128-bit\naddress are described in RFC 4291.\n•\nData. This is the payload portion of the IPv6 datagram. When the datagram\nreaches its destination, the payload will be removed from the IP datagram and\npassed on to the protocol specified in the next header field.\nThe discussion above identified the purpose of the fields that are included in the\nIPv6 datagram. Comparing the IPv6 datagram format in Figure 4.24 with the IPv4\ndatagram format that we saw in Figure 4.13, we notice that several fields appearing\nin the IPv4 datagram are no longer present in the IPv6 datagram:\n•\nFragmentation/Reassembly. IPv6 does not allow for fragmentation and reassem-\nbly at intermediate routers; these operations can be performed only by the source\nand destination. If an IPv6 datagram received by a router is too large to be for-\nwarded over the outgoing link, the router simply drops the datagram and sends a\n“Packet Too Big” ICMP error message (see below) back to the sender. The\nsender can then resend the data, using a smaller IP datagram size. Fragmentation\nand reassembly is a time-consuming operation; removing this functionality from\nthe routers and placing it squarely in the end systems considerably speeds up IP\nforwarding within the network.\n•\nHeader checksum. Because the transport-layer (for example, TCP and UDP) and\nlink-layer (for example, Ethernet) protocols in the Internet layers perform check-\nsumming, the designers of IP probably felt that this functionality was sufficiently\nredundant in the network layer that it could be removed. Once again, fast pro-\ncessing of IP packets was a central concern. Recall from our discussion of IPv4\nin Section 4.4.1 that since the IPv4 header contains a TTL field (similar to the\nhop limit field in IPv6), the IPv4 header checksum needed to be recomputed at\nevery router. As with fragmentation and reassembly, this too was a costly opera-\ntion in IPv4.\n358\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\n•\nOptions. An options field is no longer a part of the standard IP header. How-\never, it has not gone away. Instead, the options field is one of the possible next\nheaders pointed to from within the IPv6 header. That is, just as TCP or UDP\nprotocol headers can be the next header within an IP packet, so too can an\noptions field. The removal of the options field results in a fixed-length, 40-\nbyte IP header.\nRecall from our discussion in Section 4.4.3 that the ICMP protocol is used by IP\nnodes to report error conditions and provide limited information (for example, the\necho reply to a ping message) to an end system. A new version of ICMP has been\ndefined for IPv6 in RFC 4443. In addition to reorganizing the existing ICMP type\nand code definitions, ICMPv6 also added new types and codes required by the new\nIPv6 functionality. These include the “Packet Too Big” type, and an “unrecognized\nIPv6 options” error code. In addition, ICMPv6 subsumes the functionality of the\nInternet Group Management Protocol (IGMP) that we’ll study in Section 4.7. IGMP,\nwhich is used to manage a host’s joining and leaving of multicast groups, was previ-\nously a separate protocol from ICMP in IPv4.\nTransitioning from IPv4 to IPv6\nNow that we have seen the technical details of IPv6, let us consider a very practical\nmatter: How will the public Internet, which is based on IPv4, be transitioned to\nIPv6? The problem is that while new IPv6-capable systems can be made backward-\ncompatible, that is, can send, route, and receive IPv4 datagrams, already deployed\nIPv4-capable systems are not capable of handling IPv6 datagrams. Several options\nare possible [Huston 2011b].\nOne option would be to declare a flag day—a given time and date when all\nInternet machines would be turned off and upgraded from IPv4 to IPv6. The last\nmajor technology transition (from using NCP to using TCP for reliable transport\nservice) occurred almost 25 years ago. Even back then [RFC 801], when the Inter-\nnet was tiny and still being administered by a small number of “wizards,” it was\nrealized that such a flag day was not possible. A flag day involving hundreds of mil-\nlions of machines and millions of network administrators and users is even more\nunthinkable today. RFC 4213 describes two approaches (which can be used either\nalone or together) for gradually integrating IPv6 hosts and routers into an IPv4\nworld (with the long-term goal, of course, of having all IPv4 nodes eventually tran-\nsition to IPv6).\nProbably the most straightforward way to introduce IPv6-capable nodes is a\ndual-stack approach, where IPv6 nodes also have a complete IPv4 implementation.\nSuch a node, referred to as an IPv6/IPv4 node in RFC 4213, has the ability to send\nand receive both IPv4 and IPv6 datagrams. When interoperating with an IPv4 node,\nan IPv6/IPv4 node can use IPv4 datagrams; when interoperating with an IPv6 node,\nit can speak IPv6. IPv6/IPv4 nodes must have both IPv6 and IPv4 addresses. They\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n359\n\nmust furthermore be able to determine whether another node is IPv6-capable or\nIPv4-only. This problem can be solved using the DNS (see Chapter 2), which can\nreturn an IPv6 address if the node name being resolved is IPv6-capable, or other-\nwise return an IPv4 address. Of course, if the node issuing the DNS request is only\nIPv4-capable, the DNS returns only an IPv4 address.\nIn the dual-stack approach, if either the sender or the receiver is only IPv4-\ncapable, an IPv4 datagram must be used. As a result, it is possible that two IPv6-\ncapable nodes can end up, in essence, sending IPv4 datagrams to each other. This is\nillustrated in Figure 4.25. Suppose Node A is IPv6-capable and wants to send an IP\ndatagram to Node F, which is also IPv6-capable. Nodes A and B can exchange an\nIPv6 datagram. However, Node B must create an IPv4 datagram to send to C. Cer-\ntainly, the data field of the IPv6 datagram can be copied into the data field of the\nIPv4 datagram and appropriate address mapping can be done. However, in perform-\ning the conversion from IPv6 to IPv4, there will be IPv6-specific fields in the IPv6\ndatagram (for example, the flow identifier field) that have no counterpart in IPv4.\nThe information in these fields will be lost. Thus, even though E and F can exchange\nIPv6 datagrams, the arriving IPv4 datagrams at E from D do not contain all of the\nfields that were in the original IPv6 datagram sent from A.\nAn alternative to the dual-stack approach, also discussed in RFC 4213, is\nknown as tunneling. Tunneling can solve the problem noted above, allowing, for\nexample, E to receive the IPv6 datagram originated by A. The basic idea behind\ntunneling is the following. Suppose two IPv6 nodes (for example, B and E in Fig-\nure 4.25) want to interoperate using IPv6 datagrams but are connected to each\nother by intervening IPv4 routers. We refer to the intervening set of IPv4 routers\nbetween two IPv6 routers as a tunnel, as illustrated in Figure 4.26. With tunnel-\ning, the IPv6 node on the sending side of the tunnel (for example, B) takes the\nentire IPv6 datagram and puts it in the data (payload) field of an IPv4 datagram.\n360\nCHAPTER 4\n•\nTHE NETWORK LAYER\nA\nB\nC\nD\nE\nF\nIPv6\nA to B: IPv6\nB to C: IPv4\nD to E: IPv4\nE to F: IPv6\nIPv6\nIPv4\nIPv4\nIPv6\nIPv6\nFlow: X\nSource: A\nDest: F\ndata\nSource: A\nDest: F\ndata\nSource: A\nDest: F\ndata\nFlow: ??\nSource: A\nDest: F\ndata\nFigure 4.25 \u0002 A dual-stack approach\n\nThis IPv4 datagram is then addressed to the IPv6 node on the receiving side of\nthe tunnel (for example, E) and sent to the first node in the tunnel (for example,\nC). The intervening IPv4 routers in the tunnel route this IPv4 datagram among\nthemselves, just as they would any other datagram, blissfully unaware that the\nIPv4 datagram itself contains a complete IPv6 datagram. The IPv6 node on the\nreceiving side of the tunnel eventually receives the IPv4 datagram (it is the desti-\nnation of the IPv4 datagram!), determines that the IPv4 datagram contains an\nIPv6 datagram, extracts the IPv6 datagram, and then routes the IPv6 datagram\nexactly as it would if it had received the IPv6 datagram from a directly connected\nIPv6 neighbor.\nWe end this section by noting that while the adoption of IPv6 was initially\nslow to take off [Lawton 2001], momentum has been building recently. See [Hus-\nton 2008b] for discussion of IPv6 deployment as of 2008; see [NIST IPv6 2012]\nfor a snapshort of US IPv6 deployment. The proliferation of devices such as IP-\nenabled phones and other portable devices provides an additional push for more\n4.4\n•\nTHE INTERNET PROTOCOL (IP)\n361\nA\nB\nC\nD\nE\nF\nIPv6\nA to B: IPv6\nPhysical view\nB to C: IPv4\n(encapsulating IPv6)\nD to E: IPv4\n(encapsulating IPv6)\nE to F: IPv6\nIPv6\nIPv4\nIPv4\nIPv6\nIPv6\nFlow: X\nSource: A\nDest: F\ndata\nSource: B\nDest: E\nSource: B\nDest: E\nA\nB\nE\nF\nIPv6\nLogical view\nIPv6\nTunnel\nIPv6\nIPv6\nFlow: X\nSource: A\nDest: F\ndata\nFlow: X\nSource: A\nDest: F\ndata\nFlow: X\nSource: A\nDest: F\ndata\nFigure 4.26 \u0002 Tunneling"
    },
    {
      "chunk_id": "92ac2fb8-d1ee-4482-92c1-c7204bf7cf8b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.4.5 A Brief Foray into IP Security",
      "original_titles": [
        "4.4.5 A Brief Foray into IP Security"
      ],
      "path": "Chapter 4 The Network Layer > 4.4 The Internet Protocol (IP): Forwarding and Addressing in the Internet > 4.4.5 A Brief Foray into IP Security",
      "start_page": 389,
      "end_page": 389,
      "token_count": 682,
      "text": "widespread deployment of IPv6. Europe’s Third Generation Partnership Program\n[3GPP 2012] has specified IPv6 as the standard addressing scheme for mobile\nmultimedia.\nOne important lesson that we can learn from the IPv6 experience is that it is enor-\nmously difficult to change network-layer protocols. Since the early 1990s, numerous\nnew network-layer protocols have been trumpeted as the next major revolution for the\nInternet, but most of these protocols have had limited penetration to date. These proto-\ncols include IPv6, multicast protocols (Section 4.7), and resource reservation proto-\ncols (Chapter 7). Indeed, introducing new protocols into the network layer is like\nreplacing the foundation of a house—it is difficult to do without tearing the whole\nhouse down or at least temporarily relocating the house’s residents. On the other hand,\nthe Internet has witnessed rapid deployment of new protocols at the application layer.\nThe classic examples, of course, are the Web, instant messaging, and P2P file sharing.\nOther examples include audio and video streaming and distributed games. Introducing\nnew application-layer protocols is like adding a new layer of paint to a house—it is\nrelatively easy to do, and if you choose an attractive color, others in the neighborhood\nwill copy you. In summary, in the future we can expect to see changes in the Internet’s\nnetwork layer, but these changes will likely occur on a time scale that is much slower\nthan the changes that will occur at the application layer.\n4.4.5 A Brief Foray into IP Security\nSection 4.4.3 covered IPv4 in some detail, including the services it provides and\nhow those services are implemented. While reading through that section, you may\nhave noticed that there was no mention of any security services. Indeed, IPv4 was\ndesigned in an era (the 1970s) when the Internet was primarily used among mutu-\nally-trusted networking researchers. Creating a computer network that integrated a\nmultitude of link-layer technologies was already challenging enough, without hav-\ning to worry about security.\nBut with security being a major concern today, Internet researchers have moved\non to design new network-layer protocols that provide a variety of security services.\nOne of these protocols is IPsec, one of the more popular secure network-layer proto-\ncols and also widely deployed in Virtual Private Networks (VPNs). Although IPsec and\nits cryptographic underpinnings are covered in some detail in Chapter 8, we provide a\nbrief, high-level introduction into IPsec services in this section.\nIPsec has been designed to be backward compatible with IPv4 and IPv6. In par-\nticular, in order to reap the benefits of IPsec, we don’t need to replace the protocol\nstacks in all the routers and hosts in the Internet. For example, using the transport\nmode (one of two IPsec “modes”), if two hosts want to securely communicate, IPsec\nneeds to be available only in those two hosts. All other routers and hosts can con-\ntinue to run vanilla IPv4.\nFor concreteness, we’ll focus on IPsec’s transport mode here. In this mode, two\nhosts first establish an IPsec session between themselves. (Thus IPsec is connection-\noriented!) With the session in place, all TCP and UDP segments sent between the\n362\nCHAPTER 4\n•\nTHE NETWORK LAYER"
    },
    {
      "chunk_id": "73782e75-5e29-4691-9d25-5bb449ae7c8c",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.5 Routing Algorithms",
      "original_titles": [
        "4.5 Routing Algorithms"
      ],
      "path": "Chapter 4 The Network Layer > 4.5 Routing Algorithms",
      "start_page": 390,
      "end_page": 392,
      "token_count": 1944,
      "text": "two hosts enjoy the security services provided by IPsec. On the sending side, the\ntransport layer passes a segment to IPsec. IPsec then encrypts the segment, appends\nadditional security fields to the segment, and encapsulates the resulting payload in\nan ordinary IP datagram. (It’s actually a little more complicated than this, as we’ll\nsee in Chapter 8.) The sending host then sends the datagram into the Internet, which\ntransports it to the destination host. There, IPsec decrypts the segment and passes\nthe unencrypted segment to the transport layer.\nThe services provided by an IPsec session include:\n•\nCryptographic agreement. Mechanisms that allow the two communicating hosts\nto agree on cryptographic algorithms and keys.\n•\nEncryption of IP datagram payloads. When the sending host receives a segment\nfrom the transport layer, IPsec encrypts the payload. The payload can only be\ndecrypted by IPsec in the receiving host.\n•\nData integrity. IPsec allows the receiving host to verify that the datagram’s\nheader fields and encrypted payload were not modified while the datagram was\nen route from source to destination.\n•\nOrigin authentication. When a host receives an IPsec datagram from a trusted\nsource (with a trusted key—see Chapter 8), the host is assured that the source IP\naddress in the datagram is the actual source of the datagram.\nWhen two hosts have an IPsec session established between them, all TCP and\nUDP segments sent between them will be encrypted and authenticated. IPsec there-\nfore provides blanket coverage, securing all communication between the two hosts\nfor all network applications.\nA company can use IPsec to communicate securely in the nonsecure public Inter-\nnet. For illustrative purposes, we’ll just look at a simple example here. Consider a\ncompany that has a large number of traveling salespeople, each possessing a company\nlaptop computer. Suppose the salespeople need to frequently consult sensitive com-\npany information (for example, pricing and product information) that is stored on a\nserver in the company’s headquarters. Further suppose that the salespeople also need\nto send sensitive documents to each other. How can this be done with IPsec? As you\nmight guess, we install IPsec in the server and in all of the salespeople’s laptops. With\nIPsec installed in these hosts, whenever a salesperson needs to communicate with the\nserver or with another salesperson, the communication session will be secure.\n4.5 Routing Algorithms\nSo far in this chapter, we’ve mostly explored the network layer’s forwarding func-\ntion. We learned that when a packet arrives to a router, the router indexes a forward-\ning table and determines the link interface to which the packet is to be directed. We\nalso learned that routing algorithms, operating in network routers, exchange and\n4.5\n•\nROUTING ALGORITHMS\n363\n\ncompute the information that is used to configure these forwarding tables. The inter-\nplay between routing algorithms and forwarding tables was shown in Figure 4.2.\nHaving explored forwarding in some depth we now turn our attention to the other\nmajor topic of this chapter, namely, the network layer’s critical routing function.\nWhether the network layer provides a datagram service (in which case different pack-\nets between a given source-destination pair may take different routes) or a VC serv-\nice (in which case all packets between a given source and destination will take the\nsame path), the network layer must nonetheless determine the path that packets take\nfrom senders to receivers. We’ll see that the job of routing is to determine good paths\n(equivalently, routes), from senders to receivers, through the network of routers.\nTypically a host is attached directly to one router, the default router for the\nhost (also called the first-hop router for the host). Whenever a host sends a packet,\nthe packet is transferred to its default router. We refer to the default router of the\nsource host as the source router and the default router of the destination host as the\ndestination router. The problem of routing a packet from source host to destination\nhost clearly boils down to the problem of routing the packet from source router to\ndestination router, which is the focus of this section.\nThe purpose of a routing algorithm is then simple: given a set of routers, with\nlinks connecting the routers, a routing algorithm finds a “good” path from source\nrouter to destination router. Typically, a good path is one that has the least cost.\nWe’ll see, however, that in practice, real-world concerns such as policy issues (for\nexample, a rule such as “router x, belonging to organization Y, should not forward\nany packets originating from the network owned by organization Z”) also come into\nplay to complicate the conceptually simple and elegant algorithms whose theory\nunderlies the practice of routing in today’s networks.\nA graph is used to formulate routing problems. Recall that a graph G = (N,E)\nis a set N of nodes and a collection E of edges, where each edge is a pair of nodes\nfrom N. In the context of network-layer routing, the nodes in the graph represent\nrouters—the points at which packet-forwarding decisions are made—and the edges\nconnecting these nodes represent the physical links between these routers. Such a\ngraph abstraction of a computer network is shown in Figure 4.27. To view some\ngraphs representing real network maps, see [Dodge 2012, Cheswick 2000]; for a\ndiscussion of how well different graph-based models model the Internet, see\n[Zegura 1997, Faloutsos 1999, Li 2004].\nAs shown in Figure 4.27, an edge also has a value representing its cost. Typi-\ncally, an edge’s cost may reflect the physical length of the corresponding link (for\nexample, a transoceanic link might have a higher cost than a short-haul terrestrial\nlink), the link speed, or the monetary cost associated with a link. For our purposes,\nwe’ll simply take the edge costs as a given and won’t worry about how they are\ndetermined. For any edge (x,y) in E, we denote c(x,y) as the cost of the edge between\nnodes x and y. If the pair (x,y) does not belong to E, we set c(x,y) = ∞. Also, through-\nout we consider only undirected graphs (i.e., graphs whose edges do not have a\ndirection), so that edge (x,y) is the same as edge (y,x) and that c(x,y) = c(y,x). Also, a\nnode y is said to be a neighbor of node x if (x,y) belongs to E.\n364\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nGiven that costs are assigned to the various edges in the graph abstraction, a natu-\nral goal of a routing algorithm is to identify the least costly paths between sources and\ndestinations. To make this problem more precise, recall that a path in a graph G =\n(N,E) is a sequence of nodes (x1, x2,..., xp) such that each of the pairs (x1,x2),\n(x2,x3),...,(xp-1,xp) are edges in E. The cost of a path (x1,x2,..., xp) is simply the sum of\nall the edge costs along the path, that is, c(x1,x2) + c(x2,x3) + ...+ c(xp-1,xp). Given any\ntwo nodes x and y, there are typically many paths between the two nodes, with each\npath having a cost. One or more of these paths is a least-cost path. The least-cost\nproblem is therefore clear: Find a path between the source and destination that has\nleast cost. In Figure 4.27, for example, the least-cost path between source node u and\ndestination node w is (u, x, y, w) with a path cost of 3. Note that if all edges in the\ngraph have the same cost, the least-cost path is also the shortest path (that is, the\npath with the smallest number of links between the source and the destination).\nAs a simple exercise, try finding the least-cost path from node u to z in Figure\n4.27 and reflect for a moment on how you calculated that path. If you are like most\npeople, you found the path from u to z by examining Figure 4.27, tracing a few routes\nfrom u to z, and somehow convincing yourself that the path you had chosen had the\nleast cost among all possible paths. (Did you check all of the 17 possible paths\nbetween u and z? Probably not!) Such a calculation is an example of a centralized\nrouting algorithm—the routing algorithm was run in one location, your brain, with\ncomplete information about the network. Broadly, one way in which we can classify\nrouting algorithms is according to whether they are global or decentralized.\n•\nA global routing algorithm computes the least-cost path between a source and\ndestination using complete, global knowledge about the network. That is, the\nalgorithm takes the connectivity between all nodes and all link costs as inputs.\nThis then requires that the algorithm somehow obtain this information before\nactually performing the calculation. The calculation itself can be run at one site\n4.5\n•\nROUTING ALGORITHMS\n365\nx\ny\nv\n3\n5\n2\n5\n2\n3\n1\n1\n2\n1\nu\nz\nw\nFigure 4.27 \u0002 Abstract graph model of a computer network"
    },
    {
      "chunk_id": "dc816293-e89e-4663-b349-fbdff82b42e3",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.5.1 The Link-State (LS) Routing Algorithm",
      "original_titles": [
        "4.5.1 The Link-State (LS) Routing Algorithm"
      ],
      "path": "Chapter 4 The Network Layer > 4.5 Routing Algorithms > 4.5.1 The Link-State (LS) Routing Algorithm",
      "start_page": 393,
      "end_page": 397,
      "token_count": 2680,
      "text": "(a centralized global routing algorithm) or replicated at multiple sites. The key\ndistinguishing feature here, however, is that a global algorithm has complete\ninformation about connectivity and link costs. In practice, algorithms with global\nstate information are often referred to as link-state (LS) algorithms, since the\nalgorithm must be aware of the cost of each link in the network. We’ll study LS\nalgorithms in Section 4.5.1.\n•\nIn a decentralized routing algorithm, the calculation of the least-cost path is\ncarried out in an iterative, distributed manner. No node has complete information\nabout the costs of all network links. Instead, each node begins with only the\nknowledge of the costs of its own directly attached links. Then, through an itera-\ntive process of calculation and exchange of information with its neighboring\nnodes (that is, nodes that are at the other end of links to which it itself is\nattached), a node gradually calculates the least-cost path to a destination or set of\ndestinations. The decentralized routing algorithm we’ll study below in Section\n4.5.2 is called a distance-vector (DV) algorithm, because each node maintains a\nvector of estimates of the costs (distances) to all other nodes in the network.\nA second broad way to classify routing algorithms is according to whether they\nare static or dynamic. In static routing algorithms, routes change very slowly over\ntime, often as a result of human intervention (for example, a human manually edit-\ning a router’s forwarding table). Dynamic routing algorithms change the routing\npaths as the network traffic loads or topology change. A dynamic algorithm can be\nrun either periodically or in direct response to topology or link cost changes. While\ndynamic algorithms are more responsive to network changes, they are also more\nsusceptible to problems such as routing loops and oscillation in routes.\nA third way to classify routing algorithms is according to whether they are load-\nsensitive or load-insensitive. In a load-sensitive algorithm, link costs vary dynami-\ncally to reflect the current level of congestion in the underlying link. If a high cost is\nassociated with a link that is currently congested, a routing algorithm will tend to\nchoose routes around such a congested link. While early ARPAnet routing algo-\nrithms were load-sensitive [McQuillan 1980], a number of difficulties were encoun-\ntered [Huitema 1998]. Today’s Internet routing algorithms (such as RIP, OSPF, and\nBGP) are load-insensitive, as a link’s cost does not explicitly reflect its current (or\nrecent past) level of congestion.\n4.5.1 The Link-State (LS) Routing Algorithm\nRecall that in a link-state algorithm, the network topology and all link costs are\nknown, that is, available as input to the LS algorithm. In practice this is accom-\nplished by having each node broadcast link-state packets to all other nodes in the\nnetwork, with each link-state packet containing the identities and costs of its\nattached links. In practice (for example, with the Internet’s OSPF routing protocol,\ndiscussed in Section 4.6.1) this is often accomplished by a link-state broadcast\n366\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nalgorithm [Perlman 1999]. We’ll cover broadcast algorithms in Section 4.7. The\nresult of the nodes’ broadcast is that all nodes have an identical and complete view\nof the network. Each node can then run the LS algorithm and compute the same set\nof least-cost paths as every other node.\nThe link-state routing algorithm we present below is known as Dijkstra’s algo-\nrithm, named after its inventor. A closely related algorithm is Prim’s algorithm; see\n[Cormen 2001] for a general discussion of graph algorithms. Dijkstra’s algorithm\ncomputes the least-cost path from one node (the source, which we will refer to as u)\nto all other nodes in the network. Dijkstra’s algorithm is iterative and has the prop-\nerty that after the kth iteration of the algorithm, the least-cost paths are known to k\ndestination nodes, and among the least-cost paths to all destination nodes, these k\npaths will have the k smallest costs. Let us define the following notation:\n•\nD(v): cost of the least-cost path from the source node to destination v as of this\niteration of the algorithm.\n•\np(v): previous node (neighbor of v) along the current least-cost path from the\nsource to v.\n•\nN\u0004 : subset of nodes; v is in N\u0004 if the least-cost path from the source to v is defin-\nitively known.\nThe global routing algorithm consists of an initialization step followed by a\nloop. The number of times the loop is executed is equal to the number of nodes in\nthe network. Upon termination, the algorithm will have calculated the shortest paths\nfrom the source node u to every other node in the network.\nLink-State (LS) Algorithm for Source Node u\n1\nInitialization:\n2\nN’ = {u}\n3\nfor all nodes v\n4\nif v is a neighbor of u\n5\nthen D(v) = c(u,v)\n6\nelse D(v) = ∞\n7\n8\nLoop\n9\nfind w not in N’ such that D(w) is a minimum\n10\nadd w to N’\n11\nupdate D(v) for each neighbor v of w and not in N’:\n12\nD(v) = min( D(v), D(w) + c(w,v) )\n13\n/* new cost to v is either old cost to v or known\n14\nleast path cost to w plus cost from w to v */\n15 until N’= N\n4.5\n•\nROUTING ALGORITHMS\n367\n\nAs an example, let’s consider the network in Figure 4.27 and compute the\nleast-cost paths from u to all possible destinations. A tabular summary of the\nalgorithm’s computation is shown in Table 4.3, where each line in the table gives\nthe values of the algorithm’s variables at the end of the iteration. Let’s consider\nthe few first steps in detail.\n•\nIn the initialization step, the currently known least-cost paths from u to its\ndirectly attached neighbors, v, x, and w, are initialized to 2, 1, and 5, respectively.\nNote in particular that the cost to w is set to 5 (even though we will soon see that\na lesser-cost path does indeed exist) since this is the cost of the direct (one hop)\nlink from u to w. The costs to y and z are set to infinity because they are not\ndirectly connected to u.\n•\nIn the first iteration, we look among those nodes not yet added to the set N\u0004 and\nfind that node with the least cost as of the end of the previous iteration. That node\nis x, with a cost of 1, and thus x is added to the set N\u0004. Line 12 of the LS algo-\nrithm is then performed to update D(v) for all nodes v, yielding the results shown\nin the second line (Step 1) in Table 4.3. The cost of the path to v is unchanged.\nThe cost of the path to w (which was 5 at the end of the initialization) through\nnode x is found to have a cost of 4. Hence this lower-cost path is selected and w’s\npredecessor along the shortest path from u is set to x. Similarly, the cost to y\n(through x) is computed to be 2, and the table is updated accordingly.\n•\nIn the second iteration, nodes v and y are found to have the least-cost paths (2),\nand we break the tie arbitrarily and add y to the set N\u0004 so that N\u0004 now contains u,\nx, and y. The cost to the remaining nodes not yet in N\u0004, that is, nodes v, w, and z,\nare updated via line 12 of the LS algorithm, yielding the results shown in the\nthird row in the Table 4.3.\n•\nAnd so on. . . .\nWhen the LS algorithm terminates, we have, for each node, its predecessor\nalong the least-cost path from the source node. For each predecessor, we also\n368\nCHAPTER 4\n•\nTHE NETWORK LAYER\nstep\nN’\nD(v),p(v)\nD(w),p(w)\nD(x),p(x)\nD(y),p(y)\nD(z),p(z)\n0\nu\n2,u\n5,u\n1,u\n∞\n∞\n1\nux\n2,u\n4,x\n2,x\n∞\n2\nuxy\n2,u\n3,y\n4,y\n3\nuxyv\n3,y\n4,y\n4\nuxyvw\n4,y\n5\nuxyvwz\nTable 4.3 \u0002 Running the link-state algorithm on the network in Figure 4.27\nVideoNote\nDijkstra’s algorithm: \ndiscussion and example\n\nhave its predecessor, and so in this manner we can construct the entire path from\nthe source to all destinations. The forwarding table in a node, say node u, can\nthen be constructed from this information by storing, for each destination, the\nnext-hop node on the least-cost path from u to the destination. Figure 4.28\nshows the resulting least-cost paths and forwarding table in u for the network in\nFigure 4.27.\nWhat is the computational complexity of this algorithm? That is, given n\nnodes (not counting the source), how much computation must be done in the\nworst case to find the least-cost paths from the source to all destinations? In the\nfirst iteration, we need to search through all n nodes to determine the node, w, not\nin N\u0004 that has the minimum cost. In the second iteration, we need to check n – 1\nnodes to determine the minimum cost; in the third iteration n – 2 nodes, and so\non. Overall, the total number of nodes we need to search through over all the iter-\nations is n(n + 1)/2, and thus we say that the preceding implementation of the LS\nalgorithm has worst-case complexity of order n squared: O(n2). (A more sophisti-\ncated implementation of this algorithm, using a data structure known as a heap,\ncan find the minimum in line 9 in logarithmic rather than linear time, thus reduc-\ning the complexity.)\nBefore completing our discussion of the LS algorithm, let us consider a pathol-\nogy that can arise. Figure 4.29 shows a simple network topology where link costs\nare equal to the load carried on the link, for example, reflecting the delay that would\nbe experienced. In this example, link costs are not symmetric; that is, c(u,v) equals\nc(v,u) only if the load carried on both directions on the link (u,v) is the same. In this\nexample, node z originates a unit of traffic destined for w, node x also originates a\nunit of traffic destined for w, and node y injects an amount of traffic equal to e, also\ndestined for w. The initial routing is shown in Figure 4.29(a) with the link costs cor-\nresponding to the amount of traffic carried.\nWhen the LS algorithm is next run, node y determines (based on the link costs\nshown in Figure 4.29(a)) that the clockwise path to w has a cost of 1, while the\ncounterclockwise path to w (which it had been using) has a cost of 1 + e. Hence y’s\n4.5\n•\nROUTING ALGORITHMS\n369\nDestination \nLink\nv\nw\nx\ny\nz\n(u, v)\n(u, x)\n(u, x)\n(u, x)\n(u, x)\nX\nY\nV\nU\nZ\nW\nFigure 4.28 \u0002 Least cost path and forwarding table for nodule u\n\nleast-cost path to w is now clockwise. Similarly, x determines that its new least-cost\npath to w is also clockwise, resulting in costs shown in Figure 4.29(b). When the\nLS algorithm is run next, nodes x, y, and z all detect a zero-cost path to w in the\ncounterclockwise direction, and all route their traffic to the counterclockwise\nroutes. The next time the LS algorithm is run, x, y, and z all then route their traffic\nto the clockwise routes.\nWhat can be done to prevent such oscillations (which can occur in any algo-\nrithm, not just an LS algorithm, that uses a congestion or delay-based link met-\nric)? One solution would be to mandate that link costs not depend on the amount\nof traffic carried—an unacceptable solution since one goal of routing is to avoid\n370\nCHAPTER 4\n•\nTHE NETWORK LAYER\nw\ny\nz\nx\n1\n0\n0\n0\ne\n1 + e\n1\na.  Initial routing\n1\ne\nw\ny\nz\nx\n2 + e\n1 + e\n1\n0\n0\n0\nb. x, y detect better path\n     to w, clockwise\nw\ny\nz\nx\n0\n0\n0\n1\n1 + e\n2+ e\nc. x, y, z detect better path\n     to w, counterclockwise\nw\ny\nz\nx\n2 + e\n1 + e\n1\n0\n0\n0\nd. x, y, z, detect better path\n     to w, clockwise\n1\n1\ne\n1\n1\ne\n1\n1\ne\nFigure 4.29 \u0002 Oscillations with congestion-sensitive routing"
    },
    {
      "chunk_id": "c8cce6df-8b53-4161-8263-999ef2cc5ac4",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.5.2 The Distance-Vector (DV) Routing Algorithm",
      "original_titles": [
        "4.5.2 The Distance-Vector (DV) Routing Algorithm"
      ],
      "path": "Chapter 4 The Network Layer > 4.5 Routing Algorithms > 4.5.2 The Distance-Vector (DV) Routing Algorithm",
      "start_page": 398,
      "end_page": 405,
      "token_count": 4732,
      "text": "highly congested (for example, high-delay) links. Another solution is to ensure\nthat not all routers run the LS algorithm at the same time. This seems a more\nreasonable solution, since we would hope that even if routers ran the LS algorithm\nwith the same periodicity, the execution instance of the algorithm would not be\nthe same at each node. Interestingly, researchers have found that routers in the\nInternet can self-synchronize among themselves [Floyd Synchronization 1994].\nThat is, even though they initially execute the algorithm with the same period\nbut at different instants of time, the algorithm execution instance can eventually\nbecome, and remain, synchronized at the routers. One way to avoid such self-\nsynchronization is for each router to randomize the time it sends out a link\nadvertisement.\nHaving studied the LS algorithm, let’s consider the other major routing algo-\nrithm that is used in practice today—the distance-vector routing algorithm.\n4.5.2 The Distance-Vector (DV) Routing Algorithm\nWhereas the LS algorithm is an algorithm using global information, the distance-\nvector (DV) algorithm is iterative, asynchronous, and distributed. It is distributed\nin that each node receives some information from one or more of its directly\nattached neighbors, performs a calculation, and then distributes the results of its\ncalculation back to its neighbors. It is iterative in that this process continues\non until no more information is exchanged between neighbors. (Interestingly, the\nalgorithm is also self-terminating—there is no signal that the computation should\nstop; it just stops.) The algorithm is asynchronous in that it does not require all of\nthe nodes to operate in lockstep with each other. We’ll see that an asynchronous,\niterative, self-terminating, distributed algorithm is much more interesting and fun\nthan a centralized algorithm!\nBefore we present the DV algorithm, it will prove beneficial to discuss an\nimportant relationship that exists among the costs of the least-cost paths. Let dx(y)\nbe the cost of the least-cost path from node x to node y. Then the least costs are\nrelated by the celebrated Bellman-Ford equation, namely,\ndx(y) = minv{c(x,v) + dv(y)},\n(4.1)\nwhere the minv in the equation is taken over all of x’s neighbors. The Bellman-Ford\nequation is rather intuitive. Indeed, after traveling from x to v, if we then take the\nleast-cost path from v to y, the path cost will be c(x,v) + dv(y). Since we must begin\nby traveling to some neighbor v, the least cost from x to y is the minimum of c(x,v)\n+ dv(y) taken over all neighbors v.\nBut for those who might be skeptical about the validity of the equation, let’s\ncheck it for source node u and destination node z in Figure 4.27. The source node u\n4.5\n•\nROUTING ALGORITHMS\n371\n\nhas three neighbors: nodes v, x, and w. By walking along various paths in the graph,\nit is easy to see that dv(z) = 5, dx(z) = 3, and dw(z) = 3. Plugging these values into\nEquation 4.1, along with the costs c(u,v) = 2, c(u,x) = 1, and c(u,w) = 5, gives du(z) =\nmin{2 + 5, 5 + 3, 1 + 3} = 4, which is obviously true and which is exactly what the\nDijskstra algorithm gave us for the same network. This quick verification should\nhelp relieve any skepticism you may have.\nThe Bellman-Ford equation is not just an intellectual curiosity. It actually has\nsignificant practical importance. In particular, the solution to the Bellman-Ford\nequation provides the entries in node x’s forwarding table. To see this, let v* be any\nneighboring node that achieves the minimum in Equation 4.1. Then, if node x wants\nto send a packet to node y along a least-cost path, it should first forward the packet\nto node v*. Thus, node x’s forwarding table would specify node v* as the next-hop\nrouter for the ultimate destination y. Another important practical contribution of the\nBellman-Ford equation is that it suggests the form of the neighbor-to-neighbor com-\nmunication that will take place in the DV algorithm.\nThe basic idea is as follows. Each node x begins with Dx(y), an estimate of the\ncost of the least-cost path from itself to node y, for all nodes in N. Let Dx = [Dx(y): y\nin N] be node x’s distance vector, which is the vector of cost estimates from x to all\nother nodes, y, in N. With the DV algorithm, each node x maintains the following\nrouting information:\n•\nFor each neighbor v, the cost c(x,v) from x to directly attached neighbor, v\n•\nNode x’s distance vector, that is, Dx = [Dx(y): y in N], containing x’s estimate of\nits cost to all destinations, y, in N\n•\nThe distance vectors of each of its neighbors, that is, Dv = [Dv(y): y in N] for each\nneighbor v of x\nIn the distributed, asynchronous algorithm, from time to time, each node sends\na copy of its distance vector to each of its neighbors. When a node x receives a\nnew distance vector from any of its neighbors v, it saves v’s distance vector, and\nthen uses the Bellman-Ford equation to update its own distance vector as fol-\nlows:\nDx(y) \u0003 minv{c(x,v) + Dv(y)}\nfor each node y in N\nIf node x’s distance vector has changed as a result of this update step, node x will\nthen send its updated distance vector to each of its neighbors, which can in turn\nupdate their own distance vectors. Miraculously enough, as long as all the nodes\ncontinue to exchange their distance vectors in an asynchronous fashion, each cost\nestimate Dx(y) converges to dx(y), the actual cost of the least-cost path from node x\nto node y [Bertsekas 1991]!\n372\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nDistance-Vector (DV) Algorithm\nAt each node, x:\n4.5\n•\nROUTING ALGORITHMS\n373\n1\nInitialization:\n2\nfor all destinations y in N:\n3\nDx(y)\n= c(x,y)\n/* if y is not a neighbor then c(x,y) = ∞*/\n4\nfor each neighbor w\n5\nDw(y) = ? for all destinations y in N\n6\nfor each neighbor w\n7\nsend distance vector Dx = [Dx(y): y in N] to w\n8\n9\nloop\n10\nwait (until I see a link cost change to some neighbor w or\n11\nuntil I receive a distance vector from some neighbor w)\n12\n13\nfor each y in N:\n14\nDx(y) = minv{c(x,v) + Dv(y)}\n15\n16\nif Dx(y) changed for any destination y\n17\nsend distance vector Dx = [Dx(y): y in N] to all neighbors\n18\n19 forever\nIn the DV algorithm, a node x updates its distance-vector estimate when it\neither sees a cost change in one of its directly attached links or receives a distance-\nvector update from some neighbor. But to update its own forwarding table for a\ngiven destination y, what node x really needs to know is not the shortest-path\ndistance to y but instead the neighboring node v*(y) that is the next-hop router along\nthe shortest path to y. As you might expect, the next-hop router v*(y) is the neighbor\nv that achieves the minimum in Line 14 of the DV algorithm. (If there are multiple\nneighbors v that achieve the minimum, then v*(y) can be any of the minimizing\nneighbors.) Thus, in Lines 13–14, for each destination y, node x also determines\nv*(y) and updates its forwarding table for destination y.\nRecall that the LS algorithm is a global algorithm in the sense that it requires\neach node to first obtain a complete map of the network before running the Dijkstra\nalgorithm. The DV algorithm is decentralized and does not use such global infor-\nmation. Indeed, the only information a node will have is the costs of the links to its\ndirectly attached neighbors and information it receives from these neighbors. Each\nnode waits for an update from any neighbor (Lines 10–11), calculates its new dis-\ntance vector when receiving an update (Line 14), and distributes its new distance\n\nvector to its neighbors (Lines 16–17). DV-like algorithms are used in many routing\nprotocols in practice, including the Internet’s RIP and BGP, ISO IDRP, Novell IPX,\nand the original ARPAnet.\nFigure 4.30 illustrates the operation of the DV algorithm for the simple three-\nnode network shown at the top of the figure. The operation of the algorithm is illus-\ntrated in a synchronous manner, where all nodes simultaneously receive distance\nvectors from their neighbors, compute their new distance vectors, and inform their\nneighbors if their distance vectors have changed. After studying this example, you\n374\nCHAPTER 4\n•\nTHE NETWORK LAYER\nNode y table\nNode x table\n0 2 7\nx y z\n∞ ∞ ∞\n∞ ∞ ∞\nTime\n7\n2\n1\ny\nx\nz\nNode z table\nfrom\ncost to\nx\ny\nz\n0 2 3\nx y z\n2 0 1\n7 1 0\nfrom\ncost to\nx\ny\nz\n0 2 3\nx y z\n2 0 1\n3 1 0\nfrom\ncost to\nx\ny\nz\n2 0 1\nx y z\n∞ ∞ ∞\n∞ ∞ ∞\nfrom\ncost to\nx\ny\nz\n0 2 7\nx y z\n2 0 1\n7 1 0\nfrom\ncost to\nx\ny\nz\n0 2 3\nx y z\n2 0 1\n3 1 0\nfrom\ncost to\nx\ny\nz\n7 1 0\nx y z\n∞ ∞ ∞\n∞ ∞ ∞\nfrom\ncost to\nx\ny\nz\n0 2 7\nx y z\n2 0 1\n3 1 0\nfrom\ncost to\nx\ny\nz\n0 2 3\nx y z\n2 0 1\n3 1 0\nfrom\ncost to\nx\ny\nz\nFigure 4.30 \u0002 Distance-vector (DV) algorithm\n\nshould convince yourself that the algorithm operates correctly in an asynchronous\nmanner as well, with node computations and update generation/reception occurring\nat any time.\nThe leftmost column of the figure displays three initial routing tables for each\nof the three nodes. For example, the table in the upper-left corner is node x’s initial\nrouting table. Within a specific routing table, each row is a distance vector—specifi-\ncally, each node’s routing table includes its own distance vector and that of each of\nits neighbors. Thus, the first row in node x’s initial routing table is Dx = [Dx(x),\nDx(y), Dx(z)] = [0, 2, 7]. The second and third rows in this table are the most recently\nreceived distance vectors from nodes y and z, respectively. Because at initialization\nnode x has not received anything from node y or z, the entries in the second and third\nrows are initialized to infinity.\nAfter initialization, each node sends its distance vector to each of its two neigh-\nbors. This is illustrated in Figure 4.30 by the arrows from the first column of tables\nto the second column of tables. For example, node x sends its distance vector Dx =\n[0, 2, 7] to both nodes y and z. After receiving the updates, each node recomputes its\nown distance vector. For example, node x computes\nDx(x) = 0\nDx(y) = min{c(x,y) + Dy(y), c(x,z) + Dz(y)} = min{2 + 0, 7 + 1} = 2\nDx(z) = min{c(x,y) + Dy(z), c(x,z) + Dz(z)} = min{2 + 1, 7 + 0} = 3\nThe second column therefore displays, for each node, the node’s new distance vec-\ntor along with distance vectors just received from its neighbors. Note, for example,\nthat node x’s estimate for the least cost to node z, Dx(z), has changed from 7 to 3.\nAlso note that for node x, neighboring node y achieves the minimum in line 14 of\nthe DV algorithm; thus at this stage of the algorithm, we have at node x that v*(y) =\ny and v*(z) = y.\nAfter the nodes recompute their distance vectors, they again send their updated\ndistance vectors to their neighbors (if there has been a change). This is illustrated in\nFigure 4.30 by the arrows from the second column of tables to the third column of\ntables. Note that only nodes x and z send updates: node y’s distance vector didn’t\nchange so node y doesn’t send an update. After receiving the updates, the nodes then\nrecompute their distance vectors and update their routing tables, which are shown in\nthe third column.\nThe process of receiving updated distance vectors from neighbors, recomputing\nrouting table entries, and informing neighbors of changed costs of the least-cost path\nto a destination continues until no update messages are sent. At this point, since no\nupdate messages are sent, no further routing table calculations will occur and the\nalgorithm will enter a quiescent state; that is, all nodes will be performing the wait\nin Lines 10–11 of the DV algorithm. The algorithm remains in the quiescent state\nuntil a link cost changes, as discussed next.\n4.5\n•\nROUTING ALGORITHMS\n375\n\nDistance-Vector Algorithm: Link-Cost Changes and Link Failure\nWhen a node running the DV algorithm detects a change in the link cost from itself to\na neighbor (Lines 10–11), it updates its distance vector (Lines 13–14) and, if there’s a\nchange in the cost of the least-cost path, informs its neighbors (Lines 16–17) of its new\ndistance vector. Figure 4.31(a) illustrates a scenario where the link cost from y to x\nchanges from 4 to 1. We focus here only on y’ and z’s distance table entries to destina-\ntion x. The DV algorithm causes the following sequence of events to occur:\n•\nAt time t0, y detects the link-cost change (the cost has changed from 4 to 1),\nupdates its distance vector, and informs its neighbors of this change since its dis-\ntance vector has changed.\n•\nAt time t1, z receives the update from y and updates its table. It computes a new\nleast cost to x (it has decreased from a cost of 5 to a cost of 2) and sends its new\ndistance vector to its neighbors.\n•\nAt time t2, y receives z’s update and updates its distance table. y’s least costs do\nnot change and hence y does not send any message to z. The algorithm comes to\na quiescent state.\nThus, only two iterations are required for the DV algorithm to reach a quiescent\nstate. The good news about the decreased cost between x and y has propagated\nquickly through the network.\nLet’s now consider what can happen when a link cost increases. Suppose that\nthe link cost between x and y increases from 4 to 60, as shown in Figure 4.31(b).\n1. Before the link cost changes, Dy(x) = 4, Dy(z) = 1, Dz(y) = 1, and Dz(x) = 5. At\ntime t0, y detects the link-cost change (the cost has changed from 4 to 60). y\ncomputes its new minimum-cost path to x to have a cost of\nDy(x)  = min{c(y,x) + Dx(x), c(y,z) + Dz(x)} = min{60 + 0, 1 + 5} = 6 \n376\nCHAPTER 4\n•\nTHE NETWORK LAYER\n50\n4\n1\n60\n1\ny\nx\na.\nb.\nz\n50\n4\n1\ny\nx\nz\nFigure 4.31 \u0002 Changes in link cost\n\nOf course, with our global view of the network, we can see that this new cost\nvia z is wrong. But the only information node y has is that its direct cost to x is\n60 and that z has last told y that z could get to x with a cost of 5. So in order to\nget to x, y would now route through z, fully expecting that z will be able to get\nto x with a cost of 5. As of t1 we have a routing loop—in order to get to x, y\nroutes through z, and z routes through y. A routing loop is like a black hole—a\npacket destined for x arriving at y or z as of t1 will bounce back and forth\nbetween these two nodes forever (or until the forwarding tables are changed).\n2. Since node y has computed a new minimum cost to x, it informs z of its new\ndistance vector at time t1.\n3. Sometime after t1, z receives y’s new distance vector, which indicates that y’s\nminimum cost to x is 6. z knows it can get to y with a cost of 1 and hence\ncomputes a new least cost to x of Dz(x) = min{50 + 0,1 + 6} = 7. Since z’s\nleast cost to x has increased, it then informs y of its new distance vector at t2.\n4. In a similar manner, after receiving z’s new distance vector, y determines\nDy(x) = 8 and sends z its distance vector. z then determines Dz(x) = 9 and\nsends y its distance vector, and so on.\nHow long will the process continue? You should convince yourself that the loop\nwill persist for 44 iterations (message exchanges between y and z)—until z even-\ntually computes the cost of its path via y to be greater than 50. At this point, z will\n(finally!) determine that its least-cost path to x is via its direct connection to x. y\nwill then route to x via z. The result of the bad news about the increase in link\ncost has indeed traveled slowly! What would have happened if the link cost c(y,\nx) had changed from 4 to 10,000 and the cost c(z, x) had been 9,999? Because of\nsuch scenarios, the problem we have seen is sometimes referred to as the count-\nto-infinity problem.\nDistance-Vector Algorithm: Adding Poisoned Reverse\nThe specific looping scenario just described can be avoided using a technique\nknown as poisoned reverse. The idea is simple—if z routes through y to get to\ndestination x, then z will advertise to y that its distance to x is infinity, that is, z will\nadvertise to y that Dz(x) = ∞(even though z knows Dz(x) = 5 in truth). z will con-\ntinue telling this little white lie to y as long as it routes to x via y. Since y believes\nthat z has no path to x, y will never attempt to route to x via z, as long as z continues\nto route to x via y (and lies about doing so).\nLet’s now see how poisoned reverse solves the particular looping problem we\nencountered before in Figure 4.31(b). As a result of the poisoned reverse, y’s dis-\ntance table indicates Dz(x) = ∞. When the cost of the (x, y) link changes from 4 to 60\nat time t0, y updates its table and continues to route directly to x, albeit at a higher\ncost of 60, and informs z of its new cost to x, that is, Dy(x) = 60. After receiving the\n4.5\n•\nROUTING ALGORITHMS\n377\n\nupdate at t1, z immediately shifts its route to x to be via the direct (z, x) link at a cost\nof 50. Since this is a new least-cost path to x, and since the path no longer passes\nthrough y, z now informs y that Dz(x) = 50 at t2. After receiving the update from z, y\nupdates its distance table with Dy(x) = 51. Also, since z is now on y’s least-cost path\nto x, y poisons the reverse path from z to x by informing z at time t3 that Dy(x) = ∞\n(even though y knows that Dy(x) = 51 in truth).\nDoes poisoned reverse solve the general count-to-infinity problem? It does not.\nYou should convince yourself that loops involving three or more nodes (rather than\nsimply two immediately neighboring nodes) will not be detected by the poisoned\nreverse technique.\nA Comparison of LS and DV Routing Algorithms\nThe DV and LS algorithms take complementary approaches towards computing\nrouting. In the DV algorithm, each node talks to only its directly connected neigh-\nbors, but it provides its neighbors with least-cost estimates from itself to all the\nnodes (that it knows about) in the network. In the LS algorithm, each node talks with\nall other nodes (via broadcast), but it tells them only the costs of its directly con-\nnected links. Let’s conclude our study of LS and DV algorithms with a quick com-\nparison of some of their attributes. Recall that N is the set of nodes (routers) and E\nis the set of edges (links).\n•\nMessage complexity. We have seen that LS requires each node to know the\ncost of each link in the network. This requires O(|N| |E|) messages to be sent.\nAlso, whenever a link cost changes, the new link cost must be sent to all\nnodes. The DV algorithm requires message exchanges between directly con-\nnected neighbors at each iteration. We have seen that the time needed for the\nalgorithm to converge can depend on many factors. When link costs change,\nthe DV algorithm will propagate the results of the changed link cost only if\nthe new link cost results in a changed least-cost path for one of the nodes\nattached to that link.\n•\nSpeed of convergence. We have seen that our implementation of LS is an O(|N|2)\nalgorithm requiring O(|N| |E|)) messages. The DV algorithm can converge slowly\nand can have routing loops while the algorithm is converging. DV also suffers\nfrom the count-to-infinity problem.\n•\nRobustness. What can happen if a router fails, misbehaves, or is sabotaged?\nUnder LS, a router could broadcast an incorrect cost for one of its attached\nlinks (but no others). A node could also corrupt or drop any packets it received\nas part of an LS broadcast. But an LS node is computing only its own forward-\ning tables; other nodes are performing similar calculations for themselves. This\nmeans route calculations are somewhat separated under LS, providing a degree\nof robustness. Under DV, a node can advertise incorrect least-cost paths to any\nor all destinations. (Indeed, in 1997, a malfunctioning router in a small ISP\n378\nCHAPTER 4\n•\nTHE NETWORK LAYER"
    },
    {
      "chunk_id": "de2af0c3-31f7-45b0-b508-98bb6bb3d65f",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.5.3 Hierarchical Routing",
      "original_titles": [
        "4.5.3 Hierarchical Routing"
      ],
      "path": "Chapter 4 The Network Layer > 4.5 Routing Algorithms > 4.5.3 Hierarchical Routing",
      "start_page": 406,
      "end_page": 409,
      "token_count": 2421,
      "text": "provided national backbone routers with erroneous routing information. This\ncaused other routers to flood the malfunctioning router with traffic and caused\nlarge portions of the Internet to become disconnected for up to several hours\n[Neumann 1997].) More generally, we note that, at each iteration, a node’s cal-\nculation in DV is passed on to its neighbor and then indirectly to its neighbor’s\nneighbor on the next iteration. In this sense, an incorrect node calculation can\nbe diffused through the entire network under DV.\nIn the end, neither algorithm is an obvious winner over the other; indeed, both algo-\nrithms are used in the Internet.\nOther Routing Algorithms\nThe LS and DV algorithms we have studied are not only widely used in practice,\nthey are essentially the only routing algorithms used in practice today in the Inter-\nnet. Nonetheless, many routing algorithms have been proposed by researchers\nover the past 30 years, ranging from the extremely simple to the very sophisticated\nand complex. A broad class of routing algorithms is based on viewing packet traf-\nfic as flows between sources and destinations in a network. In this approach, the\nrouting problem can be formulated mathematically as a constrained optimization\nproblem known as a network flow problem [Bertsekas 1991]. Yet another set of\nrouting algorithms we mention here are those derived from the telephony world.\nThese circuit-switched routing algorithms are of interest to packet-switched\ndata networking in cases where per-link resources (for example, buffers, or a frac-\ntion of the link bandwidth) are to be reserved for each connection that is routed\nover the link. While the formulation of the routing problem might appear quite\ndifferent from the least-cost routing formulation we have seen in this chapter,\nthere are a number of similarities, at least as far as the path-finding algorithm\n(routing algorithm) is concerned. See [Ash 1998; Ross 1995; Girard 1990] for a\ndetailed discussion of this research area.\n4.5.3 Hierarchical Routing\nIn our study of LS and DV algorithms, we’ve viewed the network simply as a col-\nlection of interconnected routers. One router was indistinguishable from another in\nthe sense that all routers executed the same routing algorithm to compute routing\npaths through the entire network. In practice, this model and its view of a homoge-\nnous set of routers all executing the same routing algorithm is a bit simplistic for at\nleast two important reasons:\n•\nScale. As the number of routers becomes large, the overhead involved in\ncomputing, storing, and communicating routing information (for example, \n4.5\n•\nROUTING ALGORITHMS\n379\n\nLS updates or least-cost path changes) becomes prohibitive. Today’s public\nInternet consists of hundreds of millions of hosts. Storing routing information at\neach of these hosts would clearly require enormous amounts of memory. The\noverhead required to broadcast LS updates among all of the routers in the public\nInternet would leave no bandwidth left for sending data packets! A distance-vec-\ntor algorithm that iterated among such a large number of routers would surely\nnever converge. Clearly, something must be done to reduce the complexity of\nroute computation in networks as large as the public Internet.\n•\nAdministrative autonomy. Although researchers tend to ignore issues such as a\ncompany’s desire to run its routers as it pleases (for example, to run whatever\nrouting algorithm it chooses) or to hide aspects of its network’s internal organi-\nzation from the outside, these are important considerations. Ideally, an organiza-\ntion should be able to run and administer its network as it wishes, while still\nbeing able to connect its network to other outside networks.\nBoth of these problems can be solved by organizing routers into autonomous sys-\ntems (ASs), with each AS consisting of a group of routers that are typically under\nthe same administrative control (e.g., operated by the same ISP or belonging to the\nsame company network). Routers within the same AS all run the same routing algo-\nrithm (for example, an LS or DV algorithm) and have information about each\nother—exactly as was the case in our idealized model in the preceding section. The\nrouting algorithm running within an autonomous system is called an intra-\nautonomous system routing protocol. It will be necessary, of course, to connect\nASs to each other, and thus one or more of the routers in an AS will have the added\ntask of being responsible for forwarding packets to destinations outside the AS;\nthese routers are called gateway routers.\nFigure 4.32 provides a simple example with three ASs: AS1, AS2, and AS3.\nIn this figure, the heavy lines represent direct link connections between pairs of\nrouters. The thinner lines hanging from the routers represent subnets that are\ndirectly connected to the routers. AS1 has four routers—1a, 1b, 1c, and 1d—\nwhich run the intra-AS routing protocol used within AS1. Thus, each of these\nfour routers knows how to forward packets along the optimal path to any destina-\ntion within AS1. Similarly, autonomous systems AS2 and AS3 each have three\nrouters. Note that the intra-AS routing protocols running in AS1, AS2, and AS3\nneed not be the same. Also note that the routers 1b, 1c, 2a, and 3a are all gateway\nrouters.\nIt should now be clear how the routers in an AS determine routing paths for\nsource-destination pairs that are internal to the AS. But there is still a big missing\npiece to the end-to-end routing puzzle. How does a router, within some AS, know\nhow to route a packet to a destination that is outside the AS? It’s easy to answer\nthis question if the AS has only one gateway router that connects to only one\nother AS. In this case, because the AS’s intra-AS routing algorithm has deter-\nmined the least-cost path from each internal router to the gateway router, each\n380\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\ninternal router knows how it should forward the packet. The gateway router, upon\nreceiving the packet, forwards the packet on the one link that leads outside the\nAS. The AS on the other side of the link then takes over the responsibility \nof routing the packet to its ultimate destination. As an example, suppose router\n2b in Figure 4.32 receives a packet whose destination is outside of AS2. Router\n2b will then forward the packet to either router 2a or 2c, as specified by router\n2b’s forwarding table, which was configured by AS2’s intra-AS routing protocol.\nThe packet will eventually arrive to the gateway router 2a, which will forward\nthe packet to 1b. Once the packet has left 2a, AS2’s job is done with this one\npacket.\nSo the problem is easy when the source AS has only one link that leads outside\nthe AS. But what if the source AS has two or more links (through two or more gate-\nway routers) that lead outside the AS? Then the problem of knowing where to for-\nward the packet becomes significantly more challenging. For example, consider a\nrouter in AS1 and suppose it receives a packet whose destination is outside the AS.\nThe router should clearly forward the packet to one of its two gateway routers, 1b or\n1c, but which one? To solve this problem, AS1 needs (1) to learn which destinations\nare reachable via AS2 and which destinations are reachable via AS3, and (2) to\npropagate this reachability information to all the routers within AS1, so that each\nrouter can configure its forwarding table to handle external-AS destinations. These\n4.5\n•\nROUTING ALGORITHMS\n381\nAS1\nAS3\n3b\n3c\n3a\n1a\n1c\n1b\n1d\nAS2\n2a\n2c\n2b\nIntra-AS routing\nalgorithm\nForwarding\ntable\nInter-AS routing\nalgorithm\nFigure 4.32 \u0002 An example of interconnected autonomous systems\n\ntwo tasks—obtaining reachability information from neighboring ASs and propagat-\ning the reachability information to all routers internal to the AS—are handled by the\ninter-AS routing protocol. Since the inter-AS routing protocol involves communi-\ncation between two ASs, the two communicating ASs must run the same inter-AS\nrouting protocol. In fact, in the Internet all ASs run the same inter-AS routing proto-\ncol, called BGP4, which is discussed in the next section. As shown in Figure 4.32,\neach router receives information from an intra-AS routing protocol and an inter-AS\nrouting protocol, and uses the information from both protocols to configure its for-\nwarding table.\nAs an example, consider a subnet x (identified by its CIDRized address), and\nsuppose that AS1 learns from the inter-AS routing protocol that subnet x is reach-\nable from AS3 but is not reachable from AS2. AS1 then propagates this information\nto all of its routers. When router 1d learns that subnet x is reachable from AS3, and\nhence from gateway 1c, it then determines, from the information provided by the\nintra-AS routing protocol, the router interface that is on the least-cost path from\nrouter 1d to gateway router 1c. Say this is interface I. The router 1d can then put the\nentry (x, I) into its forwarding table. (This example, and others presented in this sec-\ntion, gets the general ideas across but is a simplification of what really happens in\nthe Internet. In the next section we’ll provide a more detailed description, albeit\nmore complicated, when we discuss BGP.)\nFollowing up on the previous example, now suppose that AS2 and AS3 con-\nnect to other ASs, which are not shown in the diagram. Also suppose that AS1\nlearns from the inter-AS routing protocol that subnet x is reachable both from AS2,\nvia gateway 1b, and from AS3, via gateway 1c. AS1 would then propagate this\ninformation to all its routers, including router 1d. In order to configure its forward-\ning table, router 1d would have to determine to which gateway router, 1b or 1c, it\nshould direct packets that are destined for subnet x. One approach, which is often\nemployed in practice, is to use hot-potato routing. In hot-potato routing, the AS\ngets rid of the packet (the hot potato) as quickly as possible (more precisely, as\ninexpensively as possible). This is done by having a router send the packet to the\ngateway router that has the smallest router-to-gateway cost among all gateways\nwith a path to the destination. In the context of the current example, hot-potato\nrouting, running in 1d, would use information from the intra-AS routing protocol\nto determine the path costs to 1b and 1c, and then choose the path with the least\ncost. Once this path is chosen, router 1d adds an entry for subnet x in its forward-\ning table. Figure 4.33 summarizes the actions taken at router 1d for adding the new\nentry for x to the forwarding table.\nWhen an AS learns about a destination from a neighboring AS, the AS can\nadvertise this routing information to some of its other neighboring ASs. For example,\nsuppose AS1 learns from AS2 that subnet x is reachable via AS2. AS1 could then tell\nAS3 that x is reachable via AS1. In this manner, if AS3 needs to route a packet\ndestined to x, AS3 would forward the packet to AS1, which would in turn forward the\npacket to AS2. As we’ll see in our discussion of BGP, an AS has quite a bit of\n382\nCHAPTER 4\n•\nTHE NETWORK LAYER"
    },
    {
      "chunk_id": "8a1044d1-ca00-4fb0-96cc-c77f4348e1f9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.6 Routing in the Internet",
      "original_titles": [
        "4.6 Routing in the Internet"
      ],
      "path": "Chapter 4 The Network Layer > 4.6 Routing in the Internet",
      "start_page": 410,
      "end_page": 410,
      "token_count": 523,
      "text": "flexibility in deciding which destinations it advertises to its neighboring ASs. This\nis a policy decision, typically depending more on economic issues than on technical\nissues.\nRecall from Section 1.5 that the Internet consists of a hierarchy of intercon-\nnected ISPs. So what is the relationship between ISPs and ASs? You might think that\nthe routers in an ISP, and the links that interconnect them, constitute a single AS.\nAlthough this is often the case, many ISPs partition their network into multiple ASs.\nFor example, some tier-1 ISPs use one AS for their entire network; others break up\ntheir ISP into tens of interconnected ASs.\nIn summary, the problems of scale and administrative authority are solved by\ndefining autonomous systems. Within an AS, all routers run the same intra-AS rout-\ning protocol. Among themselves, the ASs run the same inter-AS routing protocol.\nThe problem of scale is solved because an intra-AS router need only know about\nrouters within its AS. The problem of administrative authority is solved since an\norganization can run whatever intra-AS routing protocol it chooses; however, each\npair of connected ASs needs to run the same inter-AS routing protocol to exchange\nreachability information.\nIn the following section, we’ll examine two intra-AS routing protocols (RIP and\nOSPF) and the inter-AS routing protocol (BGP) that are used in today’s Internet.\nThese case studies will nicely round out our study of hierarchical routing.\n4.6 Routing in the Internet\nHaving studied Internet addressing and the IP protocol, we now turn our attention to\nthe Internet’s routing protocols; their job is to determine the path taken by a data-\ngram between source and destination. We’ll see that the Internet’s routing protocols\nembody many of the principles we learned earlier in this chapter. The link-state and\ndistance-vector approaches studied in Sections 4.5.1 and 4.5.2 and the notion of an\nautonomous system considered in Section 4.5.3 are all central to how routing is\ndone in today’s Internet.\n4.6\n•\nROUTING IN THE INTERNET\n383\nLearn from inter-AS\nprotocol that subnet\nx is reachable via\nmultiple gateways.\nUse routing info from\nintra-AS protocol to\ndetermine costs of\nleast-cost paths to\neach of the gateways.\nHot potato routing:\nChoose the gateway\nthat has the\nsmallest least cost.\nDetermine from\nforwarding table the\ninterface I that leads\nto least-cost gateway.\nEnter (x,I) in\nforwarding table.\nFigure 4.33 \u0002 Steps in adding an outside-AS destination in a router’s for-\nwarding table"
    },
    {
      "chunk_id": "037f128d-f694-487b-b9b8-af7ff8b122f8",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.6.1 Intra-AS Routing in the Internet: RIP",
      "original_titles": [
        "4.6.1 Intra-AS Routing in the Internet: RIP"
      ],
      "path": "Chapter 4 The Network Layer > 4.6 Routing in the Internet > 4.6.1 Intra-AS Routing in the Internet: RIP",
      "start_page": 411,
      "end_page": 414,
      "token_count": 1826,
      "text": "Recall from Section 4.5.3 that an autonomous system (AS) is a collection of\nrouters under the same administrative and technical control, and that all run the\nsame routing protocol among themselves. Each AS, in turn, typically contains mul-\ntiple subnets (where we use the term subnet in the precise, addressing sense in Sec-\ntion 4.4.2).\n4.6.1 Intra-AS Routing in the Internet: RIP\nAn intra-AS routing protocol is used to determine how routing is performed within\nan autonomous system (AS). Intra-AS routing protocols are also known as interior\ngateway protocols. Historically, two routing protocols have been used extensively\nfor routing within an autonomous system in the Internet: the Routing Information\nProtocol (RIP) and Open Shortest Path First (OSPF). A routing protocol closely\nrelated to OSPF is the IS-IS protocol [RFC 1142, Perlman 1999]. We first discuss\nRIP and then consider OSPF.\nRIP was one of the earliest intra-AS Internet routing protocols and is still in\nwidespread use today. It traces its origins and its name to the Xerox Network Sys-\ntems (XNS) architecture. The widespread deployment of RIP was due in great part\nto its inclusion in 1982 in the Berkeley Software Distribution (BSD) version of\nUNIX supporting TCP/IP. RIP version 1 is defined in [RFC 1058], with a backward-\ncompatible version 2 defined in [RFC 2453].\nRIP is a distance-vector protocol that operates in a manner very close to the\nidealized DV protocol we examined in Section 4.5.2. The version of RIP specified\nin RFC 1058 uses hop count as a cost metric; that is, each link has a cost of 1. In\nthe DV algorithm in Section 4.5.2, for simplicity, costs were defined between pairs\nof routers. In RIP (and also in OSPF), costs are actually from source router to a des-\ntination subnet. RIP uses the term hop, which is the number of subnets traversed\nalong the shortest path from source router to destination subnet, including the des-\ntination subnet. Figure 4.34 illustrates an AS with six leaf subnets. The table in the\nfigure indicates the number of hops from the source A to each of the leaf subnets.\nThe maximum cost of a path is limited to 15, thus limiting the use of RIP to\nautonomous systems that are fewer than 15 hops in diameter. Recall that in DV\nprotocols, neighboring routers exchange distance vectors with each other. The\ndistance vector for any one router is the current estimate of the shortest path\ndistances from that router to the subnets in the AS. In RIP, routing updates \nare exchanged between neighbors approximately every 30 seconds using a \nRIP response message. The response message sent by a router or host contains \na list of up to 25 destination subnets within the AS, as well as the sender’s\ndistance to each of those subnets. Response messages are also known as RIP\nadvertisements.\nLet’s take a look at a simple example of how RIP advertisements work. Con-\nsider the portion of an AS shown in Figure 4.35. In this figure, lines connecting the\nrouters denote subnets. Only selected routers (A, B, C, and D) and subnets (w, x, y,\n384\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nand z) are labeled. Dotted lines indicate that the AS continues on; thus this\nautonomous system has many more routers and links than are shown.\nEach router maintains a RIP table known as a routing table. A router’s routing\ntable includes both the router’s distance vector and the router’s forwarding table.\nFigure 4.36 shows the routing table for router D. Note that the routing table has\nthree columns. The first column is for the destination subnet, the second column\nindicates the identity of the next router along the shortest path to the destination sub-\nnet, and the third column indicates the number of hops (that is, the number of sub-\nnets that have to be traversed, including the destination subnet) to get to the\ndestination subnet along the shortest path. For this example, the table indicates that\nto send a datagram from router D to destination subnet w, the datagram should first\nbe forwarded to neighboring router A; the table also indicates that destination sub-\nnet w is two hops away along the shortest path. Similarly, the table indicates that\nsubnet z is seven hops away via router B. In principle, a routing table will have one\nrow for each subnet in the AS, although RIP version 2 allows subnet entries to be\naggregated using route aggregation techniques similar to those we examined in\n4.6\n•\nROUTING IN THE INTERNET\n385\nC\nD\nA\nu\nDestination \nHops\nu\nv\nw\nx\ny\nz\n1\n2\n2\n3\n3\n2\n \nv\n \nw\n \nx\n \ny\n \nz\nB\nFigure 4.34 \u0002 Number of hops from source router A to various subnets\nA\nC\nD\nB\nz\nw\nx\ny\nFigure 4.35 \u0002 A portion of an autonomous system\n\nSection 4.4. The table in Figure 4.36, and the subsequent tables to come, are only\npartially complete.\nNow suppose that 30 seconds later, router D receives from router A the adver-\ntisement shown in Figure 4.37. Note that this advertisement is nothing other than\nthe routing table information from router A! This information indicates, in particu-\nlar, that subnet z is only four hops away from router A. Router D, upon receiving this\nadvertisement, merges the advertisement (Figure 4.37) with the old routing table\n(Figure 4.36). In particular, router D learns that there is now a path through router A\nto subnet z that is shorter than the path through router B. Thus, router D updates its\nrouting table to account for the shorter shortest path, as shown in Figure 4.38. How\nis it, you might ask, that the shortest path to subnet z has become shorter? Possibly,\nthe decentralized distance-vector algorithm is still in the process of converging (see\nSection 4.5.2), or perhaps new links and/or routers were added to the AS, thus\nchanging the shortest paths in the AS.\nLet’s next consider a few of the implementation aspects of RIP. Recall that \nRIP routers exchange advertisements approximately every 30 seconds. If a router\ndoes not hear from its neighbor at least once every 180 seconds, that neighbor is\nconsidered to be no longer reachable; that is, either the neighbor has died or the\n386\nCHAPTER 4\n•\nTHE NETWORK LAYER\nDestination Subnet\nNext Router\nNumber of Hops to Destination\nw\nA\n2\ny\nB\n2\nz\nB\n7\nx\n—\n1\n. . . .\n. . . .\n. . . .\nFigure 4.36 \u0002 Routing table in router D before receiving advertisement\nfrom router A\nDestination Subnet\nNext Router\nNumber of Hops to Destination\nz\nC\n4\nw\n—\n1\nx\n—\n1\n. . . .\n. . . .\n. . . .\nFigure 4.37 \u0002 Advertisement from router A\n\nconnecting link has gone down. When this happens, RIP modifies the local routing\ntable and then propagates this information by sending advertisements to its neigh-\nboring routers (the ones that are still reachable). A router can also request informa-\ntion about its neighbor’s cost to a given destination using RIP’s request message.\nRouters send RIP request and response messages to each other over UDP using port\nnumber 520. The UDP segment is carried between routers in a standard IP data-\ngram. The fact that RIP uses a transport-layer protocol (UDP) on top of a network-\nlayer protocol (IP) to implement network-layer functionality (a routing algorithm)\nmay seem rather convoluted (it is!). Looking a little deeper at how RIP is imple-\nmented will clear this up.\nFigure 4.39 sketches how RIP is typically implemented in a UNIX system, for\nexample, a UNIX workstation serving as a router. A process called routed (pronounced\n“route dee”) executes RIP, that is, maintains routing information and exchanges\nmessages with routed processes running in neighboring routers. Because RIP is\nimplemented as an application-layer process (albeit a very special one that is able to\n4.6\n•\nROUTING IN THE INTERNET\n387\nDestination Subnet\nNext Router\nNumber of Hops to Destination\nw\nA\n2\ny\nB\n2\nz\nA\n5\n. . . .\n. . . .\n. . . .\nFigure 4.38 \u0002 Routing table in router D after receiving advertisement from\nrouter A\nNetwork\n(IP)\nTransport\n(UDP)\nData link\nPhysical\nForwarding\ntables\nRouted\nForwarding\ntables\nRouted\nNetwork\n(IP)\nTransport\n(UDP)\nData link\nPhysical\nFigure 4.39 \u0002 Implementation of RIP as the routed daemon"
    },
    {
      "chunk_id": "9dd57e46-cd29-4ced-9845-b40516c4a0af",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.6.2 Intra-AS Routing in the Internet: OSPF",
      "original_titles": [
        "4.6.2 Intra-AS Routing in the Internet: OSPF"
      ],
      "path": "Chapter 4 The Network Layer > 4.6 Routing in the Internet > 4.6.2 Intra-AS Routing in the Internet: OSPF",
      "start_page": 415,
      "end_page": 416,
      "token_count": 1327,
      "text": "manipulate the routing tables within the UNIX kernel), it can send and receive mes-\nsages over a standard socket and use a standard transport protocol. As shown, RIP is\nimplemented as an application-layer protocol (see Chapter 2) running over UDP. If\nyou’re interested in looking at an implementation of RIP (or the OSPF and BGP pro-\ntocols that we will study shortly), see [Quagga 2012].\n4.6.2 Intra-AS Routing in the Internet: OSPF\nLike RIP, OSPF routing is widely used for intra-AS routing in the Internet. OSPF\nand its closely related cousin, IS-IS, are typically deployed in upper-tier ISPs\nwhereas RIP is deployed in lower-tier ISPs and enterprise networks. The Open in\nOSPF indicates that the routing protocol specification is publicly available (for\nexample, as opposed to Cisco’s EIGRP protocol). The most recent version of OSPF,\nversion 2, is defined in RFC 2328, a public document.\nOSPF was conceived as the successor to RIP and as such has a number of\nadvanced features. At its heart, however, OSPF is a link-state protocol that uses\nflooding of link-state information and a Dijkstra least-cost path algorithm. With\nOSPF, a router constructs a complete topological map (that is, a graph) of the entire\nautonomous system. The router then locally runs Dijkstra’s shortest-path algorithm\nto determine a shortest-path tree to all subnets, with itself as the root node. Individ-\nual link costs are configured by the network administrator (see Principles and Prac-\ntice: Setting OSPF Weights). The administrator might choose to set all link costs to\n1, thus achieving minimum-hop routing, or might choose to set the link weights to\nbe inversely proportional to link capacity in order to discourage traffic from using\nlow-bandwidth links. OSPF does not mandate a policy for how link weights are set\n(that is the job of the network administrator), but instead provides the mechanisms\n(protocol) for determining least-cost path routing for the given set of link weights.\nWith OSPF, a router broadcasts routing information to all other routers in the\nautonomous system, not just to its neighboring routers. A router broadcasts link-\nstate information whenever there is a change in a link’s state (for example, a change\nin cost or a change in up/down status). It also broadcasts a link’s state periodically\n(at least once every 30 minutes), even if the link’s state has not changed. RFC 2328\nnotes that “this periodic updating of link state advertisements adds robustness to the\nlink state algorithm.” OSPF advertisements are contained in OSPF messages that\nare carried directly by IP, with an upper-layer protocol of 89 for OSPF. Thus, the\nOSPF protocol must itself implement functionality such as reliable message transfer\nand link-state broadcast. The OSPF protocol also checks that links are operational\n(via a HELLO message that is sent to an attached neighbor) and allows an OSPF\nrouter to obtain a neighboring router’s database of network-wide link state.\nSome of the advances embodied in OSPF include the following:\n•\nSecurity. Exchanges between OSPF routers (for example, link-state updates)\ncan be authenticated. With authentication, only trusted routers can participate\n388\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nin the OSPF protocol within an AS, thus preventing malicious intruders (or net-\nworking students taking their newfound knowledge out for a joyride) from\ninjecting incorrect information into router tables. By default, OSPF packets\nbetween routers are not authenticated and could be forged. Two types of\nauthentication can be configured—simple and MD5 (see Chapter 8 for a dis-\ncussion on MD5 and authentication in general). With simple authentication, the\nsame password is configured on each router. When a router sends an OSPF\npacket, it includes the password in plaintext. Clearly, simple authentication is\nnot very secure. MD5 authentication is based on shared secret keys that are\nconfigured in all the routers. For each OSPF packet that it sends, the router\ncomputes the MD5 hash of the content of the OSPF packet appended with the\nsecret key. (See the discussion of message authentication codes in Chapter 7.)\nThen the router includes the resulting hash value in the OSPF packet. The\nreceiving router, using the preconfigured secret key, will compute an MD5\nhash of the packet and compare it with the hash value that the packet carries,\nthus verifying the packet’s authenticity. Sequence numbers are also used with\nMD5 authentication to protect against replay attacks.\n•\nMultiple same-cost paths. When multiple paths to a destination have the same\ncost, OSPF allows multiple paths to be used (that is, a single path need not be\nchosen for carrying all traffic when multiple equal-cost paths exist).\n•\nIntegrated support for unicast and multicast routing. Multicast OSPF (MOSPF)\n[RFC 1584] provides simple extensions to OSPF to provide for multicast routing\n(a topic we cover in more depth in Section 4.7.2). MOSPF uses the existing\nOSPF link database and adds a new type of link-state advertisement to the exist-\ning OSPF link-state broadcast mechanism.\n•\nSupport for hierarchy within a single routing domain. Perhaps the most signifi-\ncant advance in OSPF is the ability to structure an autonomous system hierarchi-\ncally. Section 4.5.3 has already looked at the many advantages of hierarchical\nrouting structures. We cover the implementation of OSPF hierarchical routing in\nthe remainder of this section.\nAn OSPF autonomous system can be configured hierarchically into areas. Each\narea runs its own OSPF link-state routing algorithm, with each router in an area\nbroadcasting its link state to all other routers in that area. Within each area, one or\nmore area border routers are responsible for routing packets outside the area. Lastly,\nexactly one OSPF area in the AS is configured to be the backbone area. The primary\nrole of the backbone area is to route traffic between the other areas in the AS. The\nbackbone always contains all area border routers in the AS and may contain nonbor-\nder routers as well. Inter-area routing within the AS requires that the packet be first\nrouted to an area border router (intra-area routing), then routed through the back-\nbone to the area border router that is in the destination area, and then routed to the\nfinal destination.\n4.6\n•\nROUTING IN THE INTERNET\n389"
    },
    {
      "chunk_id": "44dfae8f-b0d1-4b11-9a33-25820dfd21b0",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.6.3 Inter-AS Routing: BGP",
      "original_titles": [
        "4.6.3 Inter-AS Routing: BGP"
      ],
      "path": "Chapter 4 The Network Layer > 4.6 Routing in the Internet > 4.6.3 Inter-AS Routing: BGP",
      "start_page": 417,
      "end_page": 425,
      "token_count": 5547,
      "text": "OSPF is a relatively complex protocol, and our coverage here has been neces-\nsarily brief; [Huitema 1998; Moy 1998; RFC 2328] provide additional details.\n4.6.3 Inter-AS Routing: BGP\nWe just learned how ISPs use RIP and OSPF to determine optimal paths for source-\ndestination pairs that are internal to the same AS. Let’s now examine how paths are\ndetermined for source-destination pairs that span multiple ASs. The Border Gate-\nway Protocol version 4, specified in RFC 4271 (see also [RFC 4274), is the de facto\nstandard inter-AS routing protocol in today’s Internet. It is commonly referred to as\nBGP4 or simply as BGP. As an inter-AS routing protocol (see Section 4.5.3), BGP\nprovides each AS a means to\n1. Obtain subnet reachability information from neighboring ASs.\n2. Propagate the reachability information to all routers internal to the AS.\n3. Determine “good” routes to subnets based on the reachability information and\non AS policy.\n390\nCHAPTER 4\n•\nTHE NETWORK LAYER\nSETTING OSPF LINK WEIGHTS\nOur discussion of link-state routing has implicitly assumed that link weights are set, a rout-\ning algorithm such as OSPF is run, and traffic flows according to the routing tables comput-\ned by the LS algorithm. In terms of cause and effect, the link weights are given (i.e., they\ncome first) and result (via Dijkstra’s algorithm) in routing paths that minimize overall cost. In\nthis viewpoint, link weights reflect the cost of using a link (e.g., if link weights are inversely\nproportional to capacity, then the use of high-capacity links would have smaller weights\nand thus be more attractive from a routing standpoint) and Disjkstra’s algorithm serves to\nminimize overall cost.\nIn practice, the cause and effect relationship between link weights and routing paths\nmay be reversed, with network operators configuring link weights in order to obtain routing\npaths that achieve certain traffic engineering goals [Fortz 2000, Fortz 2002]. For example,\nsuppose a network operator has an estimate of traffic flow entering the network at each\ningress point and destined for each egress point. The operator may then want to put in\nplace a specific routing of ingress-to-egress flows that minimizes the maximum utilization\nover all of the network’s links. But with a routing algorithm such as OSPF, the operator’s\nmain “knobs” for tuning the routing of flows through the network are the link weights. Thus,\nin order to achieve the goal of minimizing the maximum link utilization, the operator must\nfind the set of link weights that achieves this goal. This is a reversal of the cause and effect\nrelationship—the desired routing of flows is known, and the OSPF link weights must be\nfound such that the OSPF routing algorithm results in this desired routing of flows.\nPRINCIPLES IN PRACTICE\n\nMost importantly, BGP allows each subnet to advertise its existence to the rest of\nthe Internet. A subnet screams “I exist and I am here,” and BGP makes sure that all\nthe ASs in the Internet know about the subnet and how to get there. If it weren’t for\nBGP, each subnet would be isolated—alone and unknown by the rest of the Internet.\nBGP Basics\nBGP is extremely complex; entire books have been devoted to the subject and many\nissues are still not well understood [Yannuzzi 2005]. Furthermore, even after having\nread the books and RFCs, you may find it difficult to fully master BGP without hav-\ning practiced BGP for many months (if not years) as a designer or administrator of\nan upper-tier ISP. Nevertheless, because BGP is an absolutely critical protocol for\nthe Internet—in essence, it is the protocol that glues the whole thing together—we\nneed to acquire at least a rudimentary understanding of how it works. We begin by\ndescribing how BGP might work in the context of the simple example network we\nstudied earlier in Figure 4.32. In this description, we build on our discussion of hier-\narchical routing in Section 4.5.3; we encourage you to review that material.\nIn BGP, pairs of routers exchange routing information over semipermanent\nTCP connections using port 179. The semi-permanent TCP connections for the net-\nwork in Figure 4.32 are shown in Figure 4.40. There is typically one such BGP TCP\nconnection for each link that directly connects two routers in two different ASs;\nthus, in Figure 4.40, there is a TCP connection between gateway routers 3a and 1c\nand another TCP connection between gateway routers 1b and 2a. There are also\nsemipermanent BGP TCP connections between routers within an AS. In particular,\nFigure 4.40 displays a common configuration of one TCP connection for each pair\nof routers internal to an AS, creating a mesh of TCP connections within each AS.\nFor each TCP connection, the two routers at the end of the connection are called\nBGP peers, and the TCP connection along with all the BGP messages sent over the\n4.6\n•\nROUTING IN THE INTERNET\n391\nAS1\nAS3\n3b\neBGP session\nKey:\niBGP session\n3c\n3a\n1a\n1c\n1b\n1d\nAS2\n2a\n2c\n2b\nFigure 4.40 \u0002 eBGP and iBGP sessions\nVideoNote\nGluing the Internet\ntogether\n\n392\nCHAPTER 4\n•\nTHE NETWORK LAYER\nOBTAINING INTERNET PRESENCE: PUTTING THE PUZZLE TOGETHER\nSuppose you have just created a small that has a number of servers, including a public\nWeb server that describes your company’s products and services, a mail server from which\nyour employees obtain their email messages, and a DNS server. Naturally, you would like\nthe entire world to be able to surf your Web site in order to learn about your exciting prod-\nucts and services. Moreover, you would like your employees to be able to send and\nreceive email to potential customers throughout the world.\nTo meet these goals, you first need to obtain Internet connectivity, which is done by\ncontracting with, and connecting to, a local ISP. Your company will have a gateway\nrouter, which will be connected to a router in your local ISP. This connection might be\na DSL connection through the existing telephone infrastructure, a leased line to the ISP’s\nrouter, or one of the many other access solutions described in Chapter 1. Your local\nISP will also provide you with an IP address range, e.g., a /24 address range consist-\ning of 256 addresses. Once you have your physical connectivity and your IP address\nrange, you will assign one of the IP addresses (in your address range) to your Web\nserver, one to your mail server, one to your DNS server, one to your gateway router,\nand other IP addresses to other servers and networking devices in your company’s \nnetwork.\nIn addition to contracting with an ISP, you will also need to contract with an Internet regis-\ntrar to obtain a domain name for your company, as described in Chapter 2. For example, if\nyour company’s name is, say, Xanadu Inc., you will naturally try to obtain the domain name\nxanadu.com. Your company must also obtain presence in the DNS system. Specifically,\nbecause outsiders will want to contact your DNS server to obtain the IP addresses of your\nservers, you will also need to provide your registrar with the IP address of your DNS server.\nYour registrar will then put an entry for your DNS server (domain name and corresponding IP\naddress) in the .com top-level-domain servers, as described in Chapter 2. After this step is\ncompleted, any user who knows your domain name (e.g., xanadu.com) will be able to\nobtain the IP address of your DNS server via the DNS system.\nSo that people can discover the IP addresses of your Web server, in your DNS server\nyou will need to include entries that map the host name of your Web server (e.g.,\nwww.xanadu.com) to its IP address. You will want to have similar entries for other publicly\navailable servers in your company, including your mail server. In this manner, if Alice\nwants to browse your Web server, the DNS system will contact your DNS server, find the\nIP address of your Web server, and give it to Alice. Alice can then establish a TCP\nconnection directly with your Web server.\nHowever, there still remains one other necessary and crucial step to allow outsiders\nfrom around the world access your Web server. Consider what happens when Alice,\nwho knows the IP address of your Web server, sends an IP datagram (e.g., a TCP SYN\nsegment) to that IP address. This datagram will be routed through the Internet, visiting a\nseries of routers in many different ASes, and eventually reach your Web server. When\nPRINCIPLES IN PRACTICE\n\nconnection is called a BGP session. Furthermore, a BGP session that spans two ASs\nis called an external BGP (eBGP) session, and a BGP session between routers in\nthe same AS is called an internal BGP (iBGP) session. In Figure 4.40, the eBGP\nsessions are shown with the long dashes; the iBGP sessions are shown with the short\ndashes. Note that BGP session lines in Figure 4.40 do not always correspond to the\nphysical links in Figure 4.32.\nBGP allows each AS to learn which destinations are reachable via its neighbor-\ning ASs. In BGP, destinations are not hosts but instead are CIDRized prefixes, with\neach prefix representing a subnet or a collection of subnets. Thus, for example, sup-\npose there are four subnets attached to AS2: 138.16.64/24, 138.16.65/24,\n138.16.66/24, and 138.16.67/24. Then AS2 could aggregate the prefixes for these four\nsubnets and use BGP to advertise the single prefix to 138.16.64/22 to AS1. As another\nexample, suppose that only the first three of those four subnets are in AS2 and the\nfourth subnet, 138.16.67/24, is in AS3. Then, as described in the Principles and Prac-\ntice in Section 4.4.2, because routers use longest-prefix matching for forwarding data-\ngrams, AS3 could advertise to AS1 the more specific prefix 138.16.67/24 and AS2\ncould still advertise to AS1 the aggregated prefix 138.16.64/22.\nLet’s now examine how BGP would distribute prefix reachability information\nover the BGP sessions shown in Figure 4.40. As you might expect, using the eBGP\nsession between the gateway routers 3a and 1c, AS3 sends AS1 the list of prefixes\nthat are reachable from AS3; and AS1 sends AS3 the list of prefixes that are reach-\nable from AS1. Similarly, AS1 and AS2 exchange prefix reachability information\nthrough their gateway routers 1b and 2a. Also as you may expect, when a gateway\nrouter (in any AS) receives eBGP-learned prefixes, the gateway router uses its iBGP\nsessions to distribute the prefixes to the other routers in the AS. Thus, all the routers\nin AS1 learn about AS3 prefixes, including the gateway router 1b. The gateway\nrouter 1b (in AS1) can therefore re-advertise AS3’s prefixes to AS2. When a router\n(gateway or not) learns about a new prefix, it creates an entry for the prefix in its\nforwarding table, as described in Section 4.5.3.\n4.6\n•\nROUTING IN THE INTERNET\n393\nany one of the routers receives the datagram, it is going to look for an entry in its\nforwarding table to determine on which outgoing port it should forward the datagram.\nTherefore, each of the routers needs to know about the existence of your company’s\n/24 prefix (or some aggregate entry). How does a router become aware of your \ncompany’s prefix? As we have just seen, it becomes aware of it from BGP! Specifically,\nwhen your company contracts with a local ISP and gets assigned a prefix (i.e., an\naddress range), your local ISP will use BGP to advertise this prefix to the ISPs to which \nit connects. Those ISPs will then, in turn, use BGP to propagate the advertisement.\nEventually, all Internet routers will know about your prefix (or about some aggregate that\nincludes your prefix) and thus be able to appropriately forward datagrams destined to\nyour Web and mail servers.\n\nPath Attributes and BGP Routes\nHaving now a preliminary understanding of BGP, let’s get a little deeper into it\n(while still brushing some of the less important details under the rug!). In BGP, an\nautonomous system is identified by its globally unique autonomous system num-\nber (ASN) [RFC 1930]. (Technically, not every AS has an ASN. In particular, a so-\ncalled stub AS that carries only traffic for which it is a source or destination will not\ntypically have an ASN; we ignore this technicality in our discussion in order to bet-\nter see the forest for the trees.) AS numbers, like IP addresses, are assigned by\nICANN regional registries [ICANN 2012].\nWhen a router advertises a prefix across a BGP session, it includes with the pre-\nfix a number of BGP attributes. In BGP jargon, a prefix along with its attributes is\ncalled a route. Thus, BGP peers advertise routes to each other. Two of the more\nimportant attributes are AS-PATH and NEXT-HOP:\n•\nAS-PATH. This attribute contains the ASs through which the advertisement for the\nprefix has passed. When a prefix is passed into an AS, the AS adds its ASN to the AS-\nPATH attribute. For example, consider Figure 4.40 and suppose that prefix\n138.16.64/24 is first advertised from AS2 to AS1; if AS1 then advertises the prefix to\nAS3, AS-PATH would be AS2 AS1. Routers use the AS-PATH attribute to detect and\nprevent looping advertisements; specifically, if a router sees that its AS is contained\nin the path list, it will reject the advertisement. As we’ll soon discuss, routers also use\nthe AS-PATH attribute in choosing among multiple paths to the same prefix.\n•\nProviding the critical link between the inter-AS and intra-AS routing protocols, the\nNEXT-HOP attribute has a subtle but important use. The NEXT-HOP is the router\ninterface that begins the AS-PATH. To gain insight into this attribute, let’s again refer\nto Figure 4.40. Consider what happens when the gateway router 3a in AS3 advertises\na route to gateway router 1c in AS1 using eBGP. The route includes the advertised\nprefix, which we’ll call x, and an AS-PATH to the prefix. This advertisement also\nincludes the NEXT-HOP, which is the IP address of the router 3a interface that leads\nto 1c. (Recall that a router has multiple IP addresses, one for each of its interfaces.)\nNow consider what happens when router 1d learns about this route from iBGP. After\nlearning about this route to x, router 1d may want to forward packets to x along the\nroute, that is, router 1d may want to include the entry (x, l) in its forwarding table,\nwhere l is its interface that begins the least-cost path from 1d towards the gateway\nrouter 1c. To determine l, 1d provides the IP address in the NEXT-HOP attribute to\nits intra-AS routing module. Note that the intra-AS routing algorithm has determined\nthe least-cost path to all subnets attached to the routers in AS1, including to the sub-\nnet for the link between 1c and 3a. From this least-cost path from 1d to the 1c-3a sub-\nnet, 1d determines its router interface l that begins this path and then adds the entry\n(x, l) to its forwarding table. Whew! In summary, the NEXT-HOP attribute is used by\nrouters to properly configure their forwarding tables.\n•\nFigure 4.41 illustrates another situation where the NEXT-HOP is needed. In this fig-\nure, AS1 and AS2 are connected by two peering links. A router in AS1 could learn\n394\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nabout two different routes to the same prefix x. These two routes could have the same\nAS-PATH to x, but could have different NEXT-HOP values corresponding to the dif-\nferent peering links. Using the NEXT-HOP values and the intra-AS routing algo-\nrithm, the router can determine the cost of the path to each peering link, and then\napply hot-potato routing (see Section 4.5.3) to determine the appropriate interface.\nBGP also includes attributes that allow routers to assign preference metrics to\nthe routes, and an attribute that indicates how the prefix was inserted into BGP at\nthe origin AS. For a full discussion of route attributes, see [Griffin 2012; Stewart\n1999; Halabi 2000; Feamster 2004; RFC 4271].\nWhen a gateway router receives a route advertisement, it uses its import policy\nto decide whether to accept or filter the route and whether to set certain attributes\nsuch as the router preference metrics. The import policy may filter a route because\nthe AS may not want to send traffic over one of the ASs in the route’s AS-PATH.\nThe gateway router may also filter a route because it already knows of a preferable\nroute to the same prefix.\nBGP Route Selection\nAs described earlier in this section, BGP uses eBGP and iBGP to distribute routes\nto all the routers within ASs. From this distribution, a router may learn about more\nthan one route to any one prefix, in which case the router must select one of the\n4.6\n•\nROUTING IN THE INTERNET\n395\nTwo peering\nlinks between\nAS2 and AS1\nRouter learns\nabout a\nroute to x\nRouter learns about\nanother route to x\nAS2\nAS1\nKey:\nRoute advertisements\nmessage for destination x\nFigure 4.41 \u0002 NEXT-HOP attributes in advertisements are used to deter-\nmine which peering link to use\n\npossible routes. The input into this route selection process is the set of all routes that\nhave been learned and accepted by the router. If there are two or more routes to the\nsame prefix, then BGP sequentially invokes the following elimination rules until one\nroute remains:\n•\nRoutes are assigned a local preference value as one of their attributes. The local\npreference of a route could have been set by the router or could have been\nlearned by another router in the same AS. This is a policy decision that is left up\nto the AS’s network administrator. (We will shortly discuss BGP policy issues in\nsome detail.) The routes with the highest local preference values are selected.\n•\nFrom the remaining routes (all with the same local preference value), the route with\nthe shortest AS-PATH is selected. If this rule were the only rule for route selection,\nthen BGP would be using a DV algorithm for path determination, where the dis-\ntance metric uses the number of AS hops rather than the number of router hops.\n•\nFrom the remaining routes (all with the same local preference value and the same\nAS-PATH length), the route with the closest NEXT-HOP router is selected. Here,\nclosest means the router for which the cost of the least-cost path, determined by\nthe intra-AS algorithm, is the smallest. As discussed in Section 4.5.3, this process\nis called hot-potato routing.\n•\nIf more than one route still remains, the router uses BGP identifiers to select the\nroute; see [Stewart 1999].\nThe elimination rules are even more complicated than described above. To avoid\nnightmares about BGP, it’s best to learn about BGP selection rules in small doses!\n396\nCHAPTER 4\n•\nTHE NETWORK LAYER\nPUTTING IT ALL TOGETHER: HOW DOES AN ENTRY GET INTO A ROUTER’S\nFORWARDING TABLE?\nRecall that an entry in a router’s forwarding table consists of a prefix (e.g., 138.16.64/22)\nand a corresponding router output port (e.g., port 7). When a packet arrives to the router,\nthe packet’s destination IP address is compared with the prefixes in the forwarding table to\nfind the one with the longest prefix match. The packet is then forwarded (within the router)\nto the router port associated with that prefix. Let’s now summarize how a routing entry\n(prefix and associated port) gets entered into a forwarding table. This simple exercise will\ntie together a lot of what we just learned about routing and forwarding. To make things\ninteresting, let’s assume that the prefix is a “foreign prefix,” that is, it does not belong to\nthe router’s AS but to some other AS.\nIn order for a prefix to get entered into the router’s forwarding table, the router has to\nfirst become aware of the prefix (corresponding to a subnet or an aggregation of sub-\nnets). As we have just learned, the router becomes aware of the prefix via a BGP route\nPRINCIPLES IN PRACTICE\n\nRouting Policy\nLet’s illustrate some of the basic concepts of BGP routing policy with a simple exam-\nple. Figure 4.42 shows six interconnected autonomous systems: A, B, C, W, X, and Y.\nIt is important to note that A, B, C, W, X, and Y are ASs, not routers. Let’s assume that\nautonomous systems W, X, and Y are stub networks and that A, B, and C are backbone\nprovider networks. We’ll also assume that A, B, and C, all peer with each other, and\nprovide full BGP information to their customer networks. All traffic entering a stub\nnetwork must be destined for that network, and all traffic leaving a stub network must\nhave originated in that network. W and Y are clearly stub networks. X is a multi-\nhomed stub network, since it is connected to the rest of the network via two different\nproviders (a scenario that is becoming increasingly common in practice). However,\nlike W and Y, X itself must be the source/destination of all traffic leaving/entering X.\nBut how will this stub network behavior be implemented and enforced? How will X\nbe prevented from forwarding traffic between B and C? This can easily be\nadvertisement. Such an advertisement may be sent to it over an eBGP session (from a\nrouter in another AS) or over an iBGP session (from a router in the same AS).\nAfter the router becomes aware of the prefix, it needs to determine the appropriate output\nport to which datagrams destined to that prefix will be forwarded, before it can enter that\nprefix in its forwarding table. If the router receives more than one route advertisement for this\nprefix, the router uses the BGP route selection process, as described earlier in this subsection,\nto find the “best” route for the prefix. Suppose such a best route has been selected. As\ndescribed earlier, the selected route includes a NEXT-HOP attribute, which is the IP address of\nthe first router outside the router’s AS along this best route. As described above, the router\nthen uses its intra-AS routing protocol (typically OSPF) to determine the shortest path to the\nNEXT-HOP router. The router finally determines the port number to associate with the prefix\nby identifying the first link along that shortest path. The router can then (finally!) enter the \nprefix-port pair into its forwarding table! The forwarding table computed by the routing\nprocessor (see Figure 4.6) is then pushed to the router’s input port line cards.\nA\nW\nX\nY\nB\nKey:\nProvider\nnetwork\nCustomer\nnetwork\nC\nFigure 4.42 \u0002 A simple BGP scenario\n4.6\n•\nROUTING IN THE INTERNET\n397\n\n398\nCHAPTER 4\n•\nTHE NETWORK LAYER\nWHY ARE THERE DIFFERENT INTER-AS AND INTRA-AS \nROUTING PROTOCOLS?\nHaving now studied the details of specific inter-AS and intra-AS routing protocols deployed\nin today’s Internet, let’s conclude by considering perhaps the most fundamental question we\ncould ask about these protocols in the first place (hopefully, you have been wondering this\nall along, and have not lost the forest for the trees!): Why are different inter-AS and intra-\nAS routing protocols used?\nThe answer to this question gets at the heart of the differences between the goals of\nrouting within an AS and among ASs:\n•\nPolicy. Among ASs, policy issues dominate. It may well be important that traffic origi-\nnating in a given AS not be able to pass through another specific AS. Similarly, a\ngiven AS may well want to control what transit traffic it carries between other ASs. We\nhave seen that BGP carries path attributes and provides for controlled distribution of\nrouting information so that such policy-based routing decisions can be made. Within an\nAS, everything is nominally under the same administrative control, and thus policy\nissues play a much less important role in choosing routes within the AS.\n•\nScale. The ability of a routing algorithm and its data structures to scale to handle rout-\ning to/among large numbers of networks is a critical issue in inter-AS routing. Within\nan AS, scalability is less of a concern. For one thing, if a single administrative domain\nbecomes too large, it is always possible to divide it into two ASs and perform inter-AS\nrouting between the two new ASs. (Recall that OSPF allows such a hierarchy to be\nbuilt by splitting an AS into areas.)\n•\nPerformance. Because inter-AS routing is so policy oriented, the quality (for example,\nperformance) of the routes used is often of secondary concern (that is, a longer or more\ncostly route that satisfies certain policy criteria may well be taken over a route that is\nshorter but does not meet that criteria). Indeed, we saw that among ASs, there is not\neven the notion of cost (other than AS hop count) associated with routes. Within a sin-\ngle AS, however, such policy concerns are of less importance, allowing routing to focus\nmore on the level of performance realized on a route.\nPRINCIPLES IN PRACTICE\naccomplished by controlling the manner in which BGP routes are advertised. In par-\nticular, X will function as a stub network if it advertises (to its neighbors B and C) that\nit has no paths to any other destinations except itself. That is, even though X may\nknow of a path, say XCY, that reaches network Y, it will not advertise this path to B.\nSince B is unaware that X has a path to Y, B would never forward traffic destined to Y\n(or C) via X. This simple example illustrates how a selective route advertisement pol-\nicy can be used to implement customer/provider routing relationships."
    },
    {
      "chunk_id": "2ecacd69-0c0e-4c61-adb7-eeafa1a3153e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.7 Broadcast and Multicast Routing",
      "original_titles": [
        "4.7 Broadcast and Multicast Routing"
      ],
      "path": "Chapter 4 The Network Layer > 4.7 Broadcast and Multicast Routing",
      "start_page": 426,
      "end_page": 426,
      "token_count": 677,
      "text": "Let’s next focus on a provider network, say AS B. Suppose that B has learned\n(from A) that A has a path AW to W. B can thus install the route BAW into its rout-\ning information base. Clearly, B also wants to advertise the path BAW to its cus-\ntomer, X, so that X knows that it can route to W via B. But should B advertise the\npath BAW to C? If it does so, then C could route traffic to W via CBAW. If A, B,\nand C are all backbone providers, than B might rightly feel that it should not have\nto shoulder the burden (and cost!) of carrying transit traffic between A and C. B\nmight rightly feel that it is A’s and C’s job (and cost!) to make sure that C can route\nto/from A’s customers via a direct connection between A and C. There are currently\nno official standards that govern how backbone ISPs route among themselves. How-\never, a rule of thumb followed by commercial ISPs is that any traffic flowing across\nan ISP’s backbone network must have either a source or a destination (or both) in a\nnetwork that is a customer of that ISP; otherwise the traffic would be getting a free\nride on the ISP’s network. Individual peering agreements (that would govern ques-\ntions such as those raised above) are typically negotiated between pairs of ISPs and\nare often confidential; [Huston 1999a] provides an interesting discussion of peering\nagreements. For a detailed description of how routing policy reflects commercial\nrelationships among ISPs, see [Gao 2001; Dmitiropoulos 2007]. For a discussion of\nBGP routing polices from an ISP standpoint, see [Caesar 2005b].\nAs noted above, BGP is the de facto standard for inter-AS routing for the pub-\nlic Internet. To see the contents of various BGP routing tables (large!) extracted\nfrom routers in tier-1 ISPs, see http://www.routeviews.org. BGP routing tables\noften contain tens of thousands of prefixes and corresponding attributes. Statistics\nabout the size and characteristics of BGP routing tables are presented in [Potaroo\n2012].\nThis completes our brief introduction to BGP. Understanding BGP is important\nbecause it plays a central role in the Internet. We encourage you to see the references\n[Griffin 2012; Stewart 1999; Labovitz 1997; Halabi 2000; Huitema 1998; Gao\n2001; Feamster 2004; Caesar 2005b; Li 2007] to learn more about BGP.\n4.7 Broadcast and Multicast Routing\nThus far in this chapter, our focus has been on routing protocols that support unicast\n(i.e., point-to-point) communication, in which a single source node sends a packet\nto a single destination node. In this section, we turn our attention to broadcast and\nmulticast routing protocols. In broadcast routing, the network layer provides a\nservice of delivering a packet sent from a source node to all other nodes in the\nnetwork; multicast routing enables a single source node to send a copy of a packet\nto a subset of the other network nodes. In Section 4.7.1 we’ll consider broadcast\nrouting algorithms and their embodiment in routing protocols. We’ll examine multi-\ncast routing in Section 4.7.2.\n4.7\n•\nBROADCAST AND MULTICAST ROUTING\n399"
    },
    {
      "chunk_id": "3242f34f-aa36-4cbc-bf4c-bcaccd924eea",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.7.1 Broadcast Routing Algorithms",
      "original_titles": [
        "4.7.1 Broadcast Routing Algorithms"
      ],
      "path": "Chapter 4 The Network Layer > 4.7 Broadcast and Multicast Routing > 4.7.1 Broadcast Routing Algorithms",
      "start_page": 427,
      "end_page": 431,
      "token_count": 2767,
      "text": "4.7.1 Broadcast Routing Algorithms\nPerhaps the most straightforward way to accomplish broadcast communication is\nfor the sending node to send a separate copy of the packet to each destination, as\nshown in Figure 4.43(a). Given N destination nodes, the source node simply makes\nN copies of the packet, addresses each copy to a different destination, and then\ntransmits the N copies to the N destinations using unicast routing. This N-way-\nunicast approach to broadcasting is simple—no new network-layer routing proto-\ncol, packet-duplication, or forwarding functionality is needed. There are, however,\nseveral drawbacks to this approach. The first drawback is its inefficiency. If the\nsource node is connected to the rest of the network via a single link, then N separate\ncopies of the (same) packet will traverse this single link. It would clearly be more\nefficient to send only a single copy of a packet over this first hop and then have the\nnode at the other end of the first hop make and forward any additional needed\ncopies. That is, it would be more efficient for the network nodes themselves (rather\nthan just the source node) to create duplicate copies of a packet. For example, in\nFigure 4.43(b), only a single copy of a packet traverses the R1-R2 link. That packet\nis then duplicated at R2, with a single copy being sent over links R2-R3 and R2-R4.\nThe additional drawbacks of N-way-unicast are perhaps more subtle, but no less\nimportant. An implicit assumption of N-way-unicast is that broadcast recipients, and\ntheir addresses, are known to the sender. But how is this information obtained? Most\nlikely, additional protocol mechanisms (such as a broadcast membership or\ndestination-registration protocol) would be required. This would add more overhead\nand, importantly, additional complexity to a protocol that had initially seemed quite\nsimple. A final drawback of N-way-unicast relates to the purposes for which broad-\ncast is to be used. In Section 4.5, we learned that link-state routing protocols use\nbroadcast to disseminate the link-state information that is used to compute unicast\nroutes. Clearly, in situations where broadcast is used to create and update unicast\nroutes, it would be unwise (at best!) to rely on the unicast routing infrastructure to\nachieve broadcast.\n400\nCHAPTER 4\n•\nTHE NETWORK LAYER\nR2\nR4\nR3\na.\nb.\nDuplicate\nDuplicate creation/transmission\nR1\nR4\nR3\nDuplicate\nR1\nR2\nFigure 4.43 \u0002 Source-duplication versus in-network duplication\n\nGiven the several drawbacks of N-way-unicast broadcast, approaches in which\nthe network nodes themselves play an active role in packet duplication, packet for-\nwarding, and computation of the broadcast routes are clearly of interest. We’ll\nexamine several such approaches below and again adopt the graph notation intro-\nduced in Section 4.5. We again model the network as a graph, G = (N,E), where N\nis a set of nodes and a collection E of edges, where each edge is a pair of nodes from\nN. We’ll be a bit sloppy with our notation and use N to refer to both the set of nodes,\nas well as the cardinality (|N|) or size of that set when there is no confusion.\nUncontrolled Flooding\nThe most obvious technique for achieving broadcast is a flooding approach in\nwhich the source node sends a copy of the packet to all of its neighbors. When a\nnode receives a broadcast packet, it duplicates the packet and forwards it to all of its\nneighbors (except the neighbor from which it received the packet). Clearly, if the\ngraph is connected, this scheme will eventually deliver a copy of the broadcast\npacket to all nodes in the graph. Although this scheme is simple and elegant, it has a\nfatal flaw (before you read on, see if you can figure out this fatal flaw): If the graph\nhas cycles, then one or more copies of each broadcast packet will cycle indefinitely.\nFor example, in Figure 4.43, R2 will flood to R3, R3 will flood to R4, R4 will flood\nto R2, and R2 will flood (again!) to R3, and so on. This simple scenario results in\nthe endless cycling of two broadcast packets, one clockwise, and one counterclock-\nwise. But there can be an even more calamitous fatal flaw: When a node is con-\nnected to more than two other nodes, it will create and forward multiple copies of\nthe broadcast packet, each of which will create multiple copies of itself (at other\nnodes with more than two neighbors), and so on. This broadcast storm, resulting\nfrom the endless multiplication of broadcast packets, would eventually result in so\nmany broadcast packets being created that the network would be rendered useless.\n(See the homework questions at the end of the chapter for a problem analyzing the\nrate at which such a broadcast storm grows.)\nControlled Flooding\nThe key to avoiding a broadcast storm is for a node to judiciously choose when\nto flood a packet and (e.g., if it has already received and flooded an earlier copy of\na packet) when not to flood a packet. In practice, this can be done in one of several\nways.\nIn sequence-number-controlled flooding, a source node puts its address (or\nother unique identifier) as well as a broadcast sequence number into a broadcast\npacket, then sends the packet to all of its neighbors. Each node maintains a list of\nthe source address and sequence number of each broadcast packet it has already\nreceived, duplicated, and forwarded. When a node receives a broadcast packet, it\nfirst checks whether the packet is in this list. If so, the packet is dropped; if not, the\n4.7\n•\nBROADCAST AND MULTICAST ROUTING\n401\n\npacket is duplicated and forwarded to all the node’s neighbors (except the node from\nwhich the packet has just been received). The Gnutella protocol, discussed in Chap-\nter 2, uses sequence-number-controlled flooding to broadcast queries in its overlay\nnetwork. (In Gnutella, message duplication and forwarding is performed at the\napplication layer rather than at the network layer.)\nA second approach to controlled flooding is known as reverse path forwarding\n(RPF) [Dalal 1978], also sometimes referred to as reverse path broadcast (RPB). The\nidea behind RPF is simple, yet elegant. When a router receives a broadcast packet\nwith a given source address, it transmits the packet on all of its outgoing links (except\nthe one on which it was received) only if the packet arrived on the link that is on its\nown shortest unicast path back to the source. Otherwise, the router simply discards\nthe incoming packet without forwarding it on any of its outgoing links. Such a packet\ncan be dropped because the router knows it either will receive or has already received\na copy of this packet on the link that is on its own shortest path back to the sender.\n(You might want to convince yourself that this will, in fact, happen and that looping\nand broadcast storms will not occur.) Note that RPF does not use unicast routing to\nactually deliver a packet to a destination, nor does it require that a router know the\ncomplete shortest path from itself to the source. RPF need only know the next neigh-\nbor on its unicast shortest path to the sender; it uses this neighbor’s identity only to\ndetermine whether or not to flood a received broadcast packet.\nFigure 4.44 illustrates RPF. Suppose that the links drawn with thick lines repre-\nsent the least-cost paths from the receivers to the source (A). Node A initially broad-\ncasts a source-A packet to nodes C and B. Node B will forward the source-A packet\nit has received from A (since A is on its least-cost path to A) to both C and D. B will\nignore (drop, without forwarding) any source-A packets it receives from any other\n402\nCHAPTER 4\n•\nTHE NETWORK LAYER\nA\nB\nD\nG\nC\nF\nE\nKey:\npkt will be forwarded\npkt not forwarded beyond receiving router\nFigure 4.44 \u0002 Reverse path forwarding\n\n4.7\n•\nBROADCAST AND MULTICAST ROUTING\n403\na. Broadcast initiated at A\nb. Broadcast initiated at D\nA\nB\nC\nD\nG\nE\nF\nA\nB\nC\nD\nG\nE\nF\nFigure 4.45 \u0002 Broadcast along a spanning tree\nnodes (for example, from routers C or D). Let us now consider node C, which will\nreceive a source-A packet directly from A as well as from B. Since B is not on C’s\nown shortest path back to A, C will ignore any source-A packets it receives from B.\nOn the other hand, when C receives a source-A packet directly from A, it will for-\nward the packet to nodes B, E, and F.\nSpanning-Tree Broadcast\nWhile sequence-number-controlled flooding and RPF avoid broadcast storms, they\ndo not completely avoid the transmission of redundant broadcast packets. For exam-\nple, in Figure 4.44, nodes B, C, D, E, and F receive either one or two redundant\npackets. Ideally, every node should receive only one copy of the broadcast packet.\nExamining the tree consisting of the nodes connected by thick lines in Figure\n4.45(a), you can see that if broadcast packets were forwarded only along links\nwithin this tree, each and every network node would receive exactly one copy of the\nbroadcast packet—exactly the solution we were looking for! This tree is an example\nof a spanning tree—a tree that contains each and every node in a graph. More for-\nmally, a spanning tree of a graph G = (N,E) is a graph G\u0004 = (N,E\u0004) such that E\u0004 is a\nsubset of E, G\u0004 is connected, G\u0004 contains no cycles, and G\u0004 contains all the original\nnodes in G. If each link has an associated cost and the cost of a tree is the sum of the\nlink costs, then a spanning tree whose cost is the minimum of all of the graph’s\nspanning trees is called (not surprisingly) a minimum spanning tree.\nThus, another approach to providing broadcast is for the network nodes to first\nconstruct a spanning tree. When a source node wants to send a broadcast packet, it\nsends the packet out on all of the incident links that belong to the spanning tree. A\nnode receiving a broadcast packet then forwards the packet to all its neighbors in the\n\nspanning tree (except the neighbor from which it received the packet). Not only\ndoes spanning tree eliminate redundant broadcast packets, but once in place, the\nspanning tree can be used by any node to begin a broadcast, as shown in Figures\n4.45(a) and 4.45(b). Note that a node need not be aware of the entire tree; it simply\nneeds to know which of its neighbors in G are spanning-tree neighbors.\nThe main complexity associated with the spanning-tree approach is the creation\nand maintenance of the spanning tree. Numerous distributed spanning-tree algo-\nrithms have been developed [Gallager 1983, Gartner 2003]. We consider only one\nsimple algorithm here. In the center-based approach to building a spanning tree, a\ncenter node (also known as a rendezvous point or a core) is defined. Nodes then\nunicast tree-join messages addressed to the center node. A tree-join message is for-\nwarded using unicast routing toward the center until it either arrives at a node that\nalready belongs to the spanning tree or arrives at the center. In either case, the path\nthat the tree-join message has followed defines the branch of the spanning tree\nbetween the edge node that initiated the tree-join message and the center. One can\nthink of this new path as being grafted onto the existing spanning tree.\nFigure 4.46 illustrates the construction of a center-based spanning tree. Suppose\nthat node E is selected as the center of the tree. Suppose that node F first joins the tree\nand forwards a tree-join message to E. The single link EF becomes the initial span-\nning tree. Node B then joins the spanning tree by sending its tree-join message to E.\nSuppose that the unicast path route to E from B is via D. In this case, the tree-join\nmessage results in the path BDE being grafted onto the spanning tree. Node A next\njoins the spanning group by forwarding its tree-join message towards E. If A’s uni-\ncast path to E is through B, then since B has already joined the spanning tree, the\narrival of A’s tree-join message at B will result in the AB link being immediately\ngrafted onto the spanning tree. Node C joins the spanning tree next by forwarding\nits tree-join message directly to E. Finally, because the unicast routing from G to E\n404\nCHAPTER 4\n•\nTHE NETWORK LAYER\n3\n2\n4\n1\n5\na. Stepwise construction of spanning tree\nb. Constructed spanning tree\nA\nB\nC\nD\nG\nE\nF\nA\nB\nC\nD\nG\nE\nF\nFigure 4.46 \u0002 Center-based construction of a spanning tree"
    },
    {
      "chunk_id": "6e0acf7d-f918-4cc2-943f-684529629681",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.7.2 Multicast",
      "original_titles": [
        "4.7.2 Multicast"
      ],
      "path": "Chapter 4 The Network Layer > 4.7 Broadcast and Multicast Routing > 4.7.2 Multicast",
      "start_page": 432,
      "end_page": 438,
      "token_count": 3564,
      "text": "must be via node D, when G sends its tree-join message to E, the GD link is grafted\nonto the spanning tree at node D.\nBroadcast Algorithms in Practice\nBroadcast protocols are used in practice at both the application and network layers.\nGnutella [Gnutella 2009] uses application-level broadcast in order to broadcast\nqueries for content among Gnutella peers. Here, a link between two distributed\napplication-level peer processes in the Gnutella network is actually a TCP connec-\ntion. Gnutella uses a form of sequence-number-controlled flooding in which a 16-\nbit identifier and a 16-bit payload descriptor (which identifies the Gnutella message\ntype) are used to detect whether a received broadcast query has been previously\nreceived, duplicated, and forwarded. Gnutella also uses a time-to-live (TTL) field to\nlimit the number of hops over which a flooded query will be forwarded. When a\nGnutella process receives and duplicates a query, it decrements the TTL field before\nforwarding the query. Thus, a flooded Gnutella query will only reach peers that are\nwithin a given number (the initial value of TTL) of application-level hops from the\nquery initiator. Gnutella’s flooding mechanism is thus sometimes referred to as\nlimited-scope flooding.\nA form of sequence-number-controlled flooding is also used to broadcast link-state\nadvertisements (LSAs) in the OSPF [RFC 2328, Perlman 1999] routing algorithm, and in\nthe Intermediate-System-to-Intermediate-System (IS-IS) routing algorithm [RFC\n1142, Perlman 1999]. OSPF uses a 32-bit sequence number, as well as a 16-bit age field\nto identify LSAs. Recall that an OSPF node broadcasts LSAs for its attached links peri-\nodically, when a link cost to a neighbor changes, or when a link goes up/down. LSA\nsequence numbers are used to detect duplicate LSAs, but also serve a second important\nfunction in OSPF. With flooding, it is possible for an LSA generated by the source at\ntime t to arrive after a newer LSA that was generated by the same source at time t + d.\nThe sequence numbers used by the source node allow an older LSA to be distinguished\nfrom a newer LSA. The age field serves a purpose similar to that of a TTL value. The ini-\ntial age field value is set to zero and is incremented at each hop as it is flooded, and is also\nincremented as it sits in a router’s memory waiting to be flooded. Although we have only\nbriefly described the LSAflooding algorithm here, we note that designing LSAbroadcast\nprotocols can be very tricky business indeed. [RFC 789; Perlman 1999] describe an inci-\ndent in which incorrectly transmitted LSAs by two malfunctioning routers caused an\nearly version of an LSAflooding algorithm to take down the entire ARPAnet!\n4.7.2 Multicast\nWe’ve seen in the previous section that with broadcast service, packets are delivered\nto each and every node in the network. In this section we turn our attention to\nmulticast service, in which a multicast packet is delivered to only a subset of\nnetwork nodes. A number of emerging network applications require the delivery of\npackets from one or more senders to a group of receivers. These applications include\n4.7\n•\nBROADCAST AND MULTICAST ROUTING\n405\n\nbulk data transfer (for example, the transfer of a software upgrade from the software\ndeveloper to users needing the upgrade), streaming continuous media (for example,\nthe transfer of the audio, video, and text of a live lecture to a set of distributed lec-\nture participants), shared data applications (for example, a whiteboard or teleconfer-\nencing application that is shared among many distributed participants), data feeds\n(for example, stock quotes), Web cache updating, and interactive gaming (for exam-\nple, distributed interactive virtual environments or multiplayer games).\nIn multicast communication, we are immediately faced with two problems—\nhow to identify the receivers of a multicast packet and how to address a packet sent\nto these receivers. In the case of unicast communication, the IP address of the\nreceiver (destination) is carried in each IP unicast datagram and identifies the single\nrecipient; in the case of broadcast, all nodes need to receive the broadcast packet, so\nno destination addresses are needed. But in the case of multicast, we now have mul-\ntiple receivers. Does it make sense for each multicast packet to carry the IP\naddresses of all of the multiple recipients? While this approach might be workable\nwith a small number of recipients, it would not scale well to the case of hundreds or\nthousands of receivers; the amount of addressing information in the datagram would\nswamp the amount of data actually carried in the packet’s payload field. Explicit\nidentification of the receivers by the sender also requires that the sender know the\nidentities and addresses of all of the receivers. We will see shortly that there are\ncases where this requirement might be undesirable.\nFor these reasons, in the Internet architecture (and other network architectures\nsuch as ATM [Black 1995]), a multicast packet is addressed using address indirec-\ntion. That is, a single identifier is used for the group of receivers, and a copy of the\npacket that is addressed to the group using this single identifier is delivered to all of\nthe multicast receivers associated with that group. In the Internet, the single identifier\nthat represents a group of receivers is a class D multicast IP address. The group of\nreceivers associated with a class D address is referred to as a multicast group. The\nmulticast group abstraction is illustrated in Figure 4.47. Here, four hosts (shown in\nshaded color) are associated with the multicast group address of 226.17.30.197 and\nwill receive all datagrams addressed to that multicast address. The difficulty that we\nmust still address is the fact that each host has a unique IP unicast address that is com-\npletely independent of the address of the multicast group in which it is participating.\nWhile the multicast group abstraction is simple, it raises a host (pun intended)\nof questions. How does a group get started and how does it terminate? How is the\ngroup address chosen? How are new hosts added to the group (either as senders or\nreceivers)? Can anyone join a group (and send to, or receive from, that group) or is\ngroup membership restricted and, if so, by whom? Do group members know the\nidentities of the other group members as part of the network-layer protocol? How\ndo the network nodes interoperate with each other to deliver a multicast datagram to \nall group members? For the Internet, the answers to all of these questions involve\nthe Internet Group Management Protocol [RFC 3376]. So, let us next briefly con-\nsider IGMP and then return to these broader questions.\n406\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nInternet Group Management Protocol\nThe IGMP protocol version 3 [RFC 3376] operates between a host and its directly\nattached router (informally, we can think of the directly attached router as the first-\nhop router that a host would see on a path to any other host outside its own local\nnetwork, or the last-hop router on any path to that host), as shown in Figure 4.48.\nFigure 4.48 shows three first-hop multicast routers, each connected to its attached\nhosts via one outgoing local interface. This local interface is attached to a LAN in\nthis example, and while each LAN has multiple attached hosts, at most a few of\nthese hosts will typically belong to a given multicast group at any given time.\nIGMP provides the means for a host to inform its attached router that an application\nrunning on the host wants to join a specific multicast group. Given that the scope of\nIGMP interaction is limited to a host and its attached router, another protocol is clearly\nrequired to coordinate the multicast routers (including the attached routers) throughout\n4.7\n•\nBROADCAST AND MULTICAST ROUTING\n407\n128.119.40.186\nKey:\nRouter with attached\ngroup member\nRouter with no attached\ngroup member\n128.34.108.63\n128.34.108.60\n128.59.16.20\nmcast group\n226.17.30.197\nFigure 4.47 \u0002 The multicast group: A datagram addressed to the group is\ndelivered to all members of the multicast group\n\nthe Internet, so that multicast datagrams are routed to their final destinations. This latter\nfunctionality is accomplished by network-layer multicast routing algorithms, such as\nthose we will consider shortly. Network-layer multicast in the Internet thus consists of\ntwo complementary components: IGMP and multicast routing protocols.\nIGMP has only three message types. Like ICMP, IGMP messages are carried\n(encapsulated) within an IP datagram, with an IP protocol number of 2. The mem-\nbership_query message is sent by a router to all hosts on an attached interface\n(for example, to all hosts on a local area network) to determine the set of all multicast\ngroups that have been joined by the hosts on that interface. Hosts respond to a mem-\nbership_query message with an IGMP membership_report message.\nmembership_report messages can also be generated by a host when an\napplication first joins a multicast group without waiting for a membership_query\nmessage from the router. The final type of IGMP message is the leave_group\nmessage. Interestingly, this message is optional. But if it is optional, how does a\nrouter detect when a host leaves the multicast group? The answer to this question is\nthat the router infers that a host is no longer in the multicast group if it no longer\nresponds to a membership_query message with the given group address. This is\nan example of what is sometimes called soft state in an Internet protocol. In a soft-\nstate protocol, the state (in this case of IGMP, the fact that there are hosts joined to a\ngiven multicast group) is removed via a timeout event (in this case, via a periodic\nmembership_query message from the router) if it is not explicitly refreshed (in\nthis case, by a membership_report message from an attached host).\nThe term soft state was coined by Clark [Clark 1988], who described the notion\nof periodic state refresh messages being sent by an end system, and suggested that\n408\nCHAPTER 4\n•\nTHE NETWORK LAYER\nWide-area\nmulticast\nrouting\nIGMP\nIGMP\nIGMP\nIGMP\nFigure 4.48 \u0002 The two components of network-layer multicast in the\nInternet: IGMP and multicast routing protocols\n\nwith such refresh messages, state could be lost in a crash and then automatically\nrestored by subsequent refresh messages—all transparently to the end system and\nwithout invoking any explicit crash-recovery procedures:\n“. . . the state information would not be critical in maintaining the desired\ntype of service associated with the flow. Instead, that type of service would\nbe enforced by the end points, which would periodically send messages to\nensure that the proper type of service was being associated with the flow. In\nthis way, the state information associated with the flow could be lost in a\ncrash without permanent disruption of the service features being used. I call\nthis concept “soft state,” and it may very well permit us to achieve our pri-\nmary goals of survivability and flexibility. . .”\nIt has been argued that soft-state protocols result in simpler control than hard-\nstate protocols, which not only require state to be explicitly added and removed, but\nalso require mechanisms to recover from the situation where the entity responsible\nfor removing state has terminated prematurely or failed. Interesting discussions of\nsoft state can be found in [Raman 1999; Ji 2003; Lui 2004].\nMulticast Routing Algorithms\nThe multicast routing problem is illustrated in Figure 4.49. Hosts joined to the mul-\nticast group are shaded in color; their immediately attached router is also shaded in\ncolor. As shown in Figure 4.49, only a subset of routers (those with attached hosts that\nare joined to the multicast group) actually needs to receive the multicast traffic. In Fig-\nure 4.49, only routers A, B, E, and F need to receive the multicast traffic. Since none\nof the hosts attached to router D are joined to the multicast group and since router C\nhas no attached hosts, neither C nor D needs to receive the multicast group traffic. The\ngoal of multicast routing, then, is to find a tree of links that connects all of the routers\nthat have attached hosts belonging to the multicast group. Multicast packets will then\nbe routed along this tree from the sender to all of the hosts belonging to the multicast\ntree. Of course, the tree may contain routers that do not have attached hosts belonging\nto the multicast group (for example, in Figure 4.49, it is impossible to connect routers\nA, B, E, and F in a tree without involving either router C or D).\nIn practice, two approaches have been adopted for determining the multicast\nrouting tree, both of which we have already studied in the context of broadcast\nrouting, and so we will only mention them in passing here. The two approaches dif-\nfer according to whether a single group-shared tree is used to distribute the traffic\nfor all senders in the group, or whether a source-specific routing tree is constructed\nfor each individual sender.\n•\nMulticast routing using a group-shared tree. As in the case of spanning-tree broad-\ncast, multicast routing over a group-shared tree is based on building a tree that\nincludes all edge routers with attached hosts belonging to the multicast group.\nIn practice, a center-based approach is used to construct the multicast routing tree,\nwith edge routers with attached hosts belonging to the multicast group sending\n4.7\n•\nBROADCAST AND MULTICAST ROUTING\n409\n\n(via unicast) join messages addressed to the center node. As in the broadcast case, a\njoin message is forwarded using unicast routing toward the center until it either\narrives at a router that already belongs to the multicast tree or arrives at the center.\nAll routers along the path that the join message follows will then forward received\nmulticast packets to the edge router that initiated the multicast join. A critical ques-\ntion for center-based tree multicast routing is the process used to select the center.\nCenter-selection algorithms are discussed in [Wall 1980; Thaler 1997; Estrin 1997].\n•\nMulticast routing using a source-based tree. While group-shared tree multicast\nrouting constructs a single, shared routing tree to route packets from all senders,\nthe second approach constructs a multicast routing tree for each source in the\nmulticast group. In practice, an RPF algorithm (with source node x) is used to\nconstruct a multicast forwarding tree for multicast datagrams originating at\nsource x. The RPF broadcast algorithm we studied earlier requires a bit of tweak-\ning for use in multicast. To see why, consider router D in Figure 4.50. Under\nbroadcast RPF, it would forward packets to router G, even though router G has\nno attached hosts that are joined to the multicast group. While this is not so bad\nfor this case where D has only a single downstream router, G, imagine what\nwould happen if there were thousands of routers downstream from D! Each\nof these thousands of routers would receive unwanted multicast packets.\n410\nCHAPTER 4\n•\nTHE NETWORK LAYER\nA\nC\nF\nB\nD\nE\nFigure 4.49 \u0002 Multicast hosts, their attached routers, and other routers\n\n(This scenario is not as far-fetched as it might seem. The initial MBone [Casner\n1992; Macedonia 1994], the first global multicast network, suffered from pre-\ncisely this problem at first.). The solution to the problem of receiving unwanted\nmulticast packets under RPF is known as pruning. A multicast router that\nreceives multicast packets and has no attached hosts joined to that group will send\na prune message to its upstream router. If a router receives prune messages from\neach of its downstream routers, then it can forward a prune message upstream.\nMulticast Routing in the Internet\nThe first multicast routing protocol used in the Internet was the Distance-Vector Mul-\nticast Routing Protocol (DVMRP) [RFC 1075]. DVMRP implements source-based\ntrees with reverse path forwarding and pruning. DVMRP uses an RPF algorithm with\npruning, as discussed above. Perhaps the most widely used Internet multicast routing\nprotocol is the Protocol-Independent Multicast (PIM) routing protocol, which\nexplicitly recognizes two multicast distribution scenarios. In dense mode [RFC 3973],\nmulticast group members are densely located; that is, many or most of the routers in\nthe area need to be involved in routing multicast datagrams. PIM dense mode is a\nflood-and-prune reverse path forwarding technique similar in spirit to DVMRP.\n4.7\n•\nBROADCAST AND MULTICAST ROUTING\n411\nA\nC\nF\nKey:\npkt will be forwarded\nE\nG\nB\nS: source\nD\npkt not forwarded beyond receiving router\nFigure 4.50 \u0002 Reverse path forwarding, the multicast case"
    },
    {
      "chunk_id": "c0522c03-e9a2-4600-a51d-c11c415d1909",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "4.8 Summary",
      "original_titles": [
        "4.8 Summary"
      ],
      "path": "Chapter 4 The Network Layer > 4.8 Summary",
      "start_page": 439,
      "end_page": 439,
      "token_count": 661,
      "text": "In sparse mode [RFC 4601], the number of routers with attached group mem-\nbers is small with respect to the total number of routers; group members are widely\ndispersed. PIM sparse mode uses rendezvous points to set up the multicast distri-\nbution tree. In source-specific multicast (SSM) [RFC 3569, RFC 4607], only a\nsingle sender is allowed to send traffic into the multicast tree, considerably simpli-\nfying tree construction and maintenance.\nWhen PIM and DVMP are used within a domain, the network operator can con-\nfigure IP multicast routers within the domain, in much the same way that intra-\ndomain unicast routing protocols such as RIP, IS-IS, and OSPF can be configured.\nBut what happens when multicast routes are needed between different domains? Is\nthere a multicast equivalent of the inter-domain BGP protocol? The answer is (liter-\nally) yes. [RFC 4271] defines multiprotocol extensions to BGP to allow it to carry\nrouting information for other protocols, including multicast information. The Multi-\ncast Source Discovery Protocol (MSDP) [RFC 3618, RFC 4611] can be used to con-\nnect together rendezvous points in different PIM sparse mode domains. An excellent\noverview of the current state of multicast routing in the Internet is [RFC 5110].\nLet us close our discussion of IP multicast by noting that IP multicast has yet to\ntake off in a big way. For interesting discussions of the Internet multicast service\nmodel and deployment issues, see [Diot 2000, Sharma 2003]. Nonetheless, in spite\nof the lack of widespread deployment, network-level multicast is far from “dead.”\nMulticast traffic has been carried for many years on Internet 2, and the networks\nwith which it peers [Internet2 Multicast 2012]. In the United Kingdom, the BBC is\nengaged in trials of content distribution via IP multicast [BBC Multicast 2012]. At\nthe same time, application-level multicast, as we saw with PPLive in Chapter 2 and\nin other peer-to-peer systems such as End System Multicast [Chu 2002], provides\nmulticast distribution of content among peers using application-layer (rather than\nnetwork-layer) multicast protocols. Will future multicast services be primarily\nimplemented in the network layer (in the network core) or in the application layer (at\nthe network’s edge)? While the current craze for content distribution via peer-to-peer\napproaches tips the balance in favor of application-layer multicast at least in the near-\nterm future, progress continues to be made in IP multicast, and sometimes the race\nultimately goes to the slow and steady.\n4.8 Summary\nIn this chapter, we began our journey into the network core. We learned that the\nnetwork layer involves each and every host and router in the network. Because of\nthis, network-layer protocols are among the most challenging in the protocol stack.\nWe learned that a router may need to process millions of flows of packets\nbetween different source-destination pairs at the same time. To permit a router to\nprocess such a large number of flows, network designers have learned over the years\nthat the router’s tasks should be as simple as possible. Many measures can be taken\n412\nCHAPTER 4\n•\nTHE NETWORK LAYER"
    },
    {
      "chunk_id": "2523ac53-41e9-4815-a3d9-79a501839dec",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Homework Problems and Questions",
      "original_titles": [
        "Homework Problems and Questions"
      ],
      "path": "Chapter 4 The Network Layer > Homework Problems and Questions",
      "start_page": 440,
      "end_page": 455,
      "token_count": 7710,
      "text": "to make the router’s job easier, including using a datagram network layer rather than\na virtual-circuit network layer, using a streamlined and fixed-sized header (as in\nIPv6), eliminating fragmentation (also done in IPv6), and providing the one and\nonly best-effort service. Perhaps the most important trick here is not to keep track of\nindividual flows, but instead base routing decisions solely on hierarchically struc-\ntured destination addresses in the datagrams. It is interesting to note that the postal\nservice has been using this approach for many years.\nIn this chapter, we also looked at the underlying principles of routing algorithms.\nWe learned how routing algorithms abstract the computer network to a graph with\nnodes and links. With this abstraction, we can exploit the rich theory of shortest-path\nrouting in graphs, which has been developed over the past 40 years in the operations\nresearch and algorithms communities. We saw that there are two broad approaches: a\ncentralized (global) approach, in which each node obtains a complete map of the net-\nwork and independently applies a shortest-path routing algorithm; and a decentral-\nized approach, in which individual nodes have only a partial picture of the entire\nnetwork, yet the nodes work together to deliver packets along the shortest routes. We\nalso studied how hierarchy is used to deal with the problem of scale by partitioning\nlarge networks into independent administrative domains called autonomous systems\n(ASs). Each AS independently routes its datagrams through the AS, just as each\ncountry independently routes its postal mail through the country. We learned how\ncentralized, decentralized, and hierarchical approaches are embodied in the principal\nrouting protocols in the Internet: RIP, OSPF, and BGP. We concluded our study of\nrouting algorithms by considering broadcast and multicast routing.\nHaving completed our study of the network layer, our journey now takes us one\nstep further down the protocol stack, namely, to the link layer. Like the network layer,\nthe link layer is also part of the network core. But we will see in the next chapter that\nthe link layer has the much more localized task of moving packets between nodes on\nthe same link or LAN. Although this task may appear on the surface to be trivial com-\npared with that of the network layer’s tasks, we will see that the link layer involves a\nnumber of important and fascinating issues that can keep us busy for a long time.\nHomework Problems and Questions\nChapter 4 Review Questions\nSECTIONS 4.1–4.2\nR1. Let’s review some of the terminology used in this textbook. Recall that the\nname of a transport-layer packet is segment and that the name of a link-layer\npacket is frame. What is the name of a network-layer packet? Recall that both\nrouters and link-layer switches are called packet switches. What is the\nfundamental difference between a router and link-layer switch? Recall that\nwe use the term routers for both datagram networks and VC networks.\nHOMEWORK PROBLEMS AND QUESTIONS\n413\n\nR2. What are the two most important network-layer functions in a datagram net-\nwork? What are the three most important network-layer functions in a virtual-\ncircuit network?\nR3. What is the difference between routing and forwarding?\nR4. Do the routers in both datagram networks and virtual-circuit networks use for-\nwarding tables? If so, describe the forwarding tables for both classes of networks.\nR5. Describe some hypothetical services that the network layer can provide to a\nsingle packet. Do the same for a flow of packets. Are any of your hypotheti-\ncal services provided by the Internet’s network layer? Are any provided by\nATM’s CBR service model? Are any provided by ATM’s ABR service\nmodel?\nR6. List some applications that would benefit from ATM’s CBR service model.\nSECTION 4.3\nR7. Discuss why each input port in a high-speed router stores a shadow copy of\nthe forwarding table.\nR8. Three types of switching fabrics are discussed in Section 4.3. List and briefly\ndescribe each type. Which, if any, can send multiple packets across the fabric\nin parallel?\nR9. Describe how packet loss can occur at input ports. Describe how packet loss\nat input ports can be eliminated (without using infinite buffers).\nR10. Describe how packet loss can occur at output ports. Can this loss be \nprevented by increasing the switch fabric speed?\nR11. What is HOL blocking? Does it occur in input ports or output ports?\nSECTION 4.4\nR12. Do routers have IP addresses? If so, how many?\nR13. What is the 32-bit binary equivalent of the IP address 223.1.3.27?\nR14. Visit a host that uses DHCP to obtain its IP address, network mask, default\nrouter, and IP address of its local DNS server. List these values.\nR15. Suppose there are three routers between a source host and a destination host.\nIgnoring fragmentation, an IP datagram sent from the source host to the desti-\nnation host will travel over how many interfaces? How many forwarding tables\nwill be indexed to move the datagram from the source to the destination?\nR16. Suppose an application generates chunks of 40 bytes of data every 20 msec,\nand each chunk gets encapsulated in a TCP segment and then an IP datagram.\nWhat percentage of each datagram will be overhead, and what percentage\nwill be application data?\nR17. Suppose Host A sends Host B a TCP segment encapsulated in an IP datagram.\nWhen Host B receives the datagram, how does the network layer in Host B\n414\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nknow it should pass the segment (that is, the payload of the datagram) to TCP\nrather than to UDP or to something else?\nR18. Suppose you purchase a wireless router and connect it to your cable modem.\nAlso suppose that your ISP dynamically assigns your connected device (that\nis, your wireless router) one IP address. Also suppose that you have five PCs\nat home that use 802.11 to wirelessly connect to your wireless router. How\nare IP addresses assigned to the five PCs? Does the wireless router use NAT?\nWhy or why not?\nR19. Compare and contrast the IPv4 and the IPv6 header fields. Do they have any\nfields in common?\nR20. It has been said that when IPv6 tunnels through IPv4 routers, IPv6 treats the\nIPv4 tunnels as link-layer protocols. Do you agree with this statement? Why\nor why not?\nSECTION 4.5\nR21. Compare and contrast link-state and distance-vector routing algorithms.\nR22. Discuss how a hierarchical organization of the Internet has made it possible\nto scale to millions of users.\nR23. Is it necessary that every autonomous system use the same intra-AS routing\nalgorithm? Why or why not?\nSECTION 4.6\nR24. Consider Figure 4.37. Starting with the original table in D, suppose that D\nreceives from A the following advertisement:\nHOMEWORK PROBLEMS AND QUESTIONS\n415\nWill the table in D change? If so how?\nR25. Compare and contrast the advertisements used by RIP and OSPF.\nR26. Fill in the blank: RIP advertisements typically announce the number of hops\nto various destinations. BGP updates, on the other hand, announce the\n__________ to the various destinations.\nR27. Why are different inter-AS and intra-AS protocols used in the Internet?\nR28. Why are policy considerations as important for intra-AS protocols, such as\nOSPF and RIP, as they are for an inter-AS routing protocol like BGP?\nDestination Subnet\nNext Router\nNumber of Hops to Destination\nz\nC\n10\nw\n—\n1\nx\n—\n1\n. . . .\n. . . .\n. . . .\n\nR29. Define and contrast the following terms: subnet, prefix, and BGP route.\nR30. How does BGP use the NEXT-HOP attribute? How does it use the AS-PATH\nattribute?\nR31. Describe how a network administrator of an upper-tier ISP can implement\npolicy when configuring BGP.\nSECTION 4.7\nR32. What is an important difference between implementing the broadcast abstrac-\ntion via multiple unicasts, and a single network- (router-) supported broad-\ncast?\nR33. For each of the three general approaches we studied for broadcast communi-\ncation (uncontrolled flooding, controlled flooding, and spanning-tree broad-\ncast), are the following statements true or false? You may assume that no\npackets are lost due to buffer overflow and all packets are delivered on a link\nin the order in which they were sent.\na. A node may receive multiple copies of the same packet.\nb. A node may forward multiple copies of a packet over the same \noutgoing link.\nR34. When a host joins a multicast group, must it change its IP address to that of\nthe multicast group it is joining?\nR35. What are the roles played by the IGMP protocol and a wide-area multicast\nrouting protocol?\nR36. What is the difference between a group-shared tree and a source-based tree in\nthe context of multicast routing?\nProblems\nP1. In this question, we consider some of the pros and cons of virtual-circuit and\ndatagram networks.\na. Suppose that routers were subjected to conditions that might cause them\nto fail fairly often.  Would this argue in favor of a VC or datagram archi-\ntecture?  Why?\nb. Suppose that a source node and a destination require that a fixed amount\nof capacity always be available at all routers on the path between the\nsource and destination node, for the exclusive use of traffic flowing\nbetween this source and destination node. Would this argue in favor of a\nVC or datagram architecture?  Why?\nc. Suppose that the links and routers in the network never fail and that rout-\ning paths used between all source/destination pairs remains constant.  In\nthis scenario, does a VC or datagram architecture have more control traf-\nfic overhead?  Why?\n416\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\nP2. Consider a virtual-circuit network. Suppose the VC number is an 8-bit field.\na. What is the maximum number of virtual circuits that can be carried over a\nlink?\nb. Suppose a central node determines paths and VC numbers at connection\nsetup. Suppose the same VC number is used on each link along the VC’s\npath. Describe how the central node might determine the VC number at con-\nnection setup. Is it possible that there are fewer VCs in progress than the\nmaximum as determined in part (a) yet there is no common free VC number?\nc. Suppose that different VC numbers are permitted in each link along a \nVC’s path. During connection setup, after an end-to-end path is determined,\ndescribe how the links can choose their VC numbers and configure their for-\nwarding tables in a decentralized manner, without reliance on a central node.\nP3. A bare-bones forwarding table in a VC network has four columns. What is\nthe meaning of the values in each of these columns? A bare-bones forwarding\ntable in a datagram network has two columns. What is the meaning of the\nvalues in each of these columns?\nP4. Consider the network below.\na. Suppose that this network is a datagram network.  Show the forwarding\ntable in router A, such that all traffic destined to host H3 is forwarded\nthrough interface 3.\nb. Suppose that this network is a datagram network.  Can you write down a\nforwarding table in router A, such that all traffic from H1 destined to host\nH3 is forwarded through interface 3, while all traffic from H2 destined to\nhost H3 is forwarded through interface 4?  (Hint: this is a trick question.)\nc. Now suppose that this network is a virtual circuit network and that there is\none ongoing call between H1 and H3, and another ongoing call between\nH2 and H3. Write down a forwarding table in router A, such that all traffic\nfrom H1 destined to host H3 is forwarded through interface 3, while all\ntraffic from H2 destined to host H3 is forwarded through interface 4.  \nd. Assuming the same scenario as (c), write down the forwarding tables in\nnodes B, C, and D.\nPROBLEMS\n417\nB\nA\n1\n3\n2\n4\n2\nD\n1\n2\n3\nH3\nH1\nH2\n1\n1\n2\nC\nP5. Consider a VC network with a 2-bit field for the VC number. Suppose that\nthe network wants to set up a virtual circuit over four links: link A, link B,\n\nlink C, and link D. Suppose that each of these links is currently carrying two\nother virtual circuits, and the VC numbers of these other VCs are as follows:\n418\nCHAPTER 4\n•\nTHE NETWORK LAYER\nLink A\nLink B\nLink C\nLink D\n00\n01\n10\n11\n01\n10\n11\n00\nIn answering the following questions, keep in mind that each of the existing\nVCs may only be traversing one of the four links.\na. If each VC is required to use the same VC number on all links along its\npath, what VC number could be assigned to the new VC?\nb. If each VC is permitted to have different VC numbers in the different links\nalong its path (so that forwarding tables must perform VC number transla-\ntion), how many different combinations of four VC numbers (one for each \nof the four links) could be used?\nP6. In the text we have used the term connection-oriented service to describe a\ntransport-layer service and connection service for a network-layer service.\nWhy the subtle shades in terminology?\nP7. Suppose two packets arrive to two different input ports of a router at exactly\nthe same time. Also suppose there are no other packets anywhere in the\nrouter.\na. Suppose the two packets are to be forwarded to two different output ports.\nIs it possible to forward the two packets through the switch fabric at the\nsame time when the fabric uses a shared bus?\nb. Suppose the two packets are to be forwarded to two different output ports.\nIs it possible to forward the two packets through the switch fabric at the\nsame time when the fabric uses a crossbar?\nc. Suppose the two packets are to be forwarded to the same output port. Is it\npossible to forward the two packets through the switch fabric at the same\ntime when the fabric uses a crossbar?\nP8. In Section 4.3, we noted that the maximum queuing delay is (n–1)D if the\nswitching fabric is n times faster than the input line rates. Suppose that all\npackets are of the same length, n packets arrive at the same time to the n\ninput ports, and all n packets want to be forwarded to different output ports.\nWhat is the maximum delay for a packet for the (a) memory, (b) bus, and (c)\ncrossbar switching fabrics?\nP9. Consider the switch shown below. Suppose that all datagrams have the same\nfixed length, that the switch operates in a slotted, synchronous manner, and\nthat in one time slot a datagram can be transferred from an input port to an\noutput port. The switch fabric is a crossbar so that at most one datagram can\n\nPROBLEMS\n419\nbe transferred to a given output port in a time slot, but different output ports\ncan receive datagrams from different input ports in a single time slot. What is\nthe minimal number of time slots needed to transfer the packets shown from\ninput ports to their output ports, assuming any input queue scheduling order\nyou want (i.e., it need not have HOL blocking)?  What is the largest number\nof slots needed, assuming the worst-case scheduling order you can devise,\nassuming that a non-empty input queue is never idle?\nX Y\nSwitch\nfabric\nOutput port X\nOutput port Y\nOutput port Z\nX\nY\nZ\nP10. Consider a datagram network using 32-bit host addresses. Suppose a router\nhas four links, numbered 0 through 3, and packets are to be forwarded to the\nlink interfaces as follows:\nDestination Address Range                                      Link Interface\n11100000 00000000 00000000 00000000\nthrough\n0\n11100000 00111111 11111111 11111111\n11100000 01000000 00000000 00000000\nthrough                                                                 1\n11100000 01000000 11111111 11111111\n11100000 01000001 00000000 00000000\nthrough\n2\n11100001 01111111 11111111 11111111\notherwise\n3\na. Provide a forwarding table that has five entries, uses longest prefix match-\ning, and forwards packets to the correct link interfaces.\nb. Describe how your forwarding table determines the appropriate link inter-\nface for datagrams with destination addresses:\n11001000 10010001 01010001 01010101\n11100001 01000000 11000011 00111100\n11100001 10000000 00010001 01110111\n\n420\nCHAPTER 4\n•\nTHE NETWORK LAYER\nPrefix Match\nInterface\n1\n0\n10\n1\n111\n2\notherwise\n3\nFor each of the four interfaces, give the associated range of destination host\naddresses and the number of addresses in the range.\nP13. Consider a router that interconnects three subnets: Subnet 1, Subnet 2, and\nSubnet 3. Suppose all of the interfaces in each of these three subnets are\nrequired to have the prefix 223.1.17/24. Also suppose that Subnet 1 is\nrequired to support at least 60 interfaces, Subnet 2 is to support at least 90\ninterfaces, and Subnet 3 is to support at least 12 interfaces. Provide three net-\nwork addresses (of the form a.b.c.d/x) that satisfy these constraints.\nP14. In Section 4.2.2 an example forwarding table (using longest prefix matching)\nis given. Rewrite this forwarding table using the a.b.c.d/x notation instead of\nthe binary string notation.\nP15. In Problem P10 you are asked to provide a forwarding table (using longest\nprefix matching). Rewrite this forwarding table using the a.b.c.d/x notation\ninstead of the binary string notation.\nP11. Consider a datagram network using 8-bit host addresses. Suppose a router\nuses longest prefix matching and has the following forwarding table:\nPrefix Match\nInterface\n00\n0\n010\n1\n011\n2\n10\n2\n11\n3\nFor each of the four interfaces, give the associated range of destination host\naddresses and the number of addresses in the range.\nP12. Consider a datagram network using 8-bit host addresses. Suppose a\nrouter uses longest prefix matching and has the following forwarding\ntable:\n\nP16. Consider a subnet with prefix 128.119.40.128/26. Give an example of one\nIP address (of form xxx.xxx.xxx.xxx) that can be assigned to this network.\nSuppose an ISP owns the block of addresses of the form 128.119.40.64/26.\nSuppose it wants to create four subnets from this block, with each block \nhaving the same number of IP addresses. What are the prefixes (of form\na.b.c.d/x) for the four subnets?\nP17. Consider the topology shown in Figure 4.17. Denote the three subnets with\nhosts (starting clockwise at 12:00) as Networks A, B, and C. Denote the sub-\nnets without hosts as Networks D, E, and F.\na. Assign network addresses to each of these six subnets, with the follow-\ning constraints: All addresses must be allocated from 214.97.254/23;\nSubnet A should have enough addresses to support 250 interfaces; Sub-\nnet B should have enough addresses to support 120 interfaces; and \nSubnet C should have enough addresses to support 120 interfaces. Of\ncourse, subnets D, E and F should each be able to support two interfaces.\nFor each subnet, the assignment should take the form a.b.c.d/x or\na.b.c.d/x – e.f.g.h/y.\nb. Using your answer to part (a), provide the forwarding tables (using longest\nprefix matching) for each of the three routers.\nP18. Use the whois service at the American Registry for Internet Numbers\n(http://www.arin.net/whois) to determine the IP address blocks for three \nuniversities. Can the whois services be used to determine with certainty the\ngeographical location of a specific IP address? Use www.maxmind.com to\ndetermine the locations of the Web servers at each of these universities.  \nP19. Consider sending a 2400-byte datagram into a link that has an MTU of \n700 bytes. Suppose the original datagram is stamped with the identifica-\ntion number 422. How many fragments are generated? What are the \nvalues in the various fields in the IP datagram(s) generated related to\nfragmentation?\nP20. Suppose datagrams are limited to 1,500 bytes (including header) between\nsource Host A and destination Host B. Assuming a 20-byte IP header, how\nmany datagrams would be required to send an MP3 consisting of 5 million\nbytes? Explain how you computed your answer.\nP21. Consider the network setup in Figure 4.22. Suppose that the ISP instead\nassigns the router the address 24.34.112.235 and that the network address of\nthe home network is 192.168.1/24.\na. Assign addresses to all interfaces in the home network.\nb. Suppose each host has two ongoing TCP connections, all to port 80 at \nhost 128.119.40.86. Provide the six corresponding entries in the NAT\ntranslation table.\nPROBLEMS\n421\n\nP22. Suppose you are interested in detecting the number of hosts behind a NAT.\nYou observe that the IP layer stamps an identification number sequentially on\neach IP packet. The identification number of the first IP packet generated by\na host is a random number, and the identification numbers of the subsequent\nIP packets are sequentially assigned. Assume all IP packets generated by\nhosts behind the NAT are sent to the outside world. \na. Based on this observation, and assuming you can sniff all packets sent by\nthe NAT to the outside, can you outline a simple technique that detects the\nnumber of unique hosts behind a NAT? Justify your answer.\nb. If the identification numbers are not sequentially assigned but randomly\nassigned, would your technique work? Justify your answer.\nP23. In this problem we’ll explore the impact of NATs on P2P applications. \nSuppose a peer with username Arnold discovers through querying that a peer\nwith username Bernard has a file it wants to download. Also suppose that\nBernard and Arnold are both behind a NAT. Try to devise a technique that\nwill allow Arnold to establish a TCP connection with Bernard without\napplication-specific NAT configuration. If you have difficulty devising such\na technique, discuss why.\nP24. Looking at Figure 4.27, enumerate the paths from y to u that do not contain\nany loops.\nP25. Repeat Problem P24 for paths from x to z, z to u, and z to w.\nP26. Consider the following network. With the indicated link costs, use Dijkstra’s\nshortest-path algorithm to compute the shortest path from x to all network\nnodes. Show how the algorithm works by computing a table similar to \nTable 4.3.\nx\nv\nt\ny\nz\nu\nw\n6\n12\n8\n7\n8\n3\n6\n4\n3\n2\n4\n3\n422\nCHAPTER 4\n•\nTHE NETWORK LAYER\nVideoNote\nDijkstra’s algorithm:\ndiscussion and example\n\nP27. Consider the network shown in Problem P26. Using Dijkstra’s algorithm, and\nshowing your work using a table similar to Table 4.3, do the following:\na. Compute the shortest path from t to all network nodes.\nb. Compute the shortest path from u to all network nodes.\nc. Compute the shortest path from v to all network nodes.\nd. Compute the shortest path from w to all network nodes.\ne. Compute the shortest path from y to all network nodes.\nf. Compute the shortest path from z to all network nodes.\nP28. Consider the network shown below, and assume that each node initially\nknows the costs to each of its neighbors. Consider the distance-vector \nalgorithm and show the distance table entries at node z.\nP29. Consider a general topology (that is, not the specific network shown above) and a\nsynchronous version of the distance-vector algorithm. Suppose that at each itera-\ntion, a node exchanges its distance vectors with its neighbors and receives their\ndistance vectors. Assuming that the algorithm begins with each node knowing\nonly the costs to its immediate neighbors, what is the maximum number of itera-\ntions required before the distributed algorithm converges? Justify your answer.\nP30. Consider the network fragment shown below. x has only two attached neigh-\nbors, w and y. w has a minimum-cost path to destination u (not shown) of 5,\nand y has a minimum-cost path to u of 6. The complete paths from w and y to\nu (and between w and y) are not shown. All link costs in the network have\nstrictly positive integer values.\nx\ny\nw\n2\n2\n5\nu\nz\nv\ny\n2\n3\n6\n2\n3\n1\nx\nPROBLEMS\n423\n\na. Give x’s distance vector for destinations w, y, and u.\nb. Give a link-cost change for either c(x,w) or c(x,y) such that x will inform \nits neighbors of a new minimum-cost path to u as a result of executing the\ndistance-vector algorithm.\nc. Give a link-cost change for either c(x,w) or c(x,y) such that x will not\ninform its neighbors of a new minimum-cost path to u as a result of exe-\ncuting the distance-vector algorithm.\nP31. Consider the three-node topology shown in Figure 4.30. Rather than having\nthe link costs shown in Figure 4.30, the link costs are c(x,y) = 3, c(y,z) = 6,\nc(z,x) = 4. Compute the distance tables after the initialization step and after\neach iteration of a synchronous version of the distance-vector algorithm (as\nwe did in our earlier discussion of Figure 4.30).\nP32. Consider the count-to-infinity problem in the distance vector routing. Will the\ncount-to-infinity problem occur if we decrease the cost of a link? Why? How\nabout if we connect two nodes which do not have a link?\nP33. Argue that for the distance-vector algorithm in Figure 4.30, each value in the\ndistance vector D(x) is non-increasing and will eventually stabilize in a finite\nnumber of steps.\nP34. Consider Figure 4.31. Suppose there is another router w, connected to router\ny and z. The costs of all links are given as follows: c(x,y) = 4, c(x,z) = 50,\nc(y,w) = 1, c(z,w) = 1, c(y,z) = 3. Suppose that poisoned reverse is used in the\ndistance-vector routing algorithm.\na. When the distance vector routing is stabilized, router w, y, and z inform their\ndistances to x to each other. What distance values do they tell each other?\nb. Now suppose that the link cost between x and y increases to 60. Will there\nbe a count-to-infinity problem even if poisoned reverse is used? Why or\nwhy not? If there is a count-to-infinity problem, then how many iterations\nare needed for the distance-vector routing to reach a stable state again?\nJustify your answer. \nc. How do you modify c(y,z) such that there is no count-to-infinity problem\nat all if c(y,x) changes from 4 to 60? \nP35. Describe how loops in paths can be detected in BGP.\nP36. Will a BGP router always choose the loop-free route with the shortest AS-\npath length? Justify your answer.\nP37. Consider the network shown below. Suppose AS3 and AS2 are running OSPF\nfor their intra-AS routing protocol. Suppose AS1 and AS4 are running RIP\nfor their intra-AS routing protocol. Suppose eBGP and iBGP are used for the\ninter-AS routing protocol. Initially suppose there is no physical link between\nAS2 and AS4.\n424\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\na. Router 3c learns about prefix x from which routing protocol: OSPF, RIP,\neBGP, or iBGP?\nb. Router 3a learns about x from which routing protocol?\nc. Router 1c learns about x from which routing protocol?\nd. Router 1d learns about x from which routing protocol?\nP38. Referring to the previous problem, once router 1d learns about x it will put an\nentry (x, I) in its forwarding table.\na. Will I be equal to I1 or I2 for this entry? Explain why in one sentence.\nb. Now suppose that there is a physical link between AS2 and AS4, shown by\nthe dotted line. Suppose router 1d learns that x is accessible via AS2 as\nwell as via AS3. Will I be set to I1 or I2? Explain why in one sentence.\nc. Now suppose there is another AS, called AS5, which lies on the path \nbetween AS2 and AS4 (not shown in diagram). Suppose router 1d learns \nthat x is accessible via AS2 AS5 AS4 as well as via AS3 AS4. Will I be set \nto I1 or I2? Explain why in one sentence.\nP39. Consider the following network. ISP B provides national backbone service\nto regional ISP A. ISP C provides national backbone service to regional\nISP D. Each ISP consists of one AS. B and C peer with each other in two\nplaces using BGP. Consider traffic going from A to D. B would prefer to\nhand that traffic over to C on the West Coast (so that C would have to\nabsorb the cost of carrying the traffic cross-country), while C would \nprefer to get the traffic via its East Coast peering point with B (so that B\nwould have carried the traffic across the country). What BGP mechanism\nmight C use, so that B would hand over A-to-D traffic at its East Coast\nAS4\nAS3\nAS1\nAS2\nx\n4b\n4c\n4a\n3c\n3b\n3a\n1c\n1b\n1d\n1a\nI1\nI2\n2c\n2a\n2b\nPROBLEMS\n425\n\npeering point? To answer this question, you will need to dig into the BGP\nspecification.\nP40. In Figure 4.42, consider the path information that reaches stub networks W,\nX, and Y. Based on the information available at W and X, what are their\nrespective views of the network topology? Justify your answer. The topology\nview at Y is shown below.\nP41. Consider Figure 4.42. B would never forward traffic destined to Y via X\nbased on BGP routing. But there are some very popular applications for\nwhich data packets go to X first and then flow to Y.  Identify one such\napplication, and describe how data packets follow a path not given by\nBGP routing. \nP42. In Figure 4.42, suppose that there is another stub network V that is a customer of\nISPA. Suppose that B and C have a peering relationship, and A is a customer of\nboth B and C. Suppose that A would like to have the traffic destined to W to\ncome from B only, and the traffic destined to V from either B or C. How should\nA advertise its routes to B and C? What AS routes does C receive?\nP43. Suppose ASs X and Z are not directly connected but instead are connected by\nAS Y. Further suppose that X has a peering agreement with Y, and that Y has\nW\nY\nX\nA\nC\nStub network\nY’s view of\nthe topology\nISP B\nISP C\nISP D\nISP A\n426\nCHAPTER 4\n•\nTHE NETWORK LAYER\n\na peering agreement with Z. Finally, suppose that Z wants to transit all of Y’s\ntraffic but does not want to transit X’s traffic. Does BGP allow Z to imple-\nment this policy?\nP44. Consider the seven-node network (with nodes labeled t to z) in Problem P26.\nShow the minimal-cost tree rooted at z that includes (as end hosts) nodes u, v,\nw, and y. Informally argue why your tree is a minimal-cost tree.\nP45. Consider the two basic approaches identified for achieving broadcast, unicast\nemulation and network-layer (i.e., router-assisted) broadcast, and suppose\nspanning-tree broadcast is used to achive network-layer broadcast. Consider\na single sender and 32 receivers. Suppose the sender is connected to the\nreceivers by a binary tree of routers. What is the cost of sending a broadcast\npacket, in the cases of unicast emulation and network-layer broadcast, for this\ntopology? Here, each time a packet (or copy of a packet) is sent over a single\nlink, it incurs a unit of cost. What topology for interconnecting the sender,\nreceivers, and routers will bring the cost of unicast emulation and true net-\nwork-layer broadcast as far apart as possible? You can choose as many\nrouters as you’d like.\nP46. Consider the operation of the reverse path forwarding (RPF) algorithm in Figure\n4.44. Using the same topology, find a set of paths from all nodes to the source\nnode A (and indicate these paths in a graph using thicker-shaded lines as in Fig-\nure 4.44) such that if these paths were the least-cost paths, then node B would\nreceive a copy of A’s broadcast message from nodes A, C, and D under RPF.\nP47. Consider the topology shown in Figure 4.44. Suppose that all links have unit\ncost and that node E is the broadcast source. Using arrows like those shown\nin Figure 4.44 indicate links over which packets will be forwarded using\nRPF, and links over which packets will not be forwarded, given that node E is\nthe source.\nP48. Repeat Problem P47 using the graph from Problem P26. Assume that z is the\nbroadcast source, and that the link costs are as shown in Problem P26.\nP49. Consider the topology shown in Figure 4.46, and suppose that each link has\nunit cost. Suppose node C is chosen as the center in a center-based multicast\nrouting algorithm. Assuming that each attached router uses its least-cost path\nto node C to send join messages to C, draw the resulting center-based routing\ntree. Is the resulting tree a minimum-cost tree? Justify your answer.\nP50. Repeat Problem P49, using the graph from Problem P26. Assume that the\ncenter node is v.\nP51. In Section 4.5.1 we studied Dijkstra’s link-state routing algorithm for com-\nputing the unicast paths that are individually the least-cost paths from the\nsource to all destinations. The union of these paths might be thought of as\nforming a least-unicast-cost path tree (or a shortest unicast path tree, if\nall link costs are identical). By constructing a counterexample, show that\nthe least-cost path tree is not always the same as a minimum spanning tree.\nPROBLEMS\n427\n\nP52. Consider a network in which all nodes are connected to three other nodes. In\na single time step, a node can receive all transmitted broadcast packets from\nits neighbors, duplicate the packets, and send them to all of its neighbors\n(except to the node that sent a given packet). At the next time step, neighboring\nnodes can receive, duplicate, and forward these packets, and so on. Sup-\npose that uncontrolled flooding is used to provide broadcast in such a \nnetwork. At time step t, how many copies of the broadcast packet will be\ntransmitted, assuming that during time step 1, a single broadcast packet is\ntransmitted by the source node to its three neighbors.\nP53. We saw in Section 4.7 that there is no network-layer protocol that can be used\nto identify the hosts participating in a multicast group. Given this, how can\nmulticast applications learn the identities of the hosts that are participating in\na multicast group?\nP54. Design (give a pseudocode description of) an application-level protocol that\nmaintains the host addresses of all hosts participating in a multicast group.\nSpecifically identify the network service (unicast or multicast) that is used by\nyour protocol, and indicate whether your protocol is sending messages in-\nband or out-of-band (with respect to the application data flow among the\nmulticast group participants) and why.\nP55. What is the size of the multicast address space? Suppose now that two multi-\ncast groups randomly choose a multicast address. What is the probability that\nthey choose the same address? Suppose now that 1,000 multicast groups are\nongoing at the same time and choose their multicast group addresses at ran-\ndom. What is the probability that they interfere with each other?\nSocket Programming Assignment\nAt the end of Chapter 2, there are four socket programming assignments. Below,\nyou will find a fifth assignment which employs ICMP, a protocol discussed in this\nchapter.\nAssignment 5: ICMP Ping\nPing is a popular networking application used to test from a remote location whether\na particular host is up and reachable. It is also often used to measure latency\nbetween the client host and the target host. It works by sending ICMP “echo\nrequest” packets (i.e., ping packets) to the target host and listening for ICMP “echo\nresponse” replies (i.e., pong packets). Ping measures the RRT, records packet loss,\nand calculates a statistical summary of multiple ping-pong exchanges (the mini-\nmum, mean, max, and standard deviation of the round-trip times).\n428\nCHAPTER 4\n•\nTHE NETWORK LAYER"
    },
    {
      "chunk_id": "d01abd00-7169-4d93-8344-c597672f51af",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Programming Assignments",
      "original_titles": [
        "Programming Assignments"
      ],
      "path": "Chapter 4 The Network Layer > Programming Assignments",
      "start_page": 456,
      "end_page": 456,
      "token_count": 486,
      "text": "In this lab, you will write your own Ping application in Python. Your application\nwill use ICMP. But in order to keep your program simple, you will not exactly follow\nthe official specification in RFC 1739. Note that you will only need to write the client\nside of the program, as the functionality needed on the server side is built into almost\nall operating systems. You can find full details of this assignment, as well as impor-\ntant snippets of the Python code, at the Web site http://www.awl.com/kurose-ross.\nProgramming Assignment\nIn this programming assignment, you will be writing a “distributed” set of proce-\ndures that implements a distributed asynchronous distance-vector routing for the\nnetwork shown below.\nYou are to write the following routines that will “execute” asynchronously\nwithin the emulated environment provided for this assignment. For node 0, you will\nwrite the routines:\n•\nrtinit0(). This routine will be called once at the beginning of the emulation.\nrtinit0() has no arguments. It should initialize your distance table in node 0 to\nreflect the direct costs of 1, 3, and 7 to nodes 1, 2, and 3, respectively. In the fig-\nure above, all links are bidirectional and the costs in both directions are identi-\ncal. After initializing the distance table and any other data structures needed by\nyour node 0 routines, it should then send its directly connected neighbors (in\nthis case, 1, 2, and 3) the cost of its minimum-cost paths to all other network\nnodes. This minimum-cost information is sent to neighboring nodes in a routing\nupdate packet by calling the routine tolayer2(), as described in the full assign-\nment. The format of the routing update packet is also described in the full\nassignment.\n•\nrtupdate0(struct rtpkt *rcvdpkt). This routine will be called when node 0\nreceives a routing packet that was sent to it by one of its directly connected\nneighbors. The parameter *rcvdpkt is a pointer to the packet that was received.\nrtupdate0() is the “heart” of the distance-vector algorithm. The values it\nreceives in a routing update packet from some other node i contain i’s current\nshortest-path costs to all other network nodes. rtupdate0() uses these received\n3\n2\n0\n1\n7\n3\n1\n2\n1\nPROGRAMMING ASSIGNMENT\n429"
    },
    {
      "chunk_id": "095538e3-70df-4adf-8172-59b31f26c6b0",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Wireshark Labs: IP, ICMP",
      "original_titles": [
        "Wireshark Labs: IP, ICMP"
      ],
      "path": "Chapter 4 The Network Layer > Wireshark Labs: IP, ICMP",
      "start_page": 457,
      "end_page": 457,
      "token_count": 301,
      "text": "values to update its own distance table (as specified by the distance-vector algo-\nrithm). If its own minimum cost to another node changes as a result of the\nupdate, node 0 informs its directly connected neighbors of this change in mini-\nmum cost by sending them a routing packet. Recall that in the distance-vector\nalgorithm, only directly connected nodes will exchange routing packets. Thus,\nnodes 1 and 2 will communicate with each other, but nodes 1 and 3 will not\ncommunicate with each other.\nSimilar routines are defined for nodes 1, 2, and 3. Thus, you will write eight proce-\ndures in all: rtinit0(), rtinit1(), rtinit2(), rtinit3(), rtupdate0(), rtupdate1(), rtup-\ndate2(), and rtupdate3(). These routines will together implement a distributed,\nasynchronous computation of the distance tables for the topology and costs shown\nin the figure on the preceding page.\nYou can find the full details of the programming assignment, as well as C code\nthat you will need to create the simulated hardware/software environment, at\nhttp://www.awl.com/kurose-ross. A Java version of the assignment is also available.\nWireshark Labs\nIn the companion Web site for this textbook, http://www.awl.com/kurose-ross,\nyou’ll find two Wireshark lab assignments. The first lab examines the operation of\nthe IP protocol, and the IP datagram format in particular. The second lab explores\nthe use of the ICMP protocol in the ping and traceroute commands.\n430\nCHAPTER 4\n•\nTHE NETWORK LAYER"
    },
    {
      "chunk_id": "7411c9df-bd35-4758-b5cf-6d155283458a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Interview: Vinton G. Cerf",
      "original_titles": [
        "Interview: Vinton G. Cerf"
      ],
      "path": "Chapter 4 The Network Layer > Interview: Vinton G. Cerf",
      "start_page": 458,
      "end_page": 459,
      "token_count": 938,
      "text": "What brought you to specialize in networking?\nI was working as a programmer at UCLA in the late 1960s. My job was supported by the\nUS Defense Advanced Research Projects Agency (called ARPA then, called DARPA now). I\nwas working in the laboratory of Professor Leonard Kleinrock on the Network\nMeasurement Center of the newly created ARPAnet. The first node of the ARPAnet was\ninstalled at UCLA on September 1, 1969. I was responsible for programming a computer\nthat was used to capture performance information about the ARPAnet and to report this\ninformation back for comparison with mathematical models and predictions of the perform-\nance of the network.\nSeveral of the other graduate students and I were made responsible for working on\nthe so-called host-level protocols of the ARPAnet—the procedures and formats that would\nallow many different kinds of computers on the network to interact with each other. It was a\nfascinating exploration into a new world (for me) of distributed computing and communication.\nDid you imagine that IP would become as pervasive as it is today when you first designed\nthe protocol?\nWhen Bob Kahn and I first worked on this in 1973, I think we were mostly very focused on\nthe central question: How can we make heterogeneous packet networks interoperate with\none another, assuming we cannot actually change the networks themselves? We hoped that\nwe could find a way to permit an arbitrary collection of packet-switched networks to be\ninterconnected in a transparent fashion, so that host computers could communicate end-to-\nend without having to do any translations in between. I think we knew that we were dealing\nwith powerful and expandable technology but I doubt we had a clear image of what the\nworld would be like with hundreds of millions of computers all interlinked on the Internet.\n431\nVinton G. Cerf\nVinton G. Cerf is Vice President and Chief Internet Evangelist for\nGoogle. He served for over 16 years at MCI in various positions,\nending up his tenure there as Senior Vice President for Technology\nStrategy. He is widely known as the co-designer of the TCP/IP\nprotocols and the architecture of the Internet. During his time from\n1976 to 1982 at the US Department of Defense Advanced\nResearch Projects Agency (DARPA), he played a key role leading the\ndevelopment of Internet and Internet-related data packet and security\ntechniques. He received the US Presidential Medal of Freedom in\n2005 and the US National Medal of Technology in 1997. He\nholds a BS in Mathematics from Stanford University and an MS and\nPhD in computer science from UCLA.\nAN INTERVIEW WITH...\n\nWhat do you now envision for the future of networking and the Internet? What major\nchallenges/obstacles do you think lie ahead in their development?\nI believe the Internet itself and networks in general will continue to proliferate. Already\nthere is convincing evidence that there will be billions of Internet-enabled devices on the\nInternet, including appliances like cell phones, refrigerators, personal digital assistants,\nhome servers, televisions, as well as the usual array of laptops, servers, and so on. Big chal-\nlenges include support for mobility, battery life, capacity of the access links to the network,\nand ability to scale the optical core of the network up in an unlimited fashion. Designing an\ninterplanetary extension of the Internet is a project in which I am deeply engaged at the Jet\nPropulsion Laboratory. We will need to cut over from IPv4 [32-bit addresses] to IPv6 [128\nbits]. The list is long!\nWho has inspired you professionally?\nMy colleague Bob Kahn; my thesis advisor, Gerald Estrin; my best friend, Steve Crocker\n(we met in high school and he introduced me to computers in 1960!); and the thousands of\nengineers who continue to evolve the Internet today.\nDo you have any advice for students entering the networking/Internet field?\nThink outside the limitations of existing systems—imagine what might be possible; but then\ndo the hard work of figuring out how to get there from the current state of affairs. Dare to\ndream: A half dozen colleagues and I at the Jet Propulsion Laboratory have been working\non the design of an interplanetary extension of the terrestrial Internet. It may take decades to\nimplement this, mission by mission, but to paraphrase: “A man’s reach should exceed his\ngrasp, or what are the heavens for?”\n432"
    },
    {
      "chunk_id": "d4c50c12-536e-4d85-ab3e-b27d68ef7e96",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Chapter 5 The Link Layer: Links, Access Networks, and LANs",
      "original_titles": [
        "Chapter 5 The Link Layer: Links, Access Networks, and LANs",
        "5.1 Introduction to the Link Layer"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs",
      "start_page": 460,
      "end_page": 462,
      "token_count": 1053,
      "text": "CHAPTER 5\nThe Link Layer:\nLinks, Access\nNetworks, and\nLANs\n433\nIn the previous chapter, we learned that the network layer provides a communica-\ntion service between any two network hosts. Between the two hosts, datagrams\ntravel over a series of communication links, some wired and some wireless, starting\nat the source host, passing through a series of packet switches (switches and routers)\nand ending at the destination host. As we continue down the protocol stack, from the\nnetwork layer to the link layer, we naturally wonder how packets are sent across \nthe individual links that make up the end-to-end communication path. How are the\nnetwork-layer datagrams encapsulated in the link-layer frames for transmission over\na single link? Are different link-layer protocols used in the different links along the\ncommunication path? How are transmission conflicts in broadcast links resolved? Is\nthere addressing at the link layer and, if so, how does the link-layer addressing oper-\nate with the network-layer addressing we learned about in Chapter 4? And what\nexactly is the difference between a switch and a router? We’ll answer these and\nother important questions in this chapter.\nIn discussing the link layer, we’ll see that there are two fundamentally different\ntypes of link-layer channels. The first type are broadcast channels, which connect mul-\ntiple hosts in wireless LANs, satellite networks, and hybrid fiber-coaxial cable (HFC)\n434\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\naccess networks. Since many hosts are connected to the same broadcast communica-\ntion channel, a so-called medium access protocol is needed to coordinate frame\ntransmission. In some cases, a central controller may be used to coordinate transmis-\nsions; in other cases, the hosts themselves coordinate transmissions. The second type\nof link-layer channel is the point-to-point communication link, such as that often\nfound between two routers connected by a long-distance link, or between a user’s\noffice computer and the nearby Ethernet switch to which it is connected. Coordinating\naccess to a point-to-point link is simpler; the reference material on this book’s web site\nhas a detailed discussion of the Point-to-Point Protocol (PPP), which is used in set-\ntings ranging from dial-up service over a telephone line to high-speed point-to-point\nframe transport over fiber-optic links.\nWe’ll explore several important link-layer concepts and technologies in this chap-\nter. We’ll dive deeper into error detection and correction, a topic we touched on briefly\nin Chapter 3. We’ll consider multiple access networks and switched LANs, including\nEthernet—by far the most prevalent wired LAN technology. We’ll also look at virtual\nLANs, and data center networks. Although WiFi, and more generally wireless LANs,\nare link-layer topics, we’ll postpone our study of these important topics until Chapter 6.\n5.1 Introduction to the Link Layer\nLet’s begin with some important terminology. We’ll find it convenient in this chapter to\nrefer to any device that runs a link-layer (i.e., layer 2) protocol as a node. Nodes include\nhosts, routers, switches, and WiFi access points (discussed in Chapter 6). We will \nalso refer to the communication channels that connect adjacent nodes along the com-\nmunication path as links. In order for a datagram to be transferred from source host to\ndestination host, it must be moved over each of the individual links in the end-to-end\npath. As an example, in the company network shown at the bottom of Figure 5.1, con-\nsider sending a datagram from one of the wireless hosts to one of the servers. This data-\ngram will actually pass through six links: a WiFi link between sending host and WiFi\naccess point, an Ethernet link between the access point and a link-layer switch; a link\nbetween the link-layer switch and the router, a link between the two routers; an \nEthernet link between the router and a link-layer switch; and finally an Ethernet link\nbetween the switch and the server. Over a given link, a transmitting node encapsulates\nthe datagram in a link-layer frame and transmits the frame into the link.\nIn order to gain further insight into the link layer and how it relates to the network\nlayer, let’s consider a transportation analogy. Consider a travel agent who is planning a\ntrip for a tourist traveling from Princeton, New Jersey, to Lausanne, Switzerland. The\ntravel agent decides that it is most convenient for the tourist to take a limousine from\nPrinceton to JFK airport, then a plane from JFK airport to Geneva’s airport, and finally\na train from Geneva’s airport to Lausanne’s train station. Once the travel agent makes\nthe three reservations, it is the responsibility of the Princeton limousine company to get\nthe tourist from Princeton to JFK; it is the responsibility of the airline company to \n\n5.1\n•\nINTRODUCTION TO THE LINK LAYER\n435\nFigure 5.1 \u0002 Six link-layer hops between wireless host and server\nMobile Network\nNational or\nGlobal ISP\nLocal or\nRegional ISP\nEnterprise Network\nHome Network"
    },
    {
      "chunk_id": "14b19b4d-7971-4334-a0d9-b13dbbc63ae0",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.1.1 The Services Provided by the Link Layer",
      "original_titles": [
        "5.1.1 The Services Provided by the Link Layer"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.1 Introduction to the Link Layer > 5.1.1 The Services Provided by the Link Layer",
      "start_page": 463,
      "end_page": 463,
      "token_count": 648,
      "text": "get the tourist from JFK to Geneva; and it is the responsibility of the Swiss train service\nto get the tourist from Geneva to Lausanne. Each of the three segments of the trip \nis “direct” between two “adjacent” locations. Note that the three transportation seg-\nments are managed by different companies and use entirely different transportation\nmodes (limousine, plane, and train). Although the transportation modes are different,\nthey each provide the basic service of moving passengers from one location to an\nadjacent location. In this transportation analogy, the tourist is a datagram, each trans-\nportation segment is a link, the transportation mode is a link-layer protocol, and the\ntravel agent is a routing protocol.\n5.1.1 The Services Provided by the Link Layer\nAlthough the basic service of any link layer is to move a datagram from one node to\nan adjacent node over a single communication link, the details of the provided serv-\nice can vary from one link-layer protocol to the next. Possible services that can be\noffered by a link-layer protocol include:\n•\nFraming. Almost all link-layer protocols encapsulate each network-layer data-\ngram within a link-layer frame before transmission over the link. A frame con-\nsists of a data field, in which the network-layer datagram is inserted, and a\nnumber of header fields. The structure of the frame is specified by the link-layer\nprotocol. We’ll see several different frame formats when we examine specific\nlink-layer protocols in the second half of this chapter.\n•\nLink access. A medium access control (MAC) protocol specifies the rules by which\na frame is transmitted onto the link. For point-to-point links that have a single\nsender at one end of the link and a single receiver at the other end of the link, the\nMAC protocol is simple (or nonexistent)—the sender can send a frame whenever\nthe link is idle. The more interesting case is when multiple nodes share a single\nbroadcast link—the so-called multiple access problem. Here, the MAC protocol\nserves to coordinate the frame transmissions of the many nodes.\n•\nReliable delivery. When a link-layer protocol provides reliable delivery service, it\nguarantees to move each network-layer datagram across the link without error.\nRecall that certain transport-layer protocols (such as TCP) also provide a reliable\ndelivery service. Similar to a transport-layer reliable delivery service, a link-layer\nreliable delivery service can be achieved with acknowledgments and retransmis-\nsions (see Section 3.4). A link-layer reliable delivery service is often used for links\nthat are prone to high error rates, such as a wireless link, with the goal of correcting\nan error locally—on the link where the error occurs—rather than forcing an end-to-\nend retransmission of the data by a transport- or application-layer protocol. How-\never, link-layer reliable delivery can be considered an unnecessary overhead for low\nbit-error links, including fiber, coax, and many twisted-pair copper links. For this\nreason, many wired link-layer protocols do not provide a reliable delivery service.\n436\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS"
    },
    {
      "chunk_id": "419e9ab5-0467-45ba-b881-80545ae7cb96",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.1.2 Where Is the Link Layer Implemented?",
      "original_titles": [
        "5.1.2 Where Is the Link Layer Implemented?"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.1 Introduction to the Link Layer > 5.1.2 Where Is the Link Layer Implemented?",
      "start_page": 464,
      "end_page": 464,
      "token_count": 674,
      "text": "•\nError detection and correction. The link-layer hardware in a receiving node can\nincorrectly decide that a bit in a frame is zero when it was transmitted as a one,\nand vice versa. Such bit errors are introduced by signal attenuation and electro-\nmagnetic noise. Because there is no need to forward a datagram that has an error,\nmany link-layer protocols provide a mechanism to detect such bit errors. This is\ndone by having the transmitting node include error-detection bits in the frame,\nand having the receiving node perform an error check. Recall from Chapters 3\nand 4 that the Internet’s transport layer and network layer also provide a limited\nform of error detection—the Internet checksum. Error detection in the link layer\nis usually more sophisticated and is implemented in hardware. Error correction\nis similar to error detection, except that a receiver not only detects when bit\nerrors have occurred in the frame but also determines exactly where in the frame\nthe errors have occurred (and then corrects these errors).\n5.1.2 Where Is the Link Layer Implemented?\nBefore diving into our detailed study of the link layer, let’s conclude this introduc-\ntion by considering the question of where the link layer is implemented. We’ll focus\nhere on an end system, since we learned in Chapter 4 that the link layer is imple-\nmented in a router’s line card. Is a host’s link layer implemented in hardware or soft-\nware? Is it implemented on a separate card or chip, and how does it interface with\nthe rest of a host’s hardware and operating system components?\nFigure 5.2 shows a typical host architecture. For the most part, the link layer is\nimplemented in a network adapter, also sometimes known as a network interface\ncard (NIC). At the heart of the network adapter is the link-layer controller, usually\na single, special-purpose chip that implements many of the link-layer services\n(framing, link access, error detection, and so on). Thus, much of a link-layer con-\ntroller’s functionality is implemented in hardware. For example, Intel’s 8254x con-\ntroller [Intel 2012] implements the Ethernet protocols we’ll study in Section 5.5; the\nAtheros AR5006 [Atheros 2012] controller implements the 802.11 WiFi protocols\nwe’ll study in Chapter 6. Until the late 1990s, most network adapters were physi-\ncally separate cards (such as a PCMCIA card or a plug-in card fitting into a PC’s\nPCI card slot) but increasingly, network adapters are being integrated onto the host’s\nmotherboard—a so-called LAN-on-motherboard configuration.\nOn the sending side, the controller takes a datagram that has been created and\nstored in host memory by the higher layers of the protocol stack, encapsulates the\ndatagram in a link-layer frame (filling in the frame’s various fields), and then\ntransmits the frame into the communication link, following the link-access proto-\ncol. On the receiving side, a controller receives the entire frame, and extracts the\nnetwork-layer datagram. If the link layer performs error detection, then it is \nthe sending controller that sets the error-detection bits in the frame header and it\nis the receiving controller that performs error detection.\n5.1\n•\nINTRODUCTION TO THE LINK LAYER\n437"
    },
    {
      "chunk_id": "a8c51181-1e9d-426f-b152-a8854ba88e97",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.2 Error-Detection and -Correction Techniques",
      "original_titles": [
        "5.2 Error-Detection and -Correction Techniques"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.2 Error-Detection and -Correction Techniques",
      "start_page": 465,
      "end_page": 466,
      "token_count": 742,
      "text": "Figure 5.2 shows a network adapter attaching to a host’s bus (e.g., a PCI or\nPCI-X bus), where it looks much like any other I/O device to the other host com-\nponents. Figure 5.2 also shows that while most of the link layer is implemented in\nhardware, part of the link layer is implemented in software that runs on the host’s\nCPU. The software components of the link layer implement higher-level link-\nlayer functionality such as assembling link-layer addressing information and acti-\nvating the controller hardware. On the receiving side, link-layer software responds\nto controller interrupts (e.g., due to the receipt of one or more frames), handling\nerror conditions and passing a datagram up to the network layer. Thus, the link\nlayer is a combination of hardware and software—the place in the protocol stack\nwhere software meets hardware. Intel [2012] provides a readable overview (as\nwell as a detailed description) of the 8254x controller from a software-program-\nming point of view.\n5.2 Error-Detection and -Correction Techniques\nIn the previous section, we noted that bit-level error detection and correction—\ndetecting and correcting the corruption of bits in a link-layer frame sent from one\nnode to another physically connected neighboring node—are two services often \n438\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nHost\nMemory\nHost bus\n(e.g., PCI)\nCPU\nController\nPhysical\ntransmission\nNetwork adapter\nLink\nPhysical\nTransport\nNetwork\nLink\nApplication\nFigure 5.2 \u0002 Network adapter: its relationship to other host components\nand to protocol stack functionality\n\nprovided by the link layer. We saw in Chapter 3 that error-detection and -correction\nservices are also often offered at the transport layer as well. In this section, we’ll\nexamine a few of the simplest techniques that can be used to detect and, in some\ncases, correct such bit errors. A full treatment of the theory and implementation of\nthis topic is itself the topic of many textbooks (for example, [Schwartz 1980] or\n[Bertsekas 1991]), and our treatment here is necessarily brief. Our goal here is to\ndevelop an intuitive feel for the capabilities that error-detection and -correction\ntechniques provide, and to see how a few simple techniques work and are used in\npractice in the link layer.\nFigure 5.3 illustrates the setting for our study. At the sending node, data, D, to\nbe protected against bit errors is augmented with error-detection and -correction bits\n(EDC). Typically, the data to be protected includes not only the datagram passed\ndown from the network layer for transmission across the link, but also link-level\naddressing information, sequence numbers, and other fields in the link frame header.\nBoth D and EDC are sent to the receiving node in a link-level frame. At the receiv-\ning node, a sequence of bits, D\u0002 and EDC\u0002 is received. Note that D\u0002 and EDC\u0002 may\ndiffer from the original D and EDC as a result of in-transit bit flips.\nThe receiver’s challenge is to determine whether or not D\u0002 is the same as the\noriginal D, given that it has only received D\u0002 and EDC\u0002. The exact wording of the\nreceiver’s decision in Figure 5.3 (we ask whether an error is detected, not whether\nan error has occurred!) is important. Error-detection and -correction techniques\nEDC'\nD'\nDetected error\nDatagram\nEDC\nD\nd data bits\nBit error-prone link\nall\nbits in D'\nOK\n?\nN\nY\nDatagram\nHI\nFigure 5.3 \u0002 Error-detection and -correction scenario\n5.2\n•\nERROR-DETECTION AND -CORRECTION TECHNIQUES\n439"
    },
    {
      "chunk_id": "068a7b75-c8ef-46c0-8df0-347f441c93b8",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.2.1 Parity Checks",
      "original_titles": [
        "5.2.1 Parity Checks"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.2 Error-Detection and -Correction Techniques > 5.2.1 Parity Checks",
      "start_page": 467,
      "end_page": 468,
      "token_count": 1034,
      "text": "allow the receiver to sometimes, but not always, detect that bit errors have\noccurred. Even with the use of error-detection bits there still may be undetected\nbit errors; that is, the receiver may be unaware that the received information con-\ntains bit errors. As a consequence, the receiver might deliver a corrupted datagram\nto the network layer, or be unaware that the contents of a field in the frame’s\nheader has been corrupted. We thus want to choose an error-detection scheme that\nkeeps the probability of such occurrences small. Generally, more sophisticated\nerror-detection and-correction techniques (that is, those that have a smaller proba-\nbility of allowing undetected bit errors) incur a larger overhead—more computa-\ntion is needed to compute and transmit a larger number of error-detection and\n-correction bits.\nLet’s now examine three techniques for detecting errors in the transmitted data—\nparity checks (to illustrate the basic ideas behind error detection and correction),\nchecksumming methods (which are more typically used in the transport layer), and\ncyclic redundancy checks (which are more typically used in the link layer in an\nadapter).\n5.2.1 Parity Checks\nPerhaps the simplest form of error detection is the use of a single parity bit. Sup-\npose that the information to be sent, D in Figure 5.4, has d bits. In an even parity\nscheme, the sender simply includes one additional bit and chooses its value such\nthat the total number of 1s in the d + 1 bits (the original information plus a parity\nbit) is even. For odd parity schemes, the parity bit value is chosen such that there is\nan odd number of 1s. Figure 5.4 illustrates an even parity scheme, with the single\nparity bit being stored in a separate field.\nReceiver operation is also simple with a single parity bit. The receiver need\nonly count the number of 1s in the received d + 1 bits. If an odd number of 1-\nvalued bits are found with an even parity scheme, the receiver knows that at least\none bit error has occurred. More precisely, it knows that some odd number of bit\nerrors have occurred.\nBut what happens if an even number of bit errors occur? You should convince\nyourself that this would result in an undetected error. If the probability of bit\nerrors is small and errors can be assumed to occur independently from one bit to\nthe next, the probability of multiple bit errors in a packet would be extremely small.\n440\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1\n1\nd data bits\nParity\nbit\nFigure 5.4 \u0002 One-bit even parity\n\nIn this case, a single parity bit might suffice. However, measurements have shown\nthat, rather than occurring independently, errors are often clustered together in\n“bursts.” Under burst error conditions, the probability of undetected errors in a\nframe protected by single-bit parity can approach 50 percent [Spragins 1991].\nClearly, a more robust error-detection scheme is needed (and, fortunately, is used\nin practice!). But before examining error-detection schemes that are used in prac-\ntice, let’s consider a simple generalization of one-bit parity that will provide us\nwith insight into error-correction techniques.\nFigure 5.5 shows a two-dimensional generalization of the single-bit parity\nscheme. Here, the d bits in D are divided into i rows and j columns. A parity value is\ncomputed for each row and for each column. The resulting i + j + 1 parity bits com-\nprise the link-layer frame’s error-detection bits.\nSuppose now that a single bit error occurs in the original d bits of informa-\ntion. With this two-dimensional parity scheme, the parity of both the column\nand the row containing the flipped bit will be in error. The receiver can thus not\nonly detect the fact that a single bit error has occurred, but can use the column\nand row indices of the column and row with parity errors to actually identify the\nbit that was corrupted and correct that error! Figure 5.5 shows an example in\n1  0  1  0  1  1\n1  1  1  1  0  0\n0  1  1  1  0  1\n0  0  1  0  1  0\n1  0  1  0  1  1\n1  0  1  1  0  0\n0  1  1  1  0  1\n0  0  1  0  1  0\nRow parity\nParity\nerror\nParity\nerror\nNo errors\nCorrectable\nsingle-bit error\nd1,1\nd2,1\n. . .\ndi,1\ndi+1,1\n. . .\n. . .\n. . .\n. . .\n. . .\nd1, j\nd2, j\n. . .\ndi,j\ndi+1,j\nd1, j+1\nd2, j+1\n. . .\ndi,j+1\ndi+1, j+1\nColumn parity\nFigure 5.5 \u0002 Two-dimensional even parity\n5.2\n•\nERROR-DETECTION AND -CORRECTION TECHNIQUES\n441"
    },
    {
      "chunk_id": "2ec4d183-4df3-4acc-a59e-cf2933d583e4",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.2.2 Checksumming Methods",
      "original_titles": [
        "5.2.2 Checksumming Methods"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.2 Error-Detection and -Correction Techniques > 5.2.2 Checksumming Methods",
      "start_page": 469,
      "end_page": 469,
      "token_count": 664,
      "text": "which the 1-valued bit in position (2,2) is corrupted and switched to a 0—an\nerror that is both detectable and correctable at the receiver. Although our discus-\nsion has focused on the original d bits of information, a single error in the parity\nbits themselves is also detectable and correctable. Two-dimensional parity can\nalso detect (but not correct!) any combination of two errors in a packet. Other\nproperties of the two-dimensional parity scheme are explored in the problems at\nthe end of the chapter.\nThe ability of the receiver to both detect and correct errors is known as forward\nerror correction (FEC). These techniques are commonly used in audio storage and\nplayback devices such as audio CDs. In a network setting, FEC techniques can be\nused by themselves, or in conjunction with link-layer ARQ techniques similar to\nthose we examined in Chapter 3. FEC techniques are valuable because they can\ndecrease the number of sender retransmissions required. Perhaps more important,\nthey allow for immediate correction of errors at the receiver. This avoids having to\nwait for the round-trip propagation delay needed for the sender to receive a NAK\npacket and for the retransmitted packet to propagate back to the receiver—a poten-\ntially important advantage for real-time network applications [Rubenstein 1998] or\nlinks (such as deep-space links) with long propagation delays. Research examining\nthe use of FEC in error-control protocols includes [Biersack 1992; Nonnenmacher\n1998; Byers 1998; Shacham 1990].\n5.2.2 Checksumming Methods\nIn checksumming techniques, the d bits of data in Figure 5.4 are treated as a\nsequence of k-bit integers. One simple checksumming method is to simply sum\nthese k-bit integers and use the resulting sum as the error-detection bits. The\nInternet checksum is based on this approach—bytes of data are treated as 16-bit\nintegers and summed. The 1s complement of this sum then forms the Internet\nchecksum that is carried in the segment header. As discussed in Section 3.3, the\nreceiver checks the checksum by taking the 1s complement of the sum of the\nreceived data (including the checksum) and checking whether the result is all \n1 bits. If any of the bits are 0, an error is indicated. RFC 1071 discusses the Internet\nchecksum algorithm and its implementation in detail. In the TCP and UDP protocols,\nthe Internet checksum is computed over all fields (header and data fields\nincluded). In IP the checksum is computed over the IP header (since the UDP or\nTCP segment has its own checksum). In other protocols, for example, XTP\n[Strayer 1992], one checksum is computed over the header and another checksum\nis computed over the entire packet.\nChecksumming methods require relatively little packet overhead. For example,\nthe checksums in TCP and UDP use only 16 bits. However, they provide relatively\nweak protection against errors as compared with cyclic redundancy check, which is\ndiscussed below and which is often used in the link layer. A natural question at this\npoint is, Why is checksumming used at the transport layer and cyclic redundancy\n442\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS"
    },
    {
      "chunk_id": "a3887178-a94c-4ccf-a882-af4724bb3309",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.2.3 Cyclic Redundancy Check (CRC)",
      "original_titles": [
        "5.2.3 Cyclic Redundancy Check (CRC)"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.2 Error-Detection and -Correction Techniques > 5.2.3 Cyclic Redundancy Check (CRC)",
      "start_page": 470,
      "end_page": 471,
      "token_count": 1041,
      "text": "check used at the link layer? Recall that the transport layer is typically implemented\nin software in a host as part of the host’s operating system. Because transport-layer\nerror detection is implemented in software, it is important to have a simple and fast\nerror-detection scheme such as checksumming. On the other hand, error detection at\nthe link layer is implemented in dedicated hardware in adapters, which can rapidly\nperform the more complex CRC operations. Feldmeier [Feldmeier 1995] presents\nfast software implementation techniques for not only weighted checksum codes, but\nCRC (see below) and other codes as well.\n5.2.3 Cyclic Redundancy Check (CRC)\nAn error-detection technique used widely in today’s computer networks is based \non cyclic redundancy check (CRC) codes. CRC codes are also known as\npolynomial codes, since it is possible to view the bit string to be sent as a polyno-\nmial whose coefficients are the 0 and 1 values in the bit string, with operations on\nthe bit string interpreted as polynomial arithmetic.\nCRC codes operate as follows. Consider the d-bit piece of data, D, that the\nsending node wants to send to the receiving node. The sender and receiver must first\nagree on an r + 1 bit pattern, known as a generator, which we will denote as G. We\nwill require that the most significant (leftmost) bit of G be a 1. The key idea behind\nCRC codes is shown in Figure 5.6. For a given piece of data, D, the sender will\nchoose r additional bits, R, and append them to D such that the resulting d + r bit\npattern (interpreted as a binary number) is exactly divisible by G (i.e., has no\nremainder) using modulo-2 arithmetic. The process of error checking with CRCs is\nthus simple: The receiver divides the d + r received bits by G. If the remainder is\nnonzero, the receiver knows that an error has occurred; otherwise the data is accepted\nas being correct.\nAll CRC calculations are done in modulo-2 arithmetic without carries in\naddition or borrows in subtraction. This means that addition and subtraction are\nidentical, and both are equivalent to the bitwise exclusive-or (XOR) of the\noperands. Thus, for example,\n1011 XOR 0101 = 1110\n1001 XOR 1101 = 0100\nd bits\nr bits\nD: Data bits to be sent\nD • 2r  XOR\nR\nR: CRC bits\nBit pattern\nMathematical formula\nFigure 5.6 \u0002 CRC\n5.2\n•\nERROR-DETECTION AND -CORRECTION TECHNIQUES\n443\n\nAlso, we similarly have\n1011 – 0101 = 1110\n1001 – 1101 = 0100\nMultiplication and division are the same as in base-2 arithmetic, except that any\nrequired addition or subtraction is done without carries or borrows. As in regular\nbinary arithmetic, multiplication by 2k left shifts a bit pattern by k places. Thus,\ngiven D and R, the quantity D \u0003 2r XOR R yields the d + r bit pattern shown \nin Figure 5.6. We’ll use this algebraic characterization of the d + r bit pattern from\nFigure 5.6 in our discussion below.\nLet us now turn to the crucial question of how the sender computes R. Recall\nthat we want to find R such that there is an n such that\nD \u0003 2r XOR R\nnG\nThat is, we want to choose R such that G divides into D \u0003 2r XOR R without remain-\nder. If we XOR (that is, add modulo-2, without carry) R to both sides of the above\nequation, we get\nD \u0003 2r\nnG XOR R\nThis equation tells us that if we divide D \u0003 2r by G, the value of the remainder is pre-\ncisely R. In other words, we can calculate R as\nFigure 5.7 illustrates this calculation for the case of D = 101110, d = 6, G = 1001,\nand r\n3. The 9 bits transmitted in this case are 101110 011. You should check these\ncalculations for yourself and also check that indeed D \u0003 2r = 101011 \u0003 G XOR R.\nInternational standards have been defined for 8-, 12-, 16-, and 32-bit genera-\ntors, G. The CRC-32 32-bit standard, which has been adopted in a number of link-\nlevel IEEE protocols, uses a generator of\nGCRC-32\n100000100110000010001110110110111\nEach of the CRC standards can detect burst errors of fewer than r + 1 bits. (This\nmeans that all consecutive bit errors of r bits or fewer will be detected.) Furthermore,\nunder appropriate assumptions, a burst of length greater than r + 1 bits is detected with\nprobability 1 – 0.5r. Also, each of the CRC standards can detect any odd number of bit\nerrors. See [Williams 1993] for a discussion of implementing CRC checks. The theory\n=\n=\nR = remainder D \u0003 2r\nG\n=\n=\n444\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS"
    },
    {
      "chunk_id": "1eb76b30-970b-431b-b5d6-c79ab5981389",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.3 Multiple Access Links and Protocols",
      "original_titles": [
        "5.3 Multiple Access Links and Protocols"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.3 Multiple Access Links and Protocols",
      "start_page": 472,
      "end_page": 474,
      "token_count": 1305,
      "text": "behind CRC codes and even more powerful codes is beyond the scope of this text. The\ntext [Schwartz 1980] provides an excellent introduction to this topic.\n5.3 Multiple Access Links and Protocols \nIn the introduction to this chapter, we noted that there are two types of network links:\npoint-to-point links and broadcast links. A point-to-point link consists of a single\nsender at one end of the link and a single receiver at the other end of the link. Many\nlink-layer protocols have been designed for point-to-point links; the point-to-point pro-\ntocol (PPP) and high-level data link control (HDLC) are two such protocols that we’ll\ncover later in this chapter. The second type of link, a broadcast link, can have multiple\nsending and receiving nodes all connected to the same, single, shared broadcast chan-\nnel. The term broadcast is used here because when any one node transmits a frame, the\nchannel broadcasts the frame and each of the other nodes receives a copy. Ethernet and\nwireless LANs are examples of broadcast link-layer technologies. In this section we’ll\ntake a step back from specific link-layer protocols and first examine a problem of cen-\ntral importance to the link layer: how to coordinate the access of multiple sending and\nreceiving nodes to a shared broadcast channel—the multiple access problem. Broad-\ncast channels are often used in LANs, networks that are geographically concentrated in\na single building (or on a corporate or university campus). Thus, we’ll also look at how\nmultiple access channels are used in LANs at the end of this section.\n5.3\n•\nMULTIPLE ACCESS LINKS AND PROTOCOLS\n445\n1  0  0  1\n1  0  1  1  1  0  0  0  0\n1  0  1  0  1  1  \n1  0  0  1  \n1  0  1\n0  0  0\n1  0  1  0\n1  0  0  1\n1  1  0\n0  0  0\n1  1  0  0\n1  0  0  1\n1  0  1  0\n1  0  0  1\n 0  1  1\nG\nD\nR\nFigure 5.7 \u0002 A sample CRC calculation\n\nWe are all familiar with the notion of broadcasting—television has been using\nit since its invention. But traditional television is a one-way broadcast (that is, one\nfixed node transmitting to many receiving nodes), while nodes on a computer net-\nwork broadcast channel can both send and receive. Perhaps a more apt human anal-\nogy for a broadcast channel is a cocktail party, where many people gather in a large\nroom (the air providing the broadcast medium) to talk and listen. A second good\nanalogy is something many readers will be familiar with—a classroom—where\nteacher(s) and student(s) similarly share the same, single, broadcast medium. A cen-\ntral problem in both scenarios is that of determining who gets to talk (that is, trans-\nmit into the channel), and when. As humans, we’ve evolved an elaborate set of\nprotocols for sharing the broadcast channel:\n“Give everyone a chance to speak.”\n“Don’t speak until you are spoken to.”\n“Don’t monopolize the conversation.”\n“Raise your hand if you have a question.”\n“Don’t interrupt when someone is speaking.”\n“Don’t fall asleep when someone is talking.”\nComputer networks similarly have protocols—so-called multiple access \nprotocols—by which nodes regulate their transmission into the shared broadcast\nchannel. As shown in Figure 5.8, multiple access protocols are needed in a wide\nvariety of network settings, including both wired and wireless access networks, and\nsatellite networks. Although technically each node accesses the broadcast channel\nthrough its adapter, in this section we will refer to the node as the sending and\nreceiving device. In practice, hundreds or even thousands of nodes can directly\ncommunicate over a broadcast channel.\nBecause all nodes are capable of transmitting frames, more than two nodes\ncan transmit frames at the same time. When this happens, all of the nodes receive\nmultiple frames at the same time; that is, the transmitted frames collide at all of\nthe receivers. Typically, when there is a collision, none of the receiving nodes can\nmake any sense of any of the frames that were transmitted; in a sense, the signals\nof the colliding frames become inextricably tangled together. Thus, all the frames\ninvolved in the collision are lost, and the broadcast channel is wasted during the\ncollision interval. Clearly, if many nodes want to transmit frames frequently,\nmany transmissions will result in collisions, and much of the bandwidth of the\nbroadcast channel will be wasted.\nIn order to ensure that the broadcast channel performs useful work when multiple\nnodes are active, it is necessary to somehow coordinate the transmissions of the active\nnodes. This coordination job is the responsibility of the multiple access protocol. Over\nthe past 40 years, thousands of papers and hundreds of PhD dissertations have been\nwritten on multiple access protocols; a comprehensive survey of the first 20 years of\n446\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nthis body of work is [Rom 1990]. Furthermore, active research in multiple access pro-\ntocols continues due to the continued emergence of new types of links, particularly\nnew wireless links.\nOver the years, dozens of multiple access protocols have been implemented\nin a variety of link-layer technologies. Nevertheless, we can classify just about\nany multiple access protocol as belonging to one of three categories: channel\npartitioning protocols, random access protocols, and taking-turns protocols.\nWe’ll cover these categories of multiple access protocols in the following three\nsubsections.\nLet’s conclude this overview by noting that, ideally, a multiple access protocol\nfor a broadcast channel of rate R bits per second should have the following desirable\ncharacteristics:\n1. When only one node has data to send, that node has a throughput of R bps.\n2. When M nodes have data to send, each of these nodes has a throughput of\nR/M bps. This need not necessarily imply that each of the M nodes always\nShared wire\n(for example, cable access network)\nShared wireless\n(for example, WiFi)\nSatellite\nCocktail party\nHead\nend\nFigure 5.8 \u0002 Various multiple access channels\n5.3\n•\nMULTIPLE ACCESS LINKS AND PROTOCOLS\n447"
    },
    {
      "chunk_id": "c145ea5f-dd34-410c-91cd-4072c2074c4d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.3.1 Channel Partitioning Protocols",
      "original_titles": [
        "5.3.1 Channel Partitioning Protocols"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.3 Multiple Access Links and Protocols > 5.3.1 Channel Partitioning Protocols",
      "start_page": 475,
      "end_page": 475,
      "token_count": 700,
      "text": "has an instantaneous rate of R/M, but rather that each node should have an\naverage transmission rate of R/M over some suitably defined interval \nof time.\n3. The protocol is decentralized; that is, there is no master node that represents a\nsingle point of failure for the network.\n4. The protocol is simple, so that it is inexpensive to implement.\n5.3.1 Channel Partitioning Protocols\nRecall from our early discussion back in Section 1.3 that time-division multiplex-\ning (TDM) and frequency-division multiplexing (FDM) are two techniques that\ncan be used to partition a broadcast channel’s bandwidth among all nodes sharing\nthat channel. As an example, suppose the channel supports N nodes and that the\ntransmission rate of the channel is R bps. TDM divides time into time frames and\nfurther divides each time frame into N time slots. (The TDM time frame should\nnot be confused with the link-layer unit of data exchanged between sending and\nreceiving adapters, which is also called a frame. In order to reduce confusion, in\nthis subsection we’ll refer to the link-layer unit of data exchanged as a packet.)\nEach time slot is then assigned to one of the N nodes. Whenever a node has a\npacket to send, it transmits the packet’s bits during its assigned time slot in the\nrevolving TDM frame. Typically, slot sizes are chosen so that a single packet can\nbe transmitted during a slot time. Figure 5.9 shows a simple four-node TDM\nexample. Returning to our cocktail party analogy, a TDM-regulated cocktail party\nwould allow one partygoer to speak for a fixed period of time, then allow another\npartygoer to speak for the same amount of time, and so on. Once everyone had had\na chance to talk, the pattern would repeat.\nTDM is appealing because it eliminates collisions and is perfectly fair: Each\nnode gets a dedicated transmission rate of R/N bps during each frame time. How-\never, it has two major drawbacks. First, a node is limited to an average rate of\nR/N bps even when it is the only node with packets to send. A second drawback\nis that a node must always wait for its turn in the transmission sequence—again,\neven when it is the only node with a frame to send. Imagine the partygoer who is\nthe only one with anything to say (and imagine that this is the even rarer circum-\nstance where everyone wants to hear what that one person has to say). Clearly,\nTDM would be a poor choice for a multiple access protocol for this particular\nparty.\nWhile TDM shares the broadcast channel in time, FDM divides the R bps chan-\nnel into different frequencies (each with a bandwidth of R/N) and assigns each fre-\nquency to one of the N nodes. FDM thus creates N smaller channels of R/N bps out\nof the single, larger R bps channel. FDM shares both the advantages and drawbacks\nof TDM. It avoids collisions and divides the bandwidth fairly among the N nodes.\nHowever, FDM also shares a principal disadvantage with TDM—a node is limited\nto a bandwidth of R/N, even when it is the only node with packets to send.\n448\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS"
    },
    {
      "chunk_id": "0f27b3d9-994d-4f38-82a5-5b2eee8c816c",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.3.2 Random Access Protocols",
      "original_titles": [
        "5.3.2 Random Access Protocols"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.3 Multiple Access Links and Protocols > 5.3.2 Random Access Protocols",
      "start_page": 476,
      "end_page": 485,
      "token_count": 5031,
      "text": "A third channel partitioning protocol is code division multiple access\n(CDMA). While TDM and FDM assign time slots and frequencies, respectively,\nto the nodes, CDMA assigns a different code to each node. Each node then uses\nits unique code to encode the data bits it sends. If the codes are chosen carefully,\nCDMA networks have the wonderful property that different nodes can transmit\nsimultaneously and yet have their respective receivers correctly receive a sender’s\nencoded data bits (assuming the receiver knows the sender’s code) in spite of\ninterfering transmissions by other nodes. CDMA has been used in military sys-\ntems for some time (due to its anti-jamming properties) and now has widespread\ncivilian use, particularly in cellular telephony. Because CDMA’s use is so tightly\ntied to wireless channels, we’ll save our discussion of the technical details of\nCDMA until Chapter 6. For now, it will suffice to know that CDMA codes, like\ntime slots in TDM and frequencies in FDM, can be allocated to the multiple\naccess channel users.\n5.3.2 Random Access Protocols\nThe second broad class of multiple access protocols are random access protocols.\nIn a random access protocol, a transmitting node always transmits at the full rate\nof the channel, namely, R bps. When there is a collision, each node involved in\nthe collision repeatedly retransmits its frame (that is, packet) until its frame gets\n4KHz\nFDM\nTDM\nLink\n4KHz\nSlot\nAll slots labeled “2” are dedicated\nto a specific sender-receiver pair.\nFrame\n1\n2\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nKey:\nFigure 5.9 \u0002 A four-node TDM and FDM example\n5.3\n•\nMULTIPLE ACCESS LINKS AND PROTOCOLS\n449\n\nthrough without a collision. But when a node experiences a collision, it doesn’t\nnecessarily retransmit the frame right away. Instead it waits a random delay\nbefore retransmitting the frame. Each node involved in a collision chooses inde-\npendent random delays. Because the random delays are independently chosen, it\nis possible that one of the nodes will pick a delay that is sufficiently less than the\ndelays of the other colliding nodes and will therefore be able to sneak its frame\ninto the channel without a collision.\nThere are dozens if not hundreds of random access protocols described in the\nliterature [Rom 1990; Bertsekas 1991]. In this section we’ll describe a few of the\nmost commonly used random access protocols—the ALOHA protocols [Abramson\n1970; Abramson 1985; Abramson 2009] and the carrier sense multiple access\n(CSMA) protocols [Kleinrock 1975b]. Ethernet [Metcalfe 1976] is a popular and\nwidely deployed CSMA protocol.\nSlotted ALOHA\nLet’s begin our study of random access protocols with one of the simplest random\naccess protocols, the slotted ALOHA protocol. In our description of slotted\nALOHA, we assume the following:\n•\nAll frames consist of exactly L bits.\n•\nTime is divided into slots of size L/R seconds (that is, a slot equals the time to\ntransmit one frame).\n•\nNodes start to transmit frames only at the beginnings of slots.\n•\nThe nodes are synchronized so that each node knows when the slots begin.\n•\nIf two or more frames collide in a slot, then all the nodes detect the collision\nevent before the slot ends.\nLet p be a probability, that is, a number between 0 and 1. The operation of slotted\nALOHA in each node is simple:\n•\nWhen the node has a fresh frame to send, it waits until the beginning of the next\nslot and transmits the entire frame in the slot.\n•\nIf there isn’t a collision, the node has successfully transmitted its frame and thus\nneed not consider retransmitting the frame. (The node can prepare a new frame\nfor transmission, if it has one.)\n•\nIf there is a collision, the node detects the collision before the end of the slot. The\nnode retransmits its frame in each subsequent slot with probability p until the\nframe is transmitted without a collision.\nBy retransmitting with probability p, we mean that the node effectively tosses\na biased coin; the event heads corresponds to “retransmit,” which occurs with\n450\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nprobability p. The event tails corresponds to “skip the slot and toss the coin again\nin the next slot”; this occurs with probability (1 – p). All nodes involved in the col-\nlision toss their coins independently.\nSlotted ALOHA would appear to have many advantages. Unlike channel parti-\ntioning, slotted ALOHA allows a node to transmit continuously at the full rate, R,\nwhen that node is the only active node. (A node is said to be active if it has frames\nto send.) Slotted ALOHA is also highly decentralized, because each node detects\ncollisions and independently decides when to retransmit. (Slotted ALOHA does,\nhowever, require the slots to be synchronized in the nodes; shortly we’ll discuss an\nunslotted version of the ALOHA protocol, as well as CSMA protocols, none of which\nrequire such synchronization.) Slotted ALOHA is also an extremely simple protocol.\nSlotted ALOHA works well when there is only one active node, but how effi-\ncient is it when there are multiple active nodes? There are two possible efficiency\nconcerns here. First, as shown in Figure 5.10, when there are multiple active\nnodes, a certain fraction of the slots will have collisions and will therefore be\n“wasted.” The second concern is that another fraction of the slots will be empty\nbecause all active nodes refrain from transmitting as a result of the probabilistic\ntransmission policy. The only “unwasted” slots will be those in which exactly \none node transmits. A slot in which exactly one node transmits is said to be \na successful slot. The efficiency of a slotted multiple access protocol is defined \nto be the long-run fraction of successful slots in the case when there are a large\nnumber of active nodes, each always having a large number of frames to send. \nNode 3\nKey:\nC = Collision slot\nE = Empty slot\nS = Successful slot\nNode 2\nNode 1\n2\n2\n2\n1\n1\n1\n1\n3\n3\n3\nTime\nC\nE\nC\nS\nE\nC\nE\nS\nS\nFigure 5.10 \u0002 Nodes 1, 2, and 3 collide in the first slot. Node 2 finally\nsucceeds in the fourth slot, node 1 in the eighth slot, and\nnode 3 in the ninth slot\n5.3\n•\nMULTIPLE ACCESS LINKS AND PROTOCOLS\n451\n\nNote that if no form of access control were used, and each node were to immedi-\nately retransmit after each collision, the efficiency would be zero. Slotted ALOHA\nclearly increases the efficiency beyond zero, but by how much?\nWe now proceed to outline the derivation of the maximum efficiency of slot-\nted ALOHA. To keep this derivation simple, let’s modify the protocol a little and\nassume that each node attempts to transmit a frame in each slot with probability p.\n(That is, we assume that each node always has a frame to send and that the node\ntransmits with probability p for a fresh frame as well as for a frame that has\nalready suffered a collision.) Suppose there are N nodes. Then the probability that\na given slot is a successful slot is the probability that one of the nodes transmits\nand that the remaining N – 1 nodes do not transmit. The probability that a given\nnode transmits is p; the probability that the remaining nodes do not transmit is \n(1 – p)N\u00041. Therefore the probability a given node has a success is p(1 – p)N\u00041.\nBecause there are N nodes, the probability that any one of the N nodes has a suc-\ncess is Np(1 – p)N\u00041.\nThus, when there are N active nodes, the efficiency of slotted ALOHA is\nNp(1 – p)N\u00041. To obtain the maximum efficiency for N active nodes, we have to find\nthe p* that maximizes this expression. (See the homework problems for a general\noutline of this derivation.) And to obtain the maximum efficiency for a large num-\nber of active nodes, we take the limit of Np*(1 – p*)N\u00041 as N approaches infinity.\n(Again, see the homework problems.) After performing these calculations, we’ll\nfind that the maximum efficiency of the protocol is given by 1/e \u0005 0.37. That is,\nwhen a large number of nodes have many frames to transmit, then (at best) only \n37 percent of the slots do useful work. Thus the effective transmission rate of the\nchannel is not R bps but only 0.37 R bps! A similar analysis also shows that 37 percent\nof the slots go empty and 26 percent of slots have collisions. Imagine the poor \nnetwork administrator who has purchased a 100-Mbps slotted ALOHA system,\nexpecting to be able to use the network to transmit data among a large number of\nusers at an aggregate rate of, say, 80 Mbps! Although the channel is capable of trans-\nmitting a given frame at the full channel rate of 100 Mbps, in the long run, the \nsuccessful throughput of this channel will be less than 37 Mbps.\nAloha\nThe slotted ALOHA protocol required that all nodes synchronize their transmis-\nsions to start at the beginning of a slot. The first ALOHA protocol [Abramson\n1970] was actually an unslotted, fully decentralized protocol. In pure ALOHA,\nwhen a frame first arrives (that is, a network-layer datagram is passed down from\nthe network layer at the sending node), the node immediately transmits the frame\nin its entirety into the broadcast channel. If a transmitted frame experiences a colli-\nsion with one or more other transmissions, the node will then immediately (after\ncompletely transmitting its collided frame) retransmit the frame with probability p.\nOtherwise, the node waits for a frame transmission time. After this wait, it then\n452\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\ntransmits the frame with probability p, or waits (remaining idle) for another frame\ntime with probability 1 – p.\nTo determine the maximum efficiency of pure ALOHA, we focus on an indi-\nvidual node. We’ll make the same assumptions as in our slotted ALOHA analysis\nand take the frame transmission time to be the unit of time. At any given time, the\nprobability that a node is transmitting a frame is p. Suppose this frame begins trans-\nmission at time t0. As shown in Figure 5.11, in order for this frame to be success-\nfully transmitted, no other nodes can begin their transmission in the interval of time\n[t0 – 1, t0]. Such a transmission would overlap with the beginning of the transmis-\nsion of node i’s frame. The probability that all other nodes do not begin a transmis-\nsion in this interval is (1 – p)N\u00041. Similarly, no other node can begin a transmission\nwhile node i is transmitting, as such a transmission would overlap with the latter\npart of node i’s transmission. The probability that all other nodes do not begin a\ntransmission in this interval is also (1 – p)N\u00041. Thus, the probability that a given\nnode has a successful transmission is p(1 – p)2(N\u00041). By taking limits as in the slotted\nALOHA case, we find that the maximum efficiency of the pure ALOHA protocol is\nonly 1/(2e)—exactly half that of slotted ALOHA. This then is the price to be paid\nfor a fully decentralized ALOHA protocol.\nCarrier Sense Multiple Access (CSMA)\nIn both slotted and pure ALOHA, a node’s decision to transmit is made independ-\nently of the activity of the other nodes attached to the broadcast channel. In particu-\nlar, a node neither pays attention to whether another node happens to be transmitting\nwhen it begins to transmit, nor stops transmitting if another node begins to interfere\nwith its transmission. In our cocktail party analogy, ALOHA protocols are quite like\nTime\nWill overlap\nwith start of \ni ’s frame\nt0 – 1\nt0\nt0 + 1\nWill overlap\nwith end of \ni’s frame\nNode i frame\nFigure 5.11 \u0002 Interfering transmissions in pure ALOHA\n5.3\n•\nMULTIPLE ACCESS LINKS AND PROTOCOLS\n453\n\na boorish partygoer who continues to chatter away regardless of whether other peo-\nple are talking. As humans, we have human protocols that allow us not only to\nbehave with more civility, but also to decrease the amount of time spent “colliding”\nwith each other in conversation and, consequently, to increase the amount of data\nwe exchange in our conversations. Specifically, there are two important rules for\npolite human conversation:\n•\nListen before speaking. If someone else is speaking, wait until they are finished.\nIn the networking world, this is called carrier sensing—a node listens to the\nchannel before transmitting. If a frame from another node is currently being\ntransmitted into the channel, a node then waits until it detects no transmissions\nfor a short amount of time and then begins transmission.\n•\nIf someone else begins talking at the same time, stop talking. In the networking\nworld, this is called collision detection—a transmitting node listens to the channel\nwhile it is transmitting. If it detects that another node is transmitting an interfering\n454\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nNORM ABRAMSON AND ALOHANET\nNorm Abramson, a PhD engineer, had a passion for surfing and an interest in packet\nswitching. This combination of interests brought him to the University of Hawaii in\n1969. Hawaii consists of many mountainous islands, making it difficult to install and\noperate land-based networks. When not surfing, Abramson thought about how to\ndesign a network that does packet switching over radio. The network he designed\nhad one central host and several secondary nodes scattered over the Hawaiian\nIslands. The network had two channels, each using a different frequency band. The\ndownlink channel broadcasted packets from the central host to the secondary hosts;\nand the upstream channel sent packets from the secondary hosts to the central host. In\naddition to sending informational packets, the central host also sent on the down-\nstream channel an acknowledgment for each packet successfully received from the\nsecondary hosts.\nBecause the secondary hosts transmitted packets in a decentralized fashion, colli-\nsions on the upstream channel inevitably occurred. This observation led Abramson to\ndevise the pure ALOHA protocol, as described in this chapter. In 1970, with contin-\nued funding from ARPA, Abramson connected his ALOHAnet to the ARPAnet.\nAbramson’s work is important not only because it was the first example of a radio\npacket network, but also because it inspired Bob Metcalfe. A few years later,\nMetcalfe modified the ALOHA protocol to create the CSMA/CD protocol and the\nEthernet LAN.\nCASE HISTORY\n\nframe, it stops transmitting and waits a random amount of time before repeating\nthe sense-and-transmit-when-idle cycle.\nThese two rules are embodied in the family of carrier sense multiple access\n(CSMA) and CSMA with collision detection (CSMA/CD) protocols [Kleinrock\n1975b; Metcalfe 1976; Lam 1980; Rom 1990]. Many variations on CSMA and\nCSMA/CD have been proposed. Here, we’ll consider a few of the most important,\nand fundamental, characteristics of CSMA and CSMA/CD.\nThe first question that you might ask about CSMA is why, if all nodes per-\nform carrier sensing, do collisions occur in the first place? After all, a node will\nrefrain from transmitting whenever it senses that another node is transmitting. The\nanswer to the question can best be illustrated using space-time diagrams [Molle\n1987]. Figure 5.12 shows a space-time diagram of four nodes (A, B, C, D)\nattached to a linear broadcast bus. The horizontal axis shows the position of each\nnode in space; the vertical axis represents time.\nA\nTime\nTime\nSpace\nt 0\nt 1\nB\nC\nD\nFigure 5.12 \u0002 Space-time diagram of two CSMA nodes with colliding\ntransmissions\n5.3\n•\nMULTIPLE ACCESS LINKS AND PROTOCOLS\n455\n\nAt time t0, node B senses the channel is idle, as no other nodes are currently\ntransmitting. Node B thus begins transmitting, with its bits propagating in both\ndirections along the broadcast medium. The downward propagation of B’s bits in\nFigure 5.12 with increasing time indicates that a nonzero amount of time is needed\nfor B’s bits actually to propagate (albeit at near the speed of light) along the\nbroadcast medium. At time t1 (t1 > t0), node D has a frame to send. Although node\nB is currently transmitting at time t1, the bits being transmitted by B have yet to\nreach D, and thus D senses the channel idle at t1. In accordance with the CSMA\nprotocol, D thus begins transmitting its frame. A short time later, B’s transmission\nbegins to interfere with D’s transmission at D. From Figure 5.12, it is evident that\nthe end-to-end channel propagation delay of a broadcast channel—the time it\ntakes for a signal to propagate from one of the nodes to another—will play a cru-\ncial role in determining its performance. The longer this propagation delay, the\nlarger the chance that a carrier-sensing node is not yet able to sense a transmission\nthat has already begun at another node in the network.\nCarrier Sense Multiple Access with Collision Dection (CSMA/CD)\nIn Figure 5.12, nodes do not perform collision detection; both B and D continue to\ntransmit their frames in their entirety even though a collision has occurred. When a\nnode performs collision detection, it ceases transmission as soon as it detects a\ncollision. Figure 5.13 shows the same scenario as in Figure 5.12, except that the two\nnodes each abort their transmission a short time after detecting a collision. Clearly,\nadding collision detection to a multiple access protocol will help protocol perform-\nance by not transmitting a useless, damaged (by interference with a frame from\nanother node) frame in its entirety.\nBefore analyzing the CSMA/CD protocol, let us now summarize its operation\nfrom the perspective of an adapter (in a node) attached to a broadcast channel:\n1. The adapter obtains a datagram from the network layer, prepares a link-layer\nframe, and puts the frame adapter buffer.\n2. If the adapter senses that the channel is idle (that is, there is no signal energy\nentering the adapter from the channel), it starts to transmit the frame. If, on the\nother hand, the adapter senses that the channel is busy, it waits until it senses\nno signal energy and then starts to transmit the frame.\n3. While transmitting, the adapter monitors for the presence of signal energy\ncoming from other adapters using the broadcast channel.\n4. If the adapter transmits the entire frame without detecting signal energy from\nother adapters, the adapter is finished with the frame. If, on the other hand, the\nadapter detects signal energy from other adapters while transmitting, it aborts\nthe transmission (that is, it stops transmitting its frame).\n5. After aborting, the adapter waits a random amount of time and then returns \nto step 2.\n456\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nThe need to wait a random (rather than fixed) amount of time is hopefully clear—if\ntwo nodes transmitted frames at the same time and then both waited the same fixed\namount of time, they’d continue colliding forever. But what is a good interval \nof time from which to choose the random backoff time? If the interval is large and\nthe number of colliding nodes is small, nodes are likely to wait a large amount \nof time (with the channel remaining idle) before repeating the sense-and-transmit-\nwhen-idle step. On the other hand, if the interval is small and the number of collid-\ning nodes is large, it’s likely that the chosen random values will be nearly the same,\nand transmitting nodes will again collide. What we’d like is an interval that is short\nwhen the number of colliding nodes is small, and long when the number of collid-\ning nodes is large.\nThe binary exponential backoff algorithm, used in Ethernet as well as in\nDOCSIS cable network multiple access protocols [DOCSIS 2011], elegantly solves\nthis problem. Specifically, when transmitting a frame that has already experienced\nA\nTime\nTime\nCollision\ndetect/abort\ntime\nSpace\nt 0\nt 1\nB\nC\nD\nFigure 5.13 \u0002 CSMA with collision detection\n5.3\n•\nMULTIPLE ACCESS LINKS AND PROTOCOLS\n457\n\nn collisions, a node chooses the value of K at random from {0, 1, 2, . . . . 2n 1}. Thus,\nthe more collisions experienced by a frame, the larger the interval from which K is\nchosen. For Ethernet, the actual amount of time a node waits is K\n512 bit times (i.e.,\nK times the amount of time needed to send 512 bits into the Ethernet) and the maxi-\nmum value that n can take is capped at 10.\nLet’s look at an example. Suppose that a node attempts to transmit a frame for the\nfirst time and while transmitting it detects a collision. The node then chooses K\n0\nwith probability 0.5 or chooses K\n1 with probability 0.5. If the node chooses K\n0, then it immediately begins sensing the channel. If the node chooses K\n1, it waits\n512 bit times (e.g., 0.01 microseconds for a 100 Mbps Ethernet) before beginning\nthe sense-and-transmit-when-idle cycle. After a second collision, K is chosen with\nequal probability from {0,1,2,3}. After three collisions, K is chosen with equal prob-\nability from {0,1,2,3,4,5,6,7}. After 10 or more collisions, K is chosen with equal\nprobability from {0,1,2, . . . , 1023}. Thus, the size of the sets from which K is cho-\nsen grows exponentially with the number of collisions; for this reason this algorithm\nis referred to as binary exponential backoff.\nWe also note here that each time a node prepares a new frame for transmission,\nit runs the CSMA/CD algorithm, not taking into account any collisions that may\nhave occurred in the recent past. So it is possible that a node with a new frame will\nimmediately be able to sneak in a successful transmission while several other nodes\nare in the exponential backoff state.\nCSMA/CD Efficiency\nWhen only one node has a frame to send, the node can transmit at the full channel\nrate (e.g., for Ethernet typical rates are 10 Mbps, 100 Mbps, or 1 Gbps). However, if\nmany nodes have frames to transmit, the effective transmission rate of the channel\ncan be much less. We define the efficiency of CSMA/CD to be the long-run fraction\nof time during which frames are being transmitted on the channel without collisions\nwhen there is a large number of active nodes, with each node having a large number\nof frames to send. In order to present a closed-form approximation of the efficiency\nof Ethernet, let dprop denote the maximum time it takes signal energy to propagate\nbetween any two adapters. Let dtrans be the time to transmit a maximum-size frame\n(approximately 1.2 msecs for a 10 Mbps Ethernet). A derivation of the efficiency of\nCSMA/CD is beyond the scope of this book (see [Lam 1980] and [Bertsekas 1991]).\nHere we simply state the following approximation:\nWe see from this formula that as dprop approaches 0, the efficiency approaches 1. This\nmatches our intuition that if the propagation delay is zero, colliding nodes will abort\nEfficiency =\n1\n1 + 5dprop>dtrans\n=\n=\n=\n=\n\u0003\n-\n458\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS"
    },
    {
      "chunk_id": "9969f2eb-32ad-4070-b3c0-327728f7ce1e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.3.3 Taking-Turns Protocols",
      "original_titles": [
        "5.3.3 Taking-Turns Protocols"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.3 Multiple Access Links and Protocols > 5.3.3 Taking-Turns Protocols",
      "start_page": 486,
      "end_page": 486,
      "token_count": 759,
      "text": "immediately without wasting the channel. Also, as dtrans becomes very large, effi-\nciency approaches 1. This is also intuitive because when a frame grabs the channel,\nit will hold on to the channel for a very long time; thus, the channel will be doing\nproductive work most of the time.\n5.3.3 Taking-Turns Protocols\nRecall that two desirable properties of a multiple access protocol are (1) when only\none node is active, the active node has a throughput of R bps, and (2) when M nodes\nare active, then each active node has a throughput of nearly R/M bps. The ALOHA\nand CSMA protocols have this first property but not the second. This has motivated\nresearchers to create another class of protocols—the taking-turns protocols. As\nwith random access protocols, there are dozens of taking-turns protocols, and each\none of these protocols has many variations. We’ll discuss two of the more important\nprotocols here. The first one is the polling protocol. The polling protocol requires\none of the nodes to be designated as a master node. The master node polls each of\nthe nodes in a round-robin fashion. In particular, the master node first sends a mes-\nsage to node 1, saying that it (node 1) can transmit up to some maximum number of\nframes. After node 1 transmits some frames, the master node tells node 2 it (node 2)\ncan transmit up to the maximum number of frames. (The master node can determine\nwhen a node has finished sending its frames by observing the lack of a signal on the\nchannel.) The procedure continues in this manner, with the master node polling each\nof the nodes in a cyclic manner.\nThe polling protocol eliminates the collisions and empty slots that plague\nrandom access protocols. This allows polling to achieve a much higher efficiency.\nBut it also has a few drawbacks. The first drawback is that the protocol introduces a\npolling delay—the amount of time required to notify a node that it can transmit. If,\nfor example, only one node is active, then the node will transmit at a rate less than \nR bps, as the master node must poll each of the inactive nodes in turn each time the\nactive node has sent its maximum number of frames. The second drawback, which\nis potentially more serious, is that if the master node fails, the entire channel\nbecomes inoperative. The 802.15 protocol and the Bluetooth protocol we will study\nin Section 6.3 are examples of polling protocols.\nThe second taking-turns protocol is the token-passing protocol. In this protocol\nthere is no master node. A small, special-purpose frame known as a token is exchanged\namong the nodes in some fixed order. For example, node 1 might always send the token\nto node 2, node 2 might always send the token to node 3, and node N might always send\nthe token to node 1. When a node receives a token, it holds onto the token only if it has\nsome frames to transmit; otherwise, it immediately forwards the token to the next node.\nIf a node does have frames to transmit when it receives the token, it sends up to a max-\nimum number of frames and then forwards the token to the next node. Token passing is\ndecentralized and highly efficient. But it has its problems as well. For example, the fail-\nure of one node can crash the entire channel. Or if a node accidentally neglects to\n5.3\n•\nMULTIPLE ACCESS LINKS AND PROTOCOLS\n459"
    },
    {
      "chunk_id": "fb8c39fd-e810-40bf-b207-a147a5feb4b9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.3.4 DOCSIS: The Link-Layer Protocol for Cable Internet Access",
      "original_titles": [
        "5.3.4 DOCSIS: The Link-Layer Protocol for Cable Internet Access"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.3 Multiple Access Links and Protocols > 5.3.4 DOCSIS: The Link-Layer Protocol for Cable Internet Access",
      "start_page": 487,
      "end_page": 487,
      "token_count": 625,
      "text": "release the token, then some recovery procedure must be invoked to get the token back\nin circulation. Over the years many token-passing protocols have been developed,\nincluding the fiber distributed data interface (FDDI) protocol [Jain 1994] and the IEEE\n802.5 token ring protocol [IEEE 802.5 2012], and each one had to address these as well\nas other sticky issues.\n5.3.4 DOCSIS: The Link-Layer Protocol for Cable \nInternet Access\nIn the previous three subsections, we’ve learned about three broad classes of multi-\nple access protocols: channel partitioning protocols, random access protocols, and\ntaking turns protocols. A cable access network will make for an excellent case study\nhere, as we’ll find aspects of each of these three classes of multiple access protocols\nwith the cable access network!\nRecall from Section 1.2.1, that a cable access network typically connects sev-\neral thousand residential cable modems to a cable modem termination system\n(CMTS) at the cable network headend. The Data-Over-Cable Service Interface\nSpecifications (DOCSIS) [DOCSIS 2011] specifies the cable data network architec-\nture and its protocols. DOCSIS uses FDM to divide the downstream (CMTS to\nmodem) and upstream (modem to CMTS) network segments into multiple fre-\nquency channels. Each downstream channel is 6 MHz wide, with a maximum\nthroughput of approximately 40 Mbps per channel (although this data rate is seldom\nseen at a cable modem in practice); each upstream channel has a maximum channel\nwidth of 6.4 MHz, and a maximum upstream throughput of approximately 30 Mbps.\nEach upstream and downstream channel is a broadcast channel. Frames transmitted\non the downstream channel by the CMTS are received by all cable modems receiv-\ning that channel; since there is just a single CMTS transmitting into the downstream\nchannel, however, there is no multiple access problem. The upstream direction,\nhowever, is more interesting and technically challenging, since multiple cable\nmodems share the same upstream channel (frequency) to the CMTS, and thus colli-\nsions can potentially occur.\nAs illustrated in Figure 5.14, each upstream channel is divided into intervals of\ntime (TDM-like), each containing a sequence of mini-slots during which cable\nmodems can transmit to the CMTS. The CMTS explicitly grants permission to indi-\nvidual cable modems to transmit during specific mini-slots. The CMTS accom-\nplishes this by sending a control message known as a MAP message on a\ndownstream channel to specify which cable modem (with data to send) can transmit\nduring which mini-slot for the interval of time specified in the control message.\nSince mini-slots are explicitly allocated to cable modems, the CMTS can ensure\nthere are no colliding transmissions during a mini-slot.\nBut how does the CMTS know which cable modems have data to send in the\nfirst place? This is accomplished by having cable modems send mini-slot-request\nframes to the CMTS during a special set of interval mini-slots that are dedicated\n460\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS"
    },
    {
      "chunk_id": "7daf7415-b99a-4ccc-ba65-cf666c2f9863",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.4 Switched Local Area Networks",
      "original_titles": [
        "5.4 Switched Local Area Networks"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.4 Switched Local Area Networks",
      "start_page": 488,
      "end_page": 488,
      "token_count": 357,
      "text": "for this purpose, as shown in Figure 5.14. These mini-slot-request frames are\ntransmitted in a random access manner and so may collide with each other. A\ncable modem can neither sense whether the upstream channel is busy nor detect\ncollisions. Instead, the cable modem infers that its mini-slot-request frame experi-\nenced a collision if it does not receive a response to the requested allocation in the\nnext downstream control message. When a collision is inferred, a cable modem\nuses binary exponential backoff to defer the retransmission of its mini-slot\n-request frame to a future time slot. When there is little traffic on the upstream\nchannel, a cable modem may actually transmit data frames during slots nominally\nassigned for mini-slot-request frames (and thus avoid having to wait for a mini-slot\nassignment).\nA cable access network thus serves as a terrific example of multiple access pro-\ntocols in action—FDM, TDM, random access, and centrally allocated time slots all\nwithin one network!\n5.4 Switched Local Area Networks\nHaving covered broadcast networks and multiple access protocols in the previ-\nous section, let’s turn our attention next to switched local networks. Figure 5.15\nshows a switched local network connecting three departments, two servers and a\nrouter with four switches. Because these switches operate at the link layer, they\nswitch link-layer frames (rather than network-layer datagrams), don’t recognize\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n461\nFigure 5.14 \u0002 Upstream and downstream channels between CMTS and\ncable modems\nResidences with\ncable modems\nMinislots\ncontaining\nminislot\nrequest frames\nAssigned minislots\ncontaining cable\nmodem upstream\ndata frames\nCable head end\nMAP frame for \ninterval [t1,t2]\nCMTS\nDownstream channel i\nUpstream channel j\nt1\nt2"
    },
    {
      "chunk_id": "115bf341-5370-4685-a72e-b19a30a7547c",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.4.1 Link-Layer Addressing and ARP",
      "original_titles": [
        "5.4.1 Link-Layer Addressing and ARP"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.4 Switched Local Area Networks > 5.4.1 Link-Layer Addressing and ARP",
      "start_page": 489,
      "end_page": 495,
      "token_count": 3573,
      "text": "network-layer addresses, and don’t use routing algorithms like RIP or OSPF to\ndetermine paths through the network of layer-2 switches. Instead of using IP\naddresses, we will soon see that they use link-layer addresses to forward link-\nlayer frames through the network of switches. We’ll begin our study of switched\nLANs by first covering link-layer addressing (Section 5.4.1). We then examine\nthe celebrated Ethernet protocol (Section 5.5.2). After examining link-layer\naddressing and Ethernet, we’ll look at how link-layer switches operate (Section\n5.4.3), and then see (Section 5.4.4) how these switches are often used to build\nlarge-scale LANs.\n5.4.1 Link-Layer Addressing and ARP\nHosts and routers have link-layer addresses. Now you might find this surprising,\nrecalling from Chapter 4 that hosts and routers have network-layer addresses as\nwell. You might be asking, why in the world do we need to have addresses at both\nthe network and link layers? In addition to describing the syntax and function of the\nlink-layer addresses, in this section we hope to shed some light on why the two layers\n462\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nMail\nserver\nTo external\ninternet\n1 Gbps\n1\n2\n3\n4\n5\n6\n1 Gbps\n1 Gbps\nElectrical Engineering\nComputer Science\n100 Mbps\n(fiber)\n100 Mbps\n(fiber)\n100 Mbps\n(fiber)\nMixture of 10 Mbps,\n100 Mbps, 1 Gbps,\nCat 5 cable\nWeb\nserver\nComputer Engineering\nFigure 5.15 \u0002 An institutional network connected together by four switches\n\nof addresses are useful and, in fact, indispensable. We’ll also cover the Address Res-\nolution Protocol (ARP), which provides a mechanism to translate IP addresses to\nlink-layer addresses.\nMAC Addresses\nIn truth, it is not hosts and routers that have link-layer addresses but rather their\nadapters (that is, network interfaces) that have link-layer addresses. A host or\nrouter with multiple network interfaces will thus have multiple link-layer\naddresses associated with it, just as it would also have multiple IP addresses asso-\nciated with it. It's important to note, however, that link-layer switches do not have\nlink-layer addresses associated with their interfaces that connect to hosts and\nrouters. This is because the job of the link-layer switch is to carry datagrams\nbetween hosts and routers; a switch does this job transparently, that is, without the\nhost or router having to explicitly address the frame to the intervening switch.\nThis is illustrated in Figure 5.16. A link-layer address is variously called a LAN\naddress, a physical address, or a MAC address. Because MAC address seems to\nbe the most popular term, we’ll henceforth refer to link-layer addresses as MAC\naddresses. For most LANs (including Ethernet and 802.11 wireless LANs), the\nMAC address is 6 bytes long, giving 248 possible MAC addresses. As shown in\nFigure 5.16, these 6-byte addresses are typically expressed in hexadecimal nota-\ntion, with each byte of the address expressed as a pair of hexadecimal numbers.\nAlthough MAC addresses were designed to be permanent, it is now possible to\n88-B2-2F-54-1A-0F\n5C-66-AB-90-75-B1\n1A-23-F9-CD-06-9B\n49-BD-D2-C7-56-2A\nFigure 5.16 \u0002 Each interface connected to a LAN has a unique MAC\naddress\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n463\n\nchange an adapter’s MAC address via software. For the rest of this section, however,\nwe’ll assume that an adapter’s MAC address is fixed.\nOne interesting property of MAC addresses is that no two adapters have the\nsame address. This might seem surprising given that adapters are manufactured in\nmany countries by many companies. How does a company manufacturing\nadapters in Taiwan make sure that it is using different addresses from a company\nmanufacturing adapters in Belgium? The answer is that the IEEE manages the\nMAC address space. In particular, when a company wants to manufacture\nadapters, it purchases a chunk of the address space consisting of 224 addresses for\na nominal fee. IEEE allocates the chunk of 224 addresses by fixing the first 24 bits\nof a MAC address and letting the company create unique combinations of the last\n24 bits for each adapter.\nAn adapter’s MAC address has a flat structure (as opposed to a hierarchical\nstructure) and doesn’t change no matter where the adapter goes. A laptop with an\nEthernet interface always has the same MAC address, no matter where the com-\nputer goes. A smartphone with an 802.11 interface always has the same MAC\naddress, no matter where the smartphone goes. Recall that, in contrast, IP addresses\nhave a hierarchical structure (that is, a network part and a host part), and a host’s\nIP addresses needs to be changed when the host moves, i.e, changes the network\nto which it is attached. An adapter’s MAC address is analogous to a person’s\nsocial security number, which also has a flat addressing structure and which \ndoesn’t change no matter where the person goes. An IP address is analogous to a\nperson’s postal address, which is hierarchical and which must be changed when-\never a person moves. Just as a person may find it useful to have both a postal\naddress and a social security number, it is useful for a host and router interfaces to\nhave both a network-layer address and a MAC address.\nWhen an adapter wants to send a frame to some destination adapter, the send-\ning adapter inserts the destination adapter’s MAC address into the frame and then\nsends the frame into the LAN. As we will soon see, a switch occassionally broad-\ncasts an incoming frame onto all of its interfaces. We’ll see in Chapter 6 that\n802.11 also broadcasts frames. Thus, an adapter may receive a frame that isn’t\naddressed to it. Thus, when an adapter receives a frame, it will check to see\nwhether the destination MAC address in the frame matches its own MAC address.\nIf there is a match, the adapter extracts the enclosed datagram and passes the data-\ngram up the protocol stack. If there isn’t a match, the adapter discards the frame,\nwithout passing the network-layer datagram up. Thus, the destination only will be\ninterrupted when the frame is received.\nHowever, sometimes a sending adapter does want all the other adapters on the\nLAN to receive and process the frame it is about to send. In this case, the sending\nadapter inserts a special MAC broadcast address into the destination address field\nof the frame. For LANs that use 6-byte addresses (such as Ethernet and 802.11),\nthe broadcast address is a string of 48 consecutive 1s (that is, FF-FF-FF-FF-FF-\nFF in hexadecimal notation).\n464\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nAddress Resolution Protocol (ARP)\nBecause there are both network-layer addresses (for example, Internet IP addresses)\nand link-layer addresses (that is, MAC addresses), there is a need to translate\nbetween them. For the Internet, this is the job of the Address Resolution Protocol\n(ARP) [RFC 826].\nTo understand the need for a protocol such as ARP, consider the network shown\nin Figure 5.17. In this simple example, each host and router has a single IP address\nand single MAC address. As usual, IP addresses are shown in dotted-decimal notation\nand MAC addresses are shown in hexadecimal notation. For the purposes of this\ndiscussion, we will assume in this section that the switch broadcasts all frames; that\nis, whenever a switch receives a frame on one interface, it forwards the frame on all\nof its other interfaces. In the next section, we will provide a more accurate explana-\ntion of how switches operate.\nNow suppose that the host with IP address 222.222.222.220 wants to send an IP\ndatagram to host 222.222.222.222. In this example, both the source and destination\nare in the same subnet, in the addressing sense of Section 4.4.2. To send a datagram,\nthe source must give its adapter not only the IP datagram but also the MAC address\nfor destination 222.222.222.222. The sending adapter will then construct a link-\nlayer frame containing the destination’s MAC address and send the frame into \nthe LAN.\nKEEPING THE LAYERS INDEPENDENT\nThere are several reasons why hosts and router interfaces have MAC addresses in addition\nto network-layer addresses. First, LANs are designed for arbitrary network-layer protocols,\nnot just for IP and the Internet. If adapters were assigned IP addresses rather than “neutral”\nMAC addresses, then adapters would not easily be able to support other network-layer\nprotocols (for example, IPX or DECnet). Second, if adapters were to use network-layer\naddresses instead of MAC addresses, the network-layer address would have to be stored\nin the adapter RAM and reconfigured every time the adapter was moved (or powered up).\nAnother option is to not use any addresses in the adapters and have each adapter pass\nthe data (typically, an IP datagram) of each frame it receives up the protocol stack. The\nnetwork layer could then check for a matching network-layer address. One problem with\nthis option is that the host would be interrupted by every frame sent on the LAN, including\nby frames that were destined for other hosts on the same broadcast LAN. In summary, in\norder for the layers to be largely independent building blocks in a network architecture,\ndifferent layers need to have their own addressing scheme. We have now seen three types\nof addresses: host names for the application layer, IP addresses for the network layer, and\nMAC addresses for the link layer.\nPRINCIPLES IN PRACTICE\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n465\n\nThe important question addressed in this section is, How does the sending\nhost determine the MAC address for the destination host with IP address\n222.222.222.222? As you might have guessed, it uses ARP. An ARP module in the\nsending host takes any IP address on the same LAN as input, and returns the corre-\nsponding MAC address. In the example at hand, sending host 222.222.222.220\nprovides its ARP module the IP address 222.222.222.222, and the ARP module\nreturns the corresponding MAC address 49-BD-D2-C7-56-2A.\nSo we see that ARP resolves an IP address to a MAC address. In many ways it\nis analogous to DNS (studied in Section 2.5), which resolves host names to IP\naddresses. However, one important difference between the two resolvers is that\nDNS resolves host names for hosts anywhere in the Internet, whereas ARP resolves\nIP addresses only for hosts and router interfaces on the same subnet. If a node in\nCalifornia were to try to use ARP to resolve the IP address for a node in Mississippi,\nARP would return with an error.\nNow that we have explained what ARP does, let’s look at how it works. Each\nhost and router has an ARP table in its memory, which contains mappings of IP\naddresses to MAC addresses. Figure 5.18 shows what an ARP table in host\n222.222.222.220 might look like. The ARP table also contains a time-to-live (TTL)\nvalue, which indicates when each mapping will be deleted from the table. Note that\na table does not necessarily contain an entry for every host and router on the subnet;\nsome may have never been entered into the table, and others may have expired. \nA typical expiration time for an entry is 20 minutes from when an entry is placed in\nan ARP table.\n466\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nIP:222.222.222.221\nIP:222.222.222.220\nIP:222.222.222.223\nIP:222.222.222.222\n5C-66-AB-90-75-B1\n1A-23-F9-CD-06-9B\n49-BD-D2-C7-56-2A\n88-B2-2F-54-1A-0F\nA\nB\nC\nFigure 5.17 \u0002 Each interface on a LAN has an IP address and a MAC\naddress\n\nNow suppose that host 222.222.222.220 wants to send a datagram that is IP-\naddressed to another host or router on that subnet. The sending host needs to\nobtain the MAC address of the destination given the IP address. This task is easy\nif the sender’s ARP table has an entry for the destination node. But what if the\nARP table doesn’t currently have an entry for the destination? In particular, sup-\npose  222.222.222.220 wants to send a datagram to 222.222.222.222. In this case,\nthe sender uses the ARP protocol to resolve the address. First, the sender con-\nstructs a special packet called an ARP packet. An ARP packet has several fields,\nincluding the sending and receiving IP and MAC addresses. Both ARP query and\nresponse packets have the same format. The purpose of the ARP query packet is\nto query all the other hosts and routers on the subnet to determine the MAC\naddress corresponding to the IP address that is being resolved.\nReturning to our example, 222.222.222.220 passes an ARP query packet to\nthe adapter along with an indication that the adapter should send the packet to the\nMAC broadcast address, namely, FF-FF-FF-FF-FF-FF. The adapter encapsulates\nthe ARP packet in a link-layer frame, uses the broadcast address for the frame’s\ndestination address, and transmits the frame into the subnet. Recalling our social\nsecurity number/postal address analogy, an ARP query is equivalent to a person\nshouting out in a crowded room of cubicles in some company (say, AnyCorp):\n“What is the social security number of the person whose postal address is Cubicle\n13, Room 112, AnyCorp, Palo Alto, California?” The frame containing the ARP\nquery is received by all the other adapters on the subnet, and (because of the\nbroadcast address) each adapter passes the ARP packet within the frame up to its\nARP module. Each of these ARP modules checks to see if its IP address matches\nthe destination IP address in the ARP packet. The one with a match sends back to\nthe querying host a response ARP packet with the desired mapping. The querying\nhost 222.222.222.220 can then update its ARP table and send its IP datagram,\nencapsulated in a link-layer frame whose destination MAC is that of the host or\nrouter responding to the earlier ARP query.\nThere are a couple of interesting things to note about the ARP protocol. First,\nthe query ARP message is sent within a broadcast frame, whereas the response ARP\nmessage is sent within a standard frame. Before reading on you should think about\nwhy this is so. Second, ARP is plug-and-play; that is, an ARP table gets built \nautomatically—it doesn’t have to be configured by a system administrator. And if \nIP Address\nMAC Address\nTTL\n222.222.222.221\n88-B2-2F-54-1A-0F\n13:45:00\n222.222.222.223\n5C-66-AB-90-75-B1\n13:52:00\nFigure 5.18 \u0002 A possible ARP table in 222.222.222.220\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n467\n\na host becomes disconnected from the subnet, its entry is eventually deleted from\nthe other ARP tables in the subnet.\nStudents often wonder if ARP is a link-layer protocol or a network-layer pro-\ntocol. As we’ve seen, an ARP packet is encapsulated within a link-layer frame\nand thus lies architecturally above the link layer. However, an ARP packet has\nfields containing link-layer addresses and thus is arguably a link-layer protocol,\nbut it also contains network-layer addresses and thus is also arguably a network-\nlayer protocol. In the end, ARP is probably best considered a protocol that strad-\ndles the boundary between the link and network layers—not fitting neatly into\nthe simple layered protocol stack we studied in Chapter 1. Such are the complex-\nities of real-world protocols!\nSending a Datagram off the Subnet\nIt should now be clear how ARP operates when a host wants to send a datagram to\nanother host on the same subnet. But now let’s look at the more complicated situa-\ntion when a host on a subnet wants to send a network-layer datagram to a host off\nthe subnet (that is, across a router onto another subnet). Let’s discuss this issue in\nthe context of Figure 5.19, which shows a simple network consisting of two subnets\ninterconnected by a router.\nThere are several interesting things to note about Figure 5.19. Each host has\nexactly one IP address and one adapter. But, as discussed in Chapter 4, a router has\nan IP address for each of its interfaces. For each router interface there is also an ARP\nmodule (in the router) and an adapter. Because the router in Figure 5.19 has two\ninterfaces, it has two IP addresses, two ARP modules, and two adapters. Of course,\neach adapter in the network has its own MAC address.\nAlso note that Subnet 1 has the network address 111.111.111/24 and that Sub-\nnet 2 has the network address 222.222.222/24. Thus all of the interfaces connected\nto Subnet 1 have addresses of the form 111.111.111.xxx and all of the interfaces\nconnected to Subnet 2 have addresses of the form 222.222.222.xxx.\n468\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nIP:111.111.111.110\nIP:111.111.111.111\nIP:111.111.111.112\nIP:222.222.222.221\nIP:222.222.222.222\n74-29-9C-E8-FF-55\nCC-49-DE-D0-AB-7D\nE6-E9-00-17-BB-4B\n1A-23-F9-CD-06-9B\nIP:222.222.222.220\n88-B2-2F-54-1A-0F\n49-BD-D2-C7-56-2A\nFigure 5.19 \u0002 Two subnets interconnected by a router"
    },
    {
      "chunk_id": "d624cb1d-4152-4d69-ace0-3fe68b701efe",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.4.2 Ethernet",
      "original_titles": [
        "5.4.2 Ethernet"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.4 Switched Local Area Networks > 5.4.2 Ethernet",
      "start_page": 496,
      "end_page": 502,
      "token_count": 4378,
      "text": "Now let’s examine how a host on Subnet 1 would send a datagram to a host\non Subnet 2. Specifically, suppose that host 111.111.111.111 wants to send an IP\ndatagram to a host 222.222.222.222. The sending host passes the datagram to its\nadapter, as usual. But the sending host must also indicate to its adapter an appro-\npriate destination MAC address. What MAC address should the adapter use? One\nmight be tempted to guess that the appropriate MAC address is that of the adapter\nfor host 222.222.222.222, namely, 49-BD-D2-C7-56-2A. This guess, however,\nwould be wrong! If the sending adapter were to use that MAC address, then none\nof the adapters on Subnet 1 would bother to pass the IP datagram up to its net-\nwork layer, since the frame’s destination address would not match the MAC\naddress of any adapter on Subnet 1. The datagram would just die and go to data-\ngram heaven.\nIf we look carefully at Figure 5.19, we see that in order for a datagram to go\nfrom 111.111.111.111 to a host on Subnet 2, the datagram must first be sent to the\nrouter interface 111.111.111.110, which is the IP address of the first-hop router\non the path to the final destination. Thus, the appropriate MAC address for the\nframe is the address of the adapter for router interface 111.111.111.110, namely,\nE6-E9-00-17-BB-4B. How does the sending host acquire the MAC address for\n111.111.111.110? By using ARP, of course! Once the sending adapter has this\nMAC address, it creates a frame (containing the datagram addressed to\n222.222.222.222) and sends the frame into Subnet 1. The router adapter on Sub-\nnet 1 sees that the link-layer frame is addressed to it, and therefore passes the\nframe to the network layer of the router. Hooray—the IP datagram has success-\nfully been moved from source host to the router! But we are not finished. We still\nhave to move the datagram from the router to the destination. The router now has\nto determine the correct interface on which the datagram is to be forwarded. As\ndiscussed in Chapter 4, this is done by consulting a forwarding table in the router.\nThe forwarding table tells the router that the datagram is to be forwarded via\nrouter interface 222.222.222.220. This interface then passes the datagram to its\nadapter, which encapsulates the datagram in a new frame and sends the frame\ninto Subnet 2. This time, the destination MAC address of the frame is indeed the\nMAC address of the ultimate destination. And how does the router obtain this\ndestination MAC address? From ARP, of course!\nARP for Ethernet is defined in RFC 826. A nice introduction to ARP is given in\nthe TCP/IP tutorial, RFC 1180. We’ll explore ARP in more detail in the homework\nproblems.\n5.4.2 Ethernet\nEthernet has pretty much taken over the wired LAN market. In the 1980s and the\nearly 1990s, Ethernet faced many challenges from other LAN technologies, includ-\ning token ring, FDDI, and ATM. Some of these other technologies succeeded in\ncapturing a part of the LAN market for a few years. But since its invention in the\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n469\nVideoNote\nSending a datagram\nbetween subnets: \nlink-layer and \nnetwork-layer\naddressing\n\nmid-1970s, Ethernet has continued to evolve and grow and has held on to its\ndominant position. Today, Ethernet is by far the most prevalent wired LAN tech-\nnology, and it is likely to remain so for the foreseeable future. One might say that\nEthernet has been to local area networking what the Internet has been to global\nnetworking.\nThere are many reasons for Ethernet’s success. First, Ethernet was the first\nwidely deployed high-speed LAN. Because it was deployed early, network admin-\nistrators became intimately familiar with Ethernet—its wonders and its quirks—\nand were reluctant to switch over to other LAN technologies when they came on\nthe scene. Second, token ring, FDDI, and ATM were more complex and expensive\nthan Ethernet, which further discouraged network administrators from switching\nover. Third, the most compelling reason to switch to another LAN technology\n(such as FDDI or ATM) was usually the higher data rate of the new technology;\nhowever, Ethernet always fought back, producing versions that operated at equal\ndata rates or higher. Switched Ethernet was also introduced in the early 1990s,\nwhich further increased its effective data rates. Finally, because Ethernet has been\nso popular, Ethernet hardware (in particular, adapters and switches) has become a\ncommodity and is remarkably cheap.\nThe original Ethernet LAN was invented in the mid-1970s by Bob Metcalfe and\nDavid Boggs. The original Ethernet LAN used a coaxial bus to interconnect the\nnodes. Bus topologies for Ethernet actually persisted throughout the 1980s and into\nthe mid-1990s. Ethernet with a bus topology is a broadcast LAN—all transmitted\nframes travel to and are processed by all adapters connected to the bus. Recall that\nwe covered Ethernet's CSMA/CD multiple access protocol with binary exponential\nbackoff in Section 5.3.2.\nBy the late 1990s, most companies and universities had replaced their LANs\nwith Ethernet installations using a hub-based star topology. In such an installation\nthe hosts (and routers) are directly connected to a hub with twisted-pair copper\nwire. A hub is a physical-layer device that acts on individual bits rather than\nframes. When a bit, representing a zero or a one, arrives from one interface, the\nhub simply re-creates the bit, boosts its energy strength, and transmits the bit onto\nall the other interfaces. Thus, Ethernet with a hub-based star topology is also a\nbroadcast LAN—whenever a hub receives a bit from one of its interfaces, it sends\na copy out on all of its other interfaces. In particular, if a hub receives frames from\ntwo different interfaces at the same time, a collision occurs and the nodes that cre-\nated the frames must retransmit.\nIn the early 2000s Ethernet experienced yet another major evolutionary change.\nEthernet installations continued to use a star topology, but the hub at the center was\nreplaced with a switch. We’ll be examining switched Ethernet in depth later in this\nchapter. For now, we only mention that a switch is not only “collision-less” but \nis also a bona-fide store-and-forward packet switch; but unlike routers, which operate\nup through layer 3, a switch operates only up through layer 2.\n470\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nEthernet Frame Structure\nWe can learn a lot about Ethernet by examining the Ethernet frame, which is shown in\nFigure 5.20. To give this discussion about Ethernet frames a tangible context, let’s con-\nsider sending an IP datagram from one host to another host, with both hosts on the same\nEthernet LAN (for example, the Ethernet LAN in Figure 5.17.) (Although the payload\nof our Ethernet frame is an IP datagram, we note that an Ethernet frame can carry\nother network-layer packets as well.) Let the sending adapter, adapter A, have the\nMAC address AA-AA-AA-AA-AA-AA and the receiving adapter, adapter B, have the\nMAC address BB-BB-BB-BB-BB-BB. The sending adapter encapsulates the IP data-\ngram within an Ethernet frame and passes the frame to the physical layer. The receiv-\ning adapter receives the frame from the physical layer, extracts the IP datagram, and\npasses the IP datagram to the network layer. In this context, let’s now examine the six\nfields of the Ethernet frame, as shown in Figure 5.20.\n•\nData field (46 to 1,500 bytes). This field carries the IP datagram. The maximum\ntransmission unit (MTU) of Ethernet is 1,500 bytes. This means that if the IP\ndatagram exceeds 1,500 bytes, then the host has to fragment the datagram, as dis-\ncussed in Section 4.4.1. The minimum size of the data field is 46 bytes. This\nmeans that if the IP datagram is less than 46 bytes, the data field has to be\n“stuffed” to fill it out to 46 bytes. When stuffing is used, the data passed to the\nnetwork layer contains the stuffing as well as an IP datagram. The network layer\nuses the length field in the IP datagram header to remove the stuffing.\n•\nDestination address (6 bytes). This field contains the MAC address of the des-\ntination adapter, BB-BB-BB-BB-BB-BB. When adapter B receives an Ether-\nnet frame whose destination address is either BB-BB-BB-BB-BB-BB or the\nMAC broadcast address, it passes the contents of the frame’s data field to the\nnetwork layer; if it receives a frame with any other MAC address, it discards\nthe frame.\n•\nSource address (6 bytes). This field contains the MAC address of the adapter that\ntransmits the frame onto the LAN, in this example, AA-AA-AA-AA-AA-AA.\n•\nType field (2 bytes). The type field permits Ethernet to multiplex network-layer\nprotocols. To understand this, we need to keep in mind that hosts can use other\nnetwork-layer protocols besides IP. In fact, a given host may support multiple\nPreamble\nCRC\nDest.\naddress\nSource\naddress\nType\nData\nFigure 5.20 \u0002 Ethernet frame structure\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n471\n\nnetwork-layer protocols using different protocols for different applications. For\nthis reason, when the Ethernet frame arrives at adapter B, adapter B needs to\nknow to which network-layer protocol it should pass (that is, demultiplex) the\ncontents of the data field. IP and other network-layer protocols (for example,\nNovell IPX or AppleTalk) each have their own, standardized type number. Fur-\nthermore, the ARP protocol (discussed in the previous section) has its own type\nnumber, and if the arriving frame contains an ARP packet (i.e., has a type field\nof 0806 hexadecimal), the ARP packet will be demultiplexed up to the ARP pro-\ntocol. Note that the type field is analogous to the protocol field in the network-\nlayer datagram and the port-number fields in the transport-layer segment; all of\nthese fields serve to glue a protocol at one layer to a protocol at the layer above.\n•\nCyclic redundancy check (CRC) (4 bytes). As discussed in Section 5.2.3, the pur-\npose of the CRC field is to allow the receiving adapter, adapter B, to detect bit\nerrors in the frame.\n•\nPreamble (8 bytes). The Ethernet frame begins with an 8-byte preamble field.\nEach of the first 7 bytes of the preamble has a value of 10101010; the last byte is\n10101011. The first 7 bytes of the preamble serve to “wake up” the receiving\nadapters and to synchronize their clocks to that of the sender’s clock. Why\nshould the clocks be out of synchronization? Keep in mind that adapter A aims\nto transmit the frame at 10 Mbps, 100 Mbps, or 1 Gbps, depending on the type\nof Ethernet LAN. However, because nothing is absolutely perfect, adapter A will\nnot transmit the frame at exactly the target rate; there will always be some drift\nfrom the target rate, a drift which is not known a priori by the other adapters on\nthe LAN. A receiving adapter can lock onto adapter A’s clock simply by locking\nonto the bits in the first 7 bytes of the preamble. The last 2 bits of the eighth byte\nof the preamble (the first two consecutive 1s) alert adapter B that the “important\nstuff” is about to come.\nAll of the Ethernet technologies provide connectionless service to the network\nlayer. That is, when adapter A wants to send a datagram to adapter B, adapter A encap-\nsulates the datagram in an Ethernet frame and sends the frame into the LAN, without\nfirst handshaking with adapter B. This layer-2 connectionless service is analogous to\nIP’s layer-3 datagram service and UDP’s layer-4 connectionless service.\nEthernet technologies provide an unreliable service to the network layer.\nSpecifically, when adapter B receives a frame from adapter A, it runs the frame\nthrough a CRC check, but neither sends an acknowledgment when a frame passes\nthe CRC check nor sends a negative acknowledgment when a frame fails the CRC\ncheck. When a frame fails the CRC check, adapter B simply discards the frame.\nThus, adapter A has no idea whether its transmitted frame reached adapter B and\npassed the CRC check. This lack of reliable transport (at the link layer) helps to\nmake Ethernet simple and cheap. But it also means that the stream of datagrams\npassed to the network layer can have gaps.\n472\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nIf there are gaps due to discarded Ethernet frames, does the application at Host\nB see gaps as well? As we learned in Chapter 3, this depends on whether the appli-\ncation is using UDP or TCP. If the application is using UDP, then the application in\nHost B will indeed see gaps in the data. On the other hand, if the application is using\nTCP, then TCP in Host B will not acknowledge the data contained in discarded\nframes, causing TCP in Host A to retransmit. Note that when TCP retransmits data,\nthe data will eventually return to the Ethernet adapter at which it was discarded.\nThus, in this sense, Ethernet does retransmit data, although Ethernet is unaware of\nwhether it is transmitting a brand-new datagram with brand-new data, or a datagram\nthat contains data that has already been transmitted at least once.\nEthernet Technologies\nIn our discussion above, we’ve referred to Ethernet as if it were a single protocol stan-\ndard. But in fact, Ethernet comes in many different flavors, with somewhat bewilder-\ning acronyms such as 10BASE-T, 10BASE-2, 100BASE-T, 1000BASE-LX, and\nBOB METCALFE AND ETHERNET\nAs a PhD student at Harvard University in the early 1970s, Bob Metcalfe worked on\nthe ARPAnet at MIT. During his studies, he also became exposed to Abramson’s work\non ALOHA and random access protocols. After completing his PhD and just before\nbeginning a job at Xerox Palo Alto Research Center (Xerox PARC), he visited\nAbramson and his University of Hawaii colleagues for three months, getting a first-\nhand look at ALOHAnet. At Xerox PARC, Metcalfe became exposed to Alto comput-\ners, which in many ways were the forerunners of the personal computers of the\n1980s. Metcalfe saw the need to network these computers in an inexpensive manner.\nSo armed with his knowledge about ARPAnet, ALOHAnet, and random access proto-\ncols, Metcalfe—along with colleague David Boggs—invented Ethernet.\nMetcalfe and Boggs’s original Ethernet ran at 2.94 Mbps and linked up to 256\nhosts separated by up to one mile. Metcalfe and Boggs succeeded at getting most of\nthe researchers at Xerox PARC to communicate through their Alto computers.\nMetcalfe then forged an alliance between Xerox, Digital, and Intel to establish\nEthernet as a 10 Mbps Ethernet standard, ratified by the IEEE. Xerox did not show\nmuch interest in commercializing Ethernet. In 1979, Metcalfe formed his own com-\npany, 3Com, which developed and commercialized networking technology, including\nEthernet technology. In particular, 3Com developed and marketed Ethernet cards in\nthe early 1980s for the immensely popular IBM PCs. Metcalfe left 3Com in 1990,\nwhen it had 2,000 employees and $400 million in revenue.\nCASE HISTORY\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n473\n\n10GBASE-T. These and many other Ethernet technologies have been standardized\nover the years by the IEEE 802.3 CSMA/CD (Ethernet) working group [IEEE 802.3\n2012]. While these acronyms may appear bewildering, there is actually considerable\norder here. The first part of the acronym refers to the speed of the standard: 10, 100,\n1000, or 10G, for 10 Megabit (per second), 100 Megabit, Gigabit, and 10 Gigabit \nEthernet, respectively. “BASE” refers to baseband Ethernet, meaning that the physical\nmedia only carries Ethernet traffic; almost all of the 802.3 standards are for baseband\nEthernet. The final part of the acronym refers to the physical media itself; Ethernet is\nboth a link-layer and a physical-layer specification and is carried over a variety of\nphysical media including coaxial cable, copper wire, and fiber. Generally, a “T” refers\nto twisted-pair copper wires.\nHistorically, an Ethernet was initially conceived of as a segment of coaxial\ncable. The early 10BASE-2 and 10BASE-5 standards specify 10 Mbps Ethernet\nover two types of coaxial cable, each limited in length to 500 meters. Longer runs\ncould be obtained by using a repeater—a physical-layer device that receives a\nsignal on the input side, and regenerates the signal on the output side. A coaxial\ncable, as in Figure 5.20, corresponds nicely to our view of Ethernet as a broadcast\nmedium—all frames transmitted by one interface are received at other interfaces,\nand Ethernet’s CDMA/CD protocol nicely solves the multiple access problem.\nNodes simply attach to the cable, and voila, we have a local area network!\nEthernet has passed through a series of evolutionary steps over the years, and\ntoday’s Ethernet is very different from the original bus-topology designs using coax-\nial cable. In most installations today, nodes are connected to a switch via point-to-\npoint segments made of twisted-pair copper wires or fiber-optic cables, as shown in\nFigures 5.15–5.17.\nIn the mid-1990s, Ethernet was standardized at 100 Mbps, 10 times faster than\n10 Mbps Ethernet. The original Ethernet MAC protocol and frame format were pre-\nserved, but higher-speed physical layers were defined for copper wire (100BASE-T)\nand fiber (100BASE-FX, 100BASE-SX, 100BASE-BX). Figure 5.21 shows these\ndifferent standards and the common Ethernet MAC protocol and frame format. \n474\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nPhysical\nTransport\nNetwork\nLink\nApplication\n100BASE-TX\n100BASE-T4\n100BASE-T2\nMAC protocol\nand frame format\n100BASE-SX\n100BASE-FX\n100BASE-BX\nFigure 5.21 \u0002 100 Mbps Ethernet standards: a common link layer,\ndifferent physical layers\n\n100 Mbps Ethernet is limited to a 100 meter distance over twisted pair, and to sev-\neral kilometers over fiber, allowing Ethernet switches in different buildings to be\nconnected.\nGigabit Ethernet is an extension to the highly successful 10 Mbps and 100 Mbps\nEthernet standards. Offering a raw data rate of 1,000 Mbps, Gigabit Ethernet main-\ntains full compatibility with the huge installed base of Ethernet equipment. The stan-\ndard for Gigabit Ethernet, referred to as IEEE 802.3z, does the following:\n•\nUses the standard Ethernet frame format (Figure 5.20) and is backward com-\npatible with 10BASE-T and 100BASE-T technologies. This allows for easy\nintegration of Gigabit Ethernet with the existing installed base of Ethernet\nequipment.\n•\nAllows for point-to-point links as well as shared broadcast channels. Point-\nto-point links use switches while broadcast channels use hubs, as described\nearlier. In Gigabit Ethernet jargon, hubs are called buffered distributors.\n•\nUses CSMA/CD for shared broadcast channels. In order to have acceptable effi-\nciency, the maximum distance between nodes must be severely restricted.\n•\nAllows for full-duplex operation at 1,000 Mbps in both directions for point-to-\npoint channels.\nInitially operating over optical fiber, Gigabit Ethernet is now able to run over cate-\ngory 5 UTP cabling. 10 Gbps Ethernet (10GBASE-T) was standardized in 2007,\nproviding yet higher Ethernet LAN capacities.\nLet’s conclude our discussion of Ethernet technology by posing a question that\nmay have begun troubling you. In the days of bus topologies and hub-based star\ntopologies, Ethernet was clearly a broadcast link (as defined in Section 5.3) in which\nframe collisions occurred when nodes transmitted at the same time. To deal with\nthese collisions, the Ethernet standard included the CSMA/CD protocol, which is\nparticularly effective for a wired broadcast LAN spanning a small geographical\nregion. But if the prevalent use of Ethernet today is a switch-based star topology,\nusing store-and-forward packet switching, is there really a need anymore for an Eth-\nernet MAC protocol? As we’ll see shortly, a switch coordinates its transmissions and\nnever forwards more than one frame onto the same interface at any time. Further-\nmore, modern switches are full-duplex, so that a switch and a node can each send\nframes to each other at the same time without interference. In other words, in a\nswitch-based Ethernet LAN there are no collisions and, therefore, there is no need\nfor a MAC protocol!\nAs we’ve seen, today’s Ethernets are very different from the original Ethernet\nconceived by Metcalfe and Boggs more than 30 years ago—speeds have increased\nby three orders of magnitude, Ethernet frames are carried over a variety of media,\nswitched-Ethernets have become dominant, and now even the MAC protocol is\noften unnecessary! Is all of this really still Ethernet? The answer, of course, is “yes,\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n475"
    },
    {
      "chunk_id": "f7b9b078-14fb-459d-89bb-7c813ed64c66",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.4.3 Link-Layer Switches",
      "original_titles": [
        "5.4.3 Link-Layer Switches"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.4 Switched Local Area Networks > 5.4.3 Link-Layer Switches",
      "start_page": 503,
      "end_page": 508,
      "token_count": 3195,
      "text": "by definition.” It is interesting to note, however, that through all of these changes,\nthere has indeed been one enduring constant that has remained unchanged over\n30 years—Ethernet’s frame format. Perhaps this then is the one true and timeless\ncenterpiece of the Ethernet standard.\n5.4.3 Link-Layer Switches\nUp until this point, we have been purposefully vague about what a switch actually\ndoes and how it works. The role of the switch is to receive incoming link-layer frames\nand forward them onto outgoing links; we’ll study this forwarding function in detail\nin this subsection. We’ll see that the switch itself is transparent to the hosts and\nrouters in the subnet; that is, a host/router addresses a frame to another host/router\n(rather than addressing the frame to the switch) and happily sends the frame into the\nLAN, unaware that a switch will be receiving the frame and forwarding it. The rate at\nwhich frames arrive to any one of the switch’s output interfaces may temporarily\nexceed the link capacity of that interface. To accommodate this problem, switch out-\nput interfaces have buffers, in much the same way that router output interfaces have\nbuffers for datagrams. Let’s now take a closer look at how switches operate.\nForwarding and Filtering\nFiltering is the switch function that determines whether a frame should be for-\nwarded to some interface or should just be dropped. Forwarding is the switch\nfunction that determines the interfaces to which a frame should be directed, and\nthen moves the frame to those interfaces. Switch filtering and forwarding are\ndone with a switch table. The switch table contains entries for some, but not nec-\nessarily all, of the hosts and routers on a LAN. An entry in the switch table con-\ntains (1) a MAC address, (2) the switch interface that leads toward that MAC\naddress, and (3) the time at which the entry was placed in the table. An example\nswitch table for the uppermost switch in Figure 5.15 is shown in Figure 5.22.\n476\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nFigure 5.22 \u0002 Portion of a switch table for the uppermost switch in \nFigure 5.15\nAddress\nInterface\nTime\n62-FE-F7-11-89-A3\n1\n9:32\n7C-BA-B2-B4-91-10\n3\n9:36\n....\n....\n....\n\nAlthough this description of frame forwarding may sound similar to our \ndiscussion of datagram forwarding in Chapter 4, we’ll see shortly that there are\nimportant differences. One important difference is that switches forward packets\nbased on MAC addresses rather than on IP addresses. We will also see that a\nswitch table is constructed in a very different manner from a router’s forwarding\ntable.\nTo understand how switch filtering and forwarding work, suppose a frame with\ndestination address DD-DD-DD-DD-DD-DD arrives at the switch on interface x. The\nswitch indexes its table with the MAC address DD-DD-DD-DD-DD-DD. There are\nthree possible cases:\n•\nThere is no entry in the table for DD-DD-DD-DD-DD-DD. In this case, the switch\nforwards copies of the frame to the output buffers preceding all interfaces except\nfor interface x. In other words, if there is no entry for the destination address, the\nswitch broadcasts the frame.\n•\nThere is an entry in the table, associating DD-DD-DD-DD-DD-DD with inter-\nface x. In this case, the frame is coming from a LAN segment that contains\nadapter DD-DD-DD-DD-DD-DD. There being no need to forward the frame to\nany of the other interfaces, the switch performs the filtering function by discard-\ning the frame.\n•\nThere is an entry in the table, associating DD-DD-DD-DD-DD-DD with inter-\nface y\u0006x. In this case, the frame needs to be forwarded to the LAN segment\nattached to interface y. The switch performs its forwarding function by putting\nthe frame in an output buffer that precedes interface y.\nLet’s walk through these rules for the uppermost switch in Figure 5.15 and its\nswitch table in Figure 5.22. Suppose that a frame with destination address 62-FE-\nF7-11-89-A3 arrives at the switch from interface 1. The switch examines its table\nand sees that the destination is on the LAN segment connected to interface 1 (that\nis, Electrical Engineering). This means that the frame has already been broadcast on\nthe LAN segment that contains the destination. The switch therefore filters (that is,\ndiscards) the frame. Now suppose a frame with the same destination address arrives\nfrom interface 2. The switch again examines its table and sees that the destination is\nin the direction of interface 1; it therefore forwards the frame to the output buffer\npreceding interface 1. It should be clear from this example that as long as the switch\ntable is complete and accurate, the switch forwards frames towards destinations\nwithout any broadcasting.\nIn this sense, a switch is “smarter” than a hub. But how does this switch table\nget configured in the first place? Are there link-layer equivalents to network-\nlayer routing protocols? Or must an overworked manager manually configure the\nswitch table?\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n477\n\nSelf-Learning\nA switch has the wonderful property (particularly for the already-overworked net-\nwork administrator) that its table is built automatically, dynamically, and\nautonomously—without any intervention from a network administrator or from a\nconfiguration protocol. In other words, switches are self-learning. This capability is\naccomplished as follows:\n1. The switch table is initially empty.\n2. For each incoming frame received on an interface, the switch stores in its table\n(1) the MAC address in the frame’s source address field, (2) the interface from\nwhich the frame arrived, and (3) the current time. In this manner the switch\nrecords in its table the LAN segment on which the sender resides. If every host\nin the LAN eventually sends a frame, then every host will eventually get\nrecorded in the table.\n3. The switch deletes an address in the table if no frames are received with that\naddress as the source address after some period of time (the aging time). In\nthis manner, if a PC is replaced by another PC (with a different adapter), the\nMAC address of the original PC will eventually be purged from the switch\ntable.\nLet’s walk through the self-learning property for the uppermost switch in \nFigure 5.15 and its corresponding switch table in Figure 5.22. Suppose at time 9:39\na frame with source address 01-12-23-34-45-56 arrives from interface 2. Suppose\nthat this address is not in the switch table. Then the switch adds a new entry to the\ntable, as shown in Figure 5.23.\nContinuing with this same example, suppose that the aging time for this switch\nis 60 minutes, and no frames with source address 62-FE-F7-11-89-A3 arrive to the\nswitch between 9:32 and 10:32. Then at time 10:32, the switch removes this address\nfrom its table.\n478\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nFigure 5.23 \u0002 Switch learns about the location of an adapter with address\n01-12-23-34-45-56\nAddress\nInterface\nTime\n01-12-23-34-45-56\n2\n9:39\n62-FE-F7-11-89-A3\n1\n9:32\n7C-BA-B2-B4-91-10\n3\n9:36\n....\n....\n....\n\nSwitches are plug-and-play devices because they require no intervention\nfrom a network administrator or user. A network administrator wanting to install\na switch need do nothing more than connect the LAN segments to the switch\ninterfaces. The administrator need not configure the switch tables at the time of\ninstallation or when a host is removed from one of the LAN segments. Switches\nare also full-duplex, meaning any switch interface can send and receive at the\nsame time.\nProperties of Link-Layer Switching\nHaving described the basic operation of a link-layer switch, let’s now consider their\nfeatures and properties. We can identify several advantages of using switches, rather\nthan broadcast links such as buses or hub-based star topologies:\n•\nElimination of collisions. In a LAN built from switches (and without hubs), there\nis no wasted bandwidth due to collisions! The switches buffer frames and never\ntransmit more than one frame on a segment at any one time. As with a router, the\nmaximum aggregate throughput of a switch is the sum of all the switch interface\nrates. Thus, switches provide a significant performance improvement over LANs\nwith broadcast links.\n•\nHeterogeneous links. Because a switch isolates one link from another, the differ-\nent links in the LAN can operate at different speeds and can run over different\nmedia. For example, the uppermost switch in Figure 5.22 might have three\n1 Gbps 1000BASE-T copper links, two 100 Mbps 100BASE-FX fiber links, and\none 100BASE-T copper link. Thus, a switch is ideal for mixing legacy equip-\nment with new equipment.\n•\nManagement. In addition to providing enhanced security (see sidebar on Focus\non Security), a switch also eases network management. For example, if an\nadapter malfunctions and continually sends Ethernet frames (called a jabbering\nadapter), a switch can detect the problem and internally disconnect the mal-\nfunctioning adapter. With this feature, the network administrator need not get\nout of bed and drive back to work in order to correct the problem. Similarly, a\ncable cut disconnects only that host that was using the cut cable to connect to\nthe switch. In the days of coaxial cable, many a network manager spent hours\n“walking the line” (or more accurately, “crawling the floor”) to find the cable\nbreak that brought down the entire network. As discussed in Chapter 9 (Net-\nwork Management), switches also gather statistics on bandwidth usage, collision\nrates, and traffic types, and make this information available to the network\nmanager. This information can be used to debug and correct problems, and to\nplan how the LAN should evolve in the future. Researchers are exploring adding\nyet more management functionality into Ethernet LANs in prototype deploy-\nments [Casado 2007; Koponen 2011].\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n479\n\nSwitches Versus Routers\nAs we learned in Chapter 4, routers are store-and-forward packet switches that for-\nward packets using network-layer addresses. Although a switch is also a store-and-\nforward packet switch, it is fundamentally different from a router in that it forwards\npackets using MAC addresses. Whereas a router is a layer-3 packet switch, a switch\nis a layer-2 packet switch.\nEven though switches and routers are fundamentally different, network admin-\nistrators must often choose between them when installing an interconnection device.\nFor example, for the network in Figure 5.15, the network administrator could just as\neasily have used a router instead of a switch to connect the department LANs,\nservers, and internet gateway router. Indeed, a router would permit interdepartmen-\ntal communication without creating collisions. Given that both switches and routers\nare candidates for interconnection devices, what are the pros and cons of the two\napproaches?\nFirst consider the pros and cons of switches. As mentioned above, switches are\nplug-and-play, a property that is cherished by all the overworked network adminis-\ntrators of the world. Switches can also have relatively high filtering and forwarding\nrates—as shown in Figure 5.24, switches have to process frames only up through\nlayer 2, whereas routers have to process datagrams up through layer 3. On the other\n480\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nSNIFFING A SWITCHED LAN: SWITCH POISONING\nWhen a host is connected to a switch, it typically only receives frames that are being\nexplicity sent to it. For example, consider a switched LAN in Figure 5.17. When host\nA sends a frame to host B, and there is an entry for host B in the switch table, then\nthe switch will forward the frame only to host B. If host C happens to be running a\nsniffer, host C will not be able to sniff this A-to-B frame. Thus, in a switched-LAN envi-\nronment (in contrast to a broadcast link environment such as 802.11 LANs or\nhub–based Ethernet LANs), it is more difficult for an attacker to sniff frames. However,\nbecause the switch broadcasts frames that have destination addresses that are not in the\nswitch table, the sniffer at C can still sniff some frames that are not explicitly addressed\nto C. Furthermore, a sniffer will be able sniff all Ethernet broadcast frames with broad-\ncast destination address FF–FF–FF–FF–FF–FF. A well-known attack against a switch,\ncalled switch poisoning, is to send tons of packets to the switch with many different\nbogus source MAC addresses, thereby filling the switch table with bogus entries and\nleaving no room for the MAC addresses of the legitimate hosts. This causes the switch\nto broadcast most frames, which can then be picked up by the sniffer [Skoudis 2006].\nAs this attack is rather involved even for a sophisticated attacker, switches are signifi-\ncantly less vulnerable to sniffing than are hubs and wireless LANs.\nFOCUS ON SECURITY\n\nhand, to prevent the cycling of broadcast frames, the active topology of a switched\nnetwork is restricted to a spanning tree. Also, a large switched network would\nrequire large ARP tables in the hosts and routers and would generate substantial\nARP traffic and processing. Furthermore, switches are susceptible to broadcast\nstorms—if one host goes haywire and transmits an endless stream of Ethernet\nbroadcast frames, the switches will forward all of these frames, causing the entire\nnetwork to collapse.\nNow consider the pros and cons of routers. Because network addressing is\noften hierarchical (and not flat, as is MAC addressing), packets do not normally\ncycle through routers even when the network has redundant paths. (However,\npackets can cycle when router tables are misconfigured; but as we learned in\nChapter 4, IP uses a special datagram header field to limit the cycling.) Thus,\npackets are not restricted to a spanning tree and can use the best path between\nsource and destination. Because routers do not have the spanning tree restriction,\nthey have allowed the Internet to be built with a rich topology that includes, for\nexample, multiple active links between Europe and North America. Another fea-\nture of routers is that they provide firewall protection against layer-2 broadcast\nstorms. Perhaps the most significant drawback of routers, though, is that they are\nnot plug-and-play—they and the hosts that connect to them need their IP\naddresses to be configured. Also, routers often have a larger per-packet processing\ntime than switches, because they have to process up through the layer-3 fields.\nFinally, there are two different ways to pronounce the word router, either as\n“rootor” or as “rowter,” and people waste a lot of time arguing over the proper\npronunciation [Perlman 1999].\nGiven that both switches and routers have their pros and cons (as summarized\nin Table 5.1), when should an institutional network (for example, a university cam-\npus network or a corporate campus network) use switches, and when should it use\nHost\nApplication\nHost\nTransport\nNetwork\nLink\nPhysical\nLink\nPhysical\nNetwork\nSwitch\nRouter\nLink\nPhysical\nApplication\nTransport\nNetwork\nLink\nPhysical\nFigure 5.24 \u0002 Packet processing in switches, routers, and hosts\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n481"
    },
    {
      "chunk_id": "e1d3e854-dc36-46b5-b147-5022fc83926f",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.4.4 Virtual Local Area Networks (VLANs)",
      "original_titles": [
        "5.4.4 Virtual Local Area Networks (VLANs)"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.4 Switched Local Area Networks > 5.4.4 Virtual Local Area Networks (VLANs)",
      "start_page": 509,
      "end_page": 512,
      "token_count": 1822,
      "text": "routers? Typically, small networks consisting of a few hundred hosts have a few\nLAN segments. Switches suffice for these small networks, as they localize traffic and\nincrease aggregate throughput without requiring any configuration of IP addresses.\nBut larger networks consisting of thousands of hosts typically include routers within\nthe network (in addition to switches). The routers provide a more robust isolation of\ntraffic, control broadcast storms, and use more “intelligent” routes among the hosts\nin the network.\nFor more discussion of the pros and cons of switched versus routed networks,\nas well as a discussion of how switched LAN technology can be extended to accom-\nmodate two orders of magnitude more hosts than today’s Ethernets, see [Meyers\n2004; Kim 2008].\n5.4.4 Virtual Local Area Networks (VLANs)\nIn our earlier discussion of Figure 5.15, we noted that modern institutional LANs\nare often configured hierarchically, with each workgroup (department) having its\nown switched LAN connected to the switched LANs of other groups via a switch\nhierarchy. While such a configuration works well in an ideal world, the real\nworld is often far from ideal. Three drawbacks can be identified in the configura-\ntion in Figure 5.15:\n•\nLack of traffic isolation. Although the hierarchy localizes group traffic to\nwithin a single switch, broadcast traffic (e.g., frames carrying ARP and DHCP\nmessages or frames whose destination has not yet been learned by a self-\nlearning switch) must still traverse the entire institutional network. Limiting\nthe scope of such broadcast traffic would improve LAN performance. Perhaps\nmore importantly, it also may be desirable to limit LAN broadcast traffic for\nsecurity/privacy reasons. For example, if one group contains the company’s\nexecutive management team and another group contains disgruntled employ-\nees running Wireshark packet sniffers, the network manager may well prefer\nthat the executives’ traffic never even reaches employee hosts. This type of\nisolation could be provided by replacing the center switch in Figure 5.15 with\n482\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nHubs\nRouters\nSwitches\nTraffic isolation\nNo\nYes\nYes\nPlug and play\nYes\nNo\nYes\nOptimal routing\nNo\nYes\nNo\nTable 5.1 \u0002 Comparison of the typical features of popular interconnection\ndevices\n\na router. We’ll see shortly that this isolation also can be achieved via a\nswitched (layer 2) solution \n•\nInefficient use of switches. If instead of three groups, the institution had 10\ngroups, then 10 first-level switches would be required. If each group were\nsmall, say less than 10 people, then a single 96-port switch would likely be\nlarge enough to accommodate everyone, but this single switch would not pro-\nvide traffic isolation.\n•\nManaging users. If an employee moves between groups, the physical cabling\nmust be changed to connect the employee to a different switch in Figure 5.15.\nEmployees belonging to two groups make the problem even harder.\nFortunately, each of these difficulties can be handled by a switch that supports\nvirtual local area networks (VLANs). As the name suggests, a switch that sup-\nports VLANs allows multiple virtual local area networks to be defined over a sin-\ngle physical local area network infrastructure. Hosts within a VLAN communicate\nwith each other as if they (and no other hosts) were connected to the switch. In a\nport-based VLAN, the switch’s ports (interfaces) are divided into groups by the\nnetwork manager. Each group constitutes a VLAN, with the ports in each VLAN\nforming a broadcast domain (i.e., broadcast traffic from one port can only reach\nother ports in the group). Figure 5.25 shows a single switch with 16 ports. Ports\n2 to 8 belong to the EE VLAN, while ports 9 to 15 belong to the CS VLAN (ports\n1 and 16 are unassigned). This VLAN solves all of the difficulties noted above—\nEE and CS VLAN frames are isolated from each other, the two switches in Fig-\nure 5.15 have been replaced by a single switch, and if the user at switch port 8\njoins the CS Department, the network operator simply reconfigures the VLAN\nsoftware so that port 8 is now associated with the CS VLAN. One can easily\n1\nElectrical Engineering\n(VLAN ports 2–8)\nComputer Science\n(VLAN ports 9–15)\n9\n15\n2\n4\n8\n10\n16\nFigure 5.25 \u0002 A single switch with two configured VLANs\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n483\n\nimagine how the VLAN switch is configured and operates—the network manager\ndeclares a port to belong to a given VLAN (with undeclared ports belonging to a\ndefault VLAN) using switch management software, a table of port-to-VLAN\nmappings is maintained within the switch; and switch hardware only delivers\nframes between ports belonging to the same VLAN.\nBut by completely isolating the two VLANs, we have introduced a new diffi-\nculty! How can traffic from the EE Department be sent to the CS Department?\nOne way to handle this would be to connect a VLAN switch port (e.g., port 1 in\nFigure 5.25) to an external router and configure that port to belong both the EE\nand CS VLANs. In this case, even though the EE and CS departments share the\nsame physical switch, the logical configuration would look as if the EE and CS\ndepartments had separate switches connected via a router. An IP datagram going\nfrom the EE to the CS department would first cross the EE VLAN to reach the\nrouter and then be forwarded by the router back over the CS VLAN to the CS\nhost. Fortunately, switch vendors make such configurations easy for the network\nmanager by building a single device that contains both a VLAN switch and a\nrouter, so a separate external router is not needed. A homework problem at the end\nof the chapter explores this scenario in more detail.\nReturning again to Figure 5.15, let’s now suppose that rather than having a\nseparate Computer Engineering department, some EE and CS faculty are housed\nin a separate building, where (of course!) they need network access, and (of\ncourse!) they’d like to be part of their department’s VLAN. Figure 5.26 shows a\nsecond 8-port switch, where the switch ports have been defined as belonging to\nthe EE or the CS VLAN, as needed. But how should these two switches be inter-\nconnected? One easy solution would be to define a port belonging to the CS\nVLAN on each switch (similarly for the EE VLAN) and to connect these ports to\neach other, as shown in Figure 5.26(a). This solution doesn’t scale, however, since\nN VLANS would require N ports on each switch simply to interconnect the two\nswitches.\nA more scalable approach to interconnecting VLAN switches is known as\nVLAN trunking. In the VLAN trunking approach shown in Figure 5.26(b), a spe-\ncial port on each switch (port 16 on the left switch and port 1 on the right switch) is\nconfigured as a trunk port to interconnect the two VLAN switches. The trunk port\nbelongs to all VLANs, and frames sent to any VLAN are forwarded over the trunk\nlink to the other switch. But this raises yet another question: How does a switch\nknow that a frame arriving on a trunk port belongs to a particular VLAN? The IEEE\nhas defined an extended Ethernet frame format, 802.1Q, for frames crossing a\nVLAN trunk. As shown in Figure 5.27, the 802.1Q frame consists of the standard\nEthernet frame with a four-byte VLAN tag added into the header that carries the\nidentity of the VLAN to which the frame belongs. The VLAN tag is added into a\nframe by the switch at the sending side of a VLAN trunk, parsed, and removed by\nthe switch at the receiving side of the trunk. The VLAN tag itself consists of a 2-byte\n484\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\n1\n16\n1\n8\n1\nElectrical Engineering\n(VLAN ports 2–8)\nb.\na.\nElectrical Engineering\n(VLAN ports 2, 3, 6)\nTrunk\nlink\nComputer Science\n(VLAN ports 9–15)\n9\n15\n2\n4\n8\n10\n16\n1\n2\n3\n4\n5\n6\n8\n7\nComputer Science\n(VLAN ports 4, 5, 7)\nFigure 5.26 \u0002 Connecting two VLAN switches with two VLANs: (a) two\ncables (b) trunked\nPreamble\nCRC\nDest.\naddress\nSource\naddress\nType\nData\nPreamble\nCRC'\nDest.\naddress\nSource\naddress\nType\nTag Control Information\nTag Protocol Identifier\nRecomputed\nCRT\nData\nFigure 5.27 \u0002 Original Ethernet frame (top), 802.1Q-tagged Ethernet\nVLAN frame (below)\n5.4\n•\nSWITCHED LOCAL AREA NETWORKS\n485"
    },
    {
      "chunk_id": "6a0a9bea-729e-4d6a-aca7-f4f64911f256",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.5 Link Virtualization: A Network as a Link Layer",
      "original_titles": [
        "5.5 Link Virtualization: A Network as a Link Layer"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.5 Link Virtualization: A Network as a Link Layer",
      "start_page": 513,
      "end_page": 513,
      "token_count": 621,
      "text": "Tag Protocol Identifier (TPID) field (with a fixed hexadecimal value of 81-00), a \n2-byte Tag Control Information field that contains a 12-bit VLAN identifier field,\nand a 3-bit priority field that is similar in intent to the IP datagram TOS field.\nIn this discussion, we’ve only briefly touched on VLANs and have focused\non port-based VLANs. We should also mention that VLANs can be defined in\nseveral other ways. In MAC-based VLANs, the network manager specifies the\nset of MAC addresses that belong to each VLAN; whenever a device attaches to\na port, the port is connected into the appropriate VLAN based on the MAC\naddress of the device. VLANs can also be defined based on network-layer proto-\ncols (e.g., IPv4, IPv6, or Appletalk) and other criteria. See the 802.1Q standard\n[IEEE 802.1q 2005] for more details.\n5.5 Link Virtualization: A Network as a Link\nLayer\nBecause this chapter concerns link-layer protocols, and given that we’re now near-\ning the chapter’s end, let’s reflect on how our understanding of the term link has\nevolved. We began this chapter by viewing the link as a physical wire connecting\ntwo communicating hosts. In studying multiple access protocols, we saw that multi-\nple hosts could be connected by a shared wire and that the “wire” connecting the\nhosts could be radio spectra or other media. This led us to consider the link a bit\nmore abstractly as a channel, rather than as a wire. In our study of Ethernet LANs\n(Figure 5.15) we saw that the interconnecting media could actually be a rather com-\nplex switched infrastructure. Throughout this evolution, however, the hosts them-\nselves maintained the view that the interconnecting medium was simply a link-layer\nchannel connecting two or more hosts. We saw, for example, that an Ethernet host\ncan be blissfully unaware of whether it is connected to other LAN hosts by a single\nshort LAN segment (Figure 5.17) or by a geographically dispersed switched LAN\n(Figure 5.15) or by a VLAN (Figure 5.26).\nIn the case of a dialup modem connection between two hosts, the link connect-\ning the two hosts is actually the telephone network—a logically separate, global\ntelecommunications network with its own switches, links, and protocol stacks for\ndata transfer and signaling. From the Internet link-layer point of view, however, the\ndial-up connection through the telephone network is viewed as a simple “wire.” In\nthis sense, the Internet virtualizes the telephone network, viewing the telephone net-\nwork as a link-layer technology providing link-layer connectivity between two\nInternet hosts. You may recall from our discussion of overlay networks in Chapter 2\nthat an overlay network similarly views the Internet as a means for providing con-\nnectivity between overlay nodes, seeking to overlay the Internet in the same way\nthat the Internet overlays the telephone network.\n486\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS"
    },
    {
      "chunk_id": "f34a77fe-9eb6-48fe-9fbb-3fb5f62b9ff9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.5.1 Multiprotocol Label Switching (MPLS)",
      "original_titles": [
        "5.5.1 Multiprotocol Label Switching (MPLS)"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.5 Link Virtualization: A Network as a Link Layer > 5.5.1 Multiprotocol Label Switching (MPLS)",
      "start_page": 514,
      "end_page": 516,
      "token_count": 1791,
      "text": "In this section, we’ll consider Multiprotocol Label Switching (MPLS) net-\nworks. Unlike the circuit-switched telephone network, MPLS is a packet-switched,\nvirtual-circuit network in its own right. It has its own packet formats and forward-\ning behaviors. Thus, from a pedagogical viewpoint, a discussion of MPLS fits well\ninto a study of either the network layer or the link layer. From an Internet viewpoint,\nhowever, we can consider MPLS, like the telephone network and switched-\nEthernets, as a link-layer technology that serves to interconnect IP devices. Thus,\nwe’ll consider MPLS in our discussion of the link layer. Frame-relay and ATM \nnetworks can also be used to interconnect IP devices, though they represent a\nslightly older (but still deployed) technology and will not be covered here; see the\nvery readable book [Goralski 1999] for details. Our treatment of MPLS will be nec-\nessarily brief, as entire books could be (and have been) written on these networks.\nWe recommend [Davie 2000] for details on MPLS. We’ll focus here primarily on\nhow MPLS servers interconnect to IP devices, although we’ll dive a bit deeper into\nthe underlying technologies as well.\n5.5.1 Multiprotocol Label Switching (MPLS)\nMultiprotocol Label Switching (MPLS) evolved from a number of industry efforts in\nthe mid-to-late 1990s to improve the forwarding speed of IP routers by adopting a\nkey concept from the world of virtual-circuit networks: a fixed-length label. The goal\nwas not to abandon the destination-based IP datagram-forwarding infrastructure for\none based on fixed-length labels and virtual circuits, but to augment it by selectively\nlabeling datagrams and allowing routers to forward datagrams based on fixed-length\nlabels (rather than destination IP addresses) when possible. Importantly, these tech-\nniques work hand-in-hand with IP, using IP addressing and routing. The IETF uni-\nfied these efforts in the MPLS protocol [RFC 3031, RFC 3032], effectively blending\nVC techniques into a routed datagram network.\nLet’s begin our study of MPLS by considering the format of a link-layer frame\nthat is handled by an MPLS-capable router. Figure 5.28 shows that a link-layer frame\ntransmitted between MPLS-capable devices has a small MPLS header added\nbetween the layer-2 (e.g., Ethernet) header and layer-3 (i.e., IP) header. RFC 3032\nPPP or Ethernet\nheader\nMPLS header\nIP header\nRemainder of link-layer frame\nLabel\nExp\nS\nTTL\nFigure 5.28 \u0002 MPLS header: Located between link- and network-layer\nheaders\n5.5\n•\nLINK VIRTUALIZATION: A NETWORK AS A LINK LAYER\n487\n\ndefines the format of the MPLS header for such links; headers are defined for ATM\nand frame-relayed networks as well in other RFCs. Among the fields in the MPLS\nheader are the label (which serves the role of the virtual-circuit identifier that we\nencountered back in Section 4.2.1), 3 bits reserved for experimental use, a single S bit,\nwhich is used to indicate the end of a series of “stacked” MPLS headers (an advanced\ntopic that we’ll not cover here), and a time-to-live field.\nIt’s immediately evident from Figure 5.28 that an MPLS-enhanced frame can\nonly be sent between routers that are both MPLS capable (since a non-MPLS-\ncapable router would be quite confused when it found an MPLS header where it had\nexpected to find the IP header!). An MPLS-capable router is often referred to as a\nlabel-switched router, since it forwards an MPLS frame by looking up the MPLS\nlabel in its forwarding table and then immediately passing the datagram to the\nappropriate output interface. Thus, the MPLS-capable router need not extract the\ndestination IP address and perform a lookup of the longest prefix match in the for-\nwarding table. But how does a router know if its neighbor is indeed MPLS capable,\nand how does a router know what label to associate with the given IP destination?\nTo answer these questions, we’ll need to take a look at the interaction among a\ngroup of MPLS-capable routers.\nIn the example in Figure 5.29, routers R1 through R4 are MPLS capable. R5\nand R6 are standard IP routers. R1 has advertised to R2 and R3 that it (R1) can route\nto destination A, and that a received frame with MPLS label 6 will be forwarded to\ndestination A. Router R3 has advertised to router R4 that it can route to destinations\n488\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nR4\nin\nlabel\nout\nlabel\n10\n12\n8\nA\nD\nA\n0\n0\n1\ndest\nout\ninterface\nR6\nR5\nR3\nR2\nD\nA\n0\n0\n0\n1\n1\n0\nR1\nin\nlabel\nout\nlabel\n6\n9\nA\nD\n1\n0\n10\n12\ndest\nout\ninterface\nin\nlabel\nout\nlabel\n–\nA\n0\n6\ndest\nout\ninterface\nin\nlabel\nout\nlabel\n6\nA\n0\n8\ndest\nout\ninterface\nFigure 5.29 \u0002 MPLS-enhanced forwarding\n\nA and D, and that incoming frames with MPLS labels 10 and 12, respectively, will\nbe switched toward those destinations. Router R2 has also advertised to router R4\nthat it (R2) can reach destination A, and that a received frame with MPLS label 8\nwill be switched toward A. Note that router R4 is now in the interesting position of\nhaving two MPLS paths to reach A: via interface 0 with outbound MPLS label 10,\nand via interface 1 with an MPLS label of 8. The broad picture painted in Figure\n5.29 is that IP devices R5, R6, A, and D are connected together via an MPLS infra-\nstructure (MPLS-capable routers R1, R2, R3, and R4) in much the same way that a\nswitched LAN or an ATM network can connect together IP devices. And like a\nswitched LAN or ATM network, the MPLS-capable routers R1 through R4 do so\nwithout ever touching the IP header of a packet.\nIn our discussion above, we’ve not specified the specific protocol used to dis-\ntribute labels among the MPLS-capable routers, as the details of this signaling are\nwell beyond the scope of this book. We note, however, that the IETF working group\non MPLS has specified in [RFC 3468] that an extension of the RSVP protocol,\nknown as RSVP-TE [RFC 3209], will be the focus of its efforts for MPLS signal-\ning. We’ve also not discussed how MPLS actually computes the paths for packets\namong MPLS capable routers, nor how it gathers link-state information (e.g.,\namount of  link bandwidth unreserved by MPLS) to use in these path computations.\nExisting link-state routing algorithms (e.g., OSPF) have been extended to flood this\ninformation to MPLS-capable routers. Interestingly, the actual path computation\nalgorithms are not standardized, and are currently vendor-specific.\nThus far, the emphasis of our discussion of MPLS has been on the fact that\nMPLS performs switching based on labels, without needing to consider the IP\naddress of a packet. The true advantages of MPLS and the reason for current inter-\nest in MPLS, however, lie not in the potential increases in switching speeds, but\nrather in the new traffic management capabilities that MPLS enables. As noted\nabove, R4 has two MPLS paths to A. If forwarding were performed up at the IP\nlayer on the basis of IP address, the IP routing protocols we studied in Chapter 4\nwould specify only a single, least-cost path to A. Thus, MPLS provides the ability\nto forward packets along routes that would not be possible using standard IP routing\nprotocols. This is one simple form of traffic engineering using MPLS [RFC 3346;\nRFC 3272; RFC 2702; Xiao 2000], in which a network operator can override nor-\nmal IP routing and force some of the traffic headed toward a given destination along\none path, and other traffic destined toward the same destination along another path\n(whether for policy, performance, or some other reason).\nIt is also possible to use MPLS for many other purposes as well. It can be used\nto perform fast restoration of MPLS forwarding paths, e.g., to reroute traffic over a\nprecomputed failover path in response to link failure [Kar 2000; Huang 2002; RFC\n3469]. Finally, we note that MPLS can, and has, been used to implement so-called\nvirtual private networks (VPNs). In implementing a VPN for a customer, an ISP\nuses its MPLS-enabled network to connect together the customer’s various net-\nworks. MPLS can be used to isolate both the resources and addressing used by the\n5.5\n•\nLINK VIRTUALIZATION: A NETWORK AS A LINK LAYER\n489"
    },
    {
      "chunk_id": "a81338f6-28d7-4a41-8251-e76c83f6ac83",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.6 Data Center Networking",
      "original_titles": [
        "5.6 Data Center Networking"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.6 Data Center Networking",
      "start_page": 517,
      "end_page": 521,
      "token_count": 2896,
      "text": "customer’s VPN from that of other users crossing the ISP’s network; see [DeClercq\n2002] for details.\nOur discussion of MPLS has been brief, and we encourage you to consult the\nreferences we’ve mentioned. We note that with so many possible uses for MPLS,\nit appears that it is rapidly becoming the Swiss Army knife of Internet traffic\nengineering!\n5.6 Data Center Networking\nIn recent years, Internet companies such as Google, Microsoft, Facebook, and Amazon\n(as well as their counterparts in Asia and Europe) have built massive data centers, each\nhousing tens to hundreds of thousands of hosts, and concurrently supporting many \ndistinct cloud applications (e.g., search, email, social networking, and e-commerce).\nEach data center has its own data center network that interconnects its hosts with each\nother and interconnects the data center with the Internet. In this section, we provide a\nbrief introduction to data center networking for cloud applications.\nThe cost of a large data center is huge, exceeding $12 million per month for a\n100,000 host data center [Greenberg 2009a]. Of these costs, about 45 percent can be\nattributed to the hosts themselves (which need to be replaced every 3–4 years); 25\npercent to infrastructure, including transformers, uninterruptable power supplies\n(UPS) systems, generators for long-term outages, and cooling systems; 15 percent\nfor electric utility costs for the power draw; and 15 percent for networking, includ-\ning network gear (switches, routers and load balancers), external links, and transit\ntraffic costs. (In these percentages, costs for equipment are amortized so that a com-\nmon cost metric is applied for one-time purchases and ongoing expenses such as\npower.) While networking is not the largest cost, networking innovation is the key\nto reducing overall cost and maximizing performance [Greenberg 2009a].\nThe worker bees in a data center are the hosts: They serve content (e.g., Web\npages and videos), store emails and documents, and collectively perform massively\ndistributed computations (e.g., distributed index computations for search engines).\nThe hosts in data centers, called blades and resembling pizza boxes, are generally\ncommodity hosts that include CPU, memory, and disk storage. The hosts are stacked\nin racks, with each rack typically having 20 to 40 blades. At the top of each rack\nthere is a switch, aptly named the Top of Rack (TOR) switch, that interconnects\nthe hosts in the rack with each other and with other switches in the data center.\nSpecifically, each host in the rack has a network interface card that connects to its\nTOR switch, and each TOR switch has additional ports that can be connected \nto other switches. Although today hosts typically have 1 Gbps Ethernet connections\nto their TOR switches, 10 Gbps connections may become the norm. Each host is\nalso assigned its own data-center-internal IP address.\nThe data center network supports two types of traffic: traffic flowing between\nexternal clients and internal hosts and traffic flowing between internal hosts. To handle\nflows between external clients and internal hosts, the data center network includes one\n490\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nor more border routers, connecting the data center network to the public Internet. The\ndata center network therefore interconnects the racks with each other and connects the\nracks to the border routers. Figure 5.30 shows an example of a data center network.\nData center network design, the art of designing the interconnection network and pro-\ntocols that connect the racks with each other and with the border routers, has become\nan important branch of computer networking research in recent years [Al-Fares 2008;\nGreenberg 2009a; Greenberg 2009b; Mydotr 2009; Guo 2009; Chen 2010; Abu-Lib-\ndeh 2010; Alizadeh 2010; Wang 2010; Farrington 2010; Halperin 2011; Wilson 2011;\nMudigonda 2011; Ballani 2011; Curtis 2011; Raiciu 2011].\nLoad Balancing\nA cloud data center, such as a Google or Microsoft data center, provides many appli-\ncations concurrently, such as search, email, and video applications. To support\nrequests from external clients, each application is associated with a publicly visible\nIP address to which clients send their requests and from which they receive\nresponses. Inside the data center, the external requests are first directed to a load\nbalancer whose job it is to distribute requests to the hosts, balancing the load across\nthe hosts as a function of their current load. A large data center will often have several\nInternet\nA\n1\n2\n3\n4\n5\n6\n7\n8\nC\nB\nServer racks\nTOR switches\nTier-2 switches\nTier-1 switches\nAccess router\nBorder router\nLoad\nbalancer\nFigure 5.30 \u0002 A data center network with a hierarchical topology\n5.6\n•\nDATA CENTER NETWORKING\n491\n\nload balancers, each one devoted to a set of specific cloud applications. Such a load\nbalancer is sometimes referred to as a “layer-4 switch” since it makes decisions\nbased on the destination port number (layer 4) as well as destination IP address in\nthe packet. Upon receiving a request for a particular application, the load balancer\nforwards it to one of the hosts that handles the application. (A host may then invoke\nthe services of other hosts to help process the request.) When the host finishes pro-\ncessing the request, it sends its response back to the load balancer, which in turn\nrelays the response back to the external client. The load balancer not only balances\nthe work load across hosts, but also provides a NAT-like function, translating the\npublic external IP address to the internal IP address of the appropriate host, and then\ntranslating back for packets traveling in the reverse direction back to the clients.\nThis prevents clients from contacting hosts directly, which has the security benefit\nof hiding the internal network structure and preventing clients from directly inter-\nacting with the hosts.\nHierarchical Architecture\nFor a small data center housing only a few thousand hosts, a simple network con-\nsisting of a border router, a load balancer, and a few tens of racks all interconnected\nby a single Ethernet switch could possibly suffice. But to scale to tens to hundreds\nof thousands of hosts, a data center often employs a hierarchy of routers and\nswitches, such as the topology shown in Figure 5.30. At the top of the hierarchy, the\nborder router connects to access routers (only two are shown in Figure 5.30, but\nthere can be many more). Below each access router there are three tiers of switches.\nEach access router connects to a top-tier switch, and each top-tier switch connects\nto multiple second-tier switches and a load balancer. Each second-tier switch in turn\nconnects to multiple racks via the racks’ TOR switches (third-tier switches). All\nlinks typically use Ethernet for their link-layer and physical-layer protocols, with a\nmix of copper and fiber cabling. With such a hierarchical design, it is possible to\nscale a data center to hundreds of thousands of hosts.\nBecause it is critical for a cloud application provider to continually provide\napplications with high availability, data centers also include redundant network\nequipment and redundant links in their designs (not shown in Figure 5.30). \nFor example, each TOR switch can connect to two tier-2 switches, and each\naccess router, tier-1 switch, and tier-2 switch can be duplicated and integrated\ninto the design [Cisco 2012; Greenberg 2009b]. In the hierarchical design in \nFigure 5.30, observe that the hosts below each access router form a single sub-\nnet. In order to localize ARP broadcast traffic, each of these subnets is further\npartitioned into smaller VLAN subnets, each comprising a few hundred hosts\n[Greenberg 2009a].\nAlthough the conventional hierarchical architecture just described solves the\nproblem of scale, it suffers from limited host-to-host capacity [Greenberg 2009b]. To\nunderstand this limitation, consider again Figure 5.30, and suppose each host connects\n492\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nto its TOR switch with a 1 Gbps link, whereas the links between switches are 10 Gbps\nEthernet links. Two hosts in the same rack can always communicate at a full 1 Gbps,\nlimited only by the rate of the hosts’ network interface cards. However, if there are\nmany simultaneous flows in the data center network, the maximum rate between two\nhosts in different racks can be much less. To gain insight into this issue, consider a traf-\nfic pattern consisting of 40 simultaneous flows between 40 pairs of hosts in different\nracks. Specifically, suppose each of 10 hosts in rack 1 in Figure 5.30 sends a flow to a\ncorresponding host in rack 5. Similarly, there are ten simultaneous flows between pairs\nof hosts in racks 2 and 6, ten simultaneous flows between racks 3 and 7, and ten simul-\ntaneous flows between racks 4 and 8. If each flow evenly shares a link’s capacity with\nother flows traversing that link, then the 40 flows crossing the 10 Gbps A-to-B link (as\nwell as the 10 Gbps B-to-C link) will each only receive 10 Gbps / 40 = 250 Mbps,\nwhich is significantly less than the 1 Gbps network interface card rate. The problem\nbecomes even more acute for flows between hosts that need to travel higher up the\nhierarchy. One possible solution to this limitation is to deploy higher-rate switches and\nrouters. But this would significantly increase the cost of the data center, because\nswitches and routers with high port speeds are very expensive.\nSupporting high-bandwidth host-to-host communication is important because a\nkey requirement in data centers is flexibility in placement of computation and services\n[Greenberg 2009b; Farrington 2010]. For example, a large-scale Internet search\nengine may run on thousands of hosts spread across multiple racks with significant\nbandwidth requirements between all pairs of hosts. Similarly, a cloud computing serv-\nice such as EC2 may wish to place the multiple virtual machines comprising a cus-\ntomer’s service on the physical hosts with the most capacity irrespective of their\nlocation in the data center. If these physical hosts are spread across multiple racks, net-\nwork bottlenecks as described above may result in poor performance.\nTrends in Data Center Networking\nIn order to reduce the cost of data centers, and at the same time improve their delay and\nthroughput performance, Internet cloud giants such as Google, Facebook, Amazon, and\nMicrosoft are continually deploying new data center network designs. Although these\ndesigns are proprietary, many important trends can nevertheless be identified.\nOne such trend is to deploy new interconnection architectures and network proto-\ncols that overcome the drawbacks of the traditional hierarchical designs. One such\napproach is to replace the hierarchy of switches and routers with a fully connected\ntopology [Al-Fares 2008; Greenberg 2009b; Guo 2009], such as the topology shown in\nFigure 5.31. In this design, each tier-1 switch connects to all of the tier-2 switches so\nthat (1) host-to-host traffic never has to rise above the switch tiers, and (2) with n tier-1\nswitches, between any two tier-2 switches there are n disjoint paths. Such a design can\nsignificantly improve the host-to-host capacity. To see this, consider again our example\nof 40 flows. The topology in Figure 5.31 can handle such a flow pattern since there are\nfour distinct paths between the first tier-2 switch and the second tier-2 switch, together\n5.6\n•\nDATA CENTER NETWORKING\n493\n\nproviding an aggregate capacity of 40 Gbps between the first two tier-2 switches. Such\na design not only alleviates the host-to-host capacity limitation, but also creates a more\nflexible computation and service environment in which communication between any\ntwo racks not connected to the same switch is logically equivalent, irrespective of their\nlocations in the data center.\nAnother major trend is to employ shipping container–based modular data centers\n(MDCs) [YouTube 2009; Waldrop 2007]. In an MDC, a factory builds, within a stan-\ndard 12-meter shipping container, a “mini data center” and ships the container to the\ndata center location. Each container has up to a few thousand hosts, stacked in tens of\nracks, which are packed closely together. At the data center location, multiple contain-\ners are interconnected with each other and also with the Internet. Once a prefabricated\ncontainer is deployed at a data center, it is often difficult to service. Thus, each con-\ntainer is designed for graceful performance degradation: as components (servers and\nswitches) fail over time, the container continues to operate but with degraded perform-\nance. When many components have failed and performance has dropped below a\nthreshold, the entire container is removed and replaced with a fresh one.\nBuilding a data center out of containers creates new networking challenges.\nWith an MDC, there are two types of networks: the container-internal networks\nwithin each of the containers and the core network connecting each container [Guo\n2009; Farrington 2010]. Within each container, at the scale of up to a few thousand\nhosts, it is possible to build a fully connected network (as described above) using\ninexpensive commodity Gigabit Ethernet switches. However, the design of the core\nnetwork, interconnecting hundreds to thousands of containers while providing high\nhost-to-host bandwidth across containers for typical workloads, remains a challeng-\ning problem. A hybrid electrical/optical switch architecture for interconnecting the\ncontainers is proposed in [Farrington 2010].\nWhen using highly interconnected topologies, one of the major issues is design-\ning routing algorithms among the switches. One possibility [Greenberg 2009b] is to\n494\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n1\n2\n3\n4\n5\n6\n7\n8\nServer racks\nTOR switches\nTier-2 switches\nTier-1 switches\nFigure 5.31 \u0002 Highly-interconnected data network topology"
    },
    {
      "chunk_id": "40087de2-2943-4034-942a-40209d3cc68b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.7 Retrospective: A Day in the Life of a Web Page Request",
      "original_titles": [
        "5.7 Retrospective: A Day in the Life of a Web Page Request"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.7 Retrospective: A Day in the Life of a Web Page Request",
      "start_page": 522,
      "end_page": 523,
      "token_count": 1066,
      "text": "use a form of random routing. Another possibility [Guo 2009] is to deploy multiple\nnetwork interface cards in each host, connect each host to multiple low-cost commod-\nity switches, and allow the hosts themselves to intelligently route traffic among the\nswitches. Variations and extensions of these approaches are currently being deployed\nin contemporary data centers. Many more innovations in data center design are likely\nto come; interested readers are encouraged to read the many recent papers on data cen-\nter network design.\n5.7 Retrospective: A Day in the Life of a Web Page\nRequest\nNow that we’ve covered the link layer in this chapter, and the network, transport and\napplication layers in earlier chapters, our journey down the protocol stack is com-\nplete! In the very beginning of this book (Section 1.1), we wrote “much of this book\nis concerned with computer network protocols,” and in the first five chapters, we’ve\ncertainly seen that this is indeed the case! Before heading into the topical chapters in\nsecond part of this book, we’d like to wrap up our journey down the protocol stack\nby taking an integrated, holistic view of the protocols we’ve learned about so far. \nOne way then to take this “big picture” view is to identify the many (many!) proto-\ncols that are involved in satisfying even the simplest request: downloading a web page.\nFigure 5.32 illustrates our setting: a student, Bob, connects a laptop to his school’s \nEthernet switch and downloads a web page (say the home page of www.google.com).\nAs we now know, there’s a lot going on “under the hood” to satisfy this seemingly \nsimple request. A Wireshark lab at the end of this chapter examines trace files contain-\ning a number of the packets involved in similar scenarios in more detail.\n5.7.1 Getting Started: DHCP, UDP, IP, and Ethernet\nLet’s suppose that Bob boots up his laptop and then connects it to an Ethernet cable\nconnected to the school’s Ethernet switch, which in turn is connected to the school’s\nrouter, as shown in Figure 5.32. The school’s router is connected to an ISP, in this\nexample, comcast.net. In this example, comcast.net is providing the DNS service\nfor the school; thus, the DNS server resides in the Comcast network rather than the\nschool network. We’ll assume that the DHCP server is running within the router, as\nis often the case.\nWhen Bob first connects his laptop to the network, he can’t do anything (e.g.,\ndownload a Web page) without an IP address. Thus, the first network-related action\ntaken by Bob’s laptop is to run the DHCP protocol to obtain an IP address, as well\nas other information, from the local DHCP server:\n1. The operating system on Bob’s laptop creates a DHCP request message (Sec-\ntion 4.4.2) and puts this message within a UDP segment (Section 3.3) with\ndestination port 67 (DHCP server) and source port 68 (DHCP client). The UDP\nsegment is then placed within an IP datagram (Section 4.4.1) with a broadcast\n5.7\n•\nRETROSPECTIVE: A DAY IN THE LIFE OF A WEB PAGE REQUEST\n495\nVideoNote\nA day in the life of a\nWeb page request\n\nIP destination address (255.255.255.255) and a source IP address of 0.0.0.0,\nsince Bob’s laptop doesn’t yet have an IP address.\n2. The IP datagram containing the DHCP request message is then placed within\nan Ethernet frame (Section 5.4.2). The Ethernet frame has a destination MAC\naddresses of FF:FF:FF:FF:FF:FF so that the frame will be broadcast to all\ndevices connected to the switch (hopefully including a DHCP server); the\nframe’s source MAC address is that of Bob’s laptop, 00:16:D3:23:68:8A.\n3. The broadcast Ethernet frame containing the DHCP request is the first frame\nsent by Bob’s laptop to the Ethernet switch. The switch broadcasts the incom-\ning frame on all outgoing ports, including the port connected to the router.\n4. The router receives the broadcast Ethernet frame containing the DHCP request\non its interface with MAC address 00:22:6B:45:1F:1B and the IP datagram is\nextracted from the Ethernet frame. The datagram’s broadcast IP destination\naddress indicates that this IP datagram should be processed by upper layer proto-\ncols at this node, so the datagram’s payload (a UDP segment) is thus demulti-\nplexed (Section 3.2) up to UDP, and the DHCP request message is extracted\nfrom the UDP segment. The DHCP server now has the DHCP request message.\n5. Let’s suppose that the DHCP server running within the router can allocate IP\naddresses in the CIDR (Section 4.4.2) block 68.85.2.0/24. In this example, all\nIP addresses used within the school are thus within Comcast’s address block.\n496\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n00:22:6B:45:1F:1B\n68.85.2.1\n00:16:D3:23:68:8A\n68.85.2.101\ncomcast.net\nDNS server\n68.87.71.226\nwww.google.com\nWeb server\n64.233.169.105\nSchool network\n68.80.2.0/24\nComcast’s network\n68.80.0.0/13\nGoogle’s network\n64.233.160.0/19\n1–7\n8–13\n18–24\n14–17\nFigure 5.32 \u0002 A day in the life of a Web page request: network setting \nand actions"
    },
    {
      "chunk_id": "835f11ed-2b58-49bc-9145-b2c4825531c9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.7.1 Getting Started: DHCP, UDP, IP, and Ethernet",
      "original_titles": [
        "5.7.1 Getting Started: DHCP, UDP, IP, and Ethernet"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.7 Retrospective: A Day in the Life of a Web Page Request > 5.7.1 Getting Started: DHCP, UDP, IP, and Ethernet",
      "start_page": 522,
      "end_page": 523,
      "token_count": 1066,
      "text": "use a form of random routing. Another possibility [Guo 2009] is to deploy multiple\nnetwork interface cards in each host, connect each host to multiple low-cost commod-\nity switches, and allow the hosts themselves to intelligently route traffic among the\nswitches. Variations and extensions of these approaches are currently being deployed\nin contemporary data centers. Many more innovations in data center design are likely\nto come; interested readers are encouraged to read the many recent papers on data cen-\nter network design.\n5.7 Retrospective: A Day in the Life of a Web Page\nRequest\nNow that we’ve covered the link layer in this chapter, and the network, transport and\napplication layers in earlier chapters, our journey down the protocol stack is com-\nplete! In the very beginning of this book (Section 1.1), we wrote “much of this book\nis concerned with computer network protocols,” and in the first five chapters, we’ve\ncertainly seen that this is indeed the case! Before heading into the topical chapters in\nsecond part of this book, we’d like to wrap up our journey down the protocol stack\nby taking an integrated, holistic view of the protocols we’ve learned about so far. \nOne way then to take this “big picture” view is to identify the many (many!) proto-\ncols that are involved in satisfying even the simplest request: downloading a web page.\nFigure 5.32 illustrates our setting: a student, Bob, connects a laptop to his school’s \nEthernet switch and downloads a web page (say the home page of www.google.com).\nAs we now know, there’s a lot going on “under the hood” to satisfy this seemingly \nsimple request. A Wireshark lab at the end of this chapter examines trace files contain-\ning a number of the packets involved in similar scenarios in more detail.\n5.7.1 Getting Started: DHCP, UDP, IP, and Ethernet\nLet’s suppose that Bob boots up his laptop and then connects it to an Ethernet cable\nconnected to the school’s Ethernet switch, which in turn is connected to the school’s\nrouter, as shown in Figure 5.32. The school’s router is connected to an ISP, in this\nexample, comcast.net. In this example, comcast.net is providing the DNS service\nfor the school; thus, the DNS server resides in the Comcast network rather than the\nschool network. We’ll assume that the DHCP server is running within the router, as\nis often the case.\nWhen Bob first connects his laptop to the network, he can’t do anything (e.g.,\ndownload a Web page) without an IP address. Thus, the first network-related action\ntaken by Bob’s laptop is to run the DHCP protocol to obtain an IP address, as well\nas other information, from the local DHCP server:\n1. The operating system on Bob’s laptop creates a DHCP request message (Sec-\ntion 4.4.2) and puts this message within a UDP segment (Section 3.3) with\ndestination port 67 (DHCP server) and source port 68 (DHCP client). The UDP\nsegment is then placed within an IP datagram (Section 4.4.1) with a broadcast\n5.7\n•\nRETROSPECTIVE: A DAY IN THE LIFE OF A WEB PAGE REQUEST\n495\nVideoNote\nA day in the life of a\nWeb page request\n\nIP destination address (255.255.255.255) and a source IP address of 0.0.0.0,\nsince Bob’s laptop doesn’t yet have an IP address.\n2. The IP datagram containing the DHCP request message is then placed within\nan Ethernet frame (Section 5.4.2). The Ethernet frame has a destination MAC\naddresses of FF:FF:FF:FF:FF:FF so that the frame will be broadcast to all\ndevices connected to the switch (hopefully including a DHCP server); the\nframe’s source MAC address is that of Bob’s laptop, 00:16:D3:23:68:8A.\n3. The broadcast Ethernet frame containing the DHCP request is the first frame\nsent by Bob’s laptop to the Ethernet switch. The switch broadcasts the incom-\ning frame on all outgoing ports, including the port connected to the router.\n4. The router receives the broadcast Ethernet frame containing the DHCP request\non its interface with MAC address 00:22:6B:45:1F:1B and the IP datagram is\nextracted from the Ethernet frame. The datagram’s broadcast IP destination\naddress indicates that this IP datagram should be processed by upper layer proto-\ncols at this node, so the datagram’s payload (a UDP segment) is thus demulti-\nplexed (Section 3.2) up to UDP, and the DHCP request message is extracted\nfrom the UDP segment. The DHCP server now has the DHCP request message.\n5. Let’s suppose that the DHCP server running within the router can allocate IP\naddresses in the CIDR (Section 4.4.2) block 68.85.2.0/24. In this example, all\nIP addresses used within the school are thus within Comcast’s address block.\n496\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n00:22:6B:45:1F:1B\n68.85.2.1\n00:16:D3:23:68:8A\n68.85.2.101\ncomcast.net\nDNS server\n68.87.71.226\nwww.google.com\nWeb server\n64.233.169.105\nSchool network\n68.80.2.0/24\nComcast’s network\n68.80.0.0/13\nGoogle’s network\n64.233.160.0/19\n1–7\n8–13\n18–24\n14–17\nFigure 5.32 \u0002 A day in the life of a Web page request: network setting \nand actions"
    },
    {
      "chunk_id": "cb4262b6-542f-47e2-b9cf-684caaf876db",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.7.2 Still Getting Started: DNS and ARP",
      "original_titles": [
        "5.7.2 Still Getting Started: DNS and ARP"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.7 Retrospective: A Day in the Life of a Web Page Request > 5.7.2 Still Getting Started: DNS and ARP",
      "start_page": 524,
      "end_page": 524,
      "token_count": 678,
      "text": "Let’s suppose the DHCP server allocates address 68.85.2.101 to Bob’s laptop.\nThe DHCP server creates a DHCPACK message (Section 4.4.2) containing\nthis IP address, as well as the IP address of the DNS server (68.87.71.226), the\nIP address for the default gateway router (68.85.2.1), and the subnet block\n(68.85.2.0/24) (equivalently, the “network mask”). The DHCP message is put\ninside a UDP segment, which is put inside an IP datagram, which is put inside\nan Ethernet frame. The Ethernet frame has a source MAC address of the\nrouter’s interface to the home network (00:22:6B:45:1F:1B) and a destination\nMAC address of Bob’s laptop (00:16:D3:23:68:8A). \n6. The Ethernet frame containing the DHCP ACK is sent (unicast) by the router\nto the switch. Because the switch is self-learning (Section 5.4.3) and previ-\nously received an Ethernet frame (containing the DHCP request) from Bob’s\nlaptop, the switch knows to forward a frame addressed to 00:16:D3:23:68:8A\nonly to the output port leading to Bob’s laptop.\n7. Bob’s laptop receives the Ethernet frame containing the DHCP ACK, extracts\nthe IP datagram from the Ethernet frame, extracts the UDP segment from the\nIP datagram, and extracts the DHCP ACK message from the UDP segment.\nBob’s DHCP client then records its IP address and the IP address of its DNS\nserver. It also installs the address of the default gateway into its IP forwarding\ntable (Section 4.1). Bob’s laptop will send all datagrams with destination\naddress outside of its subnet 68.85.2.0/24 to the default gateway. At this point,\nBob’s laptop has initialized its networking components and is ready to begin\nprocessing the Web page fetch. (Note that only the last two DHCP steps of the\nfour presented in Chapter 4 are actually necessary.)\n5.7.2 Still Getting Started: DNS and ARP\nWhen Bob types the URL for www.google.com into his Web browser, he begins the\nlong chain of events that will eventually result in Google’s home page being dis-\nplayed by his Web browser. Bob’s Web browser begins the process by creating a\nTCP socket (Section 2.7) that will be used to send the HTTP request (Section 2.2)\nto www.google.com. In order to create the socket, Bob’s laptop will need to know\nthe IP address of www.google.com. We learned in Section 2.5, that the DNS proto-\ncol is used to provide this name-to-IP-address translation service.\n8. The operating system on Bob’s laptop thus creates a DNS query message (Section\n2.5.3), putting the string “www.google.com” in the question section of the DNS\nmessage. This DNS message is then placed within a UDP segment with a destina-\ntion port of 53 (DNS server). The UDP segment is then placed within an IP data-\ngram with an IP destination address of 68.87.71.226 (the address of the DNS server\nreturned in the DHCPACK in step 5) and a source IP address of 68.85.2.101.\n9. Bob’s laptop then places the datagram containing the DNS query message in\nan Ethernet frame. This frame will be sent (addressed, at the link layer) to the\ngateway router in Bob’s school’s network. However, even though Bob’s laptop\n5.7\n•\nRETROSPECTIVE: A DAY IN THE LIFE OF A WEB PAGE REQUEST\n497"
    },
    {
      "chunk_id": "6165da9d-351b-495b-b467-1e73b0e53870",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.7.3 Still Getting Started: Intra-Domain Routing to the DNS Server",
      "original_titles": [
        "5.7.3 Still Getting Started: Intra-Domain Routing to the DNS Server"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.7 Retrospective: A Day in the Life of a Web Page Request > 5.7.3 Still Getting Started: Intra-Domain Routing to the DNS Server",
      "start_page": 525,
      "end_page": 525,
      "token_count": 640,
      "text": "knows the IP address of the school’s gateway router (68.85.2.1) via the DHCP\nACK message in step 5 above, it doesn’t know the gateway router’s MAC\naddress. In order to obtain the MAC address of the gateway router, Bob’s lap-\ntop will need to use the ARP protocol (Section 5.4.1).\n10. Bob’s laptop creates an ARP query message with a target IP address of\n68.85.2.1 (the default gateway), places the ARP message within an Ethernet\nframe with a broadcast destination address (FF:FF:FF:FF:FF:FF) and sends the\nEthernet frame to the switch, which delivers the frame to all connected\ndevices, including the gateway router.\n11. The gateway router receives the frame containing the ARP query message on the\ninterface to the school network, and finds that the target IP address of 68.85.2.1 in\nthe ARP message matches the IP address of its interface. The gateway router thus\nprepares an ARP reply, indicating that its MAC address of 00:22:6B:45:1F:1B\ncorresponds to IP address 68.85.2.1. It places the ARP reply message in an\nEthernet frame, with a destination address of 00:16:D3:23:68:8A (Bob’s laptop)\nand sends the frame to the switch, which delivers the frame to Bob’s laptop.\n12. Bob’s laptop receives the frame containing the ARP reply message and extracts\nthe MAC address of the gateway router (00:22:6B:45:1F:1B) from the ARP\nreply message.\n13. Bob’s laptop can now ( finally!) address the Ethernet frame containing the DNS\nquery to the gateway router’s MAC address. Note that the IP datagram in this frame\nhas an IP destination address of 68.87.71.226 (the DNS server), while the frame has\na destination address of 00:22:6B:45:1F:1B (the gateway router). Bob’s laptop\nsends this frame to the switch, which delivers the frame to the gateway router.\n5.7.3 Still Getting Started: Intra-Domain Routing to the\nDNS Server\n14. The gateway router receives the frame and extracts the IP datagram containing \nthe DNS query. The router looks up the destination address of this datagram\n(68.87.71.226) and determines from its forwarding table that the datagram should\nbe sent to the leftmost router in the Comcast network in Figure 5.32. The IP data-\ngram is placed inside a link-layer frame appropriate for the link connecting the\nschool’s router to the leftmost Comcast router and the frame is sent over this link.\n15. The leftmost router in the Comcast network receives the frame, extracts the IP\ndatagram, examines the datagram’s destination address (68.87.71.226) and\ndetermines the outgoing interface on which to forward the datagram towards\nthe DNS server from its forwarding table, which has been filled in by Com-\ncast’s intra-domain protocol (such as RIP, OSPF or IS-IS, Section 4.6) as well\nas the Internet’s inter-domain protocol, BGP.\n16. Eventually the IP datagram containing the DNS query arrives at the DNS server.\nThe DNS server extracts the DNS query message, looks up the name\nwww.google.com in its DNS database (Section 2.5), and finds the DNS resource\n498\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS"
    },
    {
      "chunk_id": "675cf695-a747-43ce-b31f-7d914ace4c1c",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.7.4 Web Client-Server Interaction: TCP and HTTP",
      "original_titles": [
        "5.7.4 Web Client-Server Interaction: TCP and HTTP"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.7 Retrospective: A Day in the Life of a Web Page Request > 5.7.4 Web Client-Server Interaction: TCP and HTTP",
      "start_page": 526,
      "end_page": 526,
      "token_count": 635,
      "text": "record that contains the IP address (64.233.169.105) for www.google.com.\n(assuming that it is currently cached in the DNS server). Recall that this cached\ndata originated in the authoritative DNS server (Section 2.5.2) for googlecom.\nThe DNS server forms a DNS reply message containing this hostname-to-IP-\naddress mapping, and places the DNS reply message in a UDP segment, and the\nsegment within an IP datagram addressed to Bob’s laptop (68.85.2.101). This\ndatagram will be forwarded back through the Comcast network to the school’s\nrouter and from there, via the Ethernet switch to Bob’s laptop.\n17. Bob’s laptop extracts the IP address of the server www.google.com from the\nDNS message. Finally, after a lot of work, Bob’s laptop is now ready to contact\nthe www.google.com server!\n5.7.4 Web Client-Server Interaction: TCP and HTTP\n18. Now that Bob’s laptop has the IP address of www.google.com, it can create the\nTCP socket (Section 2.7) that will be used to send the HTTP GET message\n(Section 2.2.3) to www.google.com. When Bob creates the TCP socket, the TCP\nin Bob’s laptop must first perform a three-way handshake (Section 3.5.6) with\nthe TCP in www.google.com. Bob’s laptop thus first creates a TCP SYN segment\nwith destination port 80 (for HTTP), places the TCP segment inside an IP data-\ngram with a destination IP address of 64.233.169.105 (www.google.com), places\nthe datagram inside a frame with a destination MAC address of\n00:22:6B:45:1F:1B (the gateway router) and sends the frame to the switch.\n19. The routers in the school network, Comcast’s network, and Google’s network\nforward the datagram containing the TCP SYN towards www.google.com,\nusing the forwarding table in each router, as in steps 14–16 above. Recall that\nthe router forwarding table entries governing forwarding of packets over the\ninter-domain link between the Comcast and Google networks are determined\nby the BGP protocol (Section 4.6.3). \n20. Eventually, the datagram containing the TCP SYN arrives at www.google.com.\nThe TCP SYN message is extracted from the datagram and demultiplexed to the\nwelcome socket associated with port 80. A connection socket (Section 2.7) is\ncreated for the TCP connection between the Google HTTP server and Bob’s\nlaptop. A TCP SYNACK (Section 3.5.6) segment is generated, placed inside a\ndatagram addressed to Bob’s laptop, and finally placed inside a link-layer frame\nappropriate for the link connecting www.google.com to its first-hop router.\n21. The datagram containing the TCP SYNACK segment is forwarded through the\nGoogle, Comcast, and school networks, eventually arriving at the Ethernet card\nin Bob’s laptop. The datagram is demultiplexed within the operating system to\nthe TCP socket created in step 18, which enters the connected state.\n22. With the socket on Bob’s laptop now (finally!) ready to send bytes to www.google\n.com, Bob’s browser creates the HTTP GET message (Section 2.2.3) containing the\nURLto be fetched. The HTTPGET message is then written into the socket, with the\n5.7\n•\nRETROSPECTIVE: A DAY IN THE LIFE OF A WEB PAGE REQUEST\n499"
    },
    {
      "chunk_id": "406752f3-3e23-4168-98f0-6a7e4abdc098",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "5.8 Summary",
      "original_titles": [
        "5.8 Summary"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > 5.8 Summary",
      "start_page": 527,
      "end_page": 528,
      "token_count": 1436,
      "text": "GET message becoming the payload of a TCP segment. The TCP segment is placed\nin a datagram and sent and delivered to www.google.com as in steps 18–20 above.\n23. The HTTP server at www.google.com reads the HTTP GET message from the\nTCP socket, creates an HTTP response message (Section 2.2), places the\nrequested Web page content in the body of the HTTP response message, and\nsends the message into the TCP socket. \n24. The datagram containing the HTTP reply message is forwarded through the Google,\nComcast, and school networks, and arrives at Bob’s laptop. Bob’s Web browser pro-\ngram reads the HTTP response from the socket, extracts the html for the Web page\nfrom the body of the HTTP response, and finally ( finally!) displays the Web page!\nOur scenario above has covered a lot of networking ground! If you’ve understood\nmost or all of the above example, then you’ve also covered a lot of ground since you\nfirst read Section 1.1, where we wrote “much of this book is concerned with computer\nnetwork protocols” and you may have wondered what a protocol actually was! As\ndetailed as the above example might seem, we’ve omitted a number of possible addi-\ntional protocols (e.g., NAT running in the school’s gateway router, wireless access to\nthe school’s network, security protocols for accessing the school network or encrypt-\ning segments or datagrams, network management protocols), and considerations (Web\ncaching, the DNS hierarchy) that one would encounter in the public Internet. We’ll\ncover a number of these topics and more in the second part of this book.\nLastly, we note that our example above was an integrated and holistic, but also\nvery “nuts and bolts,” view of many of the protocols that we’ve studied in the first\npart of this book. The example focused more on the “how” than the “why.” For a\nbroader, more reflective view on the design of network protocols in general, see\n[Clark 1988, RFC 5218].\n5.8 Summary\nIn this chapter, we’ve examined the link layer—its services, the principles underly-\ning its operation, and a number of important specific protocols that use these princi-\nples in implementing link-layer services.\nWe saw that the basic service of the link layer is to move a network-layer data-\ngram from one node (host, switch, router, WiFi access point) to an adjacent node.\nWe saw that all link-layer protocols operate by encapsulating a network-layer data-\ngram within a link-layer frame before transmitting the frame over the link to the\nadjacent node. Beyond this common framing function, however, we learned that dif-\nferent link-layer protocols provide very different link access, delivery, and transmis-\nsion services. These differences are due in part to the wide variety of link types over\nwhich link-layer protocols must operate. A simple point-to-point link has a single\nsender and receiver communicating over a single “wire.” A multiple access link is\nshared among many senders and receivers; consequently, the link-layer protocol for\na multiple access channel has a protocol (its multiple access protocol) for coordinat-\ning link access. In the case of MPLS, the “link” connecting two adjacent nodes (for\n500\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nexample, two IP routers that are adjacent in an IP sense—that they are next-hop IP\nrouters toward some destination) may actually be a network in and of itself. In one\nsense, the idea of a network being considered as a link should not seem odd. A tele-\nphone link connecting a home modem/computer to a remote modem/router, for\nexample, is actually a path through a sophisticated and complex telephone network.\nAmong the principles underlying link-layer communication, we examined error-\ndetection and -correction techniques, multiple access protocols, link-layer address-\ning, virtualization (VLANs), and the construction of extended switched LANs and\ndata center networks.  Much of the focus today at the link layer is on these switched\nnetworks. In the case of error detection/correction, we examined how it is possible to\nadd additional bits to a frame’s header in order to detect, and in some cases correct,\nbit-flip errors that might occur when the frame is transmitted over the link. We cov-\nered simple parity and checksumming schemes, as well as the more robust cyclic\nredundancy check. We then moved on to the topic of multiple access protocols. We\nidentified and studied three broad approaches for coordinating access to a broadcast\nchannel: channel partitioning approaches (TDM, FDM), random access approaches\n(the ALOHA protocols and CSMA protocols), and taking-turns approaches (polling\nand token passing). We studied the cable access network and found that it uses many\nof these multiple access methods. We saw that a consequence of having multiple\nnodes share a single broadcast channel was the need to provide node addresses at the\nlink layer. We learned that link-layer addresses were quite different from network-\nlayer addresses and that, in the case of the Internet, a special protocol (ARP—the\nAddress Resolution Protocol) is used to translate between these two forms of\naddressing and studied the hugely successful Ethernet protocol in detail. We then\nexamined how nodes sharing a broadcast channel form a LAN and how multiple\nLANs can be connected together to form larger LANs—all without the intervention\nof network-layer routing to interconnect these local nodes. We also learned how mul-\ntiple virtual LANs can be created on a single physical LAN infrastructure.\nWe ended our study of the link layer by focusing on how MPLS networks pro-\nvide link-layer services when they interconnect IP routers and an overview of the net-\nwork designs for today’s massive data centers.We wrapped up this chapter (and\nindeed the first five chapters) by identifying the many protocols that are needed to\nfetch a simple Web page. Having covered the link layer, our journey down the proto-\ncol stack is now over! Certainly, the physical layer lies below the link layer, but the\ndetails of the physical layer are probably best left for another course (for example, in\ncommunication theory, rather than computer networking). We have, however,\ntouched upon several aspects of the physical layer in this chapter and in Chapter 1\n(our discussion of physical media in Section 1.2). We’ll consider the physical layer\nagain when we study wireless link characteristics in the next chapter.\nAlthough our journey down the protocol stack is over, our study of computer\nnetworking is not yet at an end. In the following four chapters we cover wireless\nnetworking, multimedia networking, network security, and network management.\nThese four topics do not fit conveniently into any one layer; indeed, each topic\ncrosscuts many layers. Understanding these topics (billed as advanced topics in\n5.8\n•\nSUMMARY\n501"
    },
    {
      "chunk_id": "a31fba97-b136-4a03-b90f-e13cdd4616e6",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Homework Problems and Questions",
      "original_titles": [
        "Homework Problems and Questions"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > Homework Problems and Questions",
      "start_page": 529,
      "end_page": 536,
      "token_count": 4578,
      "text": "some networking texts) thus requires a firm foundation in all layers of the protocol\nstack—a foundation that our study of the link layer has now completed!\nHomework Problems and Questions\nChapter 5 Review Questions\nSECTIONS 5.1–5.2\nR1. Consider the transportation analogy in Section 5.1.1. If the passenger is\nanalagous to a datagram, what is analogous to the link layer frame?\nR2. If all the links in the Internet were to provide reliable delivery service, would\nthe TCP reliable delivery service be redundant? Why or why not?\nR3. What are some of the possible services that a link-layer protocol can offer to\nthe network layer? Which of these link-layer services have corresponding\nservices in IP? In TCP?\nSECTION 5.3\nR4. Suppose two nodes start to transmit at the same time a packet of length L\nover a broadcast channel of rate R. Denote the propagation delay between the\ntwo nodes as dprop. Will there be a collision if dprop < L/R? Why or why not?\nR5. In Section 5.3, we listed four desirable characteristics of a broadcast channel.\nWhich of these characteristics does slotted ALOHA have? Which of these\ncharacteristics does token passing have?\nR6. In CSMA/CD, after the fifth collision, what is the probability that a node\nchooses K = 4? The result K = 4 corresponds to a delay of how many seconds\non a 10 Mbps Ethernet?\nR7. Describe polling and token-passing protocols using the analogy of cocktail\nparty interactions.\nR8. Why would the token-ring protocol be inefficient if a LAN had a very large\nperimeter?\nSECTION 5.4\nR9. How big is the MAC address space? The IPv4 address space? The IPv6\naddress space?\nR10. Suppose nodes A, B, and C each attach to the same broadcast LAN (through\ntheir adapters). If A sends thousands of IP datagrams to B with each encapsu-\nlating frame addressed to the MAC address of B, will C’s adapter process\nthese frames? If so, will C’s adapter pass the IP datagrams in these frames to\nthe network layer C? How would your answers change if A sends frames with\nthe MAC broadcast address?\n502\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nR11. Why is an ARP query sent within a broadcast frame? Why is an ARP\nresponse sent within a frame with a specific destination MAC address?\nR12. For the network in Figure 5.19, the router has two ARP modules, each with\nits own ARP table. Is it possible that the same MAC address appears in both\ntables?\nR13. Compare the frame structures for 10BASE-T, 100BASE-T, and Gigabit Eth-\nernet. How do they differ?\nR14. Consider Figure 5.15. How many subnetworks are there, in the addressing\nsense of Section 4.4?\nR15. What is the maximum number of VLANs that can be configured on a switch\nsupporting the 802.1Q protocol? Why?\nR16. Suppose that N switches supporting K VLAN groups are to be connected via\na trunking protocol. How many ports are needed to connect the switches?\nJustify your answer.\nProblems\nP1. Suppose the information content of a packet is the bit pattern 1110 0110 1001\n1101 and an even parity scheme is being used. What would the value of the\nfield containing the parity bits be for the case of a two-dimensional parity\nscheme? Your answer should be such that a minimum-length checksum field\nis used.\nP2. Show (give an example other than the one in Figure 5.5) that two-dimensional\nparity checks can correct and detect a single bit error. Show (give an example\nof) a double-bit error that can be detected but not corrected.\nP3. Suppose the information portion of a packet (D in Figure 5.3) contains 10\nbytes consisting of the 8-bit unsigned binary ASCII representation of string\n“Networking.” Compute the Internet checksum for this data.\nP4. Consider the previous problem, but instead suppose these 10 bytes contain\na. the binary representation of the numbers 1 through 10.\nb. the ASCII representation of the letters B through K (uppercase).\nc. the ASCII representation of the letters b through k (lowercase).\nCompute the Internet checksum for this data.\nP5. Consider the 7-bit generator, G=10011, and suppose that D has the value\n1010101010. What is the value of R?\nP6. Consider the previous problem, but suppose that D has the value\na. 1001010101.\nb. 0101101010.\nc. 1010100000.\nPROBLEMS\n503\n\nP7. In this problem, we explore some of the properties of the CRC. For the gen-\nerator G (=1001) given in Section 5.2.3, answer the following questions.\na. Why can it detect any single bit error in data D?\nb. Can the above G detect any odd number of bit errors? Why?\nP8. In Section 5.3, we provided an outline of the derivation of the efficiency of\nslotted ALOHA. In this problem we’ll complete the derivation.\na. Recall that when there are N active nodes, the efficiency of slotted ALOHA\nis Np(1 – p)N–1. Find the value of p that maximizes this expression.\nb. Using the value of p found in (a), find the efficiency of slotted ALOHA by\nletting N approach infinity. Hint: (1 – 1/N)N approaches 1/e as N\napproaches infinity.\nP9. Show that the maximum efficiency of pure ALOHA is 1/(2e). Note: This\nproblem is easy if you have completed the problem above!\nP10. Consider two nodes, A and B, that use the slotted ALOHA protocol to con-\ntend for a channel. Suppose node A has more data to transmit than node B,\nand node A’s retransmission probability pA is greater than node B’s retrans-\nmission probability, pB.\na. Provide a formula for node A’s average throughput. What is the total effi-\nciency of the protocol with these two nodes? \nb. If pA= 2pB, is node A’s average throughput twice as large as that of node B?\nWhy or why not? If not, how can you choose pA and pB to make that happen?\nc. In general, suppose there are N nodes, among which node A has retrans-\nmission probability 2p and all other nodes have retransmission probability\np. Provide expressions to compute the average throughputs of node A and\nof any other node. \nP11. Suppose four active nodes—nodes A, B, C and D—are competing for access to\na channel using slotted ALOHA. Assume each node has an infinite number\nof packets to send. Each node attempts to transmit in each slot with probability\np. The first slot is numbered slot 1, the second slot is numbered slot 2, and so on.\na. What is the probability that node A succeeds for the first time in slot 5?\nb. What is the probability that some node (either A, B, C or D) succeeds in slot 4?\nc. What is the probability that the first success occurs in slot 3?\nd. What is the efficiency of this four-node system?\nP12. Graph the efficiency of slotted ALOHA and pure ALOHA as a function of p\nfor the following values of N:\na. N\u000515.\nb. N\u000525.\nc. N\u000535.\n504\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nSubnet 3\nE\nF\nC\nSubnet 2\nD\nA\nB\nSubnet 1\nFigure 5.33 \u0002 Three subnets, interconnected by routers\nP13. Consider a broadcast channel with N nodes and a transmission rate of R bps.\nSuppose the broadcast channel uses polling (with an additional polling node)\nfor multiple access. Suppose the amount of time from when a node completes\ntransmission until the subsequent node is permitted to transmit (that is, the\npolling delay) is dpoll. Suppose that within a polling round, a given node is\nallowed to transmit at most Q bits. What is the maximum throughput of the\nbroadcast channel?\nP14. Consider three LANs interconnected by two routers, as shown in Figure 5.33.\na. Assign IP addresses to all of the interfaces. For Subnet 1 use addresses of\nthe form 192.168.1.xxx; for Subnet 2 uses addresses of the form\n192.168.2.xxx; and for Subnet 3 use addresses of the form 192.168.3.xxx.\nb. Assign MAC addresses to all of the adapters.\nc. Consider sending an IP datagram from Host E to Host B. Suppose all of\nthe ARP tables are up to date. Enumerate all the steps, as done for the \nsingle-router example in Section 5.4.1.\nd. Repeat (c), now assuming that the ARP table in the sending host is empty\n(and the other tables are up to date).\nP15. Consider Figure 5.33. Now we replace the router between subnets 1 and 2\nwith a switch S1, and label the router between subnets 2 and 3 as R1. \nPROBLEMS\n505\n\na. Consider sending an IP datagram from Host E to Host F. Will Host E ask\nrouter R1 to help forward the datagram? Why? In the Ethernet frame con-\ntaining the IP datagram, what are the source and destination IP and MAC\naddresses?\nb. Suppose E would like to send an IP datagram to B, and assume that E’s\nARP cache does not contain B’s MAC address. Will E perform an ARP\nquery to find B’s MAC address? Why? In the Ethernet frame (containing\nthe IP datagram destined to B) that is delivered to router R1, what are the\nsource and destination IP and MAC addresses? \nc. Suppose Host A would like to send an IP datagram to Host B, and neither A’s\nARP cache contains B’s MAC address nor does B’s ARP cache contain A’s\nMAC address. Further suppose that the switch S1’s forwarding table contains\nentries for Host B and router R1 only. Thus, A will broadcast an ARP request\nmessage. What actions will switch S1 perform once it receives the ARP\nrequest message? Will router R1 also receive this ARP request message? If so,\nwill R1 forward the message to Subnet 3? Once Host B receives this ARP\nrequest message, it will send back to Host A an ARP response message. But\nwill it send an ARP query message to ask for A’s MAC address? Why? What\nwill switch S1 do once it receives an ARP response message from Host B?\nP16. Consider the previous problem, but suppose now that the router between sub-\nnets 2 and 3 is replaced by a switch. Answer questions (a)–(c) in the previous\nproblem in this new context.\nP17. Recall that with the CSMA/CD protocol, the adapter waits K \u0003 512 bit times after\na collision, where K is drawn randomly. For K = 100, how long does the adapter\nwait until returning to Step 2 for a 10 Mbps broadcast channel? For a 100 Mbps\nbroadcast channel?\nP18. Suppose nodes A and B are on the same 10 Mbps broadcast channel, and the\npropagation delay between the two nodes is 325 bit times. Suppose CSMA/CD\nand Ethernet packets are used for this broadcast channel. Suppose node A begins\ntransmitting a frame and, before it finishes, node B begins transmitting a frame.\nCan A finish transmitting before it detects that B has transmitted? Why or why\nnot? If the answer is yes, then A incorrectly believes that its frame was success-\nfully transmitted without a collision. Hint: Suppose at time t = 0 bits, A begins\ntransmitting a frame. In the worst case, A transmits a minimum-sized frame of\n512 + 64 bit times. So A would finish transmitting the frame at t = 512 + 64 bit\ntimes. Thus, the answer is no, if B’s signal reaches A before bit time t = 512 + 64\nbits. In the worst case, when does B’s signal reach A?\nP19. Suppose nodes A and B are on the same 10 Mbps broadcast channel, and \nthe propagation delay between the two nodes is 245 bit times. Suppose A\nand B send Ethernet frames at the same time, the frames collide, and then \nA and B choose different values of K in the CSMA/CD algorithm. Assuming\n506\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nVideoNote\nSending a datagram\nbetween subnets: link-\nlayer and network-layer\naddressing\n\nno other nodes are active, can the retransmissions from A and B collide? For\nour purposes, it suffices to work out the following example. Suppose A and B\nbegin transmission at t = 0 bit times. They both detect collisions at t = 245 bit\ntimes. Suppose KA = 0 and KB = 1. At what time does B schedule its retrans-\nmission? At what time does A begin transmission? (Note: The nodes must wait\nfor an idle channel after returning to Step 2—see protocol.) At what time does\nA’s signal reach B? Does B refrain from transmitting at its scheduled time?\nP20. In this problem, you will derive the efficiency of a CSMA/CD-like multiple access\nprotocol. In this protocol, time is slotted and all adapters are synchronized to the\nslots. Unlike slotted ALOHA, however, the length of a slot (in seconds) is much\nless than a frame time (the time to transmit a frame). Let S be the length of a slot.\nSuppose all frames are of constant length L = kRS, where R is the transmission rate\nof the channel and k is a large integer. Suppose there are N nodes, each with an\ninfinite number of frames to send. We also assume that dprop < S, so that all nodes\ncan detect a collision before the end of a slot time. The protocol is as follows:\n•\nIf, for a given slot, no node has possession of the channel, all nodes \ncontend for the channel; in particular, each node transmits in the slot with\nprobability p. If exactly one node transmits in the slot, that node takes\npossession of the channel for the subsequent k – 1 slots and transmits its\nentire frame.\n•\nIf some node has possession of the channel, all other nodes refrain from\ntransmitting until the node that possesses the channel has finished trans-\nmitting its frame. Once this node has transmitted its frame, all nodes \ncontend for the channel.\nNote that the channel alternates between two states: the productive state,\nwhich lasts exactly k slots, and the nonproductive state, which lasts for a \nrandom number of slots. Clearly, the channel efficiency is the ratio of k/(k + x),\nwhere x is the expected number of consecutive unproductive slots.\na. For fixed N and p, determine the efficiency of this protocol.\nb. For fixed N, determine the p that maximizes the efficiency.\nc. Using the p (which is a function of N) found in (b), determine the effi-\nciency as N approaches infinity.\nd. Show that this efficiency approaches 1 as the frame length becomes\nlarge.\nP21. Consider Figure 5.33 in problem P14. Provide MAC addresses and IP\naddresses for the interfaces at Host A, both routers, and Host F. Suppose Host\nA sends a datagram to Host F. Give the source and destination MAC addresses\nin the frame encapsulating this IP datagram as the frame is transmitted (i) from\nA to the left router, (ii) from the left router to the right router, (iii) from the\nright router to F. Also give the source and destination IP addresses in the IP\ndatagram encapsulated within the frame at each of these points in time.\nPROBLEMS\n507\n\nP22. Suppose now that the leftmost router in Figure 5.33 is replaced by a switch.\nHosts A, B, C, and D and the right router are all star-connected into this\nswitch. Give the source and destination MAC addresses in the frame encap-\nsulating this IP datagram as the frame is transmitted (i) from A to the switch,\n(ii) from the switch to the right router, (iii) from the right router to F. Also\ngive the source and destination IP addresses in the IP datagram encapsulated\nwithin the frame at each of these points in time.\nP23. Consider Figure 5.15. Suppose that all links are 100 Mbps. What is the maxi-\nmum total aggregate throughput that can be achieved among the 9 hosts and\n2 servers in this network? You can assume that any host or serrver can send\nto any other host or server. Why?\nP24. Suppose the three departmental switches in Figure 5.15 are replaced by\nhubs. All links are 100 Mbps. Now answer the questions posed in problem\nP23.\nP25. Suppose that all the switches in Figure 5.15 are replaced by hubs. All links\nare 100 Mbps. Now answer the questions posed in problem P23.\nP26. Let’s consider the operation of a learning switch in the context of a net-\nwork in which 6 nodes labeled A through F are star connected into an Eth-\nernet switch. Suppose that (i) B sends a frame to E, (ii) E replies with a\nframe to B, (iii) A sends a frame to B, (iv) B replies with a frame to A. The\nswitch table is initially empty. Show the state of the switch table before\nand after each of these events. For each of these events, identify the link(s)\non which the transmitted frame will be forwarded, and briefly justify your\nanswers.\nP27. In this problem, we explore the use of small packets for Voice-over-IP appli-\ncations. One of the drawbacks of a small packet size is that a large fraction of\nlink bandwidth is consumed by overhead bytes. To this end, suppose that the\npacket consists of P bytes and 5 bytes of header.\na. Consider sending a digitally encoded voice source directly. Suppose the\nsource is encoded at a constant rate of 128 kbps. Assume each packet is\nentirely filled before the source sends the packet into the network. The\ntime required to fill a packet is the packetization delay. In terms of L,\ndetermine the packetization delay in milliseconds.\nb. Packetization delays greater than 20 msec can cause a noticeable and\nunpleasant echo. Determine the packetization delay for L = 1,500 bytes\n(roughly corresponding to a maximum-sized Ethernet packet) and for \nL = 50 (corresponding to an ATM packet).\nc. Calculate the store-and-forward delay at a single switch for a link rate of\nR = 622 Mbps for L = 1,500 bytes, and for L = 50 bytes.\nd. Comment on the advantages of using a small packet size.\n508\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\n\nP28. Consider the single switch VLAN in Figure 5.25, and assume an external\nrouter is connected to switch port 1. Assign IP addresses to the EE and CS\nhosts and router interface. Trace the steps taken at both the network layer and\nthe link layer to transfer an IP datagram from an EE host to a CS host (Hint:\nreread the discussion of Figure 5.19 in the text).\nP29. Consider the MPLS network shown in Figure 5.29, and suppose that routers\nR5 and R6 are now MPLS enabled. Suppose that we want to perform traffic\nengineering so that packets from R6 destined for A are switched to A via \nR6-R4-R3-R1, and packets from R5 destined for A are switched via \nR5-R4-R2-R1. Show the MPLS tables in R5 and R6, as well as the modified\ntable in R4, that would make this possible.\nP30. Consider again the same scenario as in the previous problem, but suppose\nthat packets from R6 destined for D are switched via R6-R4-R3, while pack-\nets from R5 destined to D are switched via R4-R2-R1-R3. Show the MPLS\ntables in all routers that would make this possible.\nP31. In this problem, you will put together much of what you have learned about\nInternet protocols. Suppose you walk into a room, connect to Ethernet, and\nwant to download a Web page. What are all the protocol steps that take place,\nstarting from powering on your PC to getting the Web page? Assume there is\nnothing in our DNS or browser caches when you power on your PC. (Hint:\nthe steps include the use of Ethernet, DHCP, ARP, DNS, TCP, and HTTP pro-\ntocols.) Explicitly indicate in your steps how you obtain the IP and MAC\naddresses of a gateway router.\nP32. Consider the data center network with hierarchical topology in Figure 5.30.\nSuppose now there are 80 pairs of flows, with ten flows between the first and\nninth rack, ten flows between the second and tenth rack, and so on. Further\nsuppose that all links in the network are 10 Gbps, except for the links\nbetween hosts and TOR switches, which are 1 Gbps.\na. Each flow has the same data rate; determine the maximum rate of a flow.\nb. For the same traffic pattern, determine the maximum rate of a flow for the\nhighly interconnected topology in Figure 5.31.\nc. Now suppose there is a similar traffic pattern, but involving 20 hosts on\neach hosts and 160 pairs of flows. Determine the maximum flow rates for\nthe two topologies.\nP33. Consider the hierarchical network in Figure 5.30 and suppose that the data\ncenter needs to support email and video distribution among other applica-\ntions. Suppose four racks of servers are reserved for email and four racks are\nreserved for video. For each of the applications, all four racks must lie below\na single tier-2 switch since the tier-2 to tier-1 links do not have sufficient\nbandwidth to support the intra-application traffic. For the email application, \nPROBLEMS\n509"
    },
    {
      "chunk_id": "86506fd5-6150-4dc8-b6e6-6d1c0a9533b3",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Wireshark Labs: Ethernet and ARP, DHCP",
      "original_titles": [
        "Wireshark Labs: Ethernet and ARP, DHCP",
        "Interview: Simon S. Lam"
      ],
      "path": "Chapter 5 The Link Layer: Links, Access Networks, and LANs > Wireshark Labs: Ethernet and ARP, DHCP",
      "start_page": 537,
      "end_page": 539,
      "token_count": 1596,
      "text": "510\nCHAPTER 5\n•\nTHE LINK LAYER: LINKS, ACCESS NETWORKS, AND LANS\nsuppose that for 99.9 percent of the time only three racks are used, and that\nthe video application has identical usage patterns.\na. For what fraction of time does the email application need to use a fourth\nrack? How about for the video application?\nb. Assuming email usage and video usage are independent, for what fraction\nof time do (equivalently, what is the probability that) both applications\nneed their fourth rack?\nc. Suppose that it is acceptable for an application to have a shortage of\nservers for 0.001 percent of time or less (causing rare periods of perform-\nance degradation for users).\nDiscuss how the topology in Figure 5.31 can be used so that only seven\nracks are collectively assigned to the two applications (assuming that the\ntopology can support all the traffic). \nWireshark Labs\nAt the companion Web site for this textbook, http://www.awl.com/kurose-ross,\nyou’ll find a Wireshark lab that examines the operation of the IEEE 802.3 protocol\nand the Wireshark frame format. A second Wireshark lab examines packet traces\ntaken in a home network scenario.\nSimon S. Lam\nAN INTERVIEW WITH...\nSimon S. Lam is Professor and Regents Chair in Computer Sciences\nat the University of Texas at Austin. From 1971 to 1974, he was\nwith the ARPA Network Measurement Center at UCLA, where he\nworked on satellite and radio packet switching. He led a research\ngroup that invented secure sockets and prototyped, in 1993, the first\nsecure sockets layer named Secure Network Programming, which\nwon the 2004 ACM Software System Award. His research interests\nare in design and analysis of network protocols and security servic-\nes. He received his BSEE from Washington State University and his\nMS and PhD from UCLA. He was elected to the National Academy\nof Engineering in 2007.\n511\nWhy did you decide to specialize in networking?\nWhen I arrived at UCLA as a new graduate student in Fall 1969, my intention was to study\ncontrol theory. Then I took the queuing theory classes of Leonard Kleinrock and was very\nimpressed by him. For a while, I was working on adaptive control of queuing systems as a pos-\nsible thesis topic. In early 1972, Larry Roberts initiated the ARPAnet Satellite System project\n(later called Packet Satellite). Professor Kleinrock asked me to join the project. The first thing\nwe did was to introduce a simple, yet realistic, backoff algorithm to the slotted ALOHA proto-\ncol. Shortly thereafter, I found many interesting research problems, such as ALOHA’s instab-\nility problem and need for adaptive backoff, which would form the core of my thesis.\nYou were active in the early days of the Internet in the 1970s, beginning with your \nstudent days at UCLA. What was it like then? Did people have any inkling of what \nthe Internet would become?\nThe atmosphere was really no different from other system-building projects I have seen in\nindustry and academia. The initially stated goal of the ARPAnet was fairly modest, that is,\nto provide access to expensive computers from remote locations so that many more scien-\ntists could use them. However, with the startup of the Packet Satellite project in 1972 and\nthe Packet Radio project in 1973, ARPA’s goal had expanded substantially. By 1973, ARPA\nwas building three different packet networks at the same time, and it became necessary for\nVint Cerf and Bob Kahn to develop an interconnection strategy.\nBack then, all of these progressive developments in networking were viewed (I believe)\nas logical rather than magical. No one could have envisioned the scale of the Internet and\npower of personal computers today. It was a decade before appearance of the first PCs. To put\nthings in perspective, most students submitted their computer programs as decks of punched\ncards for batch processing. Only some students had direct access to computers, which were\ntypically housed in a restricted area. Modems were slow and still a rarity. As a graduate stu-\ndent, I had only a phone on my desk, and I used pencil and paper to do most of my work.\n\n512\nWhere do you see the field of networking and the Internet heading in the future?\nIn the past, the simplicity of the Internet’s IP protocol was its greatest strength in vanquish-\ning competition and becoming the de facto standard for internetworking. Unlike competi-\ntors, such as X.25 in the 1980s and ATM in the 1990s, IP can run on top of any link-layer\nnetworking technology, because it offers only a best-effort datagram service. Thus, any \npacket network can connect to the Internet.\nToday, IP’s greatest strength is actually a shortcoming. IP is like a straitjacket that con-\nfines the Internet’s development to specific directions. In recent years, many researchers\nhave redirected their efforts to the application layer only. There is also a great deal of\nresearch on wireless ad hoc networks, sensor networks, and satellite networks. These net-\nworks can be viewed either as stand-alone systems or link-layer systems, which can flourish\nbecause they are outside of the IP straitjacket.\nMany people are excited about the possibility of P2P systems as a platform for novel\nInternet applications. However, P2P systems are highly inefficient in their use of Internet\nresources. A concern of mine is whether the transmission and switching capacity of the\nInternet core will continue to increase faster than the traffic demand on the Internet as it\ngrows to interconnect all kinds of devices and support future P2P-enabled applications.\nWithout substantial overprovisioning of capacity, ensuring network stability in the presence\nof malicious attacks and congestion will continue to be a significant challenge.\nThe Internet’s phenomenal growth also requires the allocation of new IP addresses at a\nrapid rate to network operators and enterprises worldwide. At the current rate, the pool of unal-\nlocated IPv4 addresses would be depleted in a few years. When that happens, large contiguous\nblocks of address space can only be allocated from the IPv6 address space. Since adoption of\nIPv6 is off to a slow start, due to lack of incentives for early adopters, IPv4 and IPv6 will most\nlikely co-exist on the Internet for many years to come. Successful migration from an IPv4-dom-\ninant Internet to an IPv6-dominant Internet will require a substantial global effort.\nWhat is the most challenging part of your job?\nThe most challenging part of my job as a professor is teaching and motivating every student\nin my class, and every doctoral student under my supervision, rather than just the high\nachievers. The very bright and motivated may require a little guidance but not much else. I\noften learn more from these students than they learn from me. Educating and motivating the\nunderachievers present a major challenge.\nWhat impacts do you foresee technology having on learning in the future?\nEventually, almost all human knowledge will be accessible through the Internet, which will\nbe the most powerful tool for learning. This vast knowledge base will have the potential of\nleveling the playing field for students all over the world. For example, motivated students in\nany country will be able to access the best-class Web sites, multimedia lectures, and teach-\ning materials. Already, it was said that the IEEE and ACM digital libraries have accelerated\nthe development of computer science researchers in China. In time, the Internet will tran-\nscend all geographic barriers to learning."
    },
    {
      "chunk_id": "59c168df-068a-4add-86cc-596c8a3047d7",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Chapter 6 Wireless and Mobile Networks",
      "original_titles": [
        "Chapter 6 Wireless and Mobile Networks"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks",
      "start_page": 540,
      "end_page": 540,
      "token_count": 323,
      "text": "CHAPTER 6\nWireless and\nMobile\nNetworks\n513\nIn the telephony world, the past 15 years have arguably been the golden years of cel-\nlular telephony. The number of worldwide mobile cellular subscribers increased\nfrom 34 million in 1993 to nearly 5.5 billion subscribers by 2011, with the number\nof cellular subscribers now surpassing the number of wired telephone lines. The\nmany advantages of cell phones are evident to all—anywhere, anytime, untethered\naccess to the global telephone network via a highly portable lightweight device.\nWith the advent of laptops, palmtops, smartphones, and their promise of anywhere,\nanytime, untethered access to the global Internet, is a similar explosion in the use of\nwireless Internet devices just around the corner?\nRegardless of the future growth of wireless Internet devices, it’s already clear\nthat wireless networks and the mobility-related services they enable are here to stay.\nFrom a networking standpoint, the challenges posed by these networks, particularly\nat the link layer and the network layer, are so different from traditional wired com-\nputer networks that an individual chapter devoted to the study of wireless and\nmobile networks (i.e., this chapter) is appropriate.\nWe’ll begin this chapter with a discussion of mobile users, wireless links, and\nnetworks, and their relationship to the larger (typically wired) networks to which\nthey connect. We’ll draw a distinction between the challenges posed by the wireless\nnature of the communication links in such networks, and by the mobility that these\nwireless links enable. Making this important distinction—between wireless and"
    },
    {
      "chunk_id": "3de1f71c-8a4f-478a-a356-eeced3262da2",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.1 Introduction",
      "original_titles": [
        "6.1 Introduction"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.1 Introduction",
      "start_page": 541,
      "end_page": 545,
      "token_count": 2561,
      "text": "514\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nmobility—will allow us to better isolate, identify, and master the key concepts in\neach area. Note that there are indeed many networked environments in which the\nnetwork nodes are wireless but not mobile (e.g., wireless home or office networks\nwith stationary workstations and large displays), and that there are limited forms of\nmobility that do not require wireless links (e.g., a worker who uses a wired laptop at\nhome, shuts down the laptop, drives to work, and attaches the laptop to the com-\npany’s wired network). Of course, many of the most exciting networked environ-\nments are those in which users are both wireless and mobile—for example, a\nscenario in which a mobile user (say in the back seat of car) maintains a Voice-over-IP\ncall and multiple ongoing TCP connections while racing down the autobahn at \n160 kilometers per hour. It is here, at the intersection of wireless and mobility, that\nwe’ll find the most interesting technical challenges!\nWe’ll begin by illustrating the setting in which we’ll consider wireless commu-\nnication and mobility—a network in which wireless (and possibly mobile) users are\nconnected into the larger network infrastructure by a wireless link at the network’s\nedge. We’ll then consider the characteristics of this wireless link in Section 6.2. We\ninclude a brief introduction to code division multiple access (CDMA), a shared-\nmedium access protocol that is often used in wireless networks, in Section 6.2. In\nSection 6.3, we’ll examine the link-level aspects of the IEEE 802.11 (WiFi) wire-\nless LAN standard in some depth; we’ll also say a few words about Bluetooth and\nother wireless personal area networks. In Section 6.4, we’ll provide an overview of\ncellular Internet access, including 3G and emerging 4G cellular technologies that\nprovide both voice and high-speed Internet access. In Section 6.5, we’ll turn our\nattention to mobility, focusing on the problems of locating a mobile user, routing to\nthe mobile user, and “handing off” the mobile user who dynamically moves from\none point of attachment to the network to another. We’ll examine how these mobil-\nity services are implemented in the mobile IP standard and in GSM, in Sections 6.6\nand 6.7, respectively. Finally, we’ll consider the impact of wireless links and mobil-\nity on transport-layer protocols and networked applications in Section 6.8.\n6.1 Introduction\nFigure 6.1 shows the setting in which we’ll consider the topics of wireless data com-\nmunication and mobility. We’ll begin by keeping our discussion general enough to\ncover a wide range of networks, including both wireless LANs such as IEEE 802.11\nand cellular networks such as a 3G network; we’ll drill down into a more detailed\ndiscussion of specific wireless architectures in later sections. We can identify the\nfollowing elements in a wireless network:\n•\nWireless hosts. As in the case of wired networks, hosts are the end-system devices\nthat run applications. A wireless host might be a laptop, palmtop, smartphone, or\ndesktop computer. The hosts themselves may or may not be mobile.\n\n6.1\n•\nINTRODUCTION\n515\n•\nWireless links. A host connects to a base station (defined below) or to another\nwireless host through a wireless communication link. Different wireless link\ntechnologies have different transmission rates and can transmit over different\ndistances. Figure 6.2 shows two key characteristics (coverage area and link rate)\nof the more popular wireless network standards. (The figure is only meant to pro-\nvide a rough idea of these characteristics. For example, some of these types of\nnetworks are only now being deployed, and some link rates can increase or\ndecrease beyond the values shown depending on distance, channel conditions,\nand the number of users in the wireless network.) We’ll cover these standards\nPUBLIC WIFI ACCESS: COMING SOON TO A LAMP POST NEAR YOU?\nWiFi hotspots—public locations where users can find 802.11 wireless access—are\nbecoming increasingly common in hotels, airports, and cafés around the world. Most\ncollege campuses offer ubiquitous wireless access, and it’s hard to find a hotel that\ndoesn’t offer wireless Internet access.\nOver the past decade a number of cities have designed, deployed, and operated\nmunicipal WiFi networks. The vision of providing ubiquitous WiFi access to the commu-\nnity as a public service (much like streetlights)—helping to bridge the digital divide by\nproviding Internet access to all citizens and to promote economic development—is\ncompelling. Many cities around the world, including Philadelphia, Toronto, Hong\nKong, Minneapolis, London, and Auckland, have plans to provide ubiquitous wireless\nwithin the city, or have already done so to varying degrees. The goal in Philadelphia\nwas to “turn Philadelphia into the nation’s largest WiFi hotspot and help to improve\neducation, bridge the digital divide, enhance neighborhood development, and reduce\nthe costs of government.” The ambitious program—an agreement between the city,\nWireless Philadelphia (a nonprofit entity), and the Internet Service Provider Earthlink—\nbuilt an operational network of 802.11b hotspots on streetlamp pole arms and traffic\ncontrol devices that covered 80 percent of the city. But financial and operational con-\ncerns caused the network to be sold to a group of private investors in 2008, who later\nsold the network back to the city in 2010. Other cities, such as Minneapolis, Toronto,\nHong Kong, and Auckland, have had success with smaller-scale efforts.\nThe fact that 802.11 networks operate in the unlicensed spectrum (and hence can\nbe deployed without purchasing expensive spectrum use rights) would seem to make\nthem financially attractive. However, 802.11 access points (see Section 6.3) have\nmuch shorter ranges than 3G cellular base stations (see Section 6.4), requiring a larg-\ner number of deployed endpoints to cover the same geographic region. Cellular data\nnetworks providing Internet access, on the other hand, operate in the licensed spec-\ntrum. Cellular providers pay billions of dollars for spectrum access rights for their net-\nworks, making cellular data networks a business rather than municipal undertaking.\nCASE HISTORY\n\nlater in the first half of this chapter; we’ll also consider other wireless link char-\nacteristics (such as their bit error rates and the causes of bit errors) in Section 6.2.\nIn Figure 6.1, wireless links connect wireless hosts located at the edge of the net-\nwork into the larger network infrastructure. We hasten to add that wireless links\nare also sometimes used within a network to connect routers, switches, and other\nnetwork equipment. However, our focus in this chapter will be on the use of\nwireless communication at the network edge, as it is here that many of the most\nexciting technical challenges, and most of the growth, are occurring.\n•\nBase station. The base station is a key part of the wireless network infrastructure.\nUnlike the wireless host and wireless link, a base station has no obvious counterpart\nin a wired network. Abase station is responsible for sending and receiving data (e.g.,\npackets) to and from a wireless host that is associated with that base station. A base\nstation will often be responsible for coordinating the transmission of multiple wire-\nless hosts with which it is associated. When we say a wireless host is “associated”\nwith a base station, we mean that (1) the host is within the wireless communication\n516\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nNetwork\ninfrastructure\nKey:\nWireless access point\nCoverage area\nWireless host\nWireless host in motion\nFigure 6.1 \u0002 Elements of a wireless network\n\ndistance of the base station, and (2) the host uses that base station to relay data\nbetween it (the host) and the larger network. Cell towers in cellular networks and\naccess points in 802.11 wireless LANs are examples of base stations.\nIn Figure 6.1, the base station is connected to the larger network (e.g., the Inter-\nnet, corporate or home network, or telephone network), thus functioning as a\nlink-layer relay between the wireless host and the rest of the world with which\nthe host communicates.\nHosts associated with a base station are often referred to as operating in\ninfrastructure mode, since all traditional network services (e.g., address assign-\nment and routing) are provided by the network to which a host is connected via\nthe base station. In ad hoc networks, wireless hosts have no such infrastructure\nwith which to connect. In the absence of such infrastructure, the hosts themselves\nmust provide for services such as routing, address assignment, DNS-like name\ntranslation, and more.\nWhen a mobile host moves beyond the range of one base station and into the\nrange of another, it will change its point of attachment into the larger network\n(i.e., change the base station with which it is associated)—a process referred to as\nhandoff. Such mobility raises many challenging questions. If a host can move,\nhow does one find the mobile host’s current location in the network so that data\ncan be forwarded to that mobile host? How is addressing performed, given that a\nhost can be in one of many possible locations? If the host moves during a TCP\n6.1\n•\nINTRODUCTION\n517\n802.11a,g\n802.11n\n802.11b\n802.15.1\n3G: UMTS/WCDMA, CDMA2000\n2G: IS-95, CDMA, GSM\nIndoor\nOutdoor\nMid range\noutdoor\nLong range\noutdoor\n10–30m\n50–200m\n200m–4Km\n5Km–20Km\n54 Mbps\n4 Mbps\n5–11 Mbps\n200 Mbps\n1 Mbps\n384 Kbps\nEnhanced 3G: HSPA\n4G: LTE\n802.11a,g point-to-point\nFigure 6.2 \u0002 Link characteristics of selected wireless network standards\n\nconnection or phone call, how is data routed so that the connection continues\nuninterrupted? These and many (many!) other questions make wireless and\nmobile networking an area of exciting networking research.\n•\nNetwork infrastructure. This is the larger network with which a wireless host\nmay wish to communicate.\nHaving discussed the “pieces” of a wireless network, we note that these\npieces can be combined in many different ways to form different types of wireless\nnetworks. You may find a taxonomy of these types of wireless networks useful as\nyou read on in this chapter, or read/learn more about wireless networks beyond\nthis book. At the highest level we can classify wireless networks according to two\ncriteria: (i) whether a packet in the wireless network crosses exactly one wireless\nhop or multiple wireless hops, and (ii) whether there is infrastructure such as a base\nstation in the network:\n•\nSingle-hop, infrastructure-based. These networks have a base station that is\nconnected to a larger wired network (e.g., the Internet). Furthermore, all com-\nmunication is between this base station and a wireless host over a single wire-\nless hop. The 802.11 networks you use in the classroom, café, or library; and\nthe 3G cellular data networks that we will learn about shortly all fall in this\ncategory.\n•\nSingle-hop, infrastructure-less. In these networks, there is no base station that is\nconnected to a wireless network. However, as we will see, one of the nodes in\nthis single-hop network may coordinate the transmissions of the other nodes.\nBluetooth networks (which we will study in Section 6.3.6) and 802.11 networks\nin ad hoc mode are single-hop, infrastructure-less networks.\n•\nMulti-hop, infrastructure-based. In these networks, a base station is present that\nis wired to the larger network. However, some wireless nodes may have to relay\ntheir communication through other wireless nodes in order to communicate via\nthe base station. Some wireless sensor networks and so-called wireless mesh\nnetworks fall in this category.\n•\nMulti-hop, infrastructure-less. There is no base station in these networks, and\nnodes may have to relay messages among several other nodes in order to reach a\ndestination. Nodes may also be mobile, with connectivity changing among\nnodes—a class of networks known as mobile ad hoc networks (MANETs). If\nthe mobile nodes are vehicles, the network is a vehicular ad hoc network\n(VANET). As you might imagine, the development of protocols for such net-\nworks is challenging and is the subject of much ongoing research.\nIn this chapter, we’ll mostly confine ourselves to single-hop networks, and then\nmostly to infrastructure-based networks.\n518\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS"
    },
    {
      "chunk_id": "784ace52-838c-4c67-8e21-4ee0805662b9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.2 Wireless Links and Network Characteristics",
      "original_titles": [
        "6.2 Wireless Links and Network Characteristics"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.2 Wireless Links and Network Characteristics",
      "start_page": 546,
      "end_page": 548,
      "token_count": 1675,
      "text": "Let’s now dig deeper into the technical challenges that arise in wireless and\nmobile networks. We’ll begin by first considering the individual wireless link, defer-\nring our discussion of mobility until later in this chapter.\n6.2 Wireless Links and Network Characteristics\nLet’s begin by considering a simple wired network, say a home network, with a\nwired Ethernet switch (see Section 5.4) interconnecting the hosts. If we replace\nthe wired Ethernet with a wireless 802.11 network, a wireless network interface\nwould replace the host’s wired Ethernet interface, and an access point would\nreplace the Ethernet switch, but virtually no changes would be needed at the net-\nwork layer or above. This suggests that we focus our attention on the link layer\nwhen looking for important differences between wired and wireless networks.\nIndeed, we can find a number of important differences between a wired link and a\nwireless link:\n•\nDecreasing signal strength. Electromagnetic radiation attenuates as it passes\nthrough matter (e.g., a radio signal passing through a wall). Even in free space,\nthe signal will disperse, resulting in decreased signal strength (sometimes\nreferred to as path loss) as the distance between sender and receiver increases.\n•\nInterference from other sources. Radio sources transmitting in the same fre-\nquency band will interfere with each other. For example, 2.4 GHz wireless\nphones and 802.11b wireless LANs transmit in the same frequency band. Thus,\nthe 802.11b wireless LAN user talking on a 2.4 GHz wireless phone can expect\nthat neither the network nor the phone will perform particularly well. In addition\nto interference from transmitting sources, electromagnetic noise within the envi-\nronment (e.g., a nearby motor, a microwave) can result in interference.\n•\nMultipath propagation. Multipath propagation occurs when portions of the\nelectromagnetic wave reflect off objects and the ground, taking paths of different\nlengths between a sender and receiver. This results in the blurring of the received\nsignal at the receiver. Moving objects between the sender and receiver can cause\nmultipath propagation to change over time.\nFor a detailed discussion of wireless channel characteristics, models, and measure-\nments, see [Anderson 1995].\nThe discussion above suggests that bit errors will be more common in wireless\nlinks than in wired links. For this reason, it is perhaps not surprising that wireless\nlink protocols (such as the 802.11 protocol we’ll examine in the following section)\nemploy not only powerful CRC error detection codes, but also link-level reliable-\ndata-transfer protocols that retransmit corrupted frames.\n6.2\n•\nWIRELESS LINKS AND NETWORK CHARACTERISTICS\n519\n\n520\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nFigure 6.3 \u0002 Bit error rate, transmission rate, and SNR\n10–7\n10–6\n10–5\n10–4\n10–3\n10–2\n10–1\n10\n20\n30\n40\n0\nSNR (dB)\nBER\nQAM16\n(4 Mbps)\nQAM256\n(8 Mbps)\nBPSK\n(1Mpbs)\nHaving considered the impairments that can occur on a wireless channel, let’s\nnext turn our attention to the host receiving the wireless signal. This host receives an\nelectromagnetic signal that is a combination of a degraded form of the original signal\ntransmitted by the sender (degraded due to the attenuation and multipath propagation\neffects that we discussed above, among others) and background noise in the environ-\nment. The signal-to-noise ratio (SNR) is a relative measure of the strength of the\nreceived signal (i.e., the information being transmitted) and this noise. The SNR is\ntypically measured in units of decibels (dB), a unit of measure that some think is used\nby electrical engineers primarily to confuse computer scientists. The SNR, measured\nin dB, is twenty times the ratio of the base-10 logarithm of the amplitude of the\nreceived signal to the amplitude of the noise. For our purposes here, we need only\nknow that a larger SNR makes it easier for the receiver to extract the transmitted sig-\nnal from the background noise.\nFigure 6.3 (adapted from [Holland 2001]) shows the bit error rate (BER)—\nroughly speaking, the probability that a transmitted bit is received in error at the\nreceiver—versus the SNR for three different modulation techniques for encoding\ninformation for transmission on an idealized wireless channel. The theory of modu-\nlation and coding, as well as signal extraction and BER, is well beyond the scope of\nthis text (see [Schwartz 1980] for a discussion of these topics). Nonetheless, Figure\n6.3 illustrates several physical-layer characteristics that are important in understand-\ning higher-layer wireless communication protocols:\n•\nFor a given modulation scheme, the higher the SNR, the lower the BER. Since\na sender can increase the SNR by increasing its transmission power, a sender\n\ncan decrease the probability that a frame is received in error by increasing its\ntransmission power. Note, however, that there is arguably little practical gain\nin increasing the power beyond a certain threshold, say to decrease the BER\nfrom 10-12 to 10-13. There are also disadvantages associated with increasing the\ntransmission power: More energy must be expended by the sender (an impor-\ntant concern for battery-powered mobile users), and the sender’s transmissions\nare more likely to interfere with the transmissions of another sender (see\nFigure 6.4(b)).\n•\nFor a given SNR, a modulation technique with a higher bit transmission rate\n(whether in error or not) will have a higher BER. For example, in Figure 6.3,\nwith an SNR of 10 dB, BPSK modulation with a transmission rate of 1 Mbps has\na BER of less than 10-7, while with QAM16 modulation with a transmission rate\nof 4 Mbps, the BER is 10-1, far too high to be practically useful. However, with\nan SNR of 20 dB, QAM16 modulation has a transmission rate of 4 Mbps and a\nBER of 10-7, while BPSK modulation has a transmission rate of only 1 Mbps and\na BER that is so low as to be (literally) “off the charts.” If one can tolerate a BER\nof 10-7, the higher transmission rate offered by QAM16 would make it the pre-\nferred modulation technique in this situation. These considerations give rise to\nthe final characteristic, described next.\n•\nDynamic selection of the physical-layer modulation technique can be used to\nadapt the modulation technique to channel conditions. The SNR (and hence the\nBER) may change as a result of mobility or due to changes in the environment.\nAdaptive modulation and coding are used in cellular data systems and in the\n802.11 WiFi and 3G cellular data networks that we’ll study in Sections 6.3 and\n6.4. This allows, for example, the selection of a modulation technique that \nprovides the highest transmission rate possible subject to a constraint on the\nBER, for given channel characteristics.\nA higher and time-varying bit error rate is not the only difference between a\nwired and wireless link. Recall that in the case of wired broadcast links, all nodes\nreceive the transmissions from all other nodes. In the case of wireless links, the situ-\nation is not as simple, as shown in Figure 6.4. Suppose that Station A is transmitting\nto Station B. Suppose also that Station C is transmitting to Station B. With the so-\ncalled hidden terminal problem, physical obstructions in the environment (for\nexample, a mountain or a building) may prevent A and C from hearing each other’s\ntransmissions, even though A’s and C’s transmissions are indeed interfering at the\ndestination, B. This is shown in Figure 6.4(a). A second scenario that results in\nundetectable collisions at the receiver results from the fading of a signal’s strength\nas it propagates through the wireless medium. Figure 6.4(b) illustrates the case\nwhere A and C are placed such that their signals are not strong enough to detect each\nother’s transmissions, yet their signals are strong enough to interfere with each other\nat station B. As we’ll see in Section 6.3, the hidden terminal problem and fading\n6.2\n•\nWIRELESS LINKS AND NETWORK CHARACTERISTICS\n521"
    },
    {
      "chunk_id": "efe213e7-5a4b-42c8-b658-89bcd0237634",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.2.1 CDMA",
      "original_titles": [
        "6.2.1 CDMA"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.2 Wireless Links and Network Characteristics > 6.2.1 CDMA",
      "start_page": 549,
      "end_page": 552,
      "token_count": 1571,
      "text": "make multiple access in a wireless network considerably more complex than in a\nwired network.\n6.2.1 CDMA\nRecall from Chapter 5 that when hosts communicate over a shared medium, a pro-\ntocol is needed so that the signals sent by multiple senders do not interfere at the\nreceivers. In Chapter 5 we described three classes of medium access protocols:\nchannel partitioning, random access, and taking turns. Code division multiple\naccess (CDMA) belongs to the family of channel partitioning protocols. It is preva-\nlent in wireless LAN and cellular technologies. Because CDMA is so important in\nthe wireless world, we’ll take a quick look at CDMA now, before getting into spe-\ncific wireless access technologies in the subsequent sections.\nIn a CDMA protocol, each bit being sent is encoded by multiplying the bit by\na signal (the code) that changes at a much faster rate (known as the chipping rate)\nthan the original sequence of data bits. Figure 6.5 shows a simple, idealized\nCDMA encoding/decoding scenario. Suppose that the rate at which original data\nbits reach the CDMA encoder defines the unit of time; that is, each original data\nbit to be transmitted requires a one-bit slot time. Let di be the value of the data bit\nfor the ith bit slot. For mathematical convenience, we represent a data bit with a 0\nvalue as –1. Each bit slot is further subdivided into M mini-slots; in Figure 6.5, M\n= 8, although in practice M is much larger. The CDMA code used by the sender\nconsists of a sequence of M values, cm, m = 1, . . . , M, each taking a +1 or –1\n522\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nA\nA\nC\nB\nC\nLocation\nb.\na.\n0\nSignal strength\nB\nFigure 6.4 \u0002 Hidden terminal problem caused by obstacle (a) and \nfading (b)\n\nvalue. In the example in Figure 6.5, the M-bit CDMA code being used by the\nsender is (1, 1, 1, –1, 1, –1, –1, –1).\nTo illustrate how CDMA works, let us focus on the ith data bit, di. For the mth\nmini-slot of the bit-transmission time of di, the output of the CDMA encoder, Zi,m, is\nthe value of di multiplied by the mth bit in the assigned CDMA code, cm:\n(6.1)\nZi, m = di \u0002 cm\n6.2\n•\nWIRELESS LINKS AND NETWORK CHARACTERISTICS\n523\nFigure 6.5 \u0002 A simple CDMA example: sender encoding, receiver \ndecoding\n1\n1 1 1\n-1\n-1 -1 -1\n1\n1 1 1\n-1\n-1 -1 -1\n1\n-1\n-1\n-1 -1\n1 1 1\n1\n-1\n-1 -1 -1\n1 1 1\nTime slot 1\nreceived input\nTime slot 0\nreceived input\nCode\n1\n-1\n-1\n-1 -1\n1 1 1\n1\n-1\n-1 -1 -1\n1 1 1\nData bits\nCode\n1\n1 1 1\n-1\n-1 -1 -1\n1\n1 1 1\n-1\n-1 -1 -1\nd1 = -1\nd0 = 1\nTime slot 1\nSender\nChannel output Zi,m\nReceiver\nZi,m\ndi • cm\n=\nZi,m • cm\nd\nM\ni\nm=1\nM\n=\n∑\nTime slot 1\nchannel output\nTime slot 0\nchannel output\nTime slot 0\nd1 = -1\nd0 = 1\n\nIn a simple world, with no interfering senders, the receiver would receive the\nencoded bits, Zi,m, and recover the original data bit, di, by computing:\n(6.2)\nThe reader might want to work through the details of the example in Figure 6.5 to\nsee that the original data bits are indeed correctly recovered at the receiver using\nEquation 6.2.\nThe world is far from ideal, however, and as noted above, CDMA must work in\nthe presence of interfering senders that are encoding and transmitting their data using\na different assigned code. But how can a CDMA receiver recover a sender’s original\ndata bits when those data bits are being tangled with bits being transmitted by other\nsenders? CDMA works under the assumption that the interfering transmitted bit\nsignals are additive. This means, for example, that if three senders send a 1 value, and\na fourth sender sends a –1 value during the same mini-slot, then the received\nsignal at all receivers during that mini-slot is a 2 (since 1 \u0003 1 \u0003 1 \u0004 1 = 2). In the\npresence of multiple senders, sender s computes its encoded transmissions, Zs\ni,m, in\nexactly the same manner as in Equation 6.1. The value received at a receiver during\nthe mth mini-slot of the ith bit slot, however, is now the sum of the transmitted bits\nfrom all N senders during that mini-slot:\nAmazingly, if the senders’ codes are chosen carefully, each receiver can recover the\ndata sent by a given sender out of the aggregate signal simply by using the sender’s\ncode in exactly the same manner as in Equation 6.2:\n(6.3)\nas shown in Figure 6.6, for a two-sender CDMAexample. The M-bit CDMAcode being\nused by the upper sender is (1, 1, 1, –1, 1, –1, –1, –1), while the CDMA code being used\nby the lower sender is (1, –1, 1, 1, 1, –1, 1, 1). Figure 6.6 illustrates a receiver recovering\nthe original data bits from the upper sender. Note that the receiver is able to extract\nthe data from sender 1 in spite of the interfering transmission from sender 2.\nRecall our cocktail analogy from Chapter 5. A CDMA protocol is similar to hav-\ning partygoers speaking in multiple languages; in such circumstances humans are\nactually quite good at locking into the conversation in the language they understand,\nwhile filtering out the remaining conversations. We see here that CDMA is a partition-\ning protocol in that it partitions the codespace (as opposed to time or frequency) and\nassigns each node a dedicated piece of the codespace.\ndi = 1\nM a\nM\nm=1\nZi, m\n*\n\u0002 cm\nZ*\ni, m = a\nN\ns=1\nZs\ni, m\ndi = 1\nM a\nM\nm=1\nZi, m \u0002 cm\n524\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\n6.2\n•\nWIRELESS LINKS AND NETWORK CHARACTERISTICS\n525\nReceiver 1\n1\n1 1 1\n-1\n-1 -1 -1\n1\n1 1 1\n-1\n-1 -1 -1\nTime slot 1\nreceived input\nTime slot 0\nreceived input\nData bits\nData bits\n1\n1 1 1\n-1\n-1 -1 -1\n1\n1 1 1\n-1\n-1 -1 -1\nCode\nSenders\n1 1 1\n-1\n1 1 1\n-1\n1\n-1\n-1\n1 1 1\n1 1\nCode\nCode\n+\n-2\n2\n2 2 2\n2\n-2\n2\n-2\n2\n2 2 2\n2\n-2\n2\nChannel, Zi,m\n*\nZi,m\ndi • cm\n=\nZi,m • cm\nd\nM\ni\nm=1\nM\n=\n∑\nd1 = -1\nd0 = 1\nd1 = 1\n2\n1\n1\n*\n2\n2\n2\nZi,m\ndi • cm\n=\n1\n1\n1\nd0 = 1\n2\n1\n1\nd1 = -1\nd0 = 1\n1\n1\nFigure 6.6 \u0002 A two-sender CDMA example\nOur discussion here of CDMA is necessarily brief; in practice a number of\ndifficult issues must be addressed. First, in order for the CDMA receivers to be\nable to extract a particular sender’s signal, the CDMA codes must be carefully\nchosen. Second, our discussion has assumed that the received signal strengths"
    },
    {
      "chunk_id": "c5c3558f-00f9-4866-95dd-7bdbca6d1139",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.3 WiFi: 802.11 Wireless LANs",
      "original_titles": [
        "6.3 WiFi: 802.11 Wireless LANs"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.3 WiFi: 802.11 Wireless LANs",
      "start_page": 553,
      "end_page": 553,
      "token_count": 449,
      "text": "from various senders are the same; in reality this can be difficult to achieve. There\nis a considerable body of literature addressing these and other issues related to\nCDMA; see [Pickholtz 1982; Viterbi 1995] for details.\n6.3 WiFi: 802.11 Wireless LANs\nPervasive in the workplace, the home, educational institutions, cafés, airports, and\nstreet corners, wireless LANs are now one of the most important access network\ntechnologies in the Internet today. Although many technologies and standards for\nwireless LANs were developed in the 1990s, one particular class of standards has\nclearly emerged as the winner: the IEEE 802.11 wireless LAN, also known as\nWiFi. In this section, we’ll take a close look at 802.11 wireless LANs, examining\nits frame structure, its medium access protocol, and its internetworking of 802.11\nLANs with wired Ethernet LANs.\nThere are several 802.11 standards for wireless LAN technology, including\n802.11b, 802.11a, and 802.11g. Table 6.1 summarizes the main characteristics of\nthese standards. 802.11g is by far the most popular technology. A number of dual-\nmode (802.11a/g) and tri-mode (802.11a/b/g) devices are also available.\nThe three 802.11 standards share many characteristics. They all use the same\nmedium access protocol, CSMA/CA, which we’ll discuss shortly. All three use the\nsame frame structure for their link-layer frames as well. All three standards have the\nability to reduce their transmission rate in order to reach out over greater distances.\nAnd all three standards allow for both “infrastructure mode” and “ad hoc mode,” as\nwe’ll also shortly discuss. However, as shown in Table 6.1, the three standards have\nsome major differences at the physical layer.\nThe 802.11b wireless LAN has a data rate of 11 Mbps and operates in the\nunlicensed frequency band of 2.4–2.485 GHz, competing for frequency spectrum\nwith 2.4 GHz phones and microwave ovens. 802.1la wireless LANs can run at\n526\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nTable 6.1 \u0002 Summary of IEEE 802.11 standards \nStandard\nFrequency Range (United States)\nData Rate\n802.11b\n2.4–2.485 GHz\nup to 11 Mbps\n802.11a\n5.1–5.8 GHz\nup to 54 Mbps\n802.11g\n2.4–2.485 GHz\nup to 54 Mbps"
    },
    {
      "chunk_id": "8e118fa5-25e2-4f04-bfa2-4f32d6117dd9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.3.1 The 802.11 Architecture",
      "original_titles": [
        "6.3.1 The 802.11 Architecture"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.3 WiFi: 802.11 Wireless LANs > 6.3.1 The 802.11 Architecture",
      "start_page": 554,
      "end_page": 557,
      "token_count": 2091,
      "text": "significantly higher bit rates, but do so at higher frequencies. By operating at a\nhigher frequency, 802.11a LANs have a shorter transmission distance for a given\npower level and suffer more from multipath propagation. 802.11g LANs, operat-\ning in the same lower-frequency band as 802.11b and being backwards compati-\nble with 802.11b (so one can upgrade 802.11b clients incrementally) yet with the\nhigher-speed transmission rates of 802.11a, allows users to have their cake and\neat it too.\nA relatively new WiFi standard, 802.11n [IEEE 802.11n 2012], uses multiple-\ninput multiple-output (MIMO) antennas; i.e., two or more antennas on the \nsending side and two or more antennas on the receiving side that are transmit-\nting/receiving different signals [Diggavi 2004]. Depending on the modulation\nscheme used, transmission rates of several hundred megabits per second are pos-\nsible with 802.11n.\n6.3.1 The 802.11 Architecture\nFigure 6.7 illustrates the principal components of the 802.11 wireless LAN archi-\ntecture. The fundamental building block of the 802.11 architecture is the basic\nservice set (BSS). A BSS contains one or more wireless stations and a central\n6.3\n•\nWIFI: 802.11 WIRELESS LANS\n527\nInternet\nSwitch or router \nAP\nBSS 1 \nBSS 2 \nAP\nFigure 6.7 \u0002 IEEE 802.11 LAN architecture\n\nbase station, known as an access point (AP) in 802.11 parlance. Figure 6.7\nshows the AP in each of two BSSs connecting to an interconnection device (such\nas a switch or router), which in turn leads to the Internet. In a typical home net-\nwork, there is one AP and one router (typically integrated together as one unit)\nthat connects the BSS to the Internet.\nAs with Ethernet devices, each 802.11 wireless station has a 6-byte MAC\naddress that is stored in the firmware of the station’s adapter (that is, 802.11 network\ninterface card). Each AP also has a MAC address for its wireless interface. As with\nEthernet, these MAC addresses are administered by IEEE and are (in theory) glob-\nally unique.\nAs noted in Section 6.1, wireless LANs that deploy APs are often referred to\nas infrastructure wireless LANs, with the “infrastructure” being the APs along\nwith the wired Ethernet infrastructure that interconnects the APs and a router.\nFigure 6.8 shows that IEEE 802.11 stations can also group themselves together\nto form an ad hoc network—a network with no central control and with no con-\nnections to the “outside world.” Here, the network is formed “on the fly,” by\nmobile devices that have found themselves in proximity to each other, that have\na need to communicate, and that find no preexisting network infrastructure in\ntheir location. An ad hoc network might be formed when people with laptops get\ntogether (for example, in a conference room, a train, or a car) and want to\nexchange data in the absence of a centralized AP. There has been tremendous\ninterest in ad hoc networking, as communicating portable devices continue to\nproliferate. In this section, though, we’ll focus our attention on infrastructure\nwireless LANs.\n528\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nBSS\nFigure 6.8 \u0002 An IEEE 802.11 ad hoc network\n\nChannels and Association\nIn 802.11, each wireless station needs to associate with an AP before it can send or\nreceive network-layer data. Although all of the 802.11 standards use association,\nwe’ll discuss this topic specifically in the context of IEEE 802.11b/g.\nWhen a network administrator installs an AP, the administrator assigns a one-\nor two-word Service Set Identifier (SSID) to the access point. (When you “view\navailable networks” in Microsoft Windows XP, for example, a list is displayed\nshowing the SSID of each AP in range.) The administrator must also assign a chan-\nnel number to the AP. To understand channel numbers, recall that 802.11 operates in\nthe frequency range of 2.4 GHz to 2.485 GHz. Within this 85 MHz band, 802.11\ndefines 11 partially overlapping channels. Any two channels are non-overlapping if\nand only if they are separated by four or more channels. In particular, the set of\nchannels 1, 6, and 11 is the only set of three non-overlapping channels. This means\nthat an administrator could create a wireless LAN with an aggregate maximum\ntransmission rate of 33 Mbps by installing three 802.11b APs at the same physical\nlocation, assigning channels 1, 6, and 11 to the APs, and interconnecting each of the\nAPs with a switch.\nNow that we have a basic understanding of 802.11 channels, let’s describe an\ninteresting (and not completely uncommon) situation—that of a WiFi jungle. A\nWiFi jungle is any physical location where a wireless station receives a sufficiently\nstrong signal from two or more APs. For example, in many cafés in New York City,\na wireless station can pick up a signal from numerous nearby APs. One of the APs\nmight be managed by the café, while the other APs might be in residential apart-\nments near the café. Each of these APs would likely be located in a different IP sub-\nnet and would have been independently assigned a channel.\nNow suppose you enter such a WiFi jungle with your portable computer,\nseeking wireless Internet access and a blueberry muffin. Suppose there are five\nAPs in the WiFi jungle. To gain Internet access, your wireless station needs to join\nexactly one of the subnets and hence needs to associate with exactly one of the\nAPs. Associating means the wireless station creates a virtual wire between itself\nand the AP. Specifically, only the associated AP will send data frames (that is,\nframes containing data, such as a datagram) to your wireless station, and your\nwireless station will send data frames into the Internet only through the associated\nAP. But how does your wireless station associate with a particular AP? And more\nfundamentally, how does your wireless station know which APs, if any, are out\nthere in the jungle?\nThe 802.11 standard requires that an AP periodically send beacon frames, each\nof which includes the AP’s SSID and MAC address. Your wireless station, knowing\nthat APs are sending out beacon frames, scans the 11 channels, seeking beacon\nframes from any APs that may be out there (some of which may be transmitting on\nthe same channel—it’s a jungle out there!). Having learned about available APs\n6.3\n•\nWIFI: 802.11 WIRELESS LANS\n529\n\nfrom the beacon frames, you (or your wireless host) select one of the APs for\nassociation.\nThe 802.11 standard does not specify an algorithm for selecting which of the\navailable APs to associate with; that algorithm is left up to the designers of the 802.11\nfirmware and software in your wireless host. Typically, the host chooses the AP\nwhose beacon frame is received with the highest signal strength. While a high signal\nstrength is good (see, e.g., Figure 6.3), signal strength is not the only AP characteris-\ntic that will determine the performance a host receives. In particular, it’s possible that\nthe selected AP may have a strong signal, but may be overloaded with other affiliated\nhosts (that will need to share the wireless bandwidth at that AP), while an unloaded\nAP is not selected due to a slightly weaker signal. A number of alternative ways of\nchoosing APs have thus recently been proposed [Vasudevan 2005; Nicholson 2006;\nSundaresan 2006]. For an interesting and down-to-earth discussion of how signal\nstrength is measured, see [Bardwell 2004].\nThe process of scanning channels and listening for beacon frames is known as\npassive scanning (see Figure 6.9a). A wireless host can also perform active scan-\nning, by broadcasting a probe frame that will be received by all APs within the wire-\nless host’s range, as shown in Figure 6.9b. APs respond to the probe request frame\nwith a probe response frame. The wireless host can then choose the AP with which\nto associate from among the responding APs.\nAfter selecting the AP with which to associate, the wireless host sends an asso-\nciation request frame to the AP, and the AP responds with an association response\nframe. Note that this second request/response handshake is needed with active scan-\nning, since an AP responding to the initial probe request frame doesn’t know which\nof the (possibly many) responding APs the host will choose to associate with, in\nmuch the same way that a DHCP client can choose from among multiple DHCP\nservers (see Figure 4.21). Once associated with an AP, the host will want to join the\nsubnet (in the IP addressing sense of Section 4.4.2) to which the AP belongs. Thus,\nthe host will typically send a DHCP discovery message (see Figure 4.21) into the\nsubnet via the AP in order to obtain an IP address on the subnet. Once the address is\nobtained, the rest of the world then views that host simply as another host with an IP\naddress in that subnet.\nIn order to create an association with a particular AP, the wireless station may\nbe required to authenticate itself to the AP. 802.11 wireless LANs provide a number\nof alternatives for authentication and access. One approach, used by many compa-\nnies, is to permit access to a wireless network based on a station’s MAC address. A\nsecond approach, used by many Internet cafés, employs usernames and passwords.\nIn both cases, the AP typically communicates with an authentication server, relay-\ning information between the wireless end-point station and the authentication server\nusing a protocol such as RADIUS [RFC 2865] or DIAMETER [RFC 3588]. Sepa-\nrating the authentication server from the AP allows one authentication server to\nserve many APs, centralizing the (often sensitive) decisions of authentication and\naccess within the single server, and keeping AP costs and complexity low. We’ll see\n530\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS"
    },
    {
      "chunk_id": "20a9b86b-b994-4607-863c-82d78c6fb04e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.3.2 The 802.11 MAC Protocol",
      "original_titles": [
        "6.3.2 The 802.11 MAC Protocol"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.3 WiFi: 802.11 Wireless LANs > 6.3.2 The 802.11 MAC Protocol",
      "start_page": 558,
      "end_page": 563,
      "token_count": 2354,
      "text": "in Section 8.8 that the new IEEE 802.11i protocol defining security aspects of the\n802.11 protocol family takes precisely this approach.\n6.3.2 The 802.11 MAC Protocol\nOnce a wireless station is associated with an AP, it can start sending and receiving\ndata frames to and from the access point. But because multiple stations may want to\ntransmit data frames at the same time over the same channel, a multiple access proto-\ncol is needed to coordinate the transmissions. Here, a station is either a wireless sta-\ntion or an AP. As discussed in Chapter 5 and Section 6.2.1, broadly speaking there\nare three classes of multiple access protocols: channel partitioning (including\nCDMA), random access, and taking turns. Inspired by the huge success of Ethernet\nand its random access protocol, the designers of 802.11 chose a random access proto-\ncol for 802.11 wireless LANs. This random access protocol is referred to as CSMA\nwith collision avoidance, or more succinctly as CSMA/CA. As with Ethernet’s\nCSMA/CD, the “CSMA” in CSMA/CA stands for “carrier sense multiple access,”\nmeaning that each station senses the channel before transmitting, and refrains from\ntransmitting when the channel is sensed busy. Although both Ethernet and 802.11 use\ncarrier-sensing random access, the two MAC protocols have important differences.\n6.3\n•\nWIFI: 802.11 WIRELESS LANS\n531\n1\n1\n3\n2\nH1\nAP 2\nAP 1\nBBS 1\na. Passive scanning\n1. Beacon frames sent from APs\n2. Association Request frame sent:\n \nH1 to selected AP\n3. Association Response frame sent:\n \nSelected AP to H1\na. Active scanning\n \n1. Probe Request frame broadcast from H1\n \n2. Probes Response frame sent from APs\n \n3. Association Request frame sent:\n \n \nH1 to selected AP\n \n4. Association Response frame sent:\n \n \nSelected AP to H1\nBBS 2\n2\n2\n4\n3\nH1\nAP 2\nAP 1\nBBS 1\nBBS 2\n1\nFigure 6.9 \u0002 Active and passive scanning for access points\n\nFirst, instead of using collision detection, 802.11 uses collision-avoidance tech-\nniques. Second, because of the relatively high bit error rates of wireless channels,\n802.11 (unlike Ethernet) uses a link-layer acknowledgment/retransmission (ARQ)\nscheme. We’ll describe 802.11’s collision-avoidance and link-layer acknowledgment\nschemes below.\nRecall from Sections 5.3.2 and 5.4.2 that with Ethernet’s collision-detection\nalgorithm, an Ethernet station listens to the channel as it transmits. If, while trans-\nmitting, it detects that another station is also transmitting, it aborts its transmission\nand tries to transmit again after waiting a small, random amount of time. Unlike the\n802.3 Ethernet protocol, the 802.11 MAC protocol does not implement collision\ndetection. There are two important reasons for this:\n•\nThe ability to detect collisions requires the ability to send (the station’s own sig-\nnal) and receive (to determine whether another station is also transmitting) at the\nsame time. Because the strength of the received signal is typically very small\ncompared to the strength of the transmitted signal at the 802.11 adapter, it is\ncostly to build hardware that can detect a collision.\n•\nMore importantly, even if the adapter could transmit and listen at the same time\n(and presumably abort transmission when it senses a busy channel), the adapter\nwould still not be able to detect all collisions, due to the hidden terminal problem\nand fading, as discussed in Section 6.2.\nBecause 802.11wireless LANs do not use collision detection, once a station\nbegins to transmit a frame, it transmits the frame in its entirety; that is, once a sta-\ntion gets started, there is no turning back. As one might expect, transmitting entire\nframes (particularly long frames) when collisions are prevalent can significantly\ndegrade a multiple access protocol’s performance. In order to reduce the likelihood\nof collisions, 802.11 employs several collision-avoidance techniques, which we’ll\nshortly discuss.\nBefore considering collision avoidance, however, we’ll first need to examine\n802.11’s link-layer acknowledgment scheme. Recall from Section 6.2 that when a\nstation in a wireless LAN sends a frame, the frame may not reach the destination\nstation intact for a variety of reasons. To deal with this non-negligible chance of fail-\nure, the 802.11 MAC protocol uses link-layer acknowledgments. As shown in\nFigure 6.10, when the destination station receives a frame that passes the CRC, it\nwaits a short period of time known as the Short Inter-frame Spacing (SIFS) and\nthen sends back an acknowledgment frame. If the transmitting station does not\nreceive an acknowledgment within a given amount of time, it assumes that an error\nhas occurred and retransmits the frame, using the CSMA/CA protocol to access the\nchannel. If an acknowledgment is not received after some fixed number of retrans-\nmissions, the transmitting station gives up and discards the frame.\n532\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\nHaving discussed how 802.11 uses link-layer acknowledgments, we’re now in\na position to describe the 802.11 CSMA/CA protocol. Suppose that a station (wire-\nless station or an AP) has a frame to transmit.\n1. If initially the station senses the channel idle, it transmits its frame after a short\nperiod of time known as the Distributed Inter-frame Space (DIFS); see\nFigure 6.10.\n2. Otherwise, the station chooses a random backoff value using binary exponen-\ntial backoff (as we encountered in  Section 5.3.2) and counts down this value\nwhen the channel is sensed idle. While the channel is sensed busy, the counter\nvalue remains frozen.\n3. When the counter reaches zero (note that this can only occur while the channel\nis sensed idle), the station transmits the entire frame and then waits for an\nacknowledgment.\n6.3\n•\nWIFI: 802.11 WIRELESS LANS\n533\nDestination\nDIFS\nSIFS\ndata\nack\nSource\nFigure 6.10 \u0002 802.11 uses link-layer acknowledgments\n\n4. If an acknowledgment is received, the transmitting station knows that its frame\nhas been correctly received at the destination station. If the station has another\nframe to send, it begins the CSMA/CA protocol at step 2. If the acknowledg-\nment isn’t received, the transmitting station reenters the backoff phase in step\n2, with the random value chosen from a larger interval.\nRecall that under Ethernet’s CSMA/CD, multiple access protocol (Section\n5.3.2), a station begins transmitting as soon as the channel is sensed idle. With\nCSMA/CA, however, the station refrains from transmitting while counting down,\neven when it senses the channel to be idle.Why do CSMA/CD and CDMA/CA take\nsuch different approaches here?\nTo answer this question, let’s consider a scenario in which two stations each\nhave a data frame to transmit, but neither station transmits immediately because\neach senses that a third station is already transmitting. With Ethernet’s CSMA/CD,\nthe two stations would each transmit as soon as they detect that the third station has\nfinished transmitting. This would cause a collision, which isn’t a serious issue in\nCSMA/CD, since both stations would abort their transmissions and thus avoid the\nuseless transmissions of the remainders of their frames. In 802.11, however, the sit-\nuation is quite different. Because 802.11 does not detect a collision and abort trans-\nmission, a frame suffering a collision will be transmitted in its entirety. The goal in\n802.11 is thus to avoid collisions whenever possible. In 802.11, if the two stations\nsense the channel busy, they both immediately enter random backoff, hopefully\nchoosing different backoff values. If these values are indeed different, once the\nchannel becomes idle, one of the two stations will begin transmitting before the\nother, and (if the two stations are not hidden from each other) the “losing station”\nwill hear the “winning station’s” signal, freeze its counter, and refrain from trans-\nmitting until the winning station has completed its transmission. In this manner, a\ncostly collision is avoided. Of course, collisions can still occur with 802.11 in this\nscenario: The two stations could be hidden from each other, or the two stations\ncould choose random backoff values that are close enough that the transmission\nfrom the station starting first have yet to reach the second station. Recall that we\nencountered this problem earlier in our discussion of random access algorithms in\nthe context of Figure 5.12.\nDealing with Hidden Terminals: RTS and CTS\nThe 802.11 MAC protocol also includes a nifty (but optional) reservation scheme\nthat helps avoid collisions even in the presence of hidden terminals. Let’s investi-\ngate this scheme in the context of Figure 6.11, which shows two wireless stations\nand one access point. Both of the wireless stations are within range of the AP\n(whose coverage is shown as a shaded circle) and both have associated with the AP.\nHowever, due to fading, the signal ranges of wireless stations are limited to the\n534\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\ninteriors of the shaded circles shown in Figure 6.11. Thus, each of the wireless sta-\ntions is hidden from the other, although neither is hidden from the AP.\nLet’s now consider why hidden terminals can be problematic. Suppose Station\nH1 is transmitting a frame and halfway through H1’s transmission, Station H2 wants\nto send a frame to the AP. H2, not hearing the transmission from H1, will first wait a\nDIFS interval and then transmit the frame, resulting in a collision. The channel will\ntherefore be wasted during the entire period of H1’s transmission as well as during\nH2’s transmission.\nIn order to avoid this problem, the IEEE 802.11 protocol allows a station to use\na short Request to Send (RTS) control frame and a short Clear to Send (CTS) con-\ntrol frame to reserve access to the channel. When a sender wants to send a DATA\nframe, it can first send an RTS frame to the AP, indicating the total time required to\ntransmit the DATA frame and the acknowledgment (ACK) frame. When the AP\nreceives the RTS frame, it responds by broadcasting a CTS frame. This CTS frame\nserves two purposes: It gives the sender explicit permission to send and also\ninstructs the other stations not to send for the reserved duration.\nThus, in Figure 6.12, before transmitting a DATA frame, H1 first broadcasts an\nRTS frame, which is heard by all stations in its circle, including the AP. The AP then\nresponds with a CTS frame, which is heard by all stations within its range, includ-\ning H1 and H2. Station H2, having heard the CTS, refrains from transmitting for the\ntime specified in the CTS frame. The RTS, CTS, DATA, and ACK frames are shown\nin Figure 6.12.\n6.3\n•\nWIFI: 802.11 WIRELESS LANS\n535\nAP\nH1\nH2\nFigure 6.11 \u0002 Hidden terminal example: H1 is hidden from H2, \nand vice versa\n\nThe use of the RTS and CTS frames can improve performance in two important\nways:\n•\nThe hidden station problem is mitigated, since a long DATA frame is transmitted\nonly after the channel has been reserved.\n536\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nDestination \nAll other nodes \nDefer access \nSource \nDIFS\nACK\nSIFS\nSIFS\nSIFS\nDATA \nCTS\nCTS\nACK\nRTS \nFigure 6.12 \u0002 Collision avoidance using the RTS and CTS frames"
    },
    {
      "chunk_id": "bd98d822-4189-4b5f-b7fc-17fd21f300aa",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.3.3 The IEEE 802.11 Frame",
      "original_titles": [
        "6.3.3 The IEEE 802.11 Frame"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.3 WiFi: 802.11 Wireless LANs > 6.3.3 The IEEE 802.11 Frame",
      "start_page": 564,
      "end_page": 567,
      "token_count": 2007,
      "text": "•\nBecause the RTS and CTS frames are short, a collision involving an RTS or CTS\nframe will last only for the duration of the short RTS or CTS frame. Once the\nRTS and CTS frames are correctly transmitted, the following DATA and ACK\nframes should be transmitted without collisions.\nYou are encouraged to check out the 802.11 applet in the textbook’s companion Web\nsite. This interactive applet illustrates the CSMA/CA protocol, including the\nRTS/CTS exchange sequence.\nAlthough the RTS/CTS exchange can help reduce collisions, it also introduces\ndelay and consumes channel resources. For this reason, the RTS/CTS exchange is\nonly used (if at all) to reserve the channel for the transmission of a long DATA\nframe. In practice, each wireless station can set an RTS threshold such that the\nRTS/CTS sequence is used only when the frame is longer than the threshold. For\nmany wireless stations, the default RTS threshold value is larger than the maximum\nframe length, so the RTS/CTS sequence is skipped for all DATA frames sent.\nUsing 802.11 as a Point-to-Point Link\nOur discussion so far has focused on the use of 802.11 in a multiple access setting.\nWe should mention that if two nodes each have a directional antenna, they can point\ntheir directional antennas at each other and run the 802.11 protocol over what is\nessentially a point-to-point link. Given the low cost of commodity 802.11 hardware,\nthe use of directional antennas and an increased transmission power allow 802.11 to\nbe used as an inexpensive means of providing wireless point-to-point connections\nover tens of kilometers distance. [Raman 2007] describes such a multi-hop wireless\nnetwork operating in the rural Ganges plains in India that contains point-to-point\n802.11 links.\n6.3.3 The IEEE 802.11 Frame\nAlthough the 802.11 frame shares many similarities with an Ethernet frame, it also\ncontains a number of fields that are specific to its use for wireless links. The 802.11\nframe is shown in Figure 6.13. The numbers above each of the fields in the frame\nrepresent the lengths of the fields in bytes; the numbers above each of the subfields\nin the frame control field represent the lengths of the subfields in bits. Let’s now\nexamine the fields in the frame as well as some of the more important subfields in\nthe frame’s control field.\nPayload and CRC Fields\nAt the heart of the frame is the payload, which typically consists of an IP datagram\nor an ARP packet. Although the field is permitted to be as long as 2,312 bytes, it is\n6.3\n•\nWIFI: 802.11 WIRELESS LANS\n537\n\ntypically fewer than 1,500 bytes, holding an IP datagram or an ARP packet. As with\nan Ethernet frame, an 802.11 frame includes a 32-bit cyclic redundancy check\n(CRC) so that the receiver can detect bit errors in the received frame. As we’ve seen,\nbit errors are much more common in wireless LANs than in wired LANs, so the\nCRC is even more useful here.\nAddress Fields\nPerhaps the most striking difference in the 802.11 frame is that it has four address\nfields, each of which can hold a 6-byte MAC address. But why four address fields?\nDoesn’t a source MAC field and destination MAC field suffice, as they do for\nEthernet? It turns out that three address fields are needed for internetworking pur-\nposes—specifically, for moving the network-layer datagram from a wireless station\nthrough an AP to a router interface. The fourth address field is used when APs for-\nward frames to each other in ad hoc mode. Since we are only considering infrastruc-\nture networks here, let’s focus our attention on the first three address fields. The\n802.11 standard defines these fields as follows:\n•\nAddress 2 is the MAC address of the station that transmits the frame. Thus, if a\nwireless station transmits the frame, that station’s MAC address is inserted in the\naddress 2 field. Similarly, if an AP transmits the frame, the AP’s MAC address is\ninserted in the address 2 field.\n•\nAddress 1 is the MAC address of the wireless station that is to receive the\nframe. Thus if a mobile wireless station transmits the frame, address 1 contains\nthe MAC address of the destination AP. Similarly, if an AP transmits the frame,\naddress 1 contains the MAC address of the destination wireless station.\n538\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nFrame\ncontrol\n2\n2\n2\n4\n1\n1\n1\n1\n1\n1\n1\n1\n2\n6\n6\n6\n2\n6\n0-2312\n4\nFrame (numbers indicate field length in bytes):\nAddress\n1\nDuration\nPayload\nCRC\nProtocol\nversion\nTo\nAP\nFrom\nAP\nMore\nfrag\nPower\nmgt\nMore\ndata\nAddress\n2\nAddress\n3\nAddress\n4\nSeq\ncontrol\nType\nSubtype\nRetry\nWEP\nRsvd\nFrame control field expanded (numbers indicate field length in bits):\nFigure 6.13 \u0002 The 802.11 frame\n\n•\nTo understand address 3, recall that the BSS (consisting of the AP and wireless sta-\ntions) is part of a subnet, and that this subnet connects to other subnets via some\nrouter interface. Address 3 contains the MAC address of this router interface.\nTo gain further insight into the purpose of address 3, let’s walk through an inter-\nnetworking example in the context of Figure 6.14. In this figure, there are two APs,\neach of which is responsible for a number of wireless stations. Each of the APs has\na direct connection to a router, which in turn connects to the global Internet. We\nshould keep in mind that an AP is a link-layer device, and thus neither “speaks” IP\nnor understands IP addresses. Consider now moving a datagram from the router\ninterface R1 to the wireless Station H1. The router is not aware that there is an AP\nbetween it and H1; from the router’s perspective, H1 is just a host in one of the sub-\nnets to which it (the router) is connected.\n•\nThe router, which knows the IP address of H1 (from the destination address of\nthe datagram), uses ARP to determine the MAC address of H1, just as in an\nordinary Ethernet LAN. After obtaining H1’s MAC address, router interface R1\nencapsulates the datagram within an Ethernet frame. The source address field of\nthis frame contains R1’s MAC address, and the destination address field contains\nH1’s MAC address.\n6.3\n•\nWIFI: 802.11 WIRELESS LANS\n539\nInternet\nRouter\nAP\nH1\nR1\nBSS 1\nBSS 2\nAP\nFigure 6.14 \u0002 The use of address fields in 802.11 frames: Sending\nframes between H1 and R1\n\n•\nWhen the Ethernet frame arrives at the AP, the AP converts the 802.3 Ethernet\nframe to an 802.11 frame before transmitting the frame into the wireless chan-\nnel. The AP fills in address 1 and address 2 with H1’s MAC address and its\nown MAC address, respectively, as described above. For address 3, the AP\ninserts the MAC address of R1. In this manner, H1 can determine (from\naddress 3) the MAC address of the router interface that sent the datagram into\nthe subnet.\nNow consider what happens when the wireless station H1 responds by moving a\ndatagram from H1 to R1.\n•\nH1 creates an 802.11 frame, filling the fields for address 1 and address 2 with the\nAP’s MAC address and H1’s MAC address, respectively, as described above. For\naddress 3, H1 inserts R1’s MAC address.\n•\nWhen the AP receives the 802.11 frame, it converts the frame to an Ethernet\nframe. The source address field for this frame is H1’s MAC address, and the des-\ntination address field is R1’s MAC address. Thus, address 3 allows the AP to\ndetermine the appropriate destination MAC address when constructing the Eth-\nernet frame.\nIn summary, address 3 plays a crucial role for internetworking the BSS with a wired\nLAN.\nSequence Number, Duration, and Frame Control Fields\nRecall that in 802.11, whenever a station correctly receives a frame from another\nstation, it sends back an acknowledgment. Because acknowledgments can get lost,\nthe sending station may send multiple copies of a given frame. As we saw in our dis-\ncussion of the rdt2.1 protocol (Section 3.4.1), the use of sequence numbers allows\nthe receiver to distinguish between a newly transmitted frame and the retransmis-\nsion of a previous frame. The sequence number field in the 802.11 frame thus serves\nexactly the same purpose here at the link layer as it did in the transport layer in\nChapter 3.\nRecall that the 802.11 protocol allows a transmitting station to reserve the chan-\nnel for a period of time that includes the time to transmit its data frame and the time\nto transmit an acknowledgment. This duration value is included in the frame’s dura-\ntion field (both for data frames and for the RTS and CTS frames).\nAs shown in Figure 6.13, the frame control field includes many subfields. We’ll\nsay just a few words about some of the more important subfields; for a more com-\nplete discussion, you are encouraged to consult the 802.11 specification [Held 2001;\nCrow 1997; IEEE 802.11 1999]. The type and subtype fields are used to distinguish\nthe association, RTS, CTS, ACK, and data frames. The to and from fields are used\nto define the meanings of the different address fields. (These meanings change\n540\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS"
    },
    {
      "chunk_id": "562833df-4f11-440c-bf70-3d6b86390163",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.3.4 Mobility in the Same IP Subnet",
      "original_titles": [
        "6.3.4 Mobility in the Same IP Subnet"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.3 WiFi: 802.11 Wireless LANs > 6.3.4 Mobility in the Same IP Subnet",
      "start_page": 568,
      "end_page": 568,
      "token_count": 416,
      "text": "depending on whether ad hoc or infrastructure modes are used and, in the case of\ninfrastructure mode, whether a wireless station or an AP is sending the frame.)\nFinally the WEP field indicates whether encryption is being used or not. (WEP is\ndiscussed in Chapter 8.)\n6.3.4 Mobility in the Same IP Subnet\nIn order to increase the physical range of a wireless LAN, companies and universities\nwill often deploy multiple BSSs within the same IP subnet. This naturally raises the\nissue of mobility among the BSSs—how do wireless stations seamlessly move from\none BSS to another while maintaining ongoing TCP sessions? As we’ll see in this sub-\nsection, mobility can be handled in a relatively straightforward manner when the BSSs\nare part of the subnet. When stations move between subnets, more sophisticated mobil-\nity management protocols will be needed, such as those we’ll study in Sections 6.5\nand 6.6.\nLet’s now look at a specific example of mobility between BSSs in the same\nsubnet. Figure 6.15 shows two interconnected BSSs with a host, H1, moving\nfrom BSS1 to BSS2. Because in this example the interconnection device that\nconnects the two BSSs is not a router, all of the stations in the two BSSs, includ-\ning the APs, belong to the same IP subnet. Thus, when H1 moves from BSS1 to\nBSS2, it may keep its IP address and all of its ongoing TCP connections. If the\ninterconnection device were a router, then H1 would have to obtain a new IP\naddress in the subnet in which it was moving. This address change would disrupt\n(and eventually terminate) any on-going TCP connections at H1. In Section 6.6,\nwe’ll see how a network-layer mobility protocol, such as mobile IP, can be used\nto avoid this problem.\n6.3\n•\nWIFI: 802.11 WIRELESS LANS\n541\nBSS 1 \nBSS 2 \nH1\nSwitch\nAP 1 \nAP 2 \nFigure 6.15 \u0002 Mobility in the same subnet"
    },
    {
      "chunk_id": "c28bcc9b-a149-4b75-8ee9-3e5723f8e90a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.3.5 Advanced Features in 802.11",
      "original_titles": [
        "6.3.5 Advanced Features in 802.11"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.3 WiFi: 802.11 Wireless LANs > 6.3.5 Advanced Features in 802.11",
      "start_page": 569,
      "end_page": 570,
      "token_count": 1297,
      "text": "But what specifically happens when H1 moves from BSS1 to BSS2? As H1\nwanders away from AP1, H1 detects a weakening signal from AP1 and starts to scan\nfor a stronger signal. H1 receives beacon frames from AP2 (which in many corpo-\nrate and university settings will have the same SSID as AP1). H1 then disassociates\nwith AP1 and associates with AP2, while keeping its IP address and maintaining its\nongoing TCP sessions.\nThis addresses the handoff problem from the host and AP viewpoint. But what\nabout the switch in Figure 6.15? How does it know that the host has moved from\none AP to another? As you may recall from Chapter 5, switches are “self-learning”\nand automatically build their forwarding tables. This self-learning feature nicely\nhandles occasional moves (for example, when an employee gets transferred from\none department to another); however, switches were not designed to support highly\nmobile users who want to maintain TCP connections while moving between BSSs.\nTo appreciate the problem here, recall that before the move, the switch has an entry\nin its forwarding table that pairs H1’s MAC address with the outgoing switch inter-\nface through which H1 can be reached. If H1 is initially in BSS1, then a datagram\ndestined to H1 will be directed to H1 via AP1. Once H1 associates with BSS2, how-\never, its frames should be directed to AP2. One solution (a bit of a hack, really) is\nfor AP2 to send a broadcast Ethernet frame with H1’s source address to the switch\njust after the new association. When the switch receives the frame, it updates its for-\nwarding table, allowing H1 to be reached via AP2. The 802.11f standards group is\ndeveloping an inter-AP protocol to handle these and related issues.\n6.3.5 Advanced Features in 802.11\nWe’ll wrap up our coverage of 802.11 with a short discussion of two advanced capa-\nbilities found in 802.11 networks. As we’ll see, these capabilities are not completely\nspecified in the 802.11 standard, but rather are made possible by mechanisms speci-\nfied in the standard. This allows different vendors to implement these capabilities\nusing their own (proprietary) approaches, presumably giving them an edge over the\ncompetition.\n802.11 Rate Adaptation\nWe saw earlier in Figure 6.3 that different modulation techniques (with the differ-\nent transmission rates that they provide) are appropriate for different SNR scenar-\nios. Consider for example a mobile 802.11 user who is initially 20 meters away\nfrom the base station, with a high signal-to-noise ratio. Given the high SNR, the\nuser can communicate with the base station using a physical-layer modulation tech-\nnique that provides high transmission rates while maintaining a low BER. This is\none happy user! Suppose now that the user becomes mobile, walking away from\n542\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\nthe base station, with the SNR falling as the distance from the base station\nincreases. In this case, if the modulation technique used in the 802.11 protocol\noperating between the base station and the user does not change, the BER will\nbecome unacceptably high as the SNR decreases, and eventually no transmitted\nframes will be received correctly.\nFor this reason, some 802.11 implementations have a rate adaptation capability\nthat adaptively selects the underlying physical-layer modulation technique to use\nbased on current or recent channel characteristics. If a node sends two frames in a\nrow without receiving an acknowledgment (an implicit indication of bit errors on\nthe channel), the transmission rate falls back to the next lower rate. If 10 frames in a\nrow are acknowledged, or if a timer that tracks the time since the last fallback\nexpires, the transmission rate increases to the next higher rate. This rate adaptation\nmechanism shares the same “probing” philosophy as TCP’s congestion-control\nmechanism—when conditions are good (reflected by ACK receipts), the transmis-\nsion rate is increased until something “bad” happens (the lack of ACK receipts);\nwhen something “bad” happens, the transmission rate is reduced. 802.11 rate adap-\ntation and TCP congestion control are thus similar to the young child who is con-\nstantly pushing his/her parents for more and more (say candy for a young child, later\ncurfew hours for the teenager) until the parents finally say “Enough!” and the child\nbacks off (only to try again later after conditions have hopefully improved!). A num-\nber of other schemes have also been proposed to improve on this basic automatic\nrate-adjustment scheme [Kamerman 1997; Holland 2001; Lacage 2004].\nPower Management\nPower is a precious resource in mobile devices, and thus the 802.11 standard pro-\nvides power-management capabilities that allow 802.11 nodes to minimize the\namount of time that their sense, transmit, and receive functions and other circuitry\nneed to be “on.” 802.11 power management operates as follows. A node is able to\nexplicitly alternate between sleep and wake states (not unlike a sleepy student in a\nclassroom!). A node indicates to the access point that it will be going to sleep by set-\nting the power-management bit in the header of an 802.11 frame to 1. A timer in the\nnode is then set to wake up the node just before the AP is scheduled to send its bea-\ncon frame (recall that an AP typically sends a beacon frame every 100 msec). Since\nthe AP knows from the set power-transmission bit that the node is going to sleep, it\n(the AP) knows that it should not send any frames to that node, and will buffer any\nframes destined for the sleeping host for later transmission.\nA node will wake up just before the AP sends a beacon frame, and quickly enter\nthe fully active state (unlike the sleepy student, this wakeup requires only 250\nmicroseconds [Kamerman 1997]!). The beacon frames sent out by the AP contain a\nlist of nodes whose frames have been buffered at the AP. If there are no buffered\nframes for the node, it can go back to sleep. Otherwise, the node can explicitly\n6.3\n•\nWIFI: 802.11 WIRELESS LANS\n543"
    },
    {
      "chunk_id": "c4d6a2db-db56-472d-8463-7419d36f4487",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.3.6 Personal Area Networks: Bluetooth and Zigbee",
      "original_titles": [
        "6.3.6 Personal Area Networks: Bluetooth and Zigbee"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.3 WiFi: 802.11 Wireless LANs > 6.3.6 Personal Area Networks: Bluetooth and Zigbee",
      "start_page": 571,
      "end_page": 572,
      "token_count": 967,
      "text": "request that the buffered frames be sent by sending a polling message to the AP.\nWith an inter-beacon time of 100 msec, a wakeup time of 250 microseconds, and a\nsimilarly small time to receive a beacon frame and check to ensure that there are no\nbuffered frames, a node that has no frames to send or receive can be asleep 99% of\nthe time, resulting in a significant energy savings.\n6.3.6 Personal Area Networks: Bluetooth and Zigbee\nAs illustrated in Figure 6.2, the IEEE 802.11 WiFi standard is aimed at communi-\ncation among devices separated by up to 100 meters (except when 802.11 is used in\na point-to-point configuration with a directional antenna). Two other IEEE 802\nprotocols—Bluetooth and Zigbee (defined in the IEEE 802.15.1 and IEEE 802.15.4\nstandards [IEEE 802.15 2012]) and WiMAX (defined in the IEEE 802.16 standard\n[IEEE 802.16d 2004; IEEE 802.16e 2005])—are standards for communicating\nover shorter and longer distances, respectively. We will touch on WiMAX briefly\nwhen we discuss cellular data networks in Section 6.4, and so here, we will focus\non networks for shorter distances.\nBluetooth\nAn IEEE 802.15.1 network operates over a short range, at low power, and at low\ncost. It is essentially a low-power, short-range, low-rate “cable replacement” technol-\nogy for interconnecting notebooks, peripheral devices, cellular phones, and smart-\nphones, whereas 802.11 is a higher-power, medium-range, higher-rate “access”\ntechnology. For this reason, 802.15.1 networks are sometimes referred to as wireless\npersonal area networks (WPANs). The link and physical layers of 802.15.1 are based\non the earlier Bluetooth specification for personal area networks [Held 2001, Bis-\ndikian 2001]. 802.15.1 networks operate in the 2.4 GHz unlicensed radio band in a\nTDM manner, with time slots of 625 microseconds. During each time slot, a sender\ntransmits on one of 79 channels, with the channel changing in a known but pseudo-\nrandom manner from slot to slot. This form of channel hopping, known as\nfrequency-hopping spread spectrum (FHSS), spreads transmissions in time over\nthe frequency spectrum. 802.15.1 can provide data rates up to 4 Mbps.\n802.15.1 networks are ad hoc networks: No network infrastructure (e.g., an\naccess point) is needed to interconnect 802.15.1 devices. Thus, 802.15.1 devices\nmust organize themselves. 802.15.1 devices are first organized into a piconet of up\nto eight active devices, as shown in Figure 6.16. One of these devices is designated\nas the master, with the remaining devices acting as slaves. The master node truly\nrules the piconet—its clock determines time in the piconet, it can transmit in each\nodd-numbered slot, and a slave can transmit only after the master has communicated\nwith it in the previous slot and even then the slave can only transmit to the master.\nIn addition to the slave devices, there can also be up to 255 parked devices in the\n544\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\nnetwork. These devices cannot communicate until their status has been changed\nfrom parked to active by the master node.\nFor more information about 802.15.1 WPANs, the interested reader should con-\nsult the Bluetooth references [Held 2001, Bisdikian 2001] or the official IEEE\n802.15 Web site [IEEE 802.15 2012].\nZigbee\nA second personal area network standardized by the IEEE is the 802.14.5 standard\n[IEEE 802.15 2012] known as Zigbee. While Bluetooth networks provide a “cable\nreplacement” data rate of over a Megabit per second, Zigbee is targeted at lower-\npowered, lower-data-rate, lower-duty-cycle applications than Bluetooth. While we\nmay tend to think that “bigger and faster is better,” not all network applications need\nhigh bandwidth and the consequent higher costs (both economic and power costs).\nFor example, home temperature and light sensors, security devices, and wall-\nmounted switches are all very simple, low-power, low-duty-cycle, low-cost devices.\nZigbee is thus well-suited for these devices. Zigbee defines channel rates of 20, 40,\n100, and 250 Kbps, depending on the channel frequency.\nNodes in a Zigbee network come in two flavors. So-called “reduced-function\ndevices” operate as slave devices under the control of a single “full-function\ndevice,” much as Bluetooth slave devices. A full-function device can operate as a\nmaster device as in Bluetooth by controlling multiple slave devices, and multiple\nfull-function devices can additionally be configured into a mesh network in\nwhich full-function devices route frames amongst themselves. Zigbee shares\n6.3\n•\nWIFI: 802.11 WIRELESS LANS\n545\nRadius of\ncoverage\nMaster device\nSlave device\nParked device\nKey:\nM\nM\nS\nS\nS\nS\nP\nP\nP\nP\nP\nFigure 6.16 \u0002 A Bluetooth piconet"
    },
    {
      "chunk_id": "db6aa085-5bc6-4e68-8081-b0a89a8b3e95",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.4 Cellular Internet Access",
      "original_titles": [
        "6.4 Cellular Internet Access"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.4 Cellular Internet Access",
      "start_page": 573,
      "end_page": 573,
      "token_count": 432,
      "text": "many protocol mechanisms that we’ve already encountered in other link-layer\nprotocols: beacon frames and link-layer acknowledgments (similar to 802.11),\ncarrier-sense random access protocols with binary exponential backoff (similar\nto 802.11 and Ethernet), and fixed, guaranteed allocation of time slots (similar to\nDOCSIS).\nZigbee networks can be configured in many different ways. Let’s consider the\nsimple case of a single full-function device controlling multiple reduced-function\ndevices in a time-slotted manner using beacon frames. Figure 6.17 shows the case\nwhere the Zigbee network divides time into recurring super frames, each of which\nbegins with a beacon frame. Each beacon frame divides the super frame into an\nactive period (during which devices may transmit) and an inactive period (during\nwhich all devices, including the controller, can sleep and thus conserve power). The\nactive period consists of 16 time slots, some of which are used by devices in a\nCSMA/CA random access manner, and some of which are allocated by the con-\ntroller to specific devices, thus providing guaranteed channel access for those\ndevices. More details about Zigbee networks can be found at [Baronti 2007, IEEE\n802.15.4 2012].\n6.4 Cellular Internet Access\nIn the previous section we examined how an Internet host can access the Internet\nwhen inside a WiFi hotspot—that is, when it is within the vicinity of an 802.11\naccess point. But most WiFi hotspots have a small coverage area of between 10 and\n100 meters in diameter. What do we do then when we have a desperate need for\nwireless Internet access and we cannot access a WiFi hotspot?\nGiven that cellular telephony is now ubiquitous in many areas throughout the\nworld, a natural strategy is to extend cellular networks so that they support not\nonly voice telephony but wireless Internet access as well. Ideally, this Internet\naccess would be at a reasonably high speed and would provide for seamless\n546\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nFigure 6.17 \u0002 Zigbee 802.14.4 super-frame structure\nBeacon\nGuaranteed slots\nContention slots\nInactive period\nSuper frame"
    },
    {
      "chunk_id": "428255f1-8c62-4afe-b85a-35152c06b633",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.4.1 An Overview of Cellular Network Architecture",
      "original_titles": [
        "6.4.1 An Overview of Cellular Network Architecture"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.4 Cellular Internet Access > 6.4.1 An Overview of Cellular Network Architecture",
      "start_page": 574,
      "end_page": 576,
      "token_count": 1452,
      "text": "mobility, allowing users to maintain their TCP sessions while traveling, for\nexample, on a bus or a train. With sufficiently high upstream and downstream bit\nrates, the user could even maintain video-conferencing sessions while roaming\nabout. This scenario is not that far-fetched. As of 2012, many cellular telephony\nproviders in the U.S. offer their subscribers a cellular Internet access service for\nunder $50 per month with typical downstream and upstream bit rates in the hun-\ndreds of kilobits per second. Data rates of several megabits per second are\nbecoming available as broadband data services such as those we will cover here\nbecome more widely deployed.\nIn this section, we provide a brief overview of current and emerging cellular\nInternet access technologies. Our focus here will be on both the wireless first hop as\nwell as the network that connects the wireless first hop into the larger telephone net-\nwork and/or the Internet; in Section 6.7 we’ll consider how calls are routed to a user\nmoving between base stations. Our brief discussion will necessarily provide only a\nsimplified and high-level description of cellular technologies. Modern cellular com-\nmunications, of course, has great breadth and depth, with many universities offering\nseveral courses on the topic. Readers seeking a deeper understanding are encouraged\nto see [Goodman 1997; Kaaranen 2001; Lin 2001; Korhonen 2003; Schiller 2003;\nScourias 2012; Turner 2012; Akyildiz 2010], as well as the particularly excellent and\nexhaustive reference [Mouly 1992].\n6.4.1 An Overview of Cellular Network Architecture\nIn our description of cellular network architecture in this section, we’ll adopt the ter-\nminology of the Global System for Mobile Communications (GSM) standards. (For\nhistory buffs, the GSM acronym was originally derived from Groupe Spécial Mobile,\nuntil the more anglicized name was adopted, preserving the original acronym letters.)\nIn the 1980s, Europeans recognized the need for a pan-European digital cellular\ntelephony system that would replace the numerous incompatible analog cellular\ntelephony systems, leading to the GSM standard [Mouly 1992]. Europeans deployed\nGSM technology with great success in the early 1990s, and since then GSM has\ngrown to be the 800-pound gorilla of the cellular telephone world, with more than\n80% of all cellular subscribers worldwide using GSM.\nWhen people talk about cellular technology, they often classify the technology\nas belonging to one of several “generations.” The earliest generations were designed\nprimarily for voice traffic. First generation (1G) systems were analog FDMA sys-\ntems designed exclusively for voice-only communication. These 1G systems are\nalmost extinct now, having been replaced by digital 2G systems. The original 2G\nsystems were also designed for voice, but later extended (2.5G) to support data (i.e.,\nInternet) as well as voice service. The 3G systems that currently are being deployed\nalso support voice and data, but with an ever increasing emphasis on data capabili-\nties and higher-speed radio access links.\n6.4\n•\nCELLULAR INTERNET ACCESS\n547\n\nCellular Network Architecture, 2G: Voice Connections to the \nTelephone Network\nThe term cellular refers to the fact that the region covered by a cellular network\nis partitioned into a number of geographic coverage areas, known as cells, shown\nas hexagons on the left side of Figure 6.18. As with the 802.11WiFi standard we\nstudied in Section 6.3.1, GSM has its own particular nomenclature. Each cell\ncontains a base transceiver station (BTS) that transmits signals to and receives\nsignals from the mobile stations in its cell. The coverage area of a cell depends\n548\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n3G CELLULAR MOBILE VERSUS WIRELESS LANS\nMany cellular mobile phone operators are deploying 3G cellular mobile systems with\nindoor data rates of 2 Mbps and outdoor data rates of 384 kbps and higher. These 3G\nsystems are being deployed in licensed radio-frequency bands, with some operators\npaying considerable sums to governments for spectrum-use licenses. 3G systems allow\nusers to access the Internet from remote outdoor locations while on the move, in a\nmanner similar to today’s cellular phone access. For example, 3G technology permits a\nuser to access road map information while driving a car, or movie theater information\nwhile sunbathing on a beach. Nevertheless, one may question the extent to which 3G\nsystems will be used, given their cost and the fact that users may often have simultane-\nous access to both wireless LANs and 3G:\n•\nThe emerging wireless LAN infrastructure may become nearly ubiquitous. IEEE 802.11\nwireless LANs, operating at 54 Mbps, are enjoying widespread deployment. Almost\nall portable computers and smartphones are factory-equipped with 802.11 LAN\ncapabilities. Furthermore, emerging Internet appliances—such as wireless cameras\nand picture frames—will also have small and low-powered wireless LAN capabilities.\n•\nWireless LAN base stations can also handle mobile phone appliances. Many\nphones are already capable of connecting to the cellular phone network or to an\nIP network either natively or using a Skype-like Voice-over-IP service, thus bypass-\ning the operator’s cellular voice and 3G data services.\nOf course, many other experts believe that 3G not only will be a major success,\nbut will also dramatically revolutionize the way we work and live. Most likely, \nboth WiFi and 3G will both become prevalent wireless technologies, with roaming \nwireless devices automatically selecting the access technology that provides the\nbest service at their current physical location.\nCASE HISTORY\n\n6.4\n•\nCELLULAR INTERNET ACCESS\n549\non many factors, including the transmitting power of the BTS, the transmitting\npower of the user devices, obstructing buildings in the cell, and the height of base\nstation antennas. Although Figure 6.18 shows each cell containing one base\ntransceiver station residing in the middle of the cell, many systems today place\nthe BTS at corners where three cells intersect, so that a single BTS with direc-\ntional antennas can service three cells.\nThe GSM standard for 2G cellular systems uses combined FDM/TDM (radio)\nfor the air interface. Recall from Chapter 1 that, with pure FDM, the channel is par-\ntitioned into a number of frequency bands with each band devoted to a call. Also\nrecall from Chapter 1 that, with pure TDM, time is partitioned into frames with each\nframe further partitioned into slots and each call being assigned the use of a particu-\nlar slot in the revolving frame. In combined FDM/TDM systems, the channel is par-\ntitioned into a number of frequency sub-bands; within each sub-band, time is\npartitioned into frames and slots. Thus, for a combined FDM/TDM system, if \nthe channel is partitioned into F sub-bands and time is partitioned into T slots, then\nBSC\nBSC\nMSC\nKey:\nBase transceiver station\n(BTS)\nBase station controller\n(BSC)\nMobile switching center\n(MSC)\nMobile subscribers\nGateway\nMSC\nBase Station System\n(BSS)\nBase Station System (BSS)\nPublic telephone\nnetwork\nG\nFigure 6.18 \u0002 Components of the GSM 2G cellular network architecture"
    },
    {
      "chunk_id": "6fa65388-4f73-4ca8-b3f8-3e9d408b2d4f",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.4.2 3G Cellular Data Networks: Extending the Internet to Cellular Subscribers",
      "original_titles": [
        "6.4.2 3G Cellular Data Networks: Extending the Internet to Cellular Subscribers"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.4 Cellular Internet Access > 6.4.2 3G Cellular Data Networks: Extending the Internet to Cellular Subscribers",
      "start_page": 577,
      "end_page": 579,
      "token_count": 1381,
      "text": "the channel will be able to support F.T simultaneous calls. Recall that we saw in \nSection 5.3.4 that cable access networks also use a combined FDM/TDM approach.\nGSM systems consist of 200-kHz frequency bands with each band supporting eight\nTDM calls. GSM encodes speech at 13 kbps and 12.2 kbps.\nA GSM network’s base station controller (BSC) will typically service several\ntens of base transceiver stations. The role of the BSC is to allocate BTS radio chan-\nnels to mobile subscribers, perform paging (finding the cell in which a mobile user\nis resident), and perform handoff of mobile users—a topic we’ll cover shortly in\nSection 6.7.2. The base station controller and its controlled base transceiver stations\ncollectively constitute a GSM base station system (BSS).\nAs we’ll see in Section 6.7, the mobile switching center (MSC) plays the cen-\ntral role in user authorization and accounting (e.g., determining whether a mobile\ndevice is allowed to connect to the cellular network), call establishment and tear-\ndown, and handoff. A single MSC will typically contain up to five BSCs, resulting\nin approximately 200K subscribers per MSC. A cellular provider’s network will\nhave a number of MSCs, with special MSCs known as gateway MSCs connecting\nthe provider’s cellular network to the larger public telephone network.\n6.4.2 3G Cellular Data Networks: Extending the Internet to\nCellular Subscribers\nOur discussion in Section 6.4.1 focused on connecting cellular voice users to the\npublic telephone network. But, of course, when we’re on the go, we’d also like to\nread email, access the Web, get location-dependent services (e.g., maps and restau-\nrant recommendations) and perhaps even watch streaming video. To do this, our\nsmartphone will need to run a full TCP/IP protocol stack (including the physical\nlink, network, transport, and application layers) and connect into the Internet via\nthe cellular data network. The topic of cellular data networks is a rather bewilder-\ning collection of competing and ever-evolving standards as one generation (and\nhalf-generation) succeeds the former and introduces new technologies and services\nwith new acronyms. To make matters worse, there’s no single official body that\nsets requirements for 2.5G, 3G, 3.5G, or 4G technologies, making it hard to sort\nout the differences among competing standards. In our discussion below, we’ll\nfocus on the UMTS (Universal Mobile Telecommunications Service) 3G standards\ndeveloped by the 3rd Generation Partnership project (3GPP) [3GPP 2012], a\nwidely deployed 3G technology.\nLet’s take a top-down look at 3G cellular data network architecture shown in\nFigure 6.19.\n3G Core Network\nThe 3G core cellular data network connects radio access networks to the public Inter-\nnet. The core network interoperates with components of the existing cellular voice\n550\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\n6.4\n•\nCELLULAR INTERNET ACCESS\n551\nGateway\nMSC\nG\nKey:\nServing GPRS\nSupport Node\n(SGSN)\nGateway GPRS\nSupport Node\n(GGSN)\nRadio Network\nController (RNC)\nGGSN\nSGSN\nG\nG\nMSC\nPublic telephone\nnetwork\nRadio Interface\n(WCDMA, HSPA)\nRadio Access Network\nUniversal Terrestrial Radio\nAccess Network (UTRAN)\nCore Network\nGeneral Packet Radio Service\n(GPRS) Core Network\nPublic\nInternet\nPublic\nInternet\nFigure 6.19 \u0002 3G system architecture\nnetwork (in particular, the MSC) that we previously encountered in Figure 6.18.\nGiven the considerable amount of existing infrastructure (and profitable services!) in\nthe existing cellular voice network, the approach taken by the designers of 3G data\nservices is clear: leave the existing core GSM cellular voice network untouched,\nadding additional cellular data functionality in parallel to the existing cellular voice\nnetwork. The alternative—integrating new data services directly into the core of the\nexisting cellular voice network—would have raised the same challenges encountered\n\nin Section 4.4.4, where we discussed integrating new (IPv6) and legacy (IPv4) tech-\nnologies in the Internet.\nThere are two types of nodes in the 3G core network: Serving GPRS Sup-\nport Nodes (SGSNs) and Gateway GPRS Support Nodes (GGSNs). (GPRS\nstands for Generalized Packet Radio Service, an early cellular data service in 2G\nnetworks; here we discuss the evolved version of GPRS in 3G networks). An\nSGSN is responsible for delivering datagrams to/from the mobile nodes in the\nradio access network to which the SGSN is attached. The SGSN interacts with\nthe cellular voice network’s MSC for that area, providing user authorization and\nhandoff, maintaining location (cell) information about active mobile nodes, and\nperforming datagram forwarding between mobile nodes in the radio access net-\nwork and a GGSN. The GGSN acts as a gateway, connecting multiple SGSNs\ninto the larger Internet. A GGSN is thus the last piece of 3G infrastructure that a\ndatagram originating at a mobile node encounters before entering the larger Inter-\nnet. To the outside world, the GGSN looks like any other gateway router; the\nmobility of the 3G nodes within the GGSN’s network is hidden from the outside\nworld behind the GGSN.\n3G Radio Access Network: The Wireless Edge\nThe 3G radio access network is the wireless first-hop network that we see as a\n3G user. The Radio Network Controller (RNC) typically controls several cell\nbase transceiver stations similar to the base stations that we encountered in 2G\nsystems (but officially known in 3G UMTS parlance as a “Node Bs”—a rather\nnon-descriptive name!). Each cell’s wireless link operates between the mobile\nnodes and a base transceiver station, just as in 2G networks. The RNC connects\nto both the circuit-switched cellular voice network via an MSC, and to the\npacket-switched Internet via an SGSN. Thus, while 3G cellular voice and cellu-\nlar data services use different core networks, they share a common first/last-hop\nradio access network.\nA significant change in 3G UMTS over 2G networks is that rather than using\nGSM’s FDMA/TDMA scheme, UMTS uses a CDMA technique known as Direct\nSequence Wideband CDMA (DS-WCDMA) [Dahlman 1998] within TDMA\nslots; TDMA slots, in turn, are available on multiple frequencies—an interesting\nuse of all three dedicated channel-sharing approaches that we earlier identified in\nChapter 5 and similar to the approach taken in wired cable access networks (see\nSection 5.3.4). This change requires a new 3G cellular wireless-access network\noperating in parallel with the 2G BSS radio network shown in Figure 6.19. The\ndata service associated with the WCDMA specification is known as HSP (High\nSpeed Packet Access) and promises downlink data rates of up to 14 Mbps.\nDetails regarding 3G networks can be found at the 3rd Generation Partnership\nProject (3GPP) Web site [3GPP 2012].\n552\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS"
    },
    {
      "chunk_id": "36fe8734-f0f8-4966-b8f3-99a74f2c6000",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.4.3 On to 4G: LTE",
      "original_titles": [
        "6.4.3 On to 4G: LTE"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.4 Cellular Internet Access > 6.4.3 On to 4G: LTE",
      "start_page": 580,
      "end_page": 581,
      "token_count": 952,
      "text": "6.4.3 On to 4G: LTE\nWith 3G systems now being deployed worldwide, can 4G systems be far behind?\nCertainly not! Indeed, the design, early testing, and initial deployment of 4G sys-\ntems are already underway. The 4G Long-Term Evolution (LTE) standard put for-\nward by the 3GPP has two important innovations over 3G systems:\n•\nEvolved Packet Core (EPC) [3GPP Network Architecture 2012]. The EPC is\na simplified all-IP core network that unifies the separate circuit-switched cel-\nlular voice network and the packet-switched cellular data network shown in\nFigure 6.19. It is an “all-IP” network in that both voice and data will be car-\nried in IP datagrams. As we’ve seen in Chapter 4 and will study in more detail\nin Chapter 7, IP’s “best effort” service model is not inherently well-suited to\nthe stringent performance requirements of Voice-over-IP (VoIP) traffic unless\nnetwork resources are carefully managed to avoid (rather than react to) con-\ngestion. Thus, a key task of the EPC is to manage network resources to pro-\nvide this high quality of service. The EPC also makes a clear separation\nbetween the network control and user data planes, with many of the mobility\nsupport features that we will study in Section 6.7 being implemented in the\ncontrol plane. The EPC allows multiple types of radio access networks,\nincluding legacy 2G and 3G radio access networks, to attach to the core\nnetwork. Two very readable introductions to the EPC are [Motorola 2007;\nAlcatel-Lucent 2009].\n•\nLTE Radio Access Network. LTE uses a combination of frequency division\nmultiplexing and time division multiplexing on the downstream channel,\nknown as orthogonal frequency division multiplexing (OFDM) [Rohde 2008;\nEricsson 2011]. (The term “orthogonal” comes from the fact the signals being\nsent on different frequency channels are created so that they interfere very lit-\ntle with each other, even when channel frequencies are tightly spaced). In LTE,\neach active mobile node is allocated one or more 0.5 ms time slots in one or\nmore of the channel frequencies. Figure 6.20 shows an allocation of eight time\nslots over four frequencies. By being allocated increasingly more time slots\n(whether on the same frequency or on different frequencies), a mobile node is\nable to achieve increasingly higher transmission rates. Slot (re)allocation\namong mobile nodes can be performed as often as once every millisecond. Dif-\nferent modulation schemes can also be used to change the transmission rate;\nsee our earlier discussion of Figure 6.3 and dynamic selection of modulation\nschemes in WiFi networks. Another innovation in the LTE radio network is the\nuse of sophisticated multiple-input, multiple output (MIMO) antennas. The\nmaximum data rate for an LTE user is 100 Mbps in the downstream direction\nand 50 Mbps in the upstream direction, when using 20 MHz worth of wireless\nspectrum.\n6.4\n•\nCELLULAR INTERNET ACCESS\n553\n\nThe particular allocation of time slots to mobile nodes is not mandated by the\nLTE standard. Instead, the decision of which mobile nodes will be allowed to\ntransmit in a given time slot on a given frequency is determined by the schedul-\ning algorithms provided by the LTE equipment vendor and/or the network opera-\ntor. With opportunistic scheduling [Bender 2000; Kolding 2003; Kulkarni 2005],\nmatching the physical-layer protocol to the channel conditions between the\nsender and receiver and choosing the receivers to which packets will be sent\nbased on channel conditions allow the radio network controller to make best use\nof the wireless medium. In addition, user priorities and contracted levels of serv-\nice (e.g., silver, gold, or platinum) can be used in scheduling downstream packet\ntransmissions. In addition to the LTE capabilities described above, LTE-Advanced\nallows for downstream bandwidths of hundreds of Mbps by allocating aggregated\nchannels to a mobile node [Akyildiz 2010].\nAn additional 4G wireless technology—WiMAX (World Interoperability for\nMicrowave Access)—is a family of IEEE 802.16 standards that differ significantly\nfrom LTE. Whether LTE or WiMAX becomes the 4G technology of choice is still to\nbe seen, but at the time of this writing (spring 2012), LTE appears to have signifi-\ncantly more momentum. A detailed discussion of WiMAX can be found on this\nbook’s Web site.\n554\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nf1\nf2\nf3\nf4\nf5\nf6\n0\n0.5\n1.0\n1.5\n2.0\n2.5\n9.0\n9.5\n10.0\nFigure 6.20 \u0002 Twenty 0.5 ms slots organized into 10 ms frames at each\nfrequency. An eight-slot allocation is shown shaded."
    },
    {
      "chunk_id": "988f9416-607d-4b67-96a4-b12b926e3afe",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.5 Mobility Management: Principles",
      "original_titles": [
        "6.5 Mobility Management: Principles"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.5 Mobility Management: Principles",
      "start_page": 582,
      "end_page": 583,
      "token_count": 1157,
      "text": "6.5 Mobility Management: Principles\nHaving covered the wireless nature of the communication links in a wireless net-\nwork, it’s now time to turn our attention to the mobility that these wireless links\nenable. In the broadest sense, a mobile node is one that changes its point of attach-\nment into the network over time. Because the term mobility has taken on many\nmeanings in both the computer and telephony worlds, it will serve us well first to\nconsider several dimensions of mobility in some detail.\n•\nFrom the network layer’s standpoint, how mobile is a user? A physically mobile\nuser will present a very different set of challenges to the network layer, depend-\ning on how he or she moves between points of attachment to the network. At one\nend of the spectrum in Figure 6.21, a user may carry a laptop with a wireless net-\nwork interface card around in a building. As we saw in Section 6.3.4, this user is\nnot mobile from a network-layer perspective. Moreover, if the user associates\nwith the same access point regardless of location, the user is not even mobile\nfrom the perspective of the link layer.\nAt the other end of the spectrum, consider the user zooming along the autobahn\nin a BMW at 150 kilometers per hour, passing through multiple wireless access\nnetworks and wanting to maintain an uninterrupted TCP connection to a remote\napplication throughout the trip. This user is definitely mobile! In between these\nextremes is a user who takes a laptop from one location (e.g., office or dormi-\ntory) into another (e.g., coffeeshop, classroom) and wants to connect into the\nnetwork in the new location. This user is also mobile (although less so than the\nBMW driver!) but does not need to maintain an ongoing connection while mov-\ning between points of attachment to the network. Figure 6.21 illustrates this spec-\ntrum of user mobility from the network layer’s perspective.\n6.5\n•\nMOBILITY MANAGEMENT: PRINCIPLES\n555\nUser moves only\nwithin same wireless\naccess network\nNo mobility\nHigh mobility\nUser moves between\naccess networks,\nshutting down while\nmoving between\nnetworks\nUser moves between\naccess networks,\nwhile maintaining\nongoing connections\nFigure 6.21 \u0002 Various degrees of mobility, from the network layer’s point\nof view\n\n•\nHow important is it for the mobile node’s address to always remain the same?\nWith mobile telephony, your phone number—essentially the network-layer\naddress of your phone—remains the same as you travel from one provider’s\nmobile phone network to another. Must a laptop similarly maintain the same IP\naddress while moving between IP networks?\nThe answer to this question will depend strongly on the applications being run.\nFor the BMW driver who wants to maintain an uninterrupted TCP connection to\na remote application while zipping along the autobahn, it would be convenient to\nmaintain the same IP address. Recall from Chapter 3 that an Internet application\nneeds to know the IP address and port number of the remote entity with which it\nis communicating. If a mobile entity is able to maintain its IP address as it\nmoves, mobility becomes invisible from the application standpoint. There is\ngreat value to this transparency—an application need not be concerned with a\npotentially changing IP address, and the same application code serves mobile\nand nonmobile connections alike. We’ll see in the following section that mobile\nIP provides this transparency, allowing a mobile node to maintain its permanent\nIP address while moving among networks.\nOn the other hand, a less glamorous mobile user might simply want to turn off an\noffice laptop, bring that laptop home, power up, and work from home. If the laptop\nfunctions primarily as a client in client-server applications (e.g., send/read e-mail,\nbrowse the Web, Telnet to a remote host) from home, the particular IP address used\nby the laptop is not that important. In particular, one could get by fine with an\naddress that is temporarily allocated to the laptop by the ISP serving the home. We\nsaw in Section 4.4 that DHCP already provides this functionality.\n•\nWhat supporting wired infrastructure is available? In all of our scenarios above,\nwe’ve implicitly assumed that there is a fixed infrastructure to which the mobile\nuser can connect—for example, the home’s ISP network, the wireless access net-\nwork in the office, or the wireless access networks lining the autobahn. What if\nno such infrastructure exists? If two users are within communication proximity\nof each other, can they establish a network connection in the absence of any other\nnetwork-layer infrastructure? Ad hoc networking provides precisely these capa-\nbilities. This rapidly developing area is at the cutting edge of mobile networking\nresearch and is beyond the scope of this book. [Perkins 2000] and the IETF\nMobile Ad Hoc Network (manet) working group Web pages [manet 2012] pro-\nvide thorough treatments of the subject.\nIn order to illustrate the issues involved in allowing a mobile user to maintain\nongoing connections while moving between networks, let’s consider a human\nanalogy. A twenty-something adult moving out of the family home becomes\nmobile, living in a series of dormitories and/or apartments, and often changing\naddresses. If an old friend wants to get in touch, how can that friend find the\naddress of her mobile friend? One common way is to contact the family, since a\n556\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS"
    },
    {
      "chunk_id": "0727cb8c-0017-4ba8-aacf-02195aade50d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.5.1 Addressing",
      "original_titles": [
        "6.5.1 Addressing"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.5 Mobility Management: Principles > 6.5.1 Addressing",
      "start_page": 584,
      "end_page": 585,
      "token_count": 971,
      "text": "6.5\n•\nMOBILITY MANAGEMENT: PRINCIPLES\n557\nmobile adult will often register his or her current address with the family (if for no\nother reason than so that the parents can send money to help pay the rent!). The\nfamily home, with its permanent address, becomes that one place that others can\ngo as a first step in communicating with the mobile adult. Later communication\nfrom the friend may be either indirect (for example, with mail being sent first to\nthe parents’ home and then forwarded to the mobile adult) or direct (for example,\nwith the friend using the address obtained from the parents to send mail directly to\nher mobile friend).\nIn a network setting, the permanent home of a mobile node (such as a laptop\nor smartphone) is known as the home network, and the entity within the home \nnetwork that performs the mobility management functions discussed below on\nbehalf of the mobile node is known as the home agent. The network in which \nthe mobile node is currently residing is known as the foreign (or visited) network,\nand the entity within the foreign network that helps the mobile node with \nthe mobility management functions discussed below is known as a foreign agent.\nFor mobile professionals, their home network might likely be their company net-\nwork, while the visited network might be the network of a colleague they are visit-\ning. A correspondent is the entity wishing to communicate with the mobile node.\nFigure 6.22 illustrates these concepts, as well as addressing concepts considered\nbelow. In Figure 6.22, note that agents are shown as being collocated with routers\n(e.g., as processes running on routers), but alternatively they could be executing on\nother hosts or servers in the network.\n6.5.1 Addressing\nWe noted above that in order for user mobility to be transparent to network applica-\ntions, it is desirable for a mobile node to keep its address as it moves from one net-\nwork to another. When a mobile node is resident in a foreign network, all traffic\naddressed to the node’s permanent address now needs to be routed to the foreign\nnetwork. How can this be done? One option is for the foreign network to advertise\nto all other networks that the mobile node is resident in its network. This could be\nvia the usual exchange of intradomain and interdomain routing information and\nwould require few changes to the existing routing infrastructure. The foreign net-\nwork could simply advertise to its neighbors that it has a highly specific route to the\nmobile node’s permanent address (that is, essentially inform other networks that it\nhas the correct path for routing datagrams to the mobile node’s permanent address;\nsee Section 4.4). These neighbors would then propagate this routing information\nthroughout the network as part of the normal procedure of updating routing infor-\nmation and forwarding tables. When the mobile node leaves one foreign network\nand joins another, the new foreign network would advertise a new, highly specific\nroute to the mobile node, and the old foreign network would withdraw its routing\ninformation regarding the mobile node.\n\n558\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nThis solves two problems at once, and it does so without making significant\nchanges to the network-layer infrastructure. Other networks know the location of\nthe mobile node, and it is easy to route datagrams to the mobile node, since the for-\nwarding tables will direct datagrams to the foreign network. A significant drawback,\nhowever, is that of scalability. If mobility management were to be the responsibility\nof network routers, the routers would have to maintain forwarding table entries for\npotentially millions of mobile nodes, and update these entries as nodes move. Some\nadditional drawbacks are explored in the problems at the end of this chapter.\nAn alternative approach (and one that has been adopted in practice) is to push\nmobility functionality from the network core to the network edge—a recurring\ntheme in our study of Internet architecture. A natural way to do this is via the mobile\nnode’s home network. In much the same way that parents of the mobile twenty-\nsomething track their child’s location, the home agent in the mobile node’s home\nnetwork can track the foreign network in which the mobile node resides. A protocol\nHome agent\nHome network:\n128.119.40/24\nVisited network:\n79.129.13/24\nMobile node\nPermanent address:\n128.119.40.186\nPermanent address:\n128.119.40.186\nForeign agent\nCare-of address:\n79.129.13.2\nCorrespondent\nWide area\nnetwork\nFigure 6.22 \u0002 Initial elements of a mobile network architecture"
    },
    {
      "chunk_id": "f3401382-3013-401f-9b08-31f7edcd7ecf",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.5.2 Routing to a Mobile Node",
      "original_titles": [
        "6.5.2 Routing to a Mobile Node"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.5 Mobility Management: Principles > 6.5.2 Routing to a Mobile Node",
      "start_page": 586,
      "end_page": 590,
      "token_count": 2600,
      "text": "6.5\n•\nMOBILITY MANAGEMENT: PRINCIPLES\n559\nbetween the mobile node (or a foreign agent representing the mobile node) and the\nhome agent will certainly be needed to update the mobile node’s location.\nLet’s now consider the foreign agent in more detail. The conceptually simplest\napproach, shown in Figure 6.22, is to locate foreign agents at the edge routers in the\nforeign network. One role of the foreign agent is to create a so-called care-of address\n(COA) for the mobile node, with the network portion of the COA matching that of\nthe foreign network. There are thus two addresses associated with a mobile node, its\npermanent address (analogous to our mobile youth’s family’s home address) and its\nCOA, sometimes known as a foreign address (analogous to the address of the house\nin which our mobile youth is currently residing). In the example in Figure 6.22, the\npermanent address of the mobile node is 128.119.40.186. When visiting network\n79.129.13/24, the mobile node has a COA of 79.129.13.2. A second role of the for-\neign agent is to inform the home agent that the mobile node is resident in its (the for-\neign agent’s) network and has the given COA. We’ll see shortly that the COA will be\nused to “reroute” datagrams to the mobile node via its foreign agent.\nAlthough we have separated the functionality of the mobile node and the for-\neign agent, it is worth noting that the mobile node can also assume the responsibili-\nties of the foreign agent. For example, the mobile node could obtain a COA in the\nforeign network (for example, using a protocol such as DHCP) and itself inform the\nhome agent of its COA.\n6.5.2 Routing to a Mobile Node\nWe have now seen how a mobile node obtains a COA and how the home agent can\nbe informed of that address. But having the home agent know the COA solves only\npart of the problem. How should datagrams be addressed and forwarded to the\nmobile node? Since only the home agent (and not network-wide routers) knows the\nlocation of the mobile node, it will no longer suffice to simply address a datagram to\nthe mobile node’s permanent address and send it into the network-layer infrastruc-\nture. Something more must be done. Two approaches can be identified, which we\nwill refer to as indirect and direct routing.\nIndirect Routing to a Mobile Node\nLet’s first consider a correspondent that wants to send a datagram to a mobile node.\nIn the indirect routing approach, the correspondent simply addresses the datagram\nto the mobile node’s permanent address and sends the datagram into the network,\nblissfully unaware of whether the mobile node is resident in its home network or is\nvisiting a foreign network; mobility is thus completely transparent to the correspon-\ndent. Such datagrams are first routed, as usual, to the mobile node’s home network.\nThis is illustrated in step 1 in Figure 6.23.\nLet’s now turn our attention to the home agent. In addition to being responsible\nfor interacting with a foreign agent to track the mobile node’s COA, the home agent\n\n560\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nhas another very important function. Its second job is to be on the lookout for arriv-\ning datagrams addressed to nodes whose home network is that of the home agent but\nthat are currently resident in a foreign network. The home agent intercepts these\ndatagrams and then forwards them to a mobile node in a two-step process. The data-\ngram is first forwarded to the foreign agent, using the mobile node’s COA (step 2 in\nFigure 6.23), and then forwarded from the foreign agent to the mobile node (step 3\nin Figure 6.23).\nIt is instructive to consider this rerouting in more detail. The home agent will\nneed to address the datagram using the mobile node’s COA, so that the network\nlayer will route the datagram to the foreign network. On the other hand, it is\ndesirable to leave the correspondent’s datagram intact, since the application receiv-\ning the datagram should be unaware that the datagram was forwarded via the home\nagent. Both goals can be satisfied by having the home agent encapsulate the corre-\nspondent’s original complete datagram within a new (larger) datagram. This larger\nHome\nagent\nHome network:\n128.119.40/24\nVisited network:\n79.129.13/24\nMobile node\nPermanent address:\n128.119.40.186\nPermanent address:\n128.119.40.186\nForeign\nagent\nCare-of\naddress:\n79.129.13.2\nWide area\nnetwork\nCorrespondent\n1\n2\n4\n3\nFigure 6.23 \u0002 Indirect routing to a mobile node\n\n6.5\n•\nMOBILITY MANAGEMENT: PRINCIPLES\n561\ndatagram is addressed and delivered to the mobile node’s COA. The foreign agent,\nwho “owns” the COA, will receive and decapsulate the datagram—that is, remove\nthe correspondent’s original datagram from within the larger encapsulating data-\ngram and forward (step 3 in Figure 6.23) the original datagram to the mobile node.\nFigure 6.24 shows a correspondent’s original datagram being sent to the home net-\nwork, an encapsulated datagram being sent to the foreign agent, and the original\ndatagram being delivered to the mobile node. The sharp reader will note that the\nencapsulation/decapsulation described here is identical to the notion of tunneling,\ndiscussed in Chapter 4 in the context of IP multicast and IPv6.\nLet’s next consider how a mobile node sends datagrams to a correspondent.\nThis is quite simple, as the mobile node can address its datagram directly to the cor-\nrespondent (using its own permanent address as the source address, and the corre-\nspondent’s address as the destination address). Since the mobile node knows the\ncorrespondent’s address, there is no need to route the datagram back through the\nhome agent. This is shown as step 4 in Figure 6.23.\nLet’s summarize our discussion of indirect routing by listing the new network-\nlayer functionality required to support mobility.\nHome\nagent\nPermanent address:\n128.119.40.186\nPermanent address:\n128.119.40.186\nForeign\nagent\nCorrespondent\ndest: 128.119.40.186\ndest: 79.129.13.2\ndest: 128.119.40.186\ndest: 128.119.40.186\nCare-of address:\n79.129.13.2\nFigure 6.24 \u0002 Encapsulation and decapsulation\n\n•\nA mobile-node–to–foreign-agent protocol. The mobile node will register with\nthe foreign agent when attaching to the foreign network. Similarly, a mobile\nnode will deregister with the foreign agent when it leaves the foreign network.\n•\nA foreign-agent–to–home-agent registration protocol. The foreign agent will\nregister the mobile node’s COA with the home agent. A foreign agent need not\nexplicitly deregister a COA when a mobile node leaves its network, because the\nsubsequent registration of a new COA, when the mobile node moves to a new\nnetwork, will take care of this.\n•\nA home-agent datagram encapsulation protocol. Encapsulation and forward-\ning of the correspondent’s original datagram within a datagram addressed to\nthe COA.\n•\nA foreign-agent decapsulation protocol. Extraction of the correspondent’s origi-\nnal datagram from the encapsulating datagram, and the forwarding of the origi-\nnal datagram to the mobile node.\nThe previous discussion provides all the pieces—foreign agents, the home\nagent, and indirect forwarding—needed for a mobile node to maintain an ongo-\ning connection while moving among networks. As an example of how these\npieces fit together, assume the mobile node is attached to foreign network A, has\nregistered a COA in network A with its home agent, and is receiving datagrams\nthat are being indirectly routed through its home agent. The mobile node now\nmoves to foreign network B and registers with the foreign agent in network B,\nwhich informs the home agent of the mobile node’s new COA. From this point\non, the home agent will reroute datagrams to foreign network B. As far as a cor-\nrespondent is concerned, mobility is transparent—datagrams are routed via the\nsame home agent both before and after the move. As far as the home agent is con-\ncerned, there is no disruption in the flow of datagrams—arriving datagrams are\nfirst forwarded to foreign network A; after the change in COA, datagrams are for-\nwarded to foreign network B. But will the mobile node see an interrupted flow of\ndatagrams as it moves between networks? As long as the time between the\nmobile node’s disconnection from network A (at which point it can no longer\nreceive datagrams via A) and its attachment to network B (at which point it will\nregister a new COA with its home agent) is small, few datagrams will be lost.\nRecall from Chapter 3 that end-to-end connections can suffer datagram loss due\nto network congestion. Hence occasional datagram loss within a connection\nwhen a node moves between networks is by no means a catastrophic problem. If\nloss-free communication is required, upper-layer mechanisms will recover from\ndatagram loss, whether such loss results from network congestion or from user\nmobility.\nAn indirect routing approach is used in the mobile IP standard [RFC 5944], as\ndiscussed in Section 6.6.\n562\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\n6.5\n•\nMOBILITY MANAGEMENT: PRINCIPLES\n563\nDirect Routing to a Mobile Node\nThe indirect routing approach illustrated in Figure 6.23 suffers from an inefficiency\nknown as the triangle routing problem—datagrams addressed to the mobile node\nmust be routed first to the home agent and then to the foreign network, even when a\nmuch more efficient route exists between the correspondent and the mobile node. In\nthe worst case, imagine a mobile user who is visiting the foreign network of a col-\nleague. The two are sitting side by side and exchanging data over the network. Data-\ngrams from the correspondent (in this case the colleague of the visitor) are routed to\nthe mobile user’s home agent and then back again to the foreign network!\nDirect routing overcomes the inefficiency of triangle routing, but does so at the\ncost of additional complexity. In the direct routing approach, a correspondent agent in\nthe correspondent’s network first learns the COA of the mobile node. This can be done\nby having the correspondent agent query the home agent, assuming that (as in the case\nof indirect routing) the mobile node has an up-to-date value for its COA registered with\nits home agent. It is also possible for the correspondent itself to perform the function of\nthe correspondent agent, just as a mobile node could perform the function of the for-\neign agent. This is shown as steps 1 and 2 in Figure 6.25. The correspondent agent then\ntunnels datagrams directly to the mobile node’s COA, in a manner analogous to the tun-\nneling performed by the home agent, steps 3 and 4 in Figure 6.25.\nWhile direct routing overcomes the triangle routing problem, it introduces two\nimportant additional challenges:\n•\nA mobile-user location protocol is needed for the correspondent agent to query\nthe home agent to obtain the mobile node’s COA (steps 1 and 2 in Figure 6.25).\n•\nWhen the mobile node moves from one foreign network to another, how will data\nnow be forwarded to the new foreign network? In the case of indirect routing, this\nproblem was easily solved by updating the COA maintained by the home\nagent. However, with direct routing, the home agent is queried for the COA by\nthe correspondent agent only once, at the beginning of the session. Thus, updat-\ning the COA at the home agent, while necessary, will not be enough to solve the\nproblem of routing data to the mobile node’s new foreign network.\nOne solution would be to create a new protocol to notify the correspondent of\nthe changing COA. An alternate solution, and one that we’ll see adopted in practice in\nGSM networks, works as follows. Suppose data is currently being forwarded to the\nmobile node in the foreign network where the mobile node was located when the ses-\nsion first started (step 1 in Figure 6.26). We’ll identify the foreign agent in that for-\neign network where the mobile node was first found as the anchor foreign agent.\nWhen the mobile node moves to a new foreign network (step 2 in Figure 6.26), the\nmobile node registers with the new foreign agent (step 3), and the new foreign agent\nprovides the anchor foreign agent with the mobile node’s new COA (step 4). When"
    },
    {
      "chunk_id": "d677c4e5-8ece-4059-9a70-27ed5b7dd48e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.6 Mobile IP",
      "original_titles": [
        "6.6 Mobile IP"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.6 Mobile IP",
      "start_page": 591,
      "end_page": 596,
      "token_count": 2122,
      "text": "564\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nHome\nagent\nHome network:\n128.119.40/24\nVisited network:\n79.129.13/24\nMobile node\nPermanent address:\n128.119.40.186\nKey:\nPermanent address:\n128.119.40.186\nForeign\nagent\nCare-of address:\n79.129.13.2\nWide area\nnetwork\nCorrespondent\nControl messages \nCorrespondent\nagent\n1\n2\n3\nData flow \n4\nFigure 6.25 \u0002 Direct routing to a mobile user\nthe anchor foreign agent receives an encapsulated datagram for a departed mobile\nnode, it can then re-encapsulate the datagram and forward it to the mobile node \n(step 5) using the new COA. If the mobile node later moves yet again to a new for-\neign network, the foreign agent in that new visited network would then contact the\nanchor foreign agent in order to set up forwarding to this new foreign network.\n6.6 Mobile IP\nThe Internet architecture and protocols for supporting mobility, collectively known\nas mobile IP, are defined primarily in RFC 5944 for IPv4. Mobile IP is a flexible\nstandard, supporting many different modes of operation (for example, operation\n\n6.6\n•\nMOBILE IP\n565\nHome\nagent\nHome network:\nForeign network\nbeing visited at\nsession start:\nNew foreign\nnetwork:\nAnchor\nforeign\nagent\nNew foreign agent\nWide area\nnetwork\nCorrespondent\nCorrespondent\nagent\n1\n4\n2\n3\n5\nFigure 6.26 \u0002 Mobile transfer between networks with direct routing\nwith or without a foreign agent), multiple ways for agents and mobile nodes to dis-\ncover each other, use of single or multiple COAs, and multiple forms of encapsula-\ntion. As such, mobile IP is a complex standard, and would require an entire book to\ndescribe in detail; indeed one such book is [Perkins 1998b]. Our modest goal here is\nto provide an overview of the most important aspects of mobile IP and to illustrate\nits use in a few common-case scenarios.\nThe mobile IP architecture contains many of the elements we have considered\nabove, including the concepts of home agents, foreign agents, care-of addresses, and\nencapsulation/decapsulation. The current standard [RFC 5944] specifies the use of\nindirect routing to the mobile node.\nThe mobile IP standard consists of three main pieces:\n•\nAgent discovery. Mobile IP defines the protocols used by a home or foreign agent\nto advertise its services to mobile nodes, and protocols for mobile nodes to solicit\nthe services of a foreign or home agent.\n\n566\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n•\nRegistration with the home agent. Mobile IP defines the protocols used by the\nmobile node and/or foreign agent to register and deregister COAs with a mobile\nnode’s home agent.\n•\nIndirect routing of datagrams. The standard also defines the manner in which\ndatagrams are forwarded to mobile nodes by a home agent, including rules for\nforwarding datagrams, rules for handling error conditions, and several forms of\nencapsulation [RFC 2003, RFC 2004].\nSecurity considerations are prominent throughout the mobile IP standard. For\nexample, authentication of a mobile node is clearly needed to ensure that a mali-\ncious user does not register a bogus care-of address with a home agent, which\ncould cause all datagrams addressed to an IP address to be redirected to the mali-\ncious user. Mobile IP achieves security using many of the mechanisms that we will\nexamine in Chapter 8, so we will not address security considerations in our discus-\nsion below.\nAgent Discovery\nA mobile IP node arriving to a new network, whether attaching to a foreign network\nor returning to its home network, must learn the identity of the corresponding for-\neign or home agent. Indeed it is the discovery of a new foreign agent, with a new\nnetwork address, that allows the network layer in a mobile node to learn that it has\nmoved into a new foreign network. This process is known as agent discovery.\nAgent discovery can be accomplished in one of two ways: via agent advertisement\nor via agent solicitation.\nWith agent advertisement, a foreign or home agent advertises its services\nusing an extension to the existing router discovery protocol [RFC 1256]. The\nagent periodically broadcasts an ICMP message with a type field of 9 (router dis-\ncovery) on all links to which it is connected. The router discovery message con-\ntains the IP address of the router (that is, the agent), thus allowing a mobile node\nto learn the agent’s IP address. The router discovery message also contains a\nmobility agent advertisement extension that contains additional information\nneeded by the mobile node. Among the more important fields in the extension are\nthe following:\n•\nHome agent bit (H). Indicates that the agent is a home agent for the network in\nwhich it resides.\n•\nForeign agent bit (F). Indicates that the agent is a foreign agent for the network\nin which it resides.\n•\nRegistration required bit (R). Indicates that a mobile user in this network must\nregister with a foreign agent. In particular, a mobile user cannot obtain a care-\nof address in the foreign network (for example, using DHCP) and assume the\n\n6.6\n•\nMOBILE IP\n567\nfunctionality of the foreign agent for itself, without registering with the for-\neign agent.\n•\nM, G encapsulation bits. Indicate whether a form of encapsulation other than IP-\nin-IP encapsulation will be used.\n•\nCare-of address (COA) fields. A list of one or more care-of addresses provided\nby the foreign agent. In our example below, the COA will be associated with the\nforeign agent, who will receive datagrams sent to the COA and then forward\nthem to the appropriate mobile node. The mobile user will select one of these\naddresses as its COA when registering with its home agent.\nFigure 6.27 illustrates some of the key fields in the agent advertisement message.\nWith agent solicitation, a mobile node wanting to learn about agents without\nwaiting to receive an agent advertisement can broadcast an agent solicitation mes-\nsage, which is simply an ICMP message with type value 10. An agent receiving the\nsolicitation will unicast an agent advertisement directly to the mobile node, which\ncan then proceed as if it had received an unsolicited advertisement.\nType = 9\nCode = 0\nType = 16\nLength\nSequence number\nRegistration lifetime\nReserved\nRBHFMGrT\nbits\nChecksum\nStandard\nICMP fields\n0\n8\n16\n24\nRouter address\n0 or more care-of addresses\nMobility agent\nadvertisement\nextension\nFigure 6.27 \u0002 ICMP router discovery message with mobility agent \nadvertisement extension\n\n568\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nRegistration with the Home Agent\nOnce a mobile IP node has received a COA, that address must be registered with the\nhome agent. This can be done either via the foreign agent (who then registers\nthe COA with the home agent) or directly by the mobile IP node itself. We consider\nthe former case below. Four steps are involved.\n1. Following the receipt of a foreign agent advertisement, a mobile node sends a\nmobile IP registration message to the foreign agent. The registration message is\ncarried within a UDP datagram and sent to port 434. The registration message\ncarries a COA advertised by the foreign agent, the address of the home agent\n(HA), the permanent address of the mobile node (MA), the requested lifetime\nof the registration, and a 64-bit registration identification. The requested regis-\ntration lifetime is the number of seconds that the registration is to be valid. If\nthe registration is not renewed at the home agent within the specified lifetime,\nthe registration will become invalid. The registration identifier acts like a\nsequence number and serves to match a received registration reply with a reg-\nistration request, as discussed below.\n2. The foreign agent receives the registration message and records the mobile node’s\npermanent IP address. The foreign agent now knows that it should be looking for\ndatagrams containing an encapsulated datagram whose destination address\nmatches the permanent address of the mobile node. The foreign agent then sends a\nmobile IP registration message (again, within a UDP datagram) to port 434 of the\nhome agent. The message contains the COA, HA, MA, encapsulation format\nrequested, requested registration lifetime, and registration identification.\n3. The home agent receives the registration request and checks for authenticity\nand correctness. The home agent binds the mobile node’s permanent IP address\nwith the COA; in the future, datagrams arriving at the home agent and\naddressed to the mobile node will now be encapsulated and tunneled to the\nCOA. The home agent sends a mobile IP registration reply containing the HA,\nMA, actual registration lifetime, and the registration identification of the\nrequest that is being satisfied with this reply.\n4. The foreign agent receives the registration reply and then forwards it to the\nmobile node.\nAt this point, registration is complete, and the mobile node can receive data-\ngrams sent to its permanent address. Figure 6.28 illustrates these steps. Note that the\nhome agent specifies a lifetime that is smaller than the lifetime requested by the\nmobile node.\nA foreign agent need not explicitly deregister a COA when a mobile node\nleaves its network. This will occur automatically, when the mobile node moves to a\nnew network (whether another foreign network or its home network) and registers a\nnew COA.\n\n6.6\n•\nMOBILE IP\n569\nThe mobile IP standard allows many additional scenarios and capabilities in\naddition to those described previously. The interested reader should consult [Perkins\n1998b; RFC 5944].\nHome agent\nHA: 128.119.40.7\nMobile agent\nMA: 128.119.40.186\nVisited network:\n79.129.13/24\nICMP agent adv.\nCOA: 79.129.13.2 \n. . . \nCOA: 79.129.13.2 \nHA:128.119.40.7\nMA: 128.119.40.186 \nLifetime: 9999 \nidentification: 714 \n. . . \nRegistration req.\nCOA: 79.129.13.2 \nHA:128.119.40.7\nMA: 128.119.40.186 \nLifetime: 9999 \nidentification: 714 \nencapsulation format \n. . . \nRegistration req.\nTime\nTime\nTime\nHA: 128.119.40.7 \nMA: 128.119.40.186 \nLifetime: 4999 \nidentification: 714 \nencapsulation format \n. . . \nRegistration reply\nHA: 128.119.40.7 \nMA: 128.119.40.186 \nLifetime: 4999 \nidentification: 714 \n. . . \nRegistration reply\nForeign agent\nCOA: 79.129.13.2\nFigure 6.28 \u0002 Agent advertisement and mobile IP registration"
    },
    {
      "chunk_id": "edad6ba7-823e-4049-97e0-483225f475ef",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.7 Managing Mobility in Cellular Networks",
      "original_titles": [
        "6.7 Managing Mobility in Cellular Networks"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.7 Managing Mobility in Cellular Networks",
      "start_page": 597,
      "end_page": 597,
      "token_count": 644,
      "text": "570\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n6.7 Managing Mobility in Cellular Networks\nHaving examined how mobility is managed in IP networks, let’s now turn our atten-\ntion to networks with an even longer history of supporting mobility—cellular\ntelephony networks. Whereas we focused on the first-hop wireless link in cellular\nnetworks in Section 6.4, we’ll focus here on mobility, using the GSM cellular net-\nwork architecture [Goodman 1997; Mouly 1992; Scourias 2012; Kaaranen 2001;\nKorhonen 2003; Turner 2012] as our case study, since it is a mature and widely\ndeployed technology. As in the case of mobile IP, we’ll see that a number of the fun-\ndamental principles we identified in Section 6.5 are embodied in GSM’s network\narchitecture.\nLike mobile IP, GSM adopts an indirect routing approach (see Section 6.5.2),\nfirst routing the correspondent’s call to the mobile user’s home network and from\nthere to the visited network. In GSM terminology, the mobile users’s home network\nis referred to as the mobile user’s home public land mobile network (home\nPLMN). Since the PLMN acronym is a bit of a mouthful, and mindful of our quest\nto avoid an alphabet soup of acronyms, we’ll refer to the GSM home PLMN simply\nas the home network. The home network is the cellular provider with which the\nmobile user has a subscription (i.e., the provider that bills the user for monthly cel-\nlular service). The visited PLMN, which we’ll refer to simply as the visited net-\nwork, is the network in which the mobile user is currently residing.\nAs in the case of mobile IP, the responsibilities of the home and visited net-\nworks are quite different.\n•\nThe home network maintains a database known as the home location register\n(HLR), which contains the permanent cell phone number and subscriber pro-\nfile information for each of its subscribers. Importantly, the HLR also con-\ntains information about the current locations of these subscribers. That is, if a\nmobile user is currently roaming in another provider’s cellular network, the\nHLR contains enough information to obtain (via a process we’ll describe\nshortly) an address in the visited network to which a call to the mobile user\nshould be routed. As we’ll see, a special switch in the home network, known\nas the Gateway Mobile services Switching Center (GMSC) is contacted by\na correspondent when a call is placed to a mobile user. Again, in our quest to\navoid an alphabet soup of acronyms, we’ll refer to the GMSC here by a more\ndescriptive term, home MSC.\n•\nThe visited network maintains a database known as the visitor location register\n(VLR). The VLR contains an entry for each mobile user that is currently in the\nportion of the network served by the VLR. VLR entries thus come and go as\nmobile users enter and leave the network. A VLR is usually co-located with the\nmobile switching center (MSC) that coordinates the setup of a call to and from\nthe visited network."
    },
    {
      "chunk_id": "debcd65f-e34e-4b90-938c-f1afee1e06e4",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.7.1 Routing Calls to a Mobile User",
      "original_titles": [
        "6.7.1 Routing Calls to a Mobile User",
        "6.7.2 Handoffs in GSM"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.7 Managing Mobility in Cellular Networks > 6.7.1 Routing Calls to a Mobile User",
      "start_page": 598,
      "end_page": 601,
      "token_count": 2049,
      "text": "6.7\n•\nMANAGING MOBILITY IN CELLULAR NETWORKS\n571\nIn practice, a provider’s cellular network will serve as a home network for its sub-\nscribers and as a visited network for mobile users whose subscription is with a dif-\nferent cellular provider.\n6.7.1 Routing Calls to a Mobile User\nWe’re now in a position to describe how a call is placed to a mobile GSM user in a\nvisited network. We’ll consider a simple example below; more complex scenarios are\ndescribed in [Mouly 1992]. The steps, as illustrated in Figure 6.29, are as follows:\n1. The correspondent dials the mobile user’s phone number. This number itself\ndoes not refer to a particular telephone line or location (after all, the phone\nnumber is fixed and the user is mobile!). The leading digits in the number are\nsufficient to globally identify the mobile’s home network. The call is routed\nfrom the correspondent through the PSTN to the home MSC in the mobile’s\nhome network. This is the first leg of the call.\n2. The home MSC receives the call and interrogates the HLR to determine the\nlocation of the mobile user. In the simplest case, the HLR returns the mobile\nMobile\nuser\nVisited\nnetwork\nHome\nnetwork\nPublic switched \ntelephone\nnetwork\n1\n3\nCorrespondent\nVLR\nHLR\n2\nFigure 6.29 \u0002 Placing a call to a mobile user: indirect routing\n572\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nstation roaming number (MSRN), which we will refer to as the roaming\nnumber. Note that this number is different from the mobile’s permanent phone\nnumber, which is associated with the mobile’s home network. The roaming\nnumber is ephemeral: It is temporarily assigned to a mobile when it enters a vis-\nited network. The roaming number serves a role similar to that of the care-of\naddress in mobile IP and, like the COA, is invisible to the correspondent and the\nmobile. If HLR does not have the roaming number, it returns the address of the\nVLR in the visited network. In this case (not shown in Figure 6.29), the home\nMSC will need to query the VLR to obtain the roaming number of the mobile\nnode. But how does the HLR get the roaming number or the VLR address in the\nfirst place? What happens to these values when the mobile user moves to\nanother visited network? We’ll consider these important questions shortly.\n3. Given the roaming number, the home MSC sets up the second leg of the call\nthrough the network to the MSC in the visited network. The call is completed,\nbeing routed from the correspondent to the home MSC, and from there to the\nvisited MSC, and from there to the base station serving the mobile user.\nAn unresolved question in step 2 is how the HLR obtains information about\nthe location of the mobile user. When a mobile telephone is switched on or enters a\npart of a visited network that is covered by a new VLR, the mobile must register\nwith the visited network. This is done through the exchange of signaling messages\nbetween the mobile and the VLR. The visited VLR, in turn, sends a location update\nrequest message to the mobile’s HLR. This message informs the HLR of either the\nroaming number at which the mobile can be contacted, or the address of the VLR\n(which can then later be queried to obtain the mobile number). As part of this\nexchange, the VLR also obtains subscriber information from the HLR about the\nmobile and determines what services (if any) should be accorded the mobile user\nby the visited network.\n6.7.2 Handoffs in GSM\nA handoff occurs when a mobile station changes its association from one base sta-\ntion to another during a call. As shown in Figure 6.30, a mobile’s call is initially\n(before handoff) routed to the mobile through one base station (which we’ll refer to\nas the old base station), and after handoff is routed to the mobile through another\nbase station (which we’ll refer to as the new base station). Note that a handoff\nbetween base stations results not only in the mobile transmitting/receiving to/from a\nnew base station, but also in the rerouting of the ongoing call from a switching point\nwithin the network to the new base station. Let’s initially assume that the old and\nnew base stations share the same MSC, and that the rerouting occurs at this MSC.\nThere may be several reasons for handoff to occur, including (1) the signal\nbetween the current base station and the mobile may have deteriorated to such an\nextent that the call is in danger of being dropped, and (2) a cell may have become\n\n6.7\n•\nMANAGING MOBILITY IN CELLULAR NETWORKS\n573\noverloaded, handling a large number of calls. This congestion may be alleviated by\nhanding off mobiles to less congested nearby cells.\nWhile it is associated with a base station, a mobile periodically measures the\nstrength of a beacon signal from its current base station as well as beacon signals from\nnearby base stations that it can “hear.” These measurements are reported once or twice\na second to the mobile’s current base station. Handoff in GSM is initiated by the old\nbase station based on these measurements, the current loads of mobiles in nearby cells,\nand other factors [Mouly 1992]. The GSM standard does not specify the specific algo-\nrithm to be used by a base station to determine whether or not to perform handoff.\nFigure 6.31 illustrates the steps involved when a base station does decide to\nhand off a mobile user:\n1. The old base station (BS) informs the visited MSC that a handoff is to be per-\nformed and the BS (or possible set of BSs) to which the mobile is to be handed off.\n2. The visited MSC initiates path setup to the new BS, allocating the resources\nneeded to carry the rerouted call, and signaling the new BS that a handoff is\nabout to occur.\n3. The new BS allocates and activates a radio channel for use by the mobile.\n4. The new BS signals back to the visited MSC and the old BS that the visited-\nMSC-to-new-BS path has been established and that the mobile should be\ninformed of the impending handoff. The new BS provides all of the informa-\ntion that the mobile will need to associate with the new BS.\n5. The mobile is informed that it should perform a handoff. Note that up until this\npoint, the mobile has been blissfully unaware that the network has been laying\nthe groundwork (e.g., allocating a channel in the new BS and allocating a path\nfrom the visited MSC to the new BS) for a handoff.\n6. The mobile and the new BS exchange one or more messages to fully activate\nthe new channel in the new BS.\nOld BS\nNew BS\nOld\nrouting\nNew\nrouting\nVLR\nFigure 6.30 \u0002 Handoff scenario between base stations with a common\nMSC\n\n574\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n7. The mobile sends a handoff complete message to the new BS, which is for-\nwarded up to the visited MSC. The visited MSC then reroutes the ongoing call\nto the mobile via the new BS.\n8. The resources allocated along the path to the old BS are then released.\nLet’s conclude our discussion of handoff by considering what happens when the\nmobile moves to a BS that is associated with a different MSC than the old BS, and what\nhappens when this inter-MSC handoff occurs more than once. As shown in Figure 6.32,\nGSM defines the notion of an anchor MSC. The anchor MSC is the MSC visited by\nthe mobile when a call first begins; the anchor MSC thus remains unchanged during\nthe call. Throughout the call’s duration and regardless of the number of inter-MSC\ntransfers performed by the mobile, the call is routed from the home MSC to the anchor\nMSC, and then from the anchor MSC to the visited MSC where the mobile is currently\nlocated. When a mobile moves from the coverage area of one MSC to another, the\nongoing call is rerouted from the anchor MSC to the new visited MSC containing the\nnew base station. Thus, at all times there are at most three MSCs (the home MSC, the\nanchor MSC, and the visited MSC) between the correspondent and the mobile. Figure\n6.32 illustrates the routing of a call among the MSCs visited by a mobile user.\nRather than maintaining a single MSC hop from the anchor MSC to the current\nMSC, an alternative approach would have been to simply chain the MSCs visited by\nthe mobile, having an old MSC forward the ongoing call to the new MSC each time\nthe mobile moves to a new MSC. Such MSC chaining can in fact occur in IS-41 cel-\nlular networks, with an optional path minimization step to remove MSCs between the\nanchor MSC and the current visited MSC [Lin 2001].\nLet’s wrap up our discussion of GSM mobility management with a comparison\nof mobility management in GSM and Mobile IP. The comparison in Table 6.2 indi-\ncates that although IP and cellular networks are fundamentally different in many\nways, they share a surprising number of common functional elements and overall\napproaches in handling mobility.\nOld\nBS\nNew\nBS\n1\n5\n7\n8\n2\n3\n6\n4\nVLR\nFigure 6.31 \u0002 Steps in accomplishing a handoff between base stations\nwith a common MSC"
    },
    {
      "chunk_id": "66d04210-f249-4f74-a0e4-e3e70a77e2aa",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.8 Wireless and Mobility: Impact on Higher-Layer Protocols",
      "original_titles": [
        "6.8 Wireless and Mobility: Impact on Higher-Layer Protocols"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.8 Wireless and Mobility: Impact on Higher-Layer Protocols",
      "start_page": 602,
      "end_page": 604,
      "token_count": 1552,
      "text": "6.8\n•\nWIRELESS AND MOBILITY: IMPACT ON HIGHER-LAYER PROTOCOLS\n575\n6.8 Wireless and Mobility: Impact on Higher-\nLayer Protocols\nIn this chapter, we’ve seen that wireless networks differ significantly from their wired\ncounterparts at both the link layer (as a result of wireless channel characteristics such as\nfading, multipath, and hidden terminals) and at the network layer (as a result of mobile\nusers who change their points of attachment to the network). But are there important dif-\nferences at the transport and application layers? It’s tempting to think that these differ-\nences will be minor, since the network layer provides the same best-effort delivery\nservice model to upper layers in both wired and wireless networks. Similarly, if proto-\ncols such as TCP or UDP are used to provide transport-layer services to applications in\nboth wired and wireless networks, then the application layer should remain unchanged as\nwell. In one sense our intuition is right—TCP and UDP can (and do) operate in networks\nwith wireless links. On the other hand, transport protocols in general, and TCP in partic-\nular, can sometimes have very different performance in wired and wireless networks,\nand it is here, in terms of performance, that differences are manifested. Let’s see why.\nRecall that TCP retransmits a segment that is either lost or corrupted on the path\nbetween sender and receiver. In the case of mobile users, loss can result from either\nHome network\nCorrespondent\na.  Before handoff\nAnchor\nMSC\nPSTN\nb.  After handoff\nCorrespondent\nAnchor\nMSC\nPSTN\nHome network\nFigure 6.32 \u0002 Rerouting via the anchor MSC\n\n576\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\nnetwork congestion (router buffer overflow) or from handoff (e.g., from delays in\nrerouting segments to a mobile’s new point of attachment to the network). In all\ncases, TCP’s receiver-to-sender ACK indicates only that a segment was not received\nintact; the sender is unaware of whether the segment was lost due to congestion,\nduring handoff, or due to detected bit errors. In all cases, the sender’s response is\nthe same—to retransmit the segment. TCP’s congestion-control response is also the\nsame in all cases—TCP decreases its congestion window, as discussed in Section\n3.7. By unconditionally decreasing its congestion window, TCP implicitly assumes\nthat segment loss results from congestion rather than corruption or handoff. We saw\nin Section 6.2 that bit errors are much more common in wireless networks than in\nwired networks. When such bit errors occur or when handoff loss occurs, there’s\nreally no reason for the TCP sender to decrease its congestion window (and thus\ndecrease its sending rate). Indeed, it may well be the case that router buffers are\nempty and packets are flowing along the end-to-end path unimpeded by congestion.\nResearchers realized in the early to mid 1990s that given high bit error rates on\nwireless links and the possibility of handoff loss, TCP’s congestion-control response\ncould be problematic in a wireless setting. Three broad classes of approaches are\npossible for dealing with this problem:\n•\nLocal recovery. Local recovery protocols recover from bit errors when and where\n(e.g., at the wireless link) they occur, e.g., the 802.11 ARQ protocol we studied\nGSM element\nComment on GSM element\nMobile IP element\nHome system\nNetwork to which the mobile user’s permanent phone \nHome network\nnumber belongs.\nGateway mobile switching center or\nHome MSC: point of contact to obtain routable address of \nHome agent\nsimply home MSC, Home \nmobile user. HLR: database in home system containing permanent \nlocation register (HLR)\nphone number, profile information, current location of mobile user,\nsubscription information.\nVisited system\nNetwork other than home system where mobile user is currently residing.\nVisited network.\nVisited mobile services switching center,\nVisited MSC: responsible for setting up calls to/from mobile nodes \nForeign agent\nVisitor location register (VLR)\nin cells associated with MSC. VLR: temporary database entry in \nvisited system, containing subscription information for each \nvisiting mobile user.\nMobile station roaming number \nRoutable address for telephone call segment between home MSC \nCare-of address\n(MSRN) or simply roaming number\nand visited MSC, visible to neither the mobile nor the correspondent.\nTable 6.2\n\u0002 Commonalities between mobile IP and GSM mobility\n\n6.8\n•\nWIRELESS AND MOBILITY: IMPACT ON HIGHER-LAYER PROTOCOLS\n577\nin Section 6.3, or more sophisticated approaches that use both ARQ and FEC\n[Ayanoglu 1995].\n•\nTCP sender awareness of wireless links. In the local recovery approaches, the\nTCP sender is blissfully unaware that its segments are traversing a wireless link.\nAn alternative approach is for the TCP sender and receiver to be aware of the\nexistence of a wireless link, to distinguish between congestive losses occurring in\nthe wired network and corruption/loss occurring at the wireless link, and to\ninvoke congestion control only in response to congestive wired-network losses.\n[Balakrishnan 1997] investigates various types of TCP, assuming that end systems\ncan make this distinction. [Liu 2003] investigates techniques for distinguishing\nbetween losses on the wired and wireless segments of an end-to-end path.\n•\nSplit-connection approaches. In a split-connection approach [Bakre 1995], the\nend-to-end connection between the mobile user and the other end point is broken\ninto two transport-layer connections: one from the mobile host to the wireless\naccess point, and one from the wireless access point to the other communication\nend point (which we’ll assume here is a wired host). The end-to-end connection\nis thus formed by the concatenation of a wireless part and a wired part. The trans-\nport layer over the wireless segment can be a standard TCP connection [Bakre\n1995], or a specially tailored error recovery protocol on top of UDP. [Yavatkar\n1994] investigates the use of a transport-layer selective repeat protocol over the\nwireless connection. Measurements reported in [Wei 2006] indicate that split\nTCP connections are widely used in cellular data networks, and that significant\nimprovements can indeed be made through the use of split TCP connections.\nOur treatment of TCP over wireless links has been necessarily brief here. In-\ndepth surveys of TCP challenges and solutions in wireless networks can be found in\n[Hanabali 2005; Leung 2006]. We encourage you to consult the references for\ndetails of this ongoing area of research.\nHaving considered transport-layer protocols, let us next consider the effect of\nwireless and mobility on application-layer protocols. Here, an important considera-\ntion is that wireless links often have relatively low bandwidths, as we saw in Figure\n6.2. As a result, applications that operate over wireless links, particularly over cellu-\nlar wireless links, must treat bandwidth as a scarce commodity. For example, a Web\nserver serving content to a Web browser executing on a 3G phone will likely not be\nable to provide the same image-rich content that it gives to a browser operating over\na wired connection. Although wireless links do provide challenges at the application\nlayer, the mobility they enable also makes possible a rich set of location-aware and\ncontext-aware applications [Chen 2000; Baldauf 2007]. More generally, wireless\nand mobile networks will play a key role in realizing the ubiquitous computing envi-\nronments of the future [Weiser 1991]. It’s fair to say that we’ve only seen the tip of\nthe iceberg when it comes to the impact of wireless and mobile networks on net-\nworked applications and their protocols!"
    },
    {
      "chunk_id": "cbcf567b-8b96-4956-b293-32c07502a19d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "6.9 Summary",
      "original_titles": [
        "6.9 Summary"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > 6.9 Summary",
      "start_page": 605,
      "end_page": 609,
      "token_count": 2782,
      "text": "6.9 Summary\nWireless and mobile networks have revolutionized telephony and are having an\nincreasingly profound impact in the world of computer networks as well. With their\nanytime, anywhere, untethered access into the global network infrastructure, they\nare not only making network access more ubiquitous, they are also enabling an\nexciting new set of location-dependent services. Given the growing importance of\nwireless and mobile networks, this chapter has focused on the principles, common\nlink technologies, and network architectures for supporting wireless and mobile\ncommunication.\nWe began this chapter with an introduction to wireless and mobile networks,\ndrawing an important distinction between the challenges posed by the wireless\nnature of the communication links in such networks, and by the mobility that these\nwireless links enable. This allowed us to better isolate, identify, and master the key\nconcepts in each area. We focused first on wireless communication, considering the\ncharacteristics of a wireless link in Section 6.2. In Sections 6.3 and 6.4, we exam-\nined the link-level aspects of the IEEE 802.11 (WiFi) wireless LAN standard, two\nIEEE 802.15 personal area networks (Bluetooth and Zigbee), and 3G and 4G cellu-\nlar Internet access. We then turned our attention to the issue of mobility. In Section\n6.5, we identified several forms of mobility, with points along this spectrum posing\ndifferent challenges and admitting different solutions. We considered the problems\nof locating and routing to a mobile user, as well as approaches for handing off the\nmobile user who dynamically moves from one point of attachment to the network to\nanother. We examined how these issues were addressed in the mobile IP standard\nand in GSM, in Sections 6.6 and 6.7, respectively. Finally, we considered the impact\nof wireless links and mobility on transport-layer protocols and networked applica-\ntions in Section 6.8.\nAlthough we have devoted an entire chapter to the study of wireless and mobile\nnetworks, an entire book (or more) would be required to fully explore this exciting\nand rapidly expanding field. We encourage you to delve more deeply into this field\nby consulting the many references provided in this chapter.\nHomework Problems and Questions\nChapter 6 Review Questions\nSECTION 6.1\nR1. What does it mean for a wireless network to be operating in “infrastructure\nmode?” If the network is not in infrastructure mode, what mode of operation\nis it in, and what is the difference between that mode of operation and infra-\nstructure mode?\n578\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\nR2. What are the four types of wireless networks identified in our taxonomy in\nSection 6.1? Which of these types of wireless networks have you used?\nSECTION 6.2\nR3. What are the differences between the following types of wireless channel\nimpairments: path loss, multipath propagation, interference from other\nsources?\nR4. As a mobile node gets farther and farther away from a base station, what are\ntwo actions that a base station could take to ensure that the loss probability of\na transmitted frame does not increase?\nSECTIONS 6.3 AND 6.4\nR5. Describe the role of the beacon frames in 802.11.\nR6. True or false: Before an 802.11 station transmits a data frame, it must first\nsend an RTS frame and receive a corresponding CTS frame.\nR7. Why are acknowledgments used in 802.11 but not in wired Ethernet?\nR8. True or false: Ethernet and 802.11 use the same frame structure.\nR9. Describe how the RTS threshold works.\nR10. Suppose the IEEE 802.11 RTS and CTS frames were as long as the standard\nDATA and ACK frames. Would there be any advantage to using the CTS and\nRTS frames? Why or why not?\nR11. Section 6.3.4 discusses 802.11 mobility, in which a wireless station moves\nfrom one BSS to another within the same subnet. When the APs are intercon-\nnected with a switch, an AP may need to send a frame with a spoofed MAC\naddress to get the switch to forward the frame properly. Why?\nR12. What are the differences between a master device in a Bluetooth network and\na base station in an 802.11 network?\nR13. What is meant by a super frame in the 802.15.4 Zigbee standard?\nR14. What is the role of the “core network” in the 3G cellular data architecture?\nR15. What is the role of the RNC in the 3G cellular data network architecture?\nWhat role does the RNC play in the cellular voice network?\nSECTIONS 6.5 AND 6.6\nR16. If a node has a wireless connection to the Internet, does that node have to be\nmobile? Explain. Suppose that a user with a laptop walks around her house\nwith her laptop, and always accesses the Internet through the same access\npoint. Is this user mobile from a network standpoint? Explain.\nR17. What is the difference between a permanent address and a care-of address?\nWho assigns a care-of address?\nHOMEWORK PROBLEMS AND QUESTIONS\n579\n\nR18. Consider a TCP connection going over Mobile IP. True or false: The TCP\nconnection phase between the correspondent and the mobile host goes\nthrough the mobile’s home network, but the data transfer phase is directly\nbetween the correspondent and the mobile host, bypassing the home network.\nSECTION 6.7\nR19. What are the purposes of the HLR and VLR in GSM networks? What ele-\nments of mobile IP are similar to the HLR and VLR?\nR20. What is the role of the anchor MSC in GSM networks?\nSECTION 6.8\nR21. What are three approaches that can be taken to avoid having a single wireless\nlink degrade the performance of an end-to-end transport-layer TCP connection?\nProblems\nP1. Consider the single-sender CDMA example in Figure 6.5. What would be the\nsender’s output (for the 2 data bits shown) if the sender’s CDMA code were\n(1, –1, 1, –1, 1, –1, 1, –1)?\nP2. Consider sender 2 in Figure 6.6. What is the sender’s output to the channel\n(before it is added to the signal from sender 1), Z2\ni,m?\nP3. Suppose that the receiver in Figure 6.6 wanted to receive the data being sent\nby sender 2. Show (by calculation) that the receiver is indeed able to recover\nsender 2’s data from the aggregate channel signal by using sender 2’s code.\nP4. For the two-sender, two-receiver example, give an example of two CDMA\ncodes containing 1 and \u00041 values that do not allow the two receivers to\nextract the original transmitted bits from the two CDMA senders.\nP5. Suppose there are two ISPs providing WiFi access in a particular café, with\neach ISP operating its own AP and having its own IP address block.\na. Further suppose that by accident, each ISP has configured its AP to oper-\nate over channel 11. Will the 802.11 protocol completely break down in\nthis situation? Discuss what happens when two stations, each associated\nwith a different ISP, attempt to transmit at the same time.\nb. Now suppose that one AP operates over channel 1 and the other over\nchannel 11. How do your answers change?\nP6. In step 4 of the CSMA/CA protocol, a station that successfully transmits a\nframe begins the CSMA/CA protocol for a second frame at step 2, rather than\nat step 1. What rationale might the designers of CSMA/CA have had in mind\nby having such a station not transmit the second frame immediately (if the\nchannel is sensed idle)?\n580\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\nP7. Suppose an 802.11b station is configured to always reserve the channel with the\nRTS/CTS sequence. Suppose this station suddenly wants to transmit 1,000 bytes\nof data, and all other stations are idle at this time. As a function of SIFS and\nDIFS, and ignoring propagation delay and assuming no bit errors, calculate the\ntime required to transmit the frame and receive the acknowledgment.\nP8. Consider the scenario shown in Figure 6.33, in which there are four wireless\nnodes, A, B, C, and D. The radio coverage of the four nodes is shown via the\nshaded ovals; all nodes share the same frequency. When A transmits, it can\nonly be heard/received by B; when B transmits, both A and C can\nhear/receive from B; when C transmits, both B and D can hear/receive from\nC; when D transmits, only C can hear/receive from D.\nSuppose now that each node has an infinite supply of messages that it wants\nto send to each of the other nodes. If a message’s destination is not an imme-\ndiate neighbor, then the message must be relayed. For example, if A wants to\nsend to D, a message from A must first be sent to B, which then sends the\nmessage to C, which then sends the message to D. Time is slotted, with a\nmessage transmission time taking exactly one time slot, e.g., as in slotted\nAloha. During a slot, a node can do one of the following: (i) send a message;\n(ii) receive a message (if exactly one message is being sent to it), (iii) remain\nsilent. As always, if a node hears two or more simultaneous transmissions, a\ncollision occurs and none of the transmitted messages are received success-\nfully. You can assume here that there are no bit-level errors, and thus if\nexactly one message is sent, it will be received correctly by those within the\ntransmission radius of the sender.\na. Suppose now that an omniscient controller (i.e., a controller that knows\nthe state of every node in the network) can command each node to do\nwhatever it (the omniscient controller) wishes, i.e., to send a message, to\nreceive a message, or to remain silent. Given this omniscient controller,\nwhat is the maximum rate at which a data message can be transferred\nfrom C to A, given that there are no other messages between any other\nsource/destination pairs?\nPROBLEMS\n581\nA\nB\nC\nD\nFigure 6.33 \u0002 Scenario for problem P8\n\nb. Suppose now that A sends messages to B, and D sends messages to C.\nWhat is the combined maximum rate at which data messages can flow\nfrom A to B and from D to C?\nc. Suppose now that A sends messages to B, and C sends messages to D.\nWhat is the combined maximum rate at which data messages can flow\nfrom A to B and from C to D?\nd. Suppose now that the wireless links are replaced by wired links. Repeat\nquestions (a) through (c) again in this wired scenario.\ne. Now suppose we are again in the wireless scenario, and that for every data\nmessage sent from source to destination, the destination will send an ACK\nmessage back to the source (e.g., as in TCP). Also suppose that each ACK\nmessage takes up one slot. Repeat questions (a) – (c) above for this scenario.\nP9. Describe the format of the 802.15.1 Bluetooth frame. You will have to do\nsome reading outside of the text to find this information. Is there anything in\nthe frame format that inherently limits the number of active nodes in an\n802.15.1 network to eight active nodes? Explain.\nP10. Consider the following idealized LTE scenario. The downstream channel\n(see Figure 6.20) is slotted in time, across F frequencies. There are four\nnodes, A, B, C, and D, reachable from the base station at rates of 10 Mbps,\n5 Mbps, 2.5 Mbps, and 1 Mbps, respectively, on the downstream channel.\nThese rates assume that the base station utilizes all time slots available on\nall F frequencies to send to just one station. The base station has an infinite\namount of data to send to each of the nodes, and can send to any one of\nthese four nodes using any of the F frequencies during any time slot in the\ndownstream sub-frame.\na. What is the maximum rate at which the base station can send to the nodes,\nassuming it can send to any node it chooses during each time slot? Is your\nsolution fair? Explain and define what you mean by “fair.”\nb. If there is a fairness requirement that each node must receive an equal\namount of data during each one second interval, what is the average \ntransmission rate by the base station (to all nodes) during the downstream\nsub-frame? Explain how you arrived at your answer.\nc. Suppose that the fairness criterion is that any node can receive at most\ntwice as much data as any other node during the sub-frame. What is the\naverage transmission rate by the base station (to all nodes) during the sub-\nframe? Explain how you arrived at your answer.\nP11. In Section 6.5, one proposed solution that allowed mobile users to maintain\ntheir IP addresses as they moved among foreign networks was to have a for-\neign network advertise a highly specific route to the mobile user and use the\nexisting routing infrastructure to propagate this information throughout the\n582\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS"
    },
    {
      "chunk_id": "61cb1c5a-0696-4718-a7ae-faa6f3be8b82",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Homework Problems and Questions",
      "original_titles": [
        "Homework Problems and Questions"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > Homework Problems and Questions",
      "start_page": 605,
      "end_page": 609,
      "token_count": 2782,
      "text": "6.9 Summary\nWireless and mobile networks have revolutionized telephony and are having an\nincreasingly profound impact in the world of computer networks as well. With their\nanytime, anywhere, untethered access into the global network infrastructure, they\nare not only making network access more ubiquitous, they are also enabling an\nexciting new set of location-dependent services. Given the growing importance of\nwireless and mobile networks, this chapter has focused on the principles, common\nlink technologies, and network architectures for supporting wireless and mobile\ncommunication.\nWe began this chapter with an introduction to wireless and mobile networks,\ndrawing an important distinction between the challenges posed by the wireless\nnature of the communication links in such networks, and by the mobility that these\nwireless links enable. This allowed us to better isolate, identify, and master the key\nconcepts in each area. We focused first on wireless communication, considering the\ncharacteristics of a wireless link in Section 6.2. In Sections 6.3 and 6.4, we exam-\nined the link-level aspects of the IEEE 802.11 (WiFi) wireless LAN standard, two\nIEEE 802.15 personal area networks (Bluetooth and Zigbee), and 3G and 4G cellu-\nlar Internet access. We then turned our attention to the issue of mobility. In Section\n6.5, we identified several forms of mobility, with points along this spectrum posing\ndifferent challenges and admitting different solutions. We considered the problems\nof locating and routing to a mobile user, as well as approaches for handing off the\nmobile user who dynamically moves from one point of attachment to the network to\nanother. We examined how these issues were addressed in the mobile IP standard\nand in GSM, in Sections 6.6 and 6.7, respectively. Finally, we considered the impact\nof wireless links and mobility on transport-layer protocols and networked applica-\ntions in Section 6.8.\nAlthough we have devoted an entire chapter to the study of wireless and mobile\nnetworks, an entire book (or more) would be required to fully explore this exciting\nand rapidly expanding field. We encourage you to delve more deeply into this field\nby consulting the many references provided in this chapter.\nHomework Problems and Questions\nChapter 6 Review Questions\nSECTION 6.1\nR1. What does it mean for a wireless network to be operating in “infrastructure\nmode?” If the network is not in infrastructure mode, what mode of operation\nis it in, and what is the difference between that mode of operation and infra-\nstructure mode?\n578\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\nR2. What are the four types of wireless networks identified in our taxonomy in\nSection 6.1? Which of these types of wireless networks have you used?\nSECTION 6.2\nR3. What are the differences between the following types of wireless channel\nimpairments: path loss, multipath propagation, interference from other\nsources?\nR4. As a mobile node gets farther and farther away from a base station, what are\ntwo actions that a base station could take to ensure that the loss probability of\na transmitted frame does not increase?\nSECTIONS 6.3 AND 6.4\nR5. Describe the role of the beacon frames in 802.11.\nR6. True or false: Before an 802.11 station transmits a data frame, it must first\nsend an RTS frame and receive a corresponding CTS frame.\nR7. Why are acknowledgments used in 802.11 but not in wired Ethernet?\nR8. True or false: Ethernet and 802.11 use the same frame structure.\nR9. Describe how the RTS threshold works.\nR10. Suppose the IEEE 802.11 RTS and CTS frames were as long as the standard\nDATA and ACK frames. Would there be any advantage to using the CTS and\nRTS frames? Why or why not?\nR11. Section 6.3.4 discusses 802.11 mobility, in which a wireless station moves\nfrom one BSS to another within the same subnet. When the APs are intercon-\nnected with a switch, an AP may need to send a frame with a spoofed MAC\naddress to get the switch to forward the frame properly. Why?\nR12. What are the differences between a master device in a Bluetooth network and\na base station in an 802.11 network?\nR13. What is meant by a super frame in the 802.15.4 Zigbee standard?\nR14. What is the role of the “core network” in the 3G cellular data architecture?\nR15. What is the role of the RNC in the 3G cellular data network architecture?\nWhat role does the RNC play in the cellular voice network?\nSECTIONS 6.5 AND 6.6\nR16. If a node has a wireless connection to the Internet, does that node have to be\nmobile? Explain. Suppose that a user with a laptop walks around her house\nwith her laptop, and always accesses the Internet through the same access\npoint. Is this user mobile from a network standpoint? Explain.\nR17. What is the difference between a permanent address and a care-of address?\nWho assigns a care-of address?\nHOMEWORK PROBLEMS AND QUESTIONS\n579\n\nR18. Consider a TCP connection going over Mobile IP. True or false: The TCP\nconnection phase between the correspondent and the mobile host goes\nthrough the mobile’s home network, but the data transfer phase is directly\nbetween the correspondent and the mobile host, bypassing the home network.\nSECTION 6.7\nR19. What are the purposes of the HLR and VLR in GSM networks? What ele-\nments of mobile IP are similar to the HLR and VLR?\nR20. What is the role of the anchor MSC in GSM networks?\nSECTION 6.8\nR21. What are three approaches that can be taken to avoid having a single wireless\nlink degrade the performance of an end-to-end transport-layer TCP connection?\nProblems\nP1. Consider the single-sender CDMA example in Figure 6.5. What would be the\nsender’s output (for the 2 data bits shown) if the sender’s CDMA code were\n(1, –1, 1, –1, 1, –1, 1, –1)?\nP2. Consider sender 2 in Figure 6.6. What is the sender’s output to the channel\n(before it is added to the signal from sender 1), Z2\ni,m?\nP3. Suppose that the receiver in Figure 6.6 wanted to receive the data being sent\nby sender 2. Show (by calculation) that the receiver is indeed able to recover\nsender 2’s data from the aggregate channel signal by using sender 2’s code.\nP4. For the two-sender, two-receiver example, give an example of two CDMA\ncodes containing 1 and \u00041 values that do not allow the two receivers to\nextract the original transmitted bits from the two CDMA senders.\nP5. Suppose there are two ISPs providing WiFi access in a particular café, with\neach ISP operating its own AP and having its own IP address block.\na. Further suppose that by accident, each ISP has configured its AP to oper-\nate over channel 11. Will the 802.11 protocol completely break down in\nthis situation? Discuss what happens when two stations, each associated\nwith a different ISP, attempt to transmit at the same time.\nb. Now suppose that one AP operates over channel 1 and the other over\nchannel 11. How do your answers change?\nP6. In step 4 of the CSMA/CA protocol, a station that successfully transmits a\nframe begins the CSMA/CA protocol for a second frame at step 2, rather than\nat step 1. What rationale might the designers of CSMA/CA have had in mind\nby having such a station not transmit the second frame immediately (if the\nchannel is sensed idle)?\n580\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS\n\nP7. Suppose an 802.11b station is configured to always reserve the channel with the\nRTS/CTS sequence. Suppose this station suddenly wants to transmit 1,000 bytes\nof data, and all other stations are idle at this time. As a function of SIFS and\nDIFS, and ignoring propagation delay and assuming no bit errors, calculate the\ntime required to transmit the frame and receive the acknowledgment.\nP8. Consider the scenario shown in Figure 6.33, in which there are four wireless\nnodes, A, B, C, and D. The radio coverage of the four nodes is shown via the\nshaded ovals; all nodes share the same frequency. When A transmits, it can\nonly be heard/received by B; when B transmits, both A and C can\nhear/receive from B; when C transmits, both B and D can hear/receive from\nC; when D transmits, only C can hear/receive from D.\nSuppose now that each node has an infinite supply of messages that it wants\nto send to each of the other nodes. If a message’s destination is not an imme-\ndiate neighbor, then the message must be relayed. For example, if A wants to\nsend to D, a message from A must first be sent to B, which then sends the\nmessage to C, which then sends the message to D. Time is slotted, with a\nmessage transmission time taking exactly one time slot, e.g., as in slotted\nAloha. During a slot, a node can do one of the following: (i) send a message;\n(ii) receive a message (if exactly one message is being sent to it), (iii) remain\nsilent. As always, if a node hears two or more simultaneous transmissions, a\ncollision occurs and none of the transmitted messages are received success-\nfully. You can assume here that there are no bit-level errors, and thus if\nexactly one message is sent, it will be received correctly by those within the\ntransmission radius of the sender.\na. Suppose now that an omniscient controller (i.e., a controller that knows\nthe state of every node in the network) can command each node to do\nwhatever it (the omniscient controller) wishes, i.e., to send a message, to\nreceive a message, or to remain silent. Given this omniscient controller,\nwhat is the maximum rate at which a data message can be transferred\nfrom C to A, given that there are no other messages between any other\nsource/destination pairs?\nPROBLEMS\n581\nA\nB\nC\nD\nFigure 6.33 \u0002 Scenario for problem P8\n\nb. Suppose now that A sends messages to B, and D sends messages to C.\nWhat is the combined maximum rate at which data messages can flow\nfrom A to B and from D to C?\nc. Suppose now that A sends messages to B, and C sends messages to D.\nWhat is the combined maximum rate at which data messages can flow\nfrom A to B and from C to D?\nd. Suppose now that the wireless links are replaced by wired links. Repeat\nquestions (a) through (c) again in this wired scenario.\ne. Now suppose we are again in the wireless scenario, and that for every data\nmessage sent from source to destination, the destination will send an ACK\nmessage back to the source (e.g., as in TCP). Also suppose that each ACK\nmessage takes up one slot. Repeat questions (a) – (c) above for this scenario.\nP9. Describe the format of the 802.15.1 Bluetooth frame. You will have to do\nsome reading outside of the text to find this information. Is there anything in\nthe frame format that inherently limits the number of active nodes in an\n802.15.1 network to eight active nodes? Explain.\nP10. Consider the following idealized LTE scenario. The downstream channel\n(see Figure 6.20) is slotted in time, across F frequencies. There are four\nnodes, A, B, C, and D, reachable from the base station at rates of 10 Mbps,\n5 Mbps, 2.5 Mbps, and 1 Mbps, respectively, on the downstream channel.\nThese rates assume that the base station utilizes all time slots available on\nall F frequencies to send to just one station. The base station has an infinite\namount of data to send to each of the nodes, and can send to any one of\nthese four nodes using any of the F frequencies during any time slot in the\ndownstream sub-frame.\na. What is the maximum rate at which the base station can send to the nodes,\nassuming it can send to any node it chooses during each time slot? Is your\nsolution fair? Explain and define what you mean by “fair.”\nb. If there is a fairness requirement that each node must receive an equal\namount of data during each one second interval, what is the average \ntransmission rate by the base station (to all nodes) during the downstream\nsub-frame? Explain how you arrived at your answer.\nc. Suppose that the fairness criterion is that any node can receive at most\ntwice as much data as any other node during the sub-frame. What is the\naverage transmission rate by the base station (to all nodes) during the sub-\nframe? Explain how you arrived at your answer.\nP11. In Section 6.5, one proposed solution that allowed mobile users to maintain\ntheir IP addresses as they moved among foreign networks was to have a for-\neign network advertise a highly specific route to the mobile user and use the\nexisting routing infrastructure to propagate this information throughout the\n582\nCHAPTER 6\n•\nWIRELESS AND MOBILE NETWORKS"
    },
    {
      "chunk_id": "aa9f3f4a-6832-4386-a261-19b4bbe83878",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Wireshark Lab: IEEE 802.11 (WiFi)",
      "original_titles": [
        "Wireshark Lab: IEEE 802.11 (WiFi)"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > Wireshark Lab: IEEE 802.11 (WiFi)",
      "start_page": 610,
      "end_page": 610,
      "token_count": 552,
      "text": "network. We identified scalability as one concern. Suppose that when a\nmobile user moves from one network to another, the new foreign network\nadvertises a specific route to the mobile user, and the old foreign network\nwithdraws its route. Consider how routing information propagates in a\ndistance-vector algorithm (particularly for the case of interdomain routing\namong networks that span the globe).\na. Will other routers be able to route datagrams immediately to the new for-\neign network as soon as the foreign network begins advertising its route?\nb. Is it possible for different routers to believe that different foreign networks\ncontain the mobile user?\nc. Discuss the timescale over which other routers in the network will eventu-\nally learn the path to the mobile users.\nP12. Suppose the correspondent in Figure 6.22 were mobile. Sketch the additional\nnetwork-layer infrastructure that would be needed to route the datagram from\nthe original mobile user to the (now mobile) correspondent. Show the struc-\nture of the datagram(s) between the original mobile user and the (now\nmobile) correspondent, as in Figure 6.23.\nP13. In mobile IP, what effect will mobility have on end-to-end delays of data-\ngrams between the source and destination?\nP14. Consider the chaining example discussed at the end of Section 6.7.2. Suppose\na mobile user visits foreign networks A, B, and C, and that a correspondent\nbegins a connection to the mobile user when it is resident in foreign network A.\nList the sequence of messages between foreign agents, and between foreign\nagents and the home agent as the mobile user moves from network A to net-\nwork B to network C. Next, suppose chaining is not performed, and the cor-\nrespondent (as well as the home agent) must be explicitly notified of the\nchanges in the mobile user’s care-of address. List the sequence of messages\nthat would need to be exchanged in this second scenario.\nP15. Consider two mobile nodes in a foreign network having a foreign agent. Is it\npossible for the two mobile nodes to use the same care-of address in mobile\nIP? Explain your answer.\nP16. In our discussion of how the VLR updated the HLR with information about\nthe mobile’s current location, what are the advantages and disadvantages of\nproviding the MSRN as opposed to the address of the VLR to the HLR?\nWireshark Lab\nAt the companion Web site for this textbook, http://www.awl.com/kurose-ross,\nyou’ll find a Wireshark lab for this chapter that captures and studies the 802.11\nframes exchanged between a wireless laptop and an access point.\nWIRESHARK LAB\n583"
    },
    {
      "chunk_id": "81cd4a07-30bb-40a1-9e8e-c636d2b161f4",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Interview: Deborah Estrin",
      "original_titles": [
        "Interview: Deborah Estrin"
      ],
      "path": "Chapter 6 Wireless and Mobile Networks > Interview: Deborah Estrin",
      "start_page": 611,
      "end_page": 613,
      "token_count": 1011,
      "text": "Please describe a few of the most exciting projects you have worked on during your\ncareer. What were the biggest challenges?\nIn the mid-90s at USC and ISI, I had the great fortune to work with the likes of Steve\nDeering, Mark Handley, and Van Jacobson on the design of multicast routing protocols (in\nparticular, PIM). I tried to carry many of the architectural design lessons from multicast into\nthe design of ecological monitoring arrays, where for the first time I really began to take\napplications and multidisciplinary research seriously. That interest in jointly innovating in\nthe social and technological space is what interests me so much about my latest area of\nresearch, mobile health. The challenges in these projects were as diverse as the problem\ndomains, but what they all had in common was the need to keep our eyes open to whether\nwe had the problem definition right as we iterated between design and deployment, proto-\ntype and pilot. None of them were problems that could be solved analytically, with simula-\ntion or even in constructed laboratory experiments. They all challenged our ability to retain\n584\nDeborah Estrin\nDeborah Estrin is Professor of Computer Science at UCLA, the \nJon Postel Chair in Computer Networks, Director of the Center for\nEmbedded Networked Sensing (CENS), and co-founder of the non-\nprofit openmhealth.org. She received her Ph.D. (1985) in Computer\nScience from M.I.T., and her B.S. (1980) from UC Berkeley. Estrin’s\nearly research focused on the design of network protocols, including\nmulticast and inter-domain routing. In 2002 Estrin founded the \nNSF-funded Science and Technology Center, CENS (http://cens\n.ucla.edu), to develop and explore environmental monitoring technologies and applications.\nCurrently Estrin and collaborators are developing participatory sensing systems, leveraging\nthe programmability, proximity, and pervasiveness of mobile phones; the primary deployment\ncontexts are mobile health (http://openmhealth.org), community data gathering, and STEM\neducation (http://mobilizingcs.org). Professor Estrin is an elected member of the American\nAcademy of Arts and Sciences (2007) and the National Academy of Engineering (2009).\nShe is a fellow of the IEEE, ACM, and AAAS. She was selected as the first ACM-W Athena\nLecturer (2006), awarded the Anita Borg Institute’s Women of Vision Award for Innovation\n(2007), inducted into the WITI hall of fame (2008) and awarded Doctor Honoris Causa from\nEPFL (2008) and Uppsala University (2011).\nAN INTERVIEW WITH...\n\nclean architectures in the presence of messy problems and contexts, and they all called for\nextensive collaboration.\nWhat changes and innovations do you see happening in wireless networks and \nmobility in the future?\nI have never put much faith into predicting the future, but I would say we might see the end\nof feature phones (i.e., those that are not programmable and are used only for voice and text\nmessaging) as smart phones become more and more powerful and the primary point of\nInternet access for many. I also think that we will see the continued proliferation of embed-\nded SIMs by which all sorts of devices have the ability to communicate via the cellular net-\nwork at low data rates.\nWhere do you see the future of networking and the Internet?\nThe efforts in named data and software-defined networking will emerge to create a more\nmanageable, evolvable, and richer infrastructure and more generally represent moving the\nrole of architecture higher up in the stack. In the beginnings of the Internet, architecture was\nlayer 4 and below, with applications being more siloed/monolithic, sitting on top. Now data\nand analytics dominate transport.\nWhat people inspired you professionally?\nThere are three people who come to mind. First, Dave Clark, the secret sauce and unsung\nhero of the Internet community. I was lucky to be around in the early days to see him act as\nthe “organizing principle” of the IAB and Internet governance; the priest of rough consen-\nsus and running code. Second, Scott Shenker, for his intellectual brilliance, integrity, and\npersistence. I strive for, but rarely attain, his clarity in defining problems and solutions. He\nis always the first person I email for advice on matters large and small. Third, my sister\nJudy Estrin, who had the creativity and courage to spend her career bringing ideas and con-\ncepts to market. Without the Judys of the world the Internet technologies would never have\ntransformed our lives.\nWhat are your recommendations for students who want careers in computer science \nand networking?\nFirst, build a strong foundation in your academic work, balanced with any and every real-\nworld work experience you can get. As you look for a working environment, seek opportu-\nnities in problem areas you really care about and with smart teams that you can learn from.\n585\n\nThis page intentionally left blank"
    },
    {
      "chunk_id": "238ccf0b-0afd-4774-aa4b-64b7646925a5",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Chapter 7 Multimedia Networking",
      "original_titles": [
        "Chapter 7 Multimedia Networking"
      ],
      "path": "Chapter 7 Multimedia Networking",
      "start_page": 614,
      "end_page": 614,
      "token_count": 327,
      "text": "CHAPTER 7\nMultimedia\nNetworking\n587\nPeople in all corners of the world are currently using the Internet to watch movies\nand television shows on demand. Internet movie and television distribution compa-\nnies such as Netflix and Hulu in North America and Youku and Kankan in China\nhave practically become household names. But people are not only watching \nInternet videos, they are using sites like YouTube to upload and distribute their own\nuser-generated content, becoming Internet video producers as well as consumers.\nMoreover, network applications such as Skype, Google Talk, and QQ (enormously\npopular in China) allow people to not only make “telephone calls” over the Internet,\nbut to also enhance those calls with video and multi-person conferencing. In \nfact, we can safely predict that by the end of the current decade almost all video dis-\ntribution and voice conversations will take place end-to-end over the Internet, often\nto wireless devices connected to the Internet via 4G and WiFi access networks.\nWe begin this chapter with a taxonomy of multimedia applications in Section 7.1.\nWe’ll see that a multimedia application can be classified as either streaming stored\naudio/video, conversational voice/video-over-IP, or streaming live audio/video.\nWe’ll see that each of these classes of applications has its own unique service\nrequirements that differ significantly from those of traditional elastic applications\nsuch as e-mail, Web browsing, and remote login. In Section 7.2, we’ll examine\nvideo streaming in some detail. We’ll explore many of the underlying principles\nbehind video streaming, including client buffering, prefetching, and adapting video"
    },
    {
      "chunk_id": "7edf80ef-44a0-48e6-a109-4b191602f0b8",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.1 Multimedia Networking Applications",
      "original_titles": [
        "7.1 Multimedia Networking Applications"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.1 Multimedia Networking Applications",
      "start_page": 615,
      "end_page": 616,
      "token_count": 1167,
      "text": "quality to available bandwidth. We will also investigate Content Distribution Net-\nworks (CDNs), which are used extensively today by the leading video streaming\nsystems. We then examine the YouTube, Netflix, and Kankan systems as case \nstudies for streaming video. In Section 7.3, we investigate conversational voice and\nvideo, which, unlike elastic applications, are highly sensitive to end-to-end delay\nbut can tolerate occasional loss of data. Here we’ll examine how techniques such as\nadaptive playout, forward error correction, and error concealment can mitigate\nagainst network-induced packet loss and delay. We’ll also examine Skype as a case\nstudy. In Section 7.4, we’ll study RTP and SIP, two popular protocols for real-time\nconversational voice and video applications. In Section 7.5, we’ll investigate mech-\nanisms within the network that can be used to distinguish one class of traffic (e.g.,\ndelay-sensitive applications such as conversational voice) from another (e.g., elastic\napplications such as browsing Web pages), and provide differentiated service among\nmultiple classes of traffic.\n7.1 Multimedia Networking Applications\nWe define a multimedia network application as any network application that\nemploys audio or video. In this section, we provide a taxonomy of multimedia appli-\ncations. We’ll see that each class of applications in the taxonomy has its own unique\nset of service requirements and design issues. But before diving into an in-depth dis-\ncussion of Internet multimedia applications, it is useful to consider the intrinsic\ncharacteristics of the audio and video media themselves.\n7.1.1 Properties of Video\nPerhaps the most salient characteristic of video is its high bit rate. Video distrib-\nuted over the Internet typically ranges from 100 kbps for low-quality video confer-\nencing to over 3 Mbps for streaming high-definition movies. To get a sense of how\nvideo bandwidth demands compare with those of other Internet applications, let’s\nbriefly consider three different users, each using a different Internet application. Our\nfirst user, Frank, is going quickly through photos posted on his friends’ Facebook\npages. Let’s assume that Frank is looking at a new photo every 10 seconds, and that\nphotos are on average 200 Kbytes in size. (As usual, throughout this discussion we\nmake the simplifying assumption that 1 Kbyte = 8,000 bits.) Our second user,\nMartha, is streaming music from the Internet (“the cloud”) to her smartphone. Let’s\nassume Martha is listening to many MP3 songs, one after the other, each encoded at\na rate of 128 kbps. Our third user, Victor, is watching a video that has been encoded\nat 2 Mbps. Finally, let’s suppose that the session length for all three users is 4,000\nseconds (approximately 67 minutes). Table 7.1 compares the bit rates and the total\nbytes transferred for these three users. We see that video streaming consumes by far\n588\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nthe most bandwidth, having a bit rate of more than ten times greater than that of the\nFacebook and music-streaming applications. Therefore, when designing networked\nvideo applications, the first thing we must keep in mind is the high bit-rate require-\nments of video. Given the popularity of video and its high bit rate, it is perhaps not\nsurprising that Cisco predicts [Cisco 2011] that streaming and stored video will be\napproximately 90 percent of global consumer Internet traffic by 2015.\nAnother important characteristic of video is that it can be compressed, thereby\ntrading off video quality with bit rate. A video is a sequence of images, typically\nbeing displayed at a constant rate, for example, at 24 or 30 images per second. An\nuncompressed, digitally encoded image consists of an array of pixels, with each\npixel encoded into a number of bits to represent luminance and color. There are two\ntypes of redundancy in video, both of which can be exploited by video compres-\nsion. Spatial redundancy is the redundancy within a given image. Intuitively, an\nimage that consists of mostly white space has a high degree of redundancy and can\nbe efficiently compressed without significantly sacrificing image quality. Temporal\nredundancy reflects repetition from image to subsequent image. If, for example, an\nimage and the subsequent image are exactly the same, there is no reason to re-\nencode the subsequent image; it is instead more efficient simply to indicate during\nencoding that the subsequent image is exactly the same. Today’s off-the-shelf com-\npression algorithms can compress a video to essentially any bit rate desired. Of\ncourse, the higher the bit rate, the better the image quality and the better the overall\nuser viewing experience.\nWe can also use compression to create multiple versions of the same video,\neach at a different quality level. For example, we can use compression to create, say,\nthree versions of the same video, at rates of 300 kbps, 1 Mbps, and 3 Mbps. Users\ncan then decide which version they want to watch as a function of their current\navailable bandwidth. Users with high-speed Internet connections might choose the\n3 Mbps version; users watching the video over 3G with a smartphone might choose\nthe 300 kbps version. Similarly, the video in a video conference application can be\ncompressed “on-the-fly” to provide the best video quality given the available end-\nto-end bandwidth between conversing users.\n7.1\n•\nMULTIMEDIA NETWORKING APPLICATIONS\n589\nBit rate\nBytes transferred in 67 min\nFacebook Frank\n160 kbps\n80 Mbytes\nMartha Music\n128 kbps\n64 Mbytes\nVictor Video\n2 Mbps\n1 Gbyte\nTable 7.1 \u0002 Comparison of bit-rate requirements of three Internet applications"
    },
    {
      "chunk_id": "5a654b8f-8830-466d-9032-dccf97ff7c6e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.1.1 Properties of Video",
      "original_titles": [
        "7.1.1 Properties of Video"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.1 Multimedia Networking Applications > 7.1.1 Properties of Video",
      "start_page": 615,
      "end_page": 616,
      "token_count": 1167,
      "text": "quality to available bandwidth. We will also investigate Content Distribution Net-\nworks (CDNs), which are used extensively today by the leading video streaming\nsystems. We then examine the YouTube, Netflix, and Kankan systems as case \nstudies for streaming video. In Section 7.3, we investigate conversational voice and\nvideo, which, unlike elastic applications, are highly sensitive to end-to-end delay\nbut can tolerate occasional loss of data. Here we’ll examine how techniques such as\nadaptive playout, forward error correction, and error concealment can mitigate\nagainst network-induced packet loss and delay. We’ll also examine Skype as a case\nstudy. In Section 7.4, we’ll study RTP and SIP, two popular protocols for real-time\nconversational voice and video applications. In Section 7.5, we’ll investigate mech-\nanisms within the network that can be used to distinguish one class of traffic (e.g.,\ndelay-sensitive applications such as conversational voice) from another (e.g., elastic\napplications such as browsing Web pages), and provide differentiated service among\nmultiple classes of traffic.\n7.1 Multimedia Networking Applications\nWe define a multimedia network application as any network application that\nemploys audio or video. In this section, we provide a taxonomy of multimedia appli-\ncations. We’ll see that each class of applications in the taxonomy has its own unique\nset of service requirements and design issues. But before diving into an in-depth dis-\ncussion of Internet multimedia applications, it is useful to consider the intrinsic\ncharacteristics of the audio and video media themselves.\n7.1.1 Properties of Video\nPerhaps the most salient characteristic of video is its high bit rate. Video distrib-\nuted over the Internet typically ranges from 100 kbps for low-quality video confer-\nencing to over 3 Mbps for streaming high-definition movies. To get a sense of how\nvideo bandwidth demands compare with those of other Internet applications, let’s\nbriefly consider three different users, each using a different Internet application. Our\nfirst user, Frank, is going quickly through photos posted on his friends’ Facebook\npages. Let’s assume that Frank is looking at a new photo every 10 seconds, and that\nphotos are on average 200 Kbytes in size. (As usual, throughout this discussion we\nmake the simplifying assumption that 1 Kbyte = 8,000 bits.) Our second user,\nMartha, is streaming music from the Internet (“the cloud”) to her smartphone. Let’s\nassume Martha is listening to many MP3 songs, one after the other, each encoded at\na rate of 128 kbps. Our third user, Victor, is watching a video that has been encoded\nat 2 Mbps. Finally, let’s suppose that the session length for all three users is 4,000\nseconds (approximately 67 minutes). Table 7.1 compares the bit rates and the total\nbytes transferred for these three users. We see that video streaming consumes by far\n588\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nthe most bandwidth, having a bit rate of more than ten times greater than that of the\nFacebook and music-streaming applications. Therefore, when designing networked\nvideo applications, the first thing we must keep in mind is the high bit-rate require-\nments of video. Given the popularity of video and its high bit rate, it is perhaps not\nsurprising that Cisco predicts [Cisco 2011] that streaming and stored video will be\napproximately 90 percent of global consumer Internet traffic by 2015.\nAnother important characteristic of video is that it can be compressed, thereby\ntrading off video quality with bit rate. A video is a sequence of images, typically\nbeing displayed at a constant rate, for example, at 24 or 30 images per second. An\nuncompressed, digitally encoded image consists of an array of pixels, with each\npixel encoded into a number of bits to represent luminance and color. There are two\ntypes of redundancy in video, both of which can be exploited by video compres-\nsion. Spatial redundancy is the redundancy within a given image. Intuitively, an\nimage that consists of mostly white space has a high degree of redundancy and can\nbe efficiently compressed without significantly sacrificing image quality. Temporal\nredundancy reflects repetition from image to subsequent image. If, for example, an\nimage and the subsequent image are exactly the same, there is no reason to re-\nencode the subsequent image; it is instead more efficient simply to indicate during\nencoding that the subsequent image is exactly the same. Today’s off-the-shelf com-\npression algorithms can compress a video to essentially any bit rate desired. Of\ncourse, the higher the bit rate, the better the image quality and the better the overall\nuser viewing experience.\nWe can also use compression to create multiple versions of the same video,\neach at a different quality level. For example, we can use compression to create, say,\nthree versions of the same video, at rates of 300 kbps, 1 Mbps, and 3 Mbps. Users\ncan then decide which version they want to watch as a function of their current\navailable bandwidth. Users with high-speed Internet connections might choose the\n3 Mbps version; users watching the video over 3G with a smartphone might choose\nthe 300 kbps version. Similarly, the video in a video conference application can be\ncompressed “on-the-fly” to provide the best video quality given the available end-\nto-end bandwidth between conversing users.\n7.1\n•\nMULTIMEDIA NETWORKING APPLICATIONS\n589\nBit rate\nBytes transferred in 67 min\nFacebook Frank\n160 kbps\n80 Mbytes\nMartha Music\n128 kbps\n64 Mbytes\nVictor Video\n2 Mbps\n1 Gbyte\nTable 7.1 \u0002 Comparison of bit-rate requirements of three Internet applications"
    },
    {
      "chunk_id": "c1f2d10d-da50-40a2-8c83-0ed31a57e3d4",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.1.2 Properties of Audio",
      "original_titles": [
        "7.1.2 Properties of Audio"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.1 Multimedia Networking Applications > 7.1.2 Properties of Audio",
      "start_page": 617,
      "end_page": 617,
      "token_count": 676,
      "text": "7.1.2 Properties of Audio\nDigital audio (including digitized speech and music) has significantly lower band-\nwidth requirements than video. Digital audio, however, has its own unique proper-\nties that must be considered when designing multimedia network applications. To\nunderstand these properties, let’s first consider how analog audio (which humans\nand musical instruments generate) is converted to a digital signal:\n•\nThe analog audio signal is sampled at some fixed rate, for example, at 8,000\nsamples per second. The value of each sample is an arbitrary real number.\n•\nEach of the samples is then rounded to one of a finite number of values. This\noperation is referred to as quantization. The number of such finite values—\ncalled quantization values—is typically a power of two, for example, 256 quan-\ntization values.\n•\nEach of the quantization values is represented by a fixed number of bits. For\nexample, if there are 256 quantization values, then each value—and hence each\naudio sample—is represented by one byte. The bit representations of all the sam-\nples are then concatenated together to form the digital representation of the sig-\nnal. As an example, if an analog audio signal is sampled at 8,000 samples per\nsecond and each sample is quantized and represented by 8 bits, then the resulting\ndigital signal will have a rate of 64,000 bits per second. For playback through\naudio speakers, the digital signal can then be converted back—that is, decoded—\nto an analog signal. However, the decoded analog signal is only an approxima-\ntion of the original signal, and the sound quality may be noticeably degraded (for\nexample, high-frequency sounds may be missing in the decoded signal). By\nincreasing the sampling rate and the number of quantization values, the decoded\nsignal can better approximate the original analog signal. Thus (as with video),\nthere is a trade-off between the quality of the decoded signal and the bit-rate and\nstorage requirements of the digital signal.\nThe basic encoding technique that we just described is called pulse code modulation\n(PCM). Speech encoding often uses PCM, with a sampling rate of 8,000 samples per\nsecond and 8 bits per sample, resulting in a rate of 64 kbps. The audio compact disk\n(CD) also uses PCM, with a sampling rate of 44,100 samples per second with 16 bits\nper sample; this gives a rate of 705.6 kbps for mono and 1.411 Mbps for stereo.\nPCM-encoded speech and music, however, are rarely used in the Internet.\nInstead, as with video, compression techniques are used to reduce the bit rates of the\nstream. Human speech can be compressed to less than 10 kbps and still be intelligi-\nble. A popular compression technique for near CD-quality stereo music is MPEG 1\nlayer 3, more commonly known as MP3. MP3 encoders can compress to many dif-\nferent rates; 128 kbps is the most common encoding rate and produces very little\nsound degradation. A related standard is Advanced Audio Coding (AAC), which\nhas been popularized by Apple. As with video, multiple versions of a prerecorded\naudio stream can be created, each at a different bit rate.\n590\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING"
    },
    {
      "chunk_id": "c3e05c61-6cec-41c2-b0b6-efe03ea2d0a7",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.1.3 Types of Multimedia Network Applications",
      "original_titles": [
        "7.1.3 Types of Multimedia Network Applications"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.1 Multimedia Networking Applications > 7.1.3 Types of Multimedia Network Applications",
      "start_page": 618,
      "end_page": 619,
      "token_count": 1202,
      "text": "Although audio bit rates are generally much less than those of video, users are\ngenerally much more sensitive to audio glitches than video glitches. Consider, for\nexample, a video conference taking place over the Internet. If, from time to time, the\nvideo signal is lost for a few seconds, the video conference can likely proceed with-\nout too much user frustration. If, however, the audio signal is frequently lost, the\nusers may have to terminate the session.\n7.1.3 Types of Multimedia Network Applications\nThe Internet supports a large variety of useful and entertaining multimedia applica-\ntions. In this subsection, we classify multimedia applications into three broad cate-\ngories: (i) streaming stored audio/video, (ii) conversational voice/video-over-IP,\nand (iii) streaming live audio/video. As we will soon see, each of these application\ncategories has its own set of service requirements and design issues.\nStreaming Stored Audio and Video\nTo keep the discussion concrete, we focus here on streaming stored video, which\ntypically combines video and audio components. Streaming stored audio (such as\nstreaming music) is very similar to streaming stored video, although the bit rates are\ntypically much lower.\nIn this class of applications, the underlying medium is prerecorded video, such\nas a movie, a television show, a prerecorded sporting event, or a prerecorded user-\ngenerated video (such as those commonly seen on YouTube). These prerecorded\nvideos are placed on servers, and users send requests to the servers to view the videos\non demand. Many Internet companies today provide streaming video, including\nYouTube (Google), Netflix, and Hulu. By some estimates, streaming stored video\nmakes up over 50 percent of the downstream traffic in the Internet access networks\ntoday [Cisco 2011]. Streaming stored video has three key distinguishing features.\n•\nStreaming.\nIn a streaming stored video application, the client typically begins\nvideo playout within a few seconds after it begins receiving the video from the\nserver. This means that the client will be playing out from one location in \nthe video while at the same time receiving later parts of the video from the\nserver. This technique, known as streaming, avoids having to download \nthe entire video file (and incurring a potentially long delay) before playout begins.\n•\nInteractivity.\nBecause the media is prerecorded, the user may pause, reposition\nforward, reposition backward, fast-forward, and so on through the video content.\nThe time from when the user makes such a request until the action manifests itself\nat the client should be less than a few seconds for acceptable responsiveness.\n•\nContinuous playout.\nOnce playout of the video begins, it should proceed\naccording to the original timing of the recording. Therefore, data must be\nreceived from the server in time for its playout at the client; otherwise, users\n7.1\n•\nMULTIMEDIA NETWORKING APPLICATIONS\n591\n\nexperience video frame freezing (when the client waits for the delayed frames)\nor frame skipping (when the client skips over delayed frames).\nBy far, the most important performance measure for streaming video is average\nthroughput. In order to provide continuous playout, the network must provide an\naverage throughput to the streaming application that is at least as large the bit rate of\nthe video itself. As we will see in Section 7.2, by using buffering and prefetching, it\nis possible to provide continuous playout even when the throughput fluctuates, as\nlong as the average throughput (averaged over 5–10 seconds) remains above the\nvideo rate [Wang 2008].\nFor many streaming video applications, prerecorded video is stored on, and\nstreamed from, a CDN rather than from a single data center. There are also many\nP2P video streaming applications for which the video is stored on users’ hosts\n(peers), with different chunks of video arriving from different peers that may spread\naround the globe. Given the prominence of Internet video streaming, we will\nexplore video streaming in some depth in Section 7.2, paying particular attention to\nclient buffering, prefetching, adapting quality to bandwidth availability, and CDN\ndistribution.\nConversational Voice- and Video-over-IP\nReal-time conversational voice over the Internet is often referred to as Internet\ntelephony, since, from the user’s perspective, it is similar to the traditional circuit-\nswitched telephone service. It is also commonly called Voice-over-IP (VoIP). Con-\nversational video is similar, except that it includes the video of the participants as\nwell as their voices. Most of today’s voice and video conversational systems allow\nusers to create conferences with three or more participants. Conversational voice\nand video are widely used in the Internet today, with the Internet companies Skype,\nQQ, and Google Talk boasting hundreds of millions of daily users.\nIn our discussion of application service requirements in Chapter 2 (Figure 2.4),\nwe identified a number of axes along which application requirements can be classi-\nfied. Two of these axes—timing considerations and tolerance of data loss—are par-\nticularly important for conversational voice and video applications. Timing\nconsiderations are important because audio and video conversational applications\nare highly delay-sensitive. For a conversation with two or more interacting speak-\ners, the delay from when a user speaks or moves until the action is manifested at the\nother end should be less than a few hundred milliseconds. For voice, delays smaller\nthan 150 milliseconds are not perceived by a human listener, delays between 150\nand 400 milliseconds can be acceptable, and delays exceeding 400 milliseconds can\nresult in frustrating, if not completely unintelligible, voice conversations.\nOn the other hand, conversational multimedia applications are loss-tolerant—\noccasional loss only causes occasional glitches in audio/video playback, and these\nlosses can often be partially or fully concealed. These delay-sensitive but loss-tolerant\n592\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING"
    },
    {
      "chunk_id": "aa5aef12-d829-4873-b87b-9acf1df67813",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.2 Streaming Stored Video",
      "original_titles": [
        "7.2 Streaming Stored Video"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.2 Streaming Stored Video",
      "start_page": 620,
      "end_page": 621,
      "token_count": 1108,
      "text": "characteristics are clearly different from those of elastic data applications such as Web\nbrowsing, e-mail, social networks, and remote login. For elastic applications, long\ndelays are annoying but not particularly harmful; the completeness and integrity of the\ntransferred data, however, are of paramount importance. We will explore conversa-\ntional voice and video in more depth in Section 7.3, paying particular attention to how\nadaptive playout, forward error correction, and error concealment can mitigate against\nnetwork-induced packet loss and delay.\nStreaming Live Audio and Video\nThis third class of applications is similar to traditional broadcast radio and televi-\nsion, except that transmission takes place over the Internet. These applications allow\na user to receive a live radio or television transmission—such as a live sporting\nevent or an ongoing news event—transmitted from any corner of the world. Today,\nthousands of radio and television stations around the world are broadcasting content\nover the Internet.\nLive, broadcast-like applications often have many users who receive the same\naudio/video program at the same time. Although the distribution of live audio/video\nto many receivers can be efficiently accomplished using the IP multicasting tech-\nniques described in Section 4.7, multicast distribution is more often accomplished\ntoday via application-layer multicast (using P2P networks or CDNs) or through mul-\ntiple separate unicast streams. As with streaming stored multimedia, the network\nmust provide each live multimedia flow with an average throughput that is larger\nthan the video consumption rate. Because the event is live, delay can also be an issue,\nalthough the timing constraints are much less stringent than those for conversational\nvoice. Delays of up to ten seconds or so from when the user chooses to view a live\ntransmission to when playout begins can be tolerated. We will not cover streaming\nlive media in this book because many of the techniques used for streaming live\nmedia—initial buffering delay, adaptive bandwidth use, and CDN distribution—are\nsimilar to those for streaming stored media.\n7.2 Streaming Stored Video \nFor streaming video applications, prerecorded videos are placed on servers, and\nusers send requests to these servers to view the videos on demand. The user may\nwatch the video from beginning to end without interruption, may stop watching the\nvideo well before it ends, or interact with the video by pausing or repositioning to a\nfuture or past scene. Streaming video systems can be classified into three categories:\nUDP streaming, HTTP streaming, and adaptive HTTP streaming. Although all\nthree types of systems are used in practice, the majority of today’s systems employ\nHTTP streaming and adaptive HTTP streaming.\n7.2\n•\nSTREAMING STORED VIDEO\n593\n\nA common characteristic of all three forms of video streaming is the extensive\nuse of client-side application buffering to mitigate the effects of varying end-to-end\ndelays and varying amounts of available bandwidth between server and client. For\nstreaming video (both stored and live), users generally can tolerate a small several-\nsecond initial delay between when the client requests a video and when video play-\nout begins at the client. Consequently, when the video starts to arrive at the client,\nthe client need not immediately begin playout, but can instead build up a reserve of\nvideo in an application buffer. Once the client has built up a reserve of several sec-\nonds of buffered-but-not-yet-played video, the client can then begin video playout.\nThere are two important advantages provided by such client buffering. First, client-\nside buffering can absorb variations in server-to-client delay. If a particular piece of\nvideo data is delayed, as long as it arrives before the reserve of received-but-not-\nyet-played video is exhausted, this long delay will not be noticed. Second, if the\nserver-to-client bandwidth briefly drops below the video consumption rate, a user\ncan continue to enjoy continuous playback, again as long as the client application\nbuffer does not become completely drained.\nFigure 7.1 illustrates client-side buffering. In this simple example, suppose that\nvideo is encoded at a fixed bit rate, and thus each video block contains video frames\nthat are to be played out over the same fixed amount of time, \n. The server trans-\nmits the first video block at \n, the second block at \n, the third block at\n, and so on. Once the client begins playout, each block should be played out\ntime units after the previous block in order to reproduce the timing of the original\nrecorded video. Because of the variable end-to-end network delays, different video\nblocks experience different delays. The first video block arrives at the client at t1\nand the second block arrives at . The network delay for the ith block is the hori-\nzontal distance between the time the block was transmitted by the server and the\nt2\n\u0002\nt0 + 2\u0002\nt0 + \u0002\nt0\n\u0002\n594\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nVariable\nnetwork\ndelay\nClient\nplayout\ndelay\nConstant bit\nrate video\ntransmission\nby server\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nConstant bit\nrate video\nplayout\nby client\nTime\nVideo block number\nt0\nt1\nt2\nt3\nt0+2Δ\nt0+Δ\nt1+Δ\nt3+Δ\nVideo\nreception\nat client\nFigure 7.1 \u0002 Client playout delay in video streaming"
    },
    {
      "chunk_id": "ce442683-a9a1-4d6b-9f0b-be2b1ef5afbb",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.2.1 UDP Streaming",
      "original_titles": [
        "7.2.1 UDP Streaming"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.2 Streaming Stored Video > 7.2.1 UDP Streaming",
      "start_page": 622,
      "end_page": 622,
      "token_count": 722,
      "text": "time it is received at the client; note that the network delay varies from one video\nblock to another. In this example, if the client were to begin playout as soon as the\nfirst block arrived at , then the second block would not have arrived in time to be\nplayed out at out at \n. In this case, video playout would either have to stall\n(waiting for block 1 to arrive) or block 1 could be skipped—both resulting in unde-\nsirable playout impairments. Instead, if the client were to delay the start of playout\nuntil , when blocks 1 through 6 have all arrived, periodic playout can proceed with\nall blocks having been received before their playout time.\n7.2.1 UDP Streaming \nWe only briefly discuss UDP streaming here, referring the reader to more in-depth dis-\ncussions of the protocols behind these systems where appropriate. With UDP stream-\ning, the server transmits video at a rate that matches the client’s video consumption rate\nby clocking out the video chunks over UDP at a steady rate. For example, if the video\nconsumption rate is 2 Mbps and each UDP packet carries 8,000 bits of video, then the\nserver would transmit one UDP packet into its socket every (8000 bits)/(2 Mbps) = \n4 msec. As we learned in Chapter 3, because UDP does not employ a congestion-control\nmechanism, the server can push packets into the network at the consumption rate of the\nvideo without the rate-control restrictions of TCP. UDP streaming typically uses a small\nclient-side buffer, big enough to hold less than a second of video.\nBefore passing the video chunks to UDP, the server will encapsulate the video\nchunks within transport packets specially designed for transporting audio and video,\nusing the Real-Time Transport Protocol (RTP) [RFC 3550] or a similar (possibly\nproprietary) scheme. We delay our coverage of RTP until Section 7.3, where we dis-\ncuss RTP in the context of conversational voice and video systems.\nAnother distinguishing property of UDP streaming is that in addition to the server-\nto-client video stream, the client and server also maintain, in parallel, a separate control\nconnection over which the client sends commands regarding session state changes\n(such as pause, resume, reposition, and so on). This control connection is in many ways\nanalogous to the FTP control connection we studied in Chapter 2. The Real-Time\nStreaming Protocol (RTSP) [RFC 2326], explained in some detail in the companion\nWeb site for this textbook, is a popular open protocol for such a control connection.\nAlthough UDP streaming has been employed in many open-source systems and\nproprietary products, it suffers from three significant drawbacks. First, due to the\nunpredictable and varying amount of available bandwidth between server and client,\nconstant-rate UDP streaming can fail to provide continuous playout. For example,\nconsider the scenario where the video consumption rate is 1 Mbps and the server-\nto-client available bandwidth is usually more than 1 Mbps, but every few minutes\nthe available bandwidth drops below 1 Mbps for several seconds. In such a scenario,\na UDP streaming system that transmits video at a constant rate of 1 Mbps over\nRTP/UDP would likely provide a poor user experience, with freezing or skipped\nframes soon after the available bandwidth falls below 1 Mbps. The second drawback\nt3\nt1 + \u0002\nt1\n7.2\n•\nSTREAMING STORED VIDEO\n595"
    },
    {
      "chunk_id": "3be3621b-7090-4f7c-9ea9-4138bca82e12",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.2.2 HTTP Streaming",
      "original_titles": [
        "7.2.2 HTTP Streaming"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.2 Streaming Stored Video > 7.2.2 HTTP Streaming",
      "start_page": 623,
      "end_page": 626,
      "token_count": 2332,
      "text": "of UDP streaming is that it requires a media control server, such as an RTSP server,\nto process client-to-server interactivity requests and to track client state (e.g., the\nclient’s playout point in the video, whether the video is being paused or played, and\nso on) for each ongoing client session. This increases the overall cost and complex-\nity of deploying a large-scale video-on-demand system. The third drawback is that\nmany firewalls are configured to block UDP traffic, preventing the users behind\nthese firewalls from receiving UDP video.\n7.2.2 HTTP Streaming\nIn HTTP streaming, the video is simply stored in an HTTP server as an ordinary file\nwith a specific URL. When a user wants to see the video, the client establishes a\nTCP connection with the server and issues an HTTP GET request for that URL. The\nserver then sends the video file, within an HTTP response message, as quickly as\npossible, that is, as quickly as TCP congestion control and flow control will allow.\nOn the client side, the bytes are collected in a client application buffer. Once the\nnumber of bytes in this buffer exceeds a predetermined threshold, the client applica-\ntion begins playback—specifically, it periodically grabs video frames from \nthe client application buffer, decompresses the frames, and displays them on the\nuser’s screen.\nWe learned in Chapter 3 that when transferring a file over TCP, the server-to-\nclient transmission rate can vary significantly due to TCP’s congestion control mecha-\nnism. In particular, it is not uncommon for the transmission rate to vary in a\n“saw-tooth” manner (for example, Figure 3.53) associated with TCP congestion con-\ntrol. Furthermore, packets can also be significantly delayed due to TCP’s retransmis-\nsion mechanism. Because of these characteristics of TCP, the conventional wisdom in\nthe 1990s was that video streaming would never work well over TCP. Over time, how-\never, designers of streaming video systems learned that TCP’s congestion control and\nreliable-data transfer mechanisms do not necessarily preclude continuous playout\nwhen client buffering and prefetching (discussed in the next section) are used.\nThe use of HTTP over TCP also allows the video to traverse firewalls and NATs\nmore easily (which are often configured to block most UDP traffic but to allow most\nHTTP traffic). Streaming over HTTP also obviates the need for a media control\nserver, such as an RTSP server, reducing the cost of a large-scale deployment over\nthe Internet. Due to all of these advantages, most video streaming applications\ntoday—including YouTube and Netflix—use HTTP streaming (over TCP) as its\nunderlying streaming protocol.\nPrefetching Video\nWe just learned, client-side buffering can be used to mitigate the effects of vary-\ning end-to-end delays and varying available bandwidth. In our earlier example in\nFigure 7.1, the server transmits video at the rate at which the video is to be played\n596\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\n7.2\n•\nSTREAMING STORED VIDEO\n597\nout. However, for streaming stored video, the client can attempt to download the\nvideo at a rate higher than the consumption rate, thereby prefetching video\nframes that are to be consumed in the future. This prefetched video is naturally\nstored in the client application buffer. Such prefetching occurs naturally with TCP\nstreaming, since TCP’s congestion avoidance mechanism will attempt to use all of\nthe available bandwidth between server and client.\nTo gain some insight into prefetching, let’s take a look at a simple example.\nSuppose the video consumption rate is 1 Mbps but the network is capable of deliv-\nering the video from server to client at a constant rate of 1.5 Mbps. Then the client\nwill not only be able to play out the video with a very small playout delay, but will\nalso be able to increase the amount of buffered video data by 500 Kbits every \nsecond. In this manner, if in the future the client receives data at a rate of less than 1\nMbps for a brief period of time, the client will be able to continue to provide contin-\nuous playback due to the reserve in its buffer. [Wang 2008] shows that when the\naverage TCP throughput is roughly twice the media bit rate, streaming over TCP\nresults in minimal starvation and low buffering delays.\nClient Application Buffer and TCP Buffers\nFigure 7.2 illustrates the interaction between client and server for HTTP streaming.\nAt the server side, the portion of the video file in white has already been sent into\nthe server’s socket, while the darkened portion is what remains to be sent. After\n“passing through the socket door,” the bytes are placed in the TCP send buffer\nbefore being transmitted into the Internet, as described in Chapter 3. In Figure 7.2,\nVideo file\nWeb server\nClient\nTCP send\nbuffer\nTCP receive\nbuffer\nTCP application\nbuffer\nFrames read\nout periodically\nfrom buffer,\ndecompressed,\nand displayed\non screen\nFigure 7.2 \u0002 Streaming stored video over HTTP/TCP\n\nbecause the TCP send buffer is shown to be full, the server is momentarily prevented\nfrom sending more bytes from the video file into the socket. On the client side, the\nclient application (media player) reads bytes from the TCP receive buffer (through\nits client socket) and places the bytes into the client application buffer. At the same\ntime, the client application periodically grabs video frames from the client application\nbuffer, decompresses the frames, and displays them on the user’s screen. Note that\nif the client application buffer is larger than the video file, then the whole process of\nmoving bytes from the server’s storage to the client’s application buffer is equiva-\nlent to an ordinary file download over HTTP—the client simply pulls the video off\nthe server as fast as TCP will allow!\nConsider now what happens when the user pauses the video during the \nstreaming process. During the pause period, bits are not removed from the client\napplication buffer, even though bits continue to enter the buffer from the server. If\nthe client application buffer is finite, it may eventually become full, which will\ncause “back pressure” all the way back to the server. Specifically, once the client\napplication buffer becomes full, bytes can no longer be removed from the \nclient TCP receive buffer, so it too becomes full. Once the client receive TCP buffer\nbecomes full, bytes can no longer be removed from the client TCP send buffer, so\nit also becomes full. Once the TCP send buffer becomes full, the server cannot send\nany more bytes into the socket. Thus, if the user pauses the video, the server may\nbe forced to stop transmitting, in which case the server will be blocked until the\nuser resumes the video.\nIn fact, even during regular playback (that is, without pausing), if the client\napplication buffer becomes full, back pressure will cause the TCP buffers to\nbecome full, which will force the server to reduce its rate. To determine the\nresulting rate, note that when the client application removes f bits, it creates room\nfor f bits in the client application buffer, which in turn allows the server to send f\nadditional bits. Thus, the server send rate can be no higher than the video con-\nsumption rate at the client. Therefore, a full client application buffer indirectly\nimposes a limit on the rate that video can be sent from server to client when\nstreaming over HTTP.\nAnalysis of Video Streaming\nSome simple modeling will provide more insight into initial playout delay and\nfreezing due to application buffer depletion. As shown in Figure 7.3, let B denote\nthe size (in bits) of the client’s application buffer, and let Q denote the number of\nbits that must be buffered before the client application begins playout. (Of course, \nQ < B.) Let r denote the video consumption rate—the rate at which the client draws\nbits out of the client application buffer during playback. So, for example, if the\nvideo’s frame rate is 30 frames/sec, and each (compressed) frame is 100,000 bits,\nthen r = 3 Mbps. To see the forest through the trees, we’ll ignore TCP’s send and\nreceive buffers.\n598\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nLet’s assume that the server sends bits at a constant rate x whenever the client\nbuffer is not full. (This is a gross simplification, since TCP’s send rate varies due to\ncongestion control; we’ll examine more realistic time-dependent rates x(t) in the\nproblems at the end of this chapter.) Suppose at time t = 0, the application buffer is\nempty and video begins arriving to the client application buffer. We now ask at what\ntime \ndoes playout begin? And while we are at it, at what time \ndoes the\nclient application buffer become full?\nFirst, let’s determine \n, the time when Q bits have entered the application\nbuffer and playout begins. Recall that bits arrive to the client application buffer at\nrate x and no bits are removed from this buffer before playout begins. Thus, the\namount of time required to build up Q bits (the initial buffering delay) is \nQ/x.\nNow let’s determine , the point in time when the client application buffer\nbecomes full. We first observe that if x < r (that is, if the server send rate is less than\nthe video consumption rate), then the client buffer will never become full! Indeed,\nstarting at time , the buffer will be depleted at rate r and will only be filled at rate \nx < r. Eventually the client buffer will empty out entirely, at which time the video\nwill freeze on the screen while the client buffer waits another \nseconds to build up\nQ bits of video. Thus, when the available rate in the network is less than the video\nrate, playout will alternate between periods of continuous playout and periods of\nfreezing. In a homework problem, you will be asked to determine the length of each\ncontinuous playout and freezing period as a function of Q, r, and x. Now let’s deter-\nmine for when x > r. In this case, starting at time , the buffer increases from Q to\nB at rate x\nr since bits are being depleted at rate r but are arriving at rate x, as\nshown in Figure 7.3. Given these hints, you will be asked in a homework problem\nto determine , the time the client buffer becomes full. Note that when the available\nrate in the network is more than the video rate, after the initial buffering delay, the\nuser will enjoy continuous playout until the video ends. \ntf\n-\ntp\ntf\ntp\ntp\ntf\ntp =\ntp\nt = tf\nt = tp\n7.2\n•\nSTREAMING STORED VIDEO\n599\nFill rate = x\nDepletion rate = r\nVideo\nserver\nInternet\nQ\nB\nClient application buffer\nFigure 7.3 \u0002 Analysis of client-side buffering for video streaming"
    },
    {
      "chunk_id": "0053421f-8051-415d-9eec-4402f701ef51",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.2.3 Adaptive Streaming and DASH",
      "original_titles": [
        "7.2.3 Adaptive Streaming and DASH"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.2 Streaming Stored Video > 7.2.3 Adaptive Streaming and DASH",
      "start_page": 627,
      "end_page": 628,
      "token_count": 1427,
      "text": "Early Termination and Repositioning the Video\nHTTP streaming systems often make use of the HTTP byte-range header in the\nHTTP GET request message, which specifies the specific range of bytes the client\ncurrently wants to retrieve from the desired video. This is particularly useful when\nthe user wants to reposition (that is, jump) to a future point in time in the video.\nWhen the user repositions to a new position, the client sends a new HTTP request,\nindicating with the byte-range header from which byte in the file should the server\nsend data. When the server receives the new HTTP request, it can forget about any\nearlier request and instead send bytes beginning with the byte indicated in the byte-\nrange request.\nWhile we are on the subject of repositioning, we briefly mention that when a user\nrepositions to a future point in the video or terminates the video early, some\nprefetched-but-not-yet-viewed data transmitted by the server will go unwatched—a\nwaste of network bandwidth and server resources. For example, suppose that the client\nbuffer is full with B bits at some time t0 into the video, and at this time the user reposi-\ntions to some instant t > t0 + B/r into the video, and then watches the video to comple-\ntion from that point on. In this case, all B bits in the buffer will be unwatched and the\nbandwidth and server resources that were used to transmit those B bits have been com-\npletely wasted. There is significant wasted bandwidth in the Internet due to early ter-\nmination, which can be quite costly, particularly for wireless links [Ihm 2011]. For this\nreason, many streaming systems use only a moderate-size client application buffer, or\nwill limit the amount of prefetched video using the byte-range header in HTTP\nrequests [Rao 2011].\nRepositioning and early termination are analogous to cooking a large meal, eat-\ning only a portion of it, and throwing the rest away, thereby wasting food. So the\nnext time your parents criticize you for wasting food by not eating all your dinner,\nyou can quickly retort by saying they are wasting bandwidth and server resources\nwhen they reposition while watching movies over the Internet! But, of course, two\nwrongs do not make a right—both food and bandwidth are not to be wasted!\n7.2.3 Adaptive Streaming and DASH\nAlthough HTTP streaming, as described in the previous subsection, has been exten-\nsively deployed in practice (for example, by YouTube since its inception), it has a\nmajor shortcoming: All clients receive the same encoding of the video, despite the\nlarge variations in the amount of bandwidth available to a client, both across differ-\nent clients and also over time for the same client. This has led to the development of\na new type of HTTP-based streaming, often referred to as Dynamic Adaptive\nStreaming over HTTP (DASH). In DASH, the video is encoded into several dif-\nferent versions, with each version having a different bit rate and, correspondingly, a\ndifferent quality level. The client dynamically requests chunks of video segments of\na few seconds in length from the different versions. When the amount of available\n600\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nbandwidth is high, the client naturally selects chunks from a high-rate version; and\nwhen the available bandwidth is low, it naturally selects from a low-rate version.\nThe client selects different chunks one at a time with HTTP GET request messages\n[Akhshabi 2011].\nOn one hand, DASH allows clients with different Internet access rates to stream\nin video at different encoding rates. Clients with low-speed 3G connections can\nreceive a low bit-rate (and low-quality) version, and clients with fiber connections\ncan receive a high-quality version. On the other hand, DASH allows a client to adapt\nto the available bandwidth if the end-to-end bandwidth changes during the session.\nThis feature is particularly important for mobile users, who typically see their band-\nwidth availability fluctuate as they move with respect to the base stations. Comcast,\nfor example, has deployed an adaptive streaming system in which each video source\nfile is encoded into 8 to 10 different MPEG-4 formats, allowing the highest quality\nvideo format to be streamed to the client, with adaptation being performed in\nresponse to changing network and device conditions.\nWith DASH, each video version is stored in the HTTP server, each with a differ-\nent URL. The HTTP server also has a manifest file, which provides a URL for each\nversion along with its bit rate. The client first requests the manifest file and learns\nabout the various versions. The client then selects one chunk at a time by specifying a\nURL and a byte range in an HTTP GET request message for each chunk. While down-\nloading chunks, the client also measures the received bandwidth and runs a rate deter-\nmination algorithm to select the chunk to request next. Naturally, if the client has a lot\nof video buffered and if the measured receive bandwidth is high, it will choose a\nchunk from a high-rate version. And naturally if the client has little video buffered and\nthe measured received bandwidth is low, it will choose a chunk from a low-rate ver-\nsion. DASH therefore allows the client to freely switch among different quality levels.\nSince a sudden drop in bit rate by changing versions may result in noticeable visual\nquality degradation, the bit-rate reduction may be achieved using multiple intermedi-\nate versions to smoothly transition to a rate where the client’s consumption rate drops\nbelow its available receive bandwidth. When the network conditions improve, the\nclient can then later choose chunks from higher bit-rate versions.\nBy dynamically monitoring the available bandwidth and client buffer level, and\nadjusting the transmission rate with version switching, DASH can often achieve\ncontinuous playout at the best possible quality level without frame freezing or skip-\nping. Furthermore, since the client (rather than the server) maintains the intelligence\nto determine which chunk to send next, the scheme also improves server-side scala-\nbility. Another benefit of this approach is that the client can use the HTTP byte-range\nrequest to precisely control the amount of prefetched video that it buffers locally.\nWe conclude our brief discussion of DASH by mentioning that for many imple-\nmentations, the server not only stores many versions of the video but also separately\nstores many versions of the audio. Each audio version has its own quality level and bit\nrate and has its own URL. In these implementations, the client dynamically selects\nboth video and audio chunks, and locally synchronizes audio and video playout.\n7.2\n•\nSTREAMING STORED VIDEO\n601"
    },
    {
      "chunk_id": "fa5cad05-2214-4207-b4f6-84c154d5a1c3",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.2.4 Content Distribution Networks",
      "original_titles": [
        "7.2.4 Content Distribution Networks"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.2 Streaming Stored Video > 7.2.4 Content Distribution Networks",
      "start_page": 629,
      "end_page": 634,
      "token_count": 3209,
      "text": "7.2.4 Content Distribution Networks\nToday, many Internet video companies are distributing on-demand multi-Mbps\nstreams to millions of users on a daily basis. YouTube, for example, with a library\nof hundreds of millions of videos, distributes hundreds of millions of video streams\nto users around the world every day [Ding 2011]. Streaming all this traffic to loca-\ntions all over the world while providing continuous playout and high interactivity is\nclearly a challenging task.\nFor an Internet video company, perhaps the most straightforward approach to\nproviding streaming video service is to build a single massive data center, store all\nof its videos in the data center, and stream the videos directly from the data center to\nclients worldwide. But there are three major problems with this approach. First, if\nthe client is far from the data center, server-to-client packets will cross many com-\nmunication links and likely pass through many ISPs, with some of the ISPs possibly\nlocated on different continents. If one of these links provides a throughput that is\nless than the video consumption rate, the end-to-end throughput will also be below\nthe consumption rate, resulting in annoying freezing delays for the user. (Recall\nfrom Chapter 1 that the end-to-end throughput of a stream is governed by the\nthroughput in the bottleneck link.) The likelihood of this happening increases as the\nnumber of links in the end-to-end path increases. A second drawback is that a popu-\nlar video will likely be sent many times over the same communication links. Not\nonly does this waste network bandwidth, but the Internet video company itself will\nbe paying its provider ISP (connected to the data center) for sending the same bytes\ninto the Internet over and over again. A third problem with this solution is that a sin-\ngle data center represents a single point of failure—if the data center or its links to\nthe Internet goes down, it would not be able to distribute any video streams.\nIn order to meet the challenge of distributing massive amounts of video data to\nusers distributed around the world, almost all major video-streaming companies\nmake use of Content Distribution Networks (CDNs). A CDN manages servers in\nmultiple geographically distributed locations, stores copies of the videos (and other\ntypes of Web content, including documents, images, and audio) in its servers, and\nattempts to direct each user request to a CDN location that will provide the best user\nexperience. The CDN may be a private CDN, that is, owned by the content provider\nitself; for example, Google’s CDN distributes YouTube videos and other types of\ncontent. The CDN may alternatively be a third-party CDN that distributes content\non behalf of multiple content providers; Akamai’s CDN, for example, is a third-\nparty CDN that distributes Netflix and Hulu content, among others. A very readable\noverview of modern CDNs is [Leighton 2009].\nCDNs typically adopt one of two different server placement philosophies\n[Huang 2008]:\n•\nEnter Deep.\nOne philosophy, pioneered by Akamai, is to enter deep into the\naccess networks of Internet Service Providers, by deploying server clusters in\naccess ISPs all over the world. (Access networks are described in Section 1.3.)\n602\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nAkamai takes this approach with clusters in approximately 1,700 locations. The\ngoal is to get close to end users, thereby improving user-perceived delay and\nthroughput by decreasing the number of links and routers between the end user and\nthe CDN cluster from which it receives content. Because of this highly distributed\ndesign, the task of maintaining and managing the clusters becomes challenging.\nGOOGLE’S NETWORK INFRASTRUCTURE\nTo support its vast array of cloud services—including search, gmail, calendar,\nYouTube video, maps, documents, and social networks—Google has deployed an\nextensive private network and CDN infrastructure. Google’s CDN infrastructure has\nthree tiers of server clusters:\n• Eight “mega data centers,” with six located in the United States and two locat-\ned in Europe [Google Locations 2012], with each data center having on the\norder of 100,000 servers. These mega data centers are responsible for serving\ndynamic (and often personalized) content, including search results and gmail\nmessages.\n• About 30 “bring-home” clusters (see discussion in 7.2.4), with each cluster con-\nsisting on the order of 100–500 servers [Adhikari 2011a]. The cluster loca-\ntions are distributed around the world, with each location typically near multi-\nple tier-1 ISP PoPs. These clusters are responsible for serving static content,\nincluding YouTube videos [Adhikari 2011a].\n• Many hundreds of “enter-deep” clusters (see discussion in 7.2.4), with each\ncluster located within an access ISP. Here a cluster typically consists of tens of\nservers within a single rack. These enter-deep servers perform TCP splitting (see\nSection 3.7) and serve static content [Chen 2011], including the static portions\nof Web pages that embody search results.\nAll of these data centers and cluster locations are networked together with Google’s\nown private network, as part of one enormous AS (AS 15169). When a user makes a\nsearch query, often the query is first sent over the local ISP to a nearby enter-deep\ncache, from where the static content is retrieved; while providing the static content to\nthe client, the nearby cache also forwards the query over Google’s private network to\none of the mega data centers, from where the personalized search results are retrieved.\nFor a YouTube video, the video itself may come from one of the bring-home caches,\nwhereas portions of the Web page surrounding the video may come from the nearby\nenter-deep cache, and the advertisements surrounding the video come from the data\ncenters. In summary, except for the local ISPs, the Google cloud services are largely\nprovided by a network infrastructure that is independent of the public Internet.\nCASE STUDY\n7.2\n•\nSTREAMING STORED VIDEO\n603\n\n•\nBring Home.\nA second design philosophy, taken by Limelight and many other\nCDN companies, is to bring the ISPs home by building large clusters at a smaller\nnumber (for example, tens) of key locations and connecting these clusters using\na private high-speed network. Instead of getting inside the access ISPs, these\nCDNs typically place each cluster at a location that is simultaneously near the\nPoPs (see Section 1.3) of many tier-1 ISPs, for example, within a few miles of\nboth AT&T and Verizon PoPs in a major city. Compared with the enter-deep\ndesign philosophy, the bring-home design typically results in lower maintenance\nand management overhead, possibly at the expense of higher delay and lower\nthroughput to end users.\nOnce its clusters are in place, the CDN replicates content across its clusters. The\nCDN may not want to place a copy of every video in each cluster, since some videos\nare rarely viewed or are only popular in some countries. In fact, many CDNs do not\npush videos to their clusters but instead use a simple pull strategy: If a client\nrequests a video from a cluster that is not storing the video, then the cluster retrieves\nthe video (from a central repository or from another cluster) and stores a copy\nlocally while streaming the video to the client at the same time. Similar to Internet\ncaches (see Chapter 2), when a cluster’s storage becomes full, it removes videos that\nare not frequently requested.\nCDN Operation\nHaving identified the two major approaches toward deploying a CDN, let’s now\ndive down into the nuts and bolts of how a CDN operates. When a browser in \na user’s host is instructed to retrieve a specific video (identified by a URL), the\nCDN must intercept the request so that it can (1) determine a suitable CDN\nserver cluster for that client at that time, and (2) redirect the client’s request to \na server in that cluster. We’ll shortly discuss how a CDN can determine a suitable\ncluster. But first let’s examine the mechanics behind intercepting and redirecting\na request.\nMost CDNs take advantage of DNS to intercept and redirect requests; an inter-\nesting discussion of such a use of the DNS is [Vixie 2009]. Let’s consider a simple\nexample to illustrate how DNS is typically involved. Suppose a content provider,\nNetCinema, employs the third-party CDN company, KingCDN, to distribute its\nvideos to its customers. On the NetCinema Web pages, each of its videos is assigned\na URL that includes the string “video” and a unique identifier for the video itself; for\nexample, Transformers 7 might be assigned http://video.netcinema.com/6Y7B23V.\nSix steps then occur, as shown in Figure 7.4:\n1. The user visits the Web page at NetCinema.\n2. When the user clicks on the link http://video.netcinema.com/6Y7B23V, the\nuser’s host sends a DNS query for video.netcinema.com.\n604\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nFigure 7.4 \u0002 DNS redirects a user’s request to a CDN server\n3. The user’s Local DNS Server (LDNS) relays the DNS query to an authorita-\ntive DNS server for NetCinema, which observes the string “video” in the\nhostname video.netcinema.com. To “hand over” the DNS query to KingCDN,\ninstead of returning an IP address, the NetCinema authoritative DNS server\nreturns to the LDNS a hostname in the KingCDN’s domain, for example,\na1105.kingcdn.com.\n4. From this point on, the DNS query enters into KingCDN’s private DNS \ninfrastructure. The user’s LDNS then sends a second query, now for\na1105.kingcdn.com, and KingCDN’s DNS system eventually returns the \nIP addresses of a KingCDN content server to the LDNS. It is thus here, \nwithin the KingCDN’s DNS system, that the CDN server from which the \nclient will receive its content is specified.\n5. The LDNS forwards the IP address of the content-serving CDN node to the\nuser’s host.\n6. Once the client receives the IP address for a KingCDN content server, it\nestablishes a direct TCP connection with the server at that IP address and\nissues an HTTP GET request for the video. If DASH is used, the server will\nfirst send to the client a manifest file with a list of URLs, one for each \nversion of the video, and the client will dynamically select chunks from the\ndifferent versions.\nLocal\nDNS server\nNetCinema authoritative\n DNS server\nwww.NetCinema.com\nKingCDN authoritative\nserver\nKingCDN content\ndistribution server\n2\n5\n6\n3\n1\n4\n7.2\n•\nSTREAMING STORED VIDEO\n605\n\nCluster Selection Strategies\nAt the core of any CDN deployment is a cluster selection strategy, that is, a mech-\nanism for dynamically directing clients to a server cluster or a data center within the\nCDN. As we just saw, the CDN learns the IP address of the client’s LDNS server via\nthe client’s DNS lookup. After learning this IP address, the CDN needs to select an\nappropriate cluster based on this IP address. CDNs generally employ proprietary\ncluster selection strategies. We now briefly survey a number of natural approaches,\neach of which has its own advantages and disadvantages.\nOne simple strategy is to assign the client to the cluster that is geographically\nclosest. Using commercial geo-location databases (such as Quova [Quova 2012]\nand Max-Mind [MaxMind 2012]), each LDNS IP address is mapped to a geographic\nlocation. When a DNS request is received from a particular LDNS, the CDN\nchooses the geographically closest cluster, that is, the cluster that is the fewest kilo-\nmeters from the LDNS “as the bird flies.” Such a solution can work reasonably well\nfor a large fraction of the clients [Agarwal 2009]. However, for some clients, the\nsolution may perform poorly, since the geographically closest cluster may not be the\nclosest cluster along the network path. Furthermore, a problem inherent with all\nDNS-based approaches is that some end-users are configured to use remotely\nlocated LDNSs [Shaikh 2001; Mao 2002], in which case the LDNS location may be\nfar from the client’s location. Moreover, this simple strategy ignores the variation in\ndelay and available bandwidth over time of Internet paths, always assigning the\nsame cluster to a particular client.\nIn order to determine the best cluster for a client based on the current traffic\nconditions, CDNs can instead perform periodic real-time measurements of delay\nand loss performance between their clusters and clients. For instance, a CDN can\nhave each of its clusters periodically send probes (for example, ping messages or\nDNS queries) to all of the LDNSs around the world. One drawback of this approach\nis that many LDNSs are configured to not respond to such probes.\nAn alternative to sending extraneous traffic for measuring path properties is to\nuse the characteristics of recent and ongoing traffic between the clients and CDN\nservers. For instance, the delay between a client and a cluster can be estimated by\nexamining the gap between server-to-client SYNACK and client-to-server ACK \nduring the TCP three-way handshake. Such solutions, however, require redirecting\nclients to (possibly) suboptimal clusters from time to time in order to measure the\nproperties of paths to these clusters. Although only a small number of requests need\nto serve as probes, the selected clients can suffer significant performance degradation\nwhen receiving content (video or otherwise) [Andrews 2002; Krishnan 2009].\nAnother alternative for cluster-to-client path probing is to use DNS query traffic to\nmeasure the delay between clients and clusters. Specifically, during the DNS phase\n(within Step 4 in Figure 7.4), the client’s LDNS can be occasionally directed to dif-\nferent DNS authoritative servers installed at the various cluster locations, yielding\nDNS traffic that can then be measured between the LDNS and these cluster locations. \n606\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nIn this scheme, the DNS servers continue to return the optimal cluster for the client,\nso that delivery of videos and other Web objects does not suffer [Huang 2010].\nA very different approach to matching clients with CDN servers is to use IP\nanycast [RFC 1546]. The idea behind IP anycast is to have the routers in the Inter-\nnet route the client’s packets to the “closest” cluster, as determined by BGP. Specifi-\ncally, as shown in Figure 7.5, during the IP-anycast configuration stage, the CDN\ncompany assigns the same IP address to each of its clusters, and uses standard BGP\nto advertise this IP address from each of the different cluster locations. When a BGP\nrouter receives multiple route advertisements for this same IP address, it treats these\nadvertisements as providing different paths to the same physical location (when, in\nfact, the advertisements are for different paths to different physical locations).\nFollowing standard operating procedures, the BGP router will then pick the “best”\n(for example, closest, as determined by AS-hop counts) route to the IP address\naccording to its local route selection mechanism. For example, if one BGP route\nAS1\nAS3\n3b\n3c\n3a\n1a\n1c\n1b\n1d\nAS2\nAS4\n2a\n2c\n4a\n4c\n4b\nAdvertise\n212.21.21.21\nCDN Server B\nCDN Server A\nAdvertise\n212.21.21.21\nReceive BGP \nadvertisements for\n212.21.21.21 from\nAS1 and from AS4.\nForward towards\nServer B since it is\ncloser.\n2b\nFigure 7.5 \u0002 Using IP anycast to route clients to closest CDN cluster\n7.2\n•\nSTREAMING STORED VIDEO\n607"
    },
    {
      "chunk_id": "a4f9853f-9f0d-4e9a-bd21-cde3f13c1f4c",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.2.5 Case Studies: Netflix, YouTube, and Kankan",
      "original_titles": [
        "7.2.5 Case Studies: Netflix, YouTube, and Kankan"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.2 Streaming Stored Video > 7.2.5 Case Studies: Netflix, YouTube, and Kankan",
      "start_page": 635,
      "end_page": 638,
      "token_count": 2285,
      "text": "(corresponding to one location) is only one AS hop away from the router, and all\nother BGP routes (corresponding to other locations) are two or more AS hops away,\nthen the BGP router would typically choose to route packets to the location that\nneeds to traverse only one AS (see Section 4.6). After this initial configuration phase,\nthe CDN can do its main job of distributing content. When any client wants to see\nany video, the CDN’s DNS returns the anycast address, no matter where the client is\nlocated. When the client sends a packet to that IP address, the packet is routed to the\n“closest” cluster as determined by the preconfigured forwarding tables, which were\nconfigured with BGP as just described. This approach has the advantage of finding\nthe cluster that is closest to the client rather than the cluster that is closest to the\nclient’s LDNS. However, the IP anycast strategy again does not take into account the\ndynamic nature of the Internet over short time scales [Ballani 2006].\nBesides network-related considerations such as delay, loss, and bandwidth per-\nformance, there are many additional important factors that go into designing a clus-\nter selection strategy. Load on the clusters is one such factor—clients should not be\ndirected to overloaded clusters. ISP delivery cost is another factor—the clusters may\nbe chosen so that specific ISPs are used to carry CDN-to-client traffic, taking into\naccount the different cost structures in the contractual relationships between ISPs\nand cluster operators.\n7.2.5 Case Studies: Netflix, YouTube, and Kankan\nWe conclude our discussion of streaming stored video by taking a look at three\nhighly successful large-scale deployments: Netflix, YouTube, and Kankan. We’ll see\nthat all these systems take very different approaches, yet employ many of the under-\nlying principles discussed in this section.\nNetflix\nGenerating almost 30 percent of the downstream U.S. Internet traffic in 2011, Netflix\nhas become the leading service provider for online movies and TV shows in the United\nStates [Sandvine 2011]. In order to rapidly deploy its large-scale service, Netflix has\nmade extensive use of third-party cloud services and CDNs. Indeed, Netflix is an inter-\nesting example of a company deploying a large-scale online service by renting servers,\nbandwidth, storage, and database services from third parties while using hardly any\ninfrastructure of its own. The following discussion is adapted from a very readable\nmeasurement study of the Netflix architecture [Adhikari 2012]. As we’ll see, Netflix\nemploys many of the techniques covered earlier in this section, including video distri-\nbution using a CDN (actually multiple CDNs) and adaptive streaming over HTTP.\nFigure 7.6 shows the basic architecture of the Netflix video-streaming platform.\nIt has four major components: the registration and payment servers, the Amazon\ncloud, multiple CDN providers, and clients. In its own hardware infrastructure, Net-\nflix maintains registration and payment servers, which handle registration of new\n608\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nFigure 7.6 \u0002 Netflix video streaming platform\naccounts and capture credit-card payment information. Except for these basic func-\ntions, Netflix runs its online service by employing machines (or virtual machines) in\nthe Amazon cloud. Some of the functions taking place in the Amazon cloud include:\n•\nContent ingestion.\nBefore Netflix can distribute a movie to its customers, it\nmust first ingest and process the movie. Netflix receives studio master versions\nof movies and uploads them to hosts in the Amazon cloud.\n•\nContent processing.\nThe machines in the Amazon cloud create many different\nformats for each movie, suitable for a diverse array of client video players run-\nning on desktop computers, smartphones, and game consoles connected to tele-\nvisions. A different version is created for each of these formats and at multiple\nbit rates, allowing for adaptive streaming over HTTP using DASH.\n•\nUploading versions to the CDNs.\nOnce all of the versions of a movie have\nbeen created, the hosts in the Amazon cloud upload the versions to the CDNs.\nTo deliver the movies to its customers on demand, Netflix makes extensive use of\nCDN technology. In fact, as of this writing in 2012, Netflix employs not one but three\nthird-party CDN companies at the same time—Akamai, Limelight, and Level-3.\nHaving described the components of the Netflix architecture, let’s take a closer\nlook at the interaction between the client and the various servers that are involved in\nAmazon Cloud\nCDN server\nCDN server\nUpload\nversions\nto CDNs\nNetflix\nregistration and\npayment servers\nCDN server\nClient\nManifest\nfile\nRegistration\nand payment\nVideo\nchunks\n(DASH)\n7.2\n•\nSTREAMING STORED VIDEO\n609\n\nmovie delivery. The Web pages for browsing the Netflix video library are served\nfrom servers in the Amazon cloud. When the user selects a movie to “Play Now,”\nthe user’s client obtains a manifest file, also from servers in the Amazon cloud. The\nmanifest file includes a variety of information, including a ranked list of CDNs and\nthe URLs for the different versions of the movie, which are used for DASH play-\nback. The ranking of the CDNs is determined by Netflix, and may change from one\nstreaming session to the next. Typically the client will select the CDN that is ranked\nhighest in the manifest file. After the client selects a CDN, the CDN leverages DNS\nto redirect the client to a specific CDN server, as described in Section 7.2.4. The\nclient and that CDN server then interact using DASH. Specifically, as described in\nSection 7.2.3, the client uses the byte-range header in HTTP GET request messages,\nto request chunks from the different versions of the movie. Netflix uses chunks that\nare approximately four-seconds long [Adhikari 2012]. While the chunks are being\ndownloaded, the client measures the received throughput and runs a rate-determination\nalgorithm to determine the quality of the next chunk to request.\nNetflix embodies many of the key principles discussed earlier in this section,\nincluding adaptive streaming and CDN distribution. Netflix also nicely illustrates\nhow a major Internet service, generating almost 30 percent of Internet traffic, can\nrun almost entirely on a third-party cloud and third-party CDN infrastructures, using\nvery little infrastructure of its own!\nYouTube\nWith approximately half a billion videos in its library and half a billion video views\nper day [Ding 2011], YouTube is indisputably the world’s largest video-sharing site.\nYouTube began its service in April 2005 and was acquired by Google in November\n2006. Although the Google/YouTube design and protocols are proprietary, through\nseveral independent measurement efforts we can gain a basic understanding about\nhow YouTube operates [Zink 2009; Torres 2011; Adhikari 2011a].\nAs with Netflix, YouTube makes extensive use of CDN technology to dis-\ntribute its videos [Torres 2011]. Unlike Netflix, however, Google does not\nemploy third-party CDNs but instead uses its own private CDN to distribute\nYouTube videos. Google has installed server clusters in many hundreds of differ-\nent locations. From a subset of about 50 of these locations, Google distributes\nYouTube videos [Adhikari 2011a]. Google uses DNS to redirect a customer\nrequest to a specific cluster, as described in Section 7.2.4. Most of the time,\nGoogle’s cluster selection strategy directs the client to the cluster for which the\nRTT between client and cluster is the lowest; however, in order to balance the\nload across clusters, sometimes the client is directed (via DNS) to a more distant\ncluster [Torres 2011]. Furthermore, if a cluster does not have the requested video,\ninstead of fetching it from somewhere else and relaying it to the client, the clus-\nter may return an HTTP redirect message, thereby redirecting the client to\nanother cluster [Torres 2011].\n610\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nYouTube employs HTTP streaming, as discussed in Section 7.2.2. YouTube\noften makes a small number of different versions available for a video, each with a\ndifferent bit rate and corresponding quality level. As of 2011, YouTube does not\nemploy adaptive streaming (such as DASH), but instead requires the user to manu-\nally select a version. In order to save bandwidth and server resources that would be\nwasted by repositioning or early termination, YouTube uses the HTTP byte range\nrequest to limit the flow of transmitted data after a target amount of video is\nprefetched.\nA few million videos are uploaded to YouTube every day. Not only are\nYouTube videos streamed from server to client over HTTP, but YouTube uploaders\nalso upload their videos from client to server over HTTP. YouTube processes each\nvideo it receives, converting it to a YouTube video format and creating multiple ver-\nsions at different bit rates. This processing takes place entirely within Google data\ncenters. Thus, in stark contrast to Netflix, which runs its service almost entirely on\nthird-party infrastructures, Google runs the entire YouTube service within its own\nvast infrastructure of data centers, private CDN, and private global network \ninterconnecting its data centers and CDN clusters. (See the case study on Google’s\nnetwork infrastructure in Section 7.2.4.)\nKankan\nWe just saw that for both the Netflix and YouTube services, servers operated by\nCDNs (either third-party or private CDNs) stream videos to clients. Netflix and\nYouTube not only have to pay for the server hardware (either directly through own-\nership or indirectly through rent), but also for the bandwidth the servers use to dis-\ntribute the videos. Given the scale of these services and the amount of bandwidth\nthey are consuming, such a “client-server” deployment is extremely costly.\nWe conclude this section by describing an entirely different approach for provid-\ning video on demand over the Internet at a large scale—one that allows the service\nprovider to significantly reduce its infrastructure and bandwidth costs. As you might\nsuspect, this approach uses P2P delivery instead of client-server (via CDNs) delivery.\nP2P video delivery is used with great success by several companies in China, includ-\ning Kankan (owned and operated by Xunlei), PPTV (formerly PPLive), and PPs (for-\nmerly PPstream). Kankan, currently the leading P2P-based video-on-demand provider\nin China, has over 20 million unique users viewing its videos every month.\nAt a high level, P2P video streaming is very similar to BitTorrent file down-\nloading (discussed in Chapter 2). When a peer wants to see a video, it contacts a\ntracker (which may be centralized or peer-based using a DHT) to discover other\npeers in the system that have a copy of that video. This peer then requests chunks\nof the video file in parallel from these other peers that have the file. Different from\ndownloading with BitTorrent, however, requests are preferentially made for\nchunks that are to be played back in the near future in order to ensure continuous\nplayback.\n7.2\n•\nSTREAMING STORED VIDEO\n611"
    },
    {
      "chunk_id": "956e2319-b038-44db-ba4d-2cbe5732fe1f",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.3 Voice-over-IP",
      "original_titles": [
        "7.3 Voice-over-IP"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.3 Voice-over-IP",
      "start_page": 639,
      "end_page": 640,
      "token_count": 1167,
      "text": "The Kankan design employs a tracker and its own DHT for tracking content.\nSwarm sizes for the most popular content involve tens of thousands of peers, typi-\ncally larger than the largest swarms in BitTorrent [Dhungel 2012]. The Kankan pro-\ntocols—for communication between peer and tracker, between peer and DHT, and\namong peers—are all proprietary. Interestingly, for distributing video chunks among\npeers, Kankan uses UDP whenever possible, leading to massive amounts of UDP\ntraffic within China’s Internet [Zhang M 2010].\n7.3 Voice-over-IP\nReal-time conversational voice over the Internet is often referred to as Internet\ntelephony, since, from the user’s perspective, it is similar to the traditional \ncircuit-switched telephone service. It is also commonly called Voice-over-IP\n(VoIP). In this section we describe the principles and protocols underlying VoIP.\nConversational video is similar in many respects to VoIP, except that it includes\nthe video of the participants as well as their voices. To keep the discussion focused\nand concrete, we focus here only on voice in this section rather than combined\nvoice and video.\n7.3.1 Limitations of the Best-Effort IP Service\nThe Internet’s network-layer protocol, IP, provides best-effort service. That is to say\nthe service makes its best effort to move each datagram from source to destination\nas quickly as possible but makes no promises whatsoever about getting the packet\nto the destination within some delay bound or about a limit on the percentage of\npackets lost. The lack of such guarantees poses significant challenges to the design\nof real-time conversational applications, which are acutely sensitive to packet delay,\njitter, and loss.\nIn this section, we’ll cover several ways in which the performance of \nVoIP over a best-effort network can be enhanced. Our focus will be on applica-\ntion-layer techniques, that is, approaches that do not require any changes in the\nnetwork core or even in the transport layer at the end hosts. To keep the discus-\nsion concrete, we’ll discuss the limitations of best-effort IP service in the context\nof a specific VoIP example. The sender generates bytes at a rate of 8,000 bytes\nper second; every 20 msecs the sender gathers these bytes into a chunk. A chunk\nand a special header (discussed below) are encapsulated in a UDP segment, via a\ncall to the socket interface. Thus, the number of bytes in a chunk is (20 msecs)·\n(8,000 bytes/sec) = 160 bytes, and a UDP segment is sent every 20 msecs.\nIf each packet makes it to the receiver with a constant end-to-end delay, then\npackets arrive at the receiver periodically every 20 msecs. In these ideal conditions,\n612\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nthe receiver can simply play back each chunk as soon as it arrives. But unfortu-\nnately, some packets can be lost and most packets will not have the same end-to-end\ndelay, even in a lightly congested Internet. For this reason, the receiver must take\nmore care in determining (1) when to play back a chunk, and (2) what to do with a\nmissing chunk. \nPacket Loss\nConsider one of the UDP segments generated by our VoIP application. The UDP\nsegment is encapsulated in an IP datagram. As the datagram wanders through the\nnetwork, it passes through router buffers (that is, queues) while waiting for trans-\nmission on outbound links. It is possible that one or more of the buffers in the path\nfrom sender to receiver is full, in which case the arriving IP datagram may be dis-\ncarded, never to arrive at the receiving application.\nLoss could be eliminated by sending the packets over TCP (which provides\nfor reliable data transfer) rather than over UDP. However, retransmission mecha-\nnisms are often considered unacceptable for conversational real-time audio appli-\ncations such as VoIP, because they increase end-to-end delay [Bolot 1996].\nFurthermore, due to TCP congestion control, packet loss may result in a reduc-\ntion of the TCP sender’s transmission rate to a rate that is lower than the\nreceiver’s drain rate, possibly leading to buffer starvation. This can have a severe\nimpact on voice intelligibility at the receiver. For these reasons, most existing\nVoIP applications run over UDP by default. [Baset 2006] reports that UDP is\nused by Skype unless a user is behind a NAT or firewall that blocks UDP\nsegments (in which case TCP is used).\nBut losing packets is not necessarily as disastrous as one might think.\nIndeed, packet loss rates between 1 and 20 percent can be tolerated, depending\non how voice is encoded and transmitted, and on how the loss is concealed at the\nreceiver. For example, forward error correction (FEC) can help conceal packet\nloss. We’ll see below that with FEC, redundant information is transmitted along\nwith the original information so that some of the lost original data can be recov-\nered from the redundant information. Nevertheless, if one or more of the links\nbetween sender and receiver is severely congested, and packet loss exceeds 10 to\n20 percent (for example, on a wireless link), then there is really nothing that can\nbe done to achieve acceptable audio quality. Clearly, best-effort service has its\nlimitations.\nEnd-to-End Delay\nEnd-to-end delay is the accumulation of transmission, processing, and queuing\ndelays in routers; propagation delays in links; and end-system processing delays.\nFor real-time conversational applications, such as VoIP, end-to-end delays smaller\nthan 150 msecs are not perceived by a human listener; delays between 150 and 400\n7.3\n•\nVOICE-OVER-IP\n613"
    },
    {
      "chunk_id": "6b197fda-506d-4ed3-aa2f-3163db71d30b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.3.1 Limitations of the Best-Effort IP Service",
      "original_titles": [
        "7.3.1 Limitations of the Best-Effort IP Service"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.3 Voice-over-IP > 7.3.1 Limitations of the Best-Effort IP Service",
      "start_page": 639,
      "end_page": 640,
      "token_count": 1167,
      "text": "The Kankan design employs a tracker and its own DHT for tracking content.\nSwarm sizes for the most popular content involve tens of thousands of peers, typi-\ncally larger than the largest swarms in BitTorrent [Dhungel 2012]. The Kankan pro-\ntocols—for communication between peer and tracker, between peer and DHT, and\namong peers—are all proprietary. Interestingly, for distributing video chunks among\npeers, Kankan uses UDP whenever possible, leading to massive amounts of UDP\ntraffic within China’s Internet [Zhang M 2010].\n7.3 Voice-over-IP\nReal-time conversational voice over the Internet is often referred to as Internet\ntelephony, since, from the user’s perspective, it is similar to the traditional \ncircuit-switched telephone service. It is also commonly called Voice-over-IP\n(VoIP). In this section we describe the principles and protocols underlying VoIP.\nConversational video is similar in many respects to VoIP, except that it includes\nthe video of the participants as well as their voices. To keep the discussion focused\nand concrete, we focus here only on voice in this section rather than combined\nvoice and video.\n7.3.1 Limitations of the Best-Effort IP Service\nThe Internet’s network-layer protocol, IP, provides best-effort service. That is to say\nthe service makes its best effort to move each datagram from source to destination\nas quickly as possible but makes no promises whatsoever about getting the packet\nto the destination within some delay bound or about a limit on the percentage of\npackets lost. The lack of such guarantees poses significant challenges to the design\nof real-time conversational applications, which are acutely sensitive to packet delay,\njitter, and loss.\nIn this section, we’ll cover several ways in which the performance of \nVoIP over a best-effort network can be enhanced. Our focus will be on applica-\ntion-layer techniques, that is, approaches that do not require any changes in the\nnetwork core or even in the transport layer at the end hosts. To keep the discus-\nsion concrete, we’ll discuss the limitations of best-effort IP service in the context\nof a specific VoIP example. The sender generates bytes at a rate of 8,000 bytes\nper second; every 20 msecs the sender gathers these bytes into a chunk. A chunk\nand a special header (discussed below) are encapsulated in a UDP segment, via a\ncall to the socket interface. Thus, the number of bytes in a chunk is (20 msecs)·\n(8,000 bytes/sec) = 160 bytes, and a UDP segment is sent every 20 msecs.\nIf each packet makes it to the receiver with a constant end-to-end delay, then\npackets arrive at the receiver periodically every 20 msecs. In these ideal conditions,\n612\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nthe receiver can simply play back each chunk as soon as it arrives. But unfortu-\nnately, some packets can be lost and most packets will not have the same end-to-end\ndelay, even in a lightly congested Internet. For this reason, the receiver must take\nmore care in determining (1) when to play back a chunk, and (2) what to do with a\nmissing chunk. \nPacket Loss\nConsider one of the UDP segments generated by our VoIP application. The UDP\nsegment is encapsulated in an IP datagram. As the datagram wanders through the\nnetwork, it passes through router buffers (that is, queues) while waiting for trans-\nmission on outbound links. It is possible that one or more of the buffers in the path\nfrom sender to receiver is full, in which case the arriving IP datagram may be dis-\ncarded, never to arrive at the receiving application.\nLoss could be eliminated by sending the packets over TCP (which provides\nfor reliable data transfer) rather than over UDP. However, retransmission mecha-\nnisms are often considered unacceptable for conversational real-time audio appli-\ncations such as VoIP, because they increase end-to-end delay [Bolot 1996].\nFurthermore, due to TCP congestion control, packet loss may result in a reduc-\ntion of the TCP sender’s transmission rate to a rate that is lower than the\nreceiver’s drain rate, possibly leading to buffer starvation. This can have a severe\nimpact on voice intelligibility at the receiver. For these reasons, most existing\nVoIP applications run over UDP by default. [Baset 2006] reports that UDP is\nused by Skype unless a user is behind a NAT or firewall that blocks UDP\nsegments (in which case TCP is used).\nBut losing packets is not necessarily as disastrous as one might think.\nIndeed, packet loss rates between 1 and 20 percent can be tolerated, depending\non how voice is encoded and transmitted, and on how the loss is concealed at the\nreceiver. For example, forward error correction (FEC) can help conceal packet\nloss. We’ll see below that with FEC, redundant information is transmitted along\nwith the original information so that some of the lost original data can be recov-\nered from the redundant information. Nevertheless, if one or more of the links\nbetween sender and receiver is severely congested, and packet loss exceeds 10 to\n20 percent (for example, on a wireless link), then there is really nothing that can\nbe done to achieve acceptable audio quality. Clearly, best-effort service has its\nlimitations.\nEnd-to-End Delay\nEnd-to-end delay is the accumulation of transmission, processing, and queuing\ndelays in routers; propagation delays in links; and end-system processing delays.\nFor real-time conversational applications, such as VoIP, end-to-end delays smaller\nthan 150 msecs are not perceived by a human listener; delays between 150 and 400\n7.3\n•\nVOICE-OVER-IP\n613"
    },
    {
      "chunk_id": "eb2daf37-733c-47ea-bb76-3ff10df59af9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.3.2 Removing Jitter at the Receiver for Audio",
      "original_titles": [
        "7.3.2 Removing Jitter at the Receiver for Audio"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.3 Voice-over-IP > 7.3.2 Removing Jitter at the Receiver for Audio",
      "start_page": 641,
      "end_page": 643,
      "token_count": 1617,
      "text": "msecs can be acceptable but are not ideal; and delays exceeding 400 msecs can seri-\nously hinder the interactivity in voice conversations. The receiving side of a VoIP\napplication will typically disregard any packets that are delayed more than a certain\nthreshold, for example, more than 400 msecs. Thus, packets that are delayed by\nmore than the threshold are effectively lost.\nPacket Jitter\nA crucial component of end-to-end delay is the varying queuing delays that a packet\nexperiences in the network’s routers. Because of these varying delays, the time from\nwhen a packet is generated at the source until it is received at the receiver can fluctu-\nate from packet to packet, as shown in Figure 7.1. This phenomenon is called jitter.\nAs an example, consider two consecutive packets in our VoIP application. The sender\nsends the second packet 20 msecs after sending the first packet. But at the receiver,\nthe spacing between these packets can become greater than 20 msecs. To see this,\nsuppose the first packet arrives at a nearly empty queue at a router, but just before the\nsecond packet arrives at the queue a large number of packets from other sources\narrive at the same queue. Because the first packet experiences a small queuing delay\nand the second packet suffers a large queuing delay at this router, the first and second\npackets become spaced by more than 20 msecs. The spacing between consecutive\npackets can also become less than 20 msecs. To see this, again consider two consecu-\ntive packets. Suppose the first packet joins the end of a queue with a large number of\npackets, and the second packet arrives at the queue before this first packet is trans-\nmitted and before any packets from other sources arrive at the queue. In this case, our\ntwo packets find themselves one right after the other in the queue. If the time it takes\nto transmit a packet on the router’s outbound link is less than 20 msecs, then the spac-\ning between first and second packets becomes less than 20 msecs.\nThe situation is analogous to driving cars on roads. Suppose you and your\nfriend are each driving in your own cars from San Diego to Phoenix. Suppose \nyou and your friend have similar driving styles, and that you both drive at \n100 km/hour, traffic permitting. If your friend starts out one hour before you,\ndepending on intervening traffic, you may arrive at Phoenix more or less than one\nhour after your friend.\nIf the receiver ignores the presence of jitter and plays out chunks as soon as\nthey arrive, then the resulting audio quality can easily become unintelligible at the\nreceiver. Fortunately, jitter can often be removed by using sequence numbers,\ntimestamps, and a playout delay, as discussed below.\n7.3.2 Removing Jitter at the Receiver for Audio\nFor our VoIP application, where packets are being generated periodically, the\nreceiver should attempt to provide periodic playout of voice chunks in the presence\n614\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nof random network jitter. This is typically done by combining the following two\nmechanisms:\n•\nPrepending each chunk with a timestamp. The sender stamps each chunk with\nthe time at which the chunk was generated.\n•\nDelaying playout of chunks at the receiver. As we saw in our earlier discussion\nof Figure 7.1, the playout delay of the received audio chunks must be long\nenough so that most of the packets are received before their scheduled playout\ntimes. This playout delay can either be fixed throughout the duration of the audio\nsession or vary adaptively during the audio session lifetime.\nWe now discuss how these three mechanisms, when combined, can alleviate or\neven eliminate the effects of jitter. We examine two playback strategies: fixed play-\nout delay and adaptive playout delay.\nFixed Playout Delay\nWith the fixed-delay strategy, the receiver attempts to play out each chunk exactly q\nmsecs after the chunk is generated. So if a chunk is timestamped at the sender at\ntime t, the receiver plays out the chunk at time t + q, assuming the chunk has arrived\nby that time. Packets that arrive after their scheduled playout times are discarded\nand considered lost.\nWhat is a good choice for q? VoIP can support delays up to about 400 msecs,\nalthough a more satisfying conversational experience is achieved with smaller values\nof q. On the other hand, if q is made much smaller than 400 msecs, then many packets\nmay miss their scheduled playback times due to the network-induced packet jitter.\nRoughly speaking, if large variations in end-to-end delay are typical, it is preferable to\nuse a large q; on the other hand, if delay is small and variations in delay are also small,\nit is preferable to use a small q, perhaps less than 150 msecs.\nThe trade-off between the playback delay and packet loss is illustrated in\nFigure 7.7. The figure shows the times at which packets are generated and played\nout for a single talk spurt. Two distinct initial playout delays are considered. As\nshown by the leftmost staircase, the sender generates packets at regular inter-\nvals—say, every 20 msecs. The first packet in this talk spurt is received at time r.\nAs shown in the figure, the arrivals of subsequent packets are not evenly spaced\ndue to the network jitter.\nFor the first playout schedule, the fixed initial playout delay is set to p – r. With\nthis schedule, the fourth packet does not arrive by its scheduled playout time, and\nthe receiver considers it lost. For the second playout schedule, the fixed initial play-\nout delay is set to p\u0002 – r. For this schedule, all packets arrive before their scheduled\nplayout times, and there is therefore no loss.\n7.3\n•\nVOICE-OVER-IP\n615\n\n616\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nPackets\ngenerated\nTime\nPackets\nr p\np'\nPlayout\nschedule\np–r\nPlayout\nschedule\np'–r\nPackets\nreceived\nMissed\nplayout\nFigure 7.7 \u0002 Packet loss for different fixed playout delays\nAdaptive Playout Delay\nThe previous example demonstrates an important delay-loss trade-off that arises\nwhen designing a playout strategy with fixed playout delays. By making the initial\nplayout delay large, most packets will make their deadlines and there will therefore\nbe negligible loss; however, for conversational services such as VoIP, long delays\ncan become bothersome if not intolerable. Ideally, we would like the playout delay\nto be minimized subject to the constraint that the loss be below a few percent.\nThe natural way to deal with this trade-off is to estimate the network delay and\nthe variance of the network delay, and to adjust the playout delay accordingly at the\nbeginning of each talk spurt. This adaptive adjustment of playout delays at the\nbeginning of the talk spurts will cause the sender’s silent periods to be compressed\nand elongated; however, compression and elongation of silence by a small amount\nis not noticeable in speech.\nFollowing [Ramjee 1994], we now describe a generic algorithm that the\nreceiver can use to adaptively adjust its playout delays. To this end, let\nti = the timestamp of the ith packet = the time the packet was generated by the\nsender\nri = the time packet i is received by receiver\npi = the time packet i is played at receiver\nThe end-to-end network delay of the ith packet is ri – ti. Due to network jitter,\nthis delay will vary from packet to packet. Let di denote an estimate of the average"
    },
    {
      "chunk_id": "51f87868-6f91-48f3-b264-9fbe29458532",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.3.3 Recovering from Packet Loss",
      "original_titles": [
        "7.3.3 Recovering from Packet Loss"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.3 Voice-over-IP > 7.3.3 Recovering from Packet Loss",
      "start_page": 644,
      "end_page": 646,
      "token_count": 1751,
      "text": "network delay upon reception of the ith packet. This estimate is constructed from\nthe timestamps as follows:\ndi = (1 – u) di–1 + u (ri – ti)\nwhere u is a fixed constant (for example, u = 0.01). Thus di is a smoothed average\nof the observed network delays r1 – t1, . . . , ri – ti. The estimate places more weight\non the recently observed network delays than on the observed network delays of the\ndistant past. This form of estimate should not be completely unfamiliar; a similar\nidea is used to estimate round-trip times in TCP, as discussed in Chapter 3. Let vi\ndenote an estimate of the average deviation of the delay from the estimated average\ndelay. This estimate is also constructed from the timestamps:\nvi = (1 – u) vi–1 + u | ri – ti – di |\nThe estimates di and vi are calculated for every packet received, although they are\nused only to determine the playout point for the first packet in any talk spurt.\nOnce having calculated these estimates, the receiver employs the following\nalgorithm for the playout of packets. If packet i is the first packet of a talk spurt, its\nplayout time, pi, is computed as:\npi = ti + di + Kvi\nwhere K is a positive constant (for example, K = 4). The purpose of the Kvi term is to\nset the playout time far enough into the future so that only a small fraction of the arriv-\ning packets in the talk spurt will be lost due to late arrivals. The playout point for any\nsubsequent packet in a talk spurt is computed as an offset from the point in time when\nthe first packet in the talk spurt was played out. In particular, let\nqi = pi – ti\nbe the length of time from when the first packet in the talk spurt is generated until it\nis played out. If packet j also belongs to this talk spurt, it is played out at time\npj = tj + qi\nThe algorithm just described makes perfect sense assuming that the receiver can\ntell whether a packet is the first packet in the talk spurt. This can be done by exam-\nining the signal energy in each received packet.\n7.3.3 Recovering from Packet Loss\nWe have discussed in some detail how a VoIP application can deal with packet jitter.\nWe now briefly describe several schemes that attempt to preserve acceptable audio\n7.3\n•\nVOICE-OVER-IP\n617\n\nquality in the presence of packet loss. Such schemes are called loss recovery schemes.\nHere we define packet loss in a broad sense: A packet is lost either if it never arrives at\nthe receiver or if it arrives after its scheduled playout time. Our VoIP example will\nagain serve as a context for describing loss recovery schemes.\nAs mentioned at the beginning of this section, retransmitting lost packets may not\nbe feasible in a real-time conversational application such as VoIP. Indeed, retransmit-\nting a packet that has missed its playout deadline serves absolutely no purpose. And\nretransmitting a packet that overflowed a router queue cannot normally be accom-\nplished quickly enough. Because of these considerations, VoIP applications often use\nsome type of loss anticipation scheme. Two types of loss anticipation schemes are\nforward error correction (FEC) and interleaving.\nForward Error Correction (FEC)\nThe basic idea of FEC is to add redundant information to the original packet stream.\nFor the cost of marginally increasing the transmission rate, the redundant information\ncan be used to reconstruct approximations or exact versions of some of the lost packets.\nFollowing [Bolot 1996] and [Perkins 1998], we now outline two simple FEC mecha-\nnisms. The first mechanism sends a redundant encoded chunk after every n chunks. The\nredundant chunk is obtained by exclusive OR-ing the n original chunks [Shacham\n1990]. In this manner if any one packet of the group of n + 1 packets is lost, the receiver\ncan fully reconstruct the lost packet. But if two or more packets in a group are lost, the\nreceiver cannot reconstruct the lost packets. By keeping n + 1, the group size, small, a\nlarge fraction of the lost packets can be recovered when loss is not excessive. However,\nthe smaller the group size, the greater the relative increase of the transmission rate. In\nparticular, the transmission rate increases by a factor of 1/n, so that, if n = 3, then the\ntransmission rate increases by 33 percent. Furthermore, this simple scheme increases\nthe playout delay, as the receiver must wait to receive the entire group of packets before\nit can begin playout. For more practical details about how FEC works for multimedia\ntransport see [RFC 5109].\nThe second FEC mechanism is to send a lower-resolution audio stream as the\nredundant information. For example, the sender might create a nominal audio\nstream and a corresponding low-resolution, low-bit rate audio stream. (The nominal\nstream could be a PCM encoding at 64 kbps, and the lower-quality stream could be\na GSM encoding at 13 kbps.) The low-bit rate stream is referred to as the redundant\nstream. As shown in Figure 7.8, the sender constructs the nth packet by taking the\nnth chunk from the nominal stream and appending to it the (n – 1)st chunk from the\nredundant stream. In this manner, whenever there is nonconsecutive packet loss, the\nreceiver can conceal the loss by playing out the low-bit rate encoded chunk that\narrives with the subsequent packet. Of course, low-bit rate chunks give lower qual-\nity than the nominal chunks. However, a stream of mostly high-quality chunks,\noccasional low-quality chunks, and no missing chunks gives good overall audio\nquality. Note that in this scheme, the receiver only has to receive two packets before\nplayback, so that the increased playout delay is small. Furthermore, if the low-bit\n618\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nrate encoding is much less than the nominal encoding, then the marginal increase in\nthe transmission rate will be small.\nIn order to cope with consecutive loss, we can use a simple variation. Instead of\nappending just the (n – 1)st low-bit rate chunk to the nth nominal chunk, the sender\ncan append the (n – 1)st and (n – 2)nd low-bit rate chunk, or append the (n – 1)st\nand (n – 3)rd low-bit rate chunk, and so on. By appending more low-bit rate chunks\nto each nominal chunk, the audio quality at the receiver becomes acceptable for a\nwider variety of harsh best-effort environments. On the other hand, the additional\nchunks increase the transmission bandwidth and the playout delay.\nInterleaving\nAs an alternative to redundant transmission, a VoIP application can send interleaved\naudio. As shown in Figure 7.9, the sender resequences units of audio data before trans-\nmission, so that originally adjacent units are separated by a certain distance in the trans-\nmitted stream. Interleaving can mitigate the effect of packet losses. If, for example,\nunits are 5 msecs in length and chunks are 20 msecs (that is, four units per chunk), then\nthe first chunk could contain units 1, 5, 9, and 13; the second chunk could contain units\n2, 6, 10, and 14; and so on. Figure 7.9 shows that the loss of a single packet from an\ninterleaved stream results in multiple small gaps in the reconstructed stream, as\nopposed to the single large gap that would occur in a noninterleaved stream.\nInterleaving can significantly improve the perceived quality of an audio stream\n[Perkins 1998]. It also has low overhead. The obvious disadvantage of interleaving is\nthat it increases latency. This limits its use for conversational applications such as VoIP,\nalthough it can perform well for streaming stored audio. A major advantage of inter-\nleaving is that it does not increase the bandwidth requirements of a stream.\n1\n1\n1\n1\n1\n2\n2\n2\n2\n3\n3\nloss\n3\n4\n3\n4\n1\n2\n3\n4\n4\nRedundancy\nReceived\nstream\nOriginal\nstream\nReconstructed\nstream\nFigure 7.8 \u0002 Piggybacking lower-quality redundant information\n7.3\n•\nVOICE-OVER-IP\n619"
    },
    {
      "chunk_id": "b06df84c-a799-4df5-9404-0ff1458d2a86",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.3.4 Case Study: VoIP with Skype",
      "original_titles": [
        "7.3.4 Case Study: VoIP with Skype"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.3 Voice-over-IP > 7.3.4 Case Study: VoIP with Skype",
      "start_page": 647,
      "end_page": 649,
      "token_count": 1466,
      "text": "Error Concealment\nError concealment schemes attempt to produce a replacement for a lost packet that\nis similar to the original. As discussed in [Perkins 1998], this is possible since audio\nsignals, and in particular speech, exhibit large amounts of short-term self-similarity.\nAs such, these techniques work for relatively small loss rates (less than 15 percent),\nand for small packets (4–40 msecs). When the loss length approaches the length of a\nphoneme (5–100 msecs) these techniques break down, since whole phonemes may\nbe missed by the listener.\nPerhaps the simplest form of receiver-based recovery is packet repetition.\nPacket repetition replaces lost packets with copies of the packets that arrived\nimmediately before the loss. It has low computational complexity and performs\nreasonably well. Another form of receiver-based recovery is interpolation, which\nuses audio before and after the loss to interpolate a suitable packet to cover the\nloss. Interpolation performs somewhat better than packet repetition but is signifi-\ncantly more computationally intensive [Perkins 1998].\n7.3.4 Case Study: VoIP with Skype\nSkype is an immensely popular VoIP application with over 50 million accounts\nactive on a daily basis. In addition to providing host-to-host VoIP service, Skype\noffers host-to-phone services, phone-to-host services, and multi-party host-to-host\n620\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nOriginal\nstream\nInterleaved\nstream\nReceived\nstream\nReconstructed\nstream\n5\n9\n13\n1\n2\n4\n1\n2\n3\n4\n1\n5\n9\n13\n1\n2\n10 14\n6\n5\n8\n6\n5\n7\n8\n6\n2\n10 14\nloss\n6\n7\n11 15\n3\n10\n12\n9\n10 11 12\n9\n4\n12\n16\n8\n13\n16\n14\n13\n15\n16\n14\n4\n12\n16\n8\nFigure 7.9 \u0002 Sending interleaved audio\n\nvideo conferencing services. (Here, a host is again any Internet connected IP device,\nincluding PCs, tablets, and smartphones.) Skype was acquired by Microsoft in 2011\nfor over $8 billion.\nBecause the Skype protocol is proprietary, and because all Skype’s control and\nmedia packets are encrypted, it is difficult to precisely determine how Skype operates.\nNevertheless, from the Skype Web site and several measurement studies, researchers\nhave learned how Skype generally works [Baset 2006; Guha 2006; Chen 2006; Suh\n2006; Ren 2006; Zhang X 2012]. For both voice and video, the Skype clients have at\ntheir disposal many different codecs, which are capable of encoding the media at a wide\nrange of rates and qualities. For example, video rates for Skype have been measured to\nbe as low as 30 kbps for a low-quality session up to almost  1 Mbps for a high quality\nsession [Zhang X 2012]. Typically, Skype’s audio quality is better than the “POTS”\n(Plain Old Telephone Service) quality provided by the wire-line phone system. (Skype\ncodecs typically sample voice at 16,000 samples/sec or higher, which provides richer\ntones than POTS, which samples at 8,000/sec.) By default, Skype sends audio and\nvideo packets over UDP. However, control packets are sent over TCP, and media pack-\nets are also sent over TCP when firewalls block UDP streams. Skype uses FEC for loss\nrecovery for both voice and video streams sent over UDP. The Skype client also adapts\nthe audio and video streams it sends to current network conditions, by changing video\nquality and FEC overhead [Zhang X 2012].\nSkype uses P2P techniques in a number of innovative ways, nicely illustrating\nhow P2P can be used in applications that go beyond content distribution and file\nsharing. As with instant messaging, host-to-host Internet telephony is inherently P2P\nsince, at the heart of the application, pairs of users (that is, peers) communicate with\neach other in real time. But Skype also employs P2P techniques for two other impor-\ntant functions, namely, for user location and for NAT traversal.\nAs shown in Figure 7.10, the peers (hosts) in Skype are organized into a hierar-\nchical overlay network, with each peer classified as a super peer or an ordinary peer.\nSkype maintains an index that maps Skype usernames to current IP addresses (and\nport numbers). This index is distributed over the super peers. When Alice wants to\ncall Bob, her Skype client searches the distributed index to determine Bob’s current\nIP address. Because the Skype protocol is proprietary, it is currently not known how\nthe index mappings are organized across the super peers, although some form of\nDHT organization is very possible.\nP2P techniques are also used in Skype relays, which are useful for establish-\ning calls between hosts in home networks. Many home network configurations\nprovide access to the Internet through NATs, as discussed in Chapter 4. Recall that\na NAT prevents a host from outside the home network from initiating a connec-\ntion to a host within the home network. If both Skype callers have NATs, then\nthere is a problem—neither can accept a call initiated by the other, making a call\nseemingly impossible. The clever use of super peers and relays nicely solves this\nproblem. Suppose that when Alice signs in, she is assigned to a non-NATed super\npeer and initiates a session to that super peer. (Since Alice is initiating the session,\nher NAT permits this session.) This session allows Alice and her super peer to\n7.3\n•\nVOICE-OVER-IP\n621\n\nexchange control messages. The same happens for Bob when he signs in. Now,\nwhen Alice wants to call Bob, she informs her super peer, who in turn informs\nBob’s super peer, who in turn informs Bob of Alice’s incoming call. If Bob\naccepts the call, the two super peers select a third non-NATed super peer—the\nrelay peer—whose job will be to relay data between Alice and Bob. Alice’s and\nBob’s super peers then instruct Alice and Bob respectively to initiate a session\nwith the relay. As shown in Figure 7.10, Alice then sends voice packets to the\nrelay over the Alice-to-relay connection (which was initiated by Alice), and the\nrelay then forwards these packets over the relay-to-Bob connection (which was\ninitiated by Bob); packets from Bob to Alice flow over these same two relay con-\nnections in reverse. And voila!—Bob and Alice have an end-to-end connection\neven though neither can accept a session originating from outside.\nUp to now, our discussion on Skype has focused on calls involving two persons.\nNow let’s examine multi-party audio conference calls. With N > 2 participants, if each\nuser were to send  a copy of its audio stream to each of the N\n1 other users, then a\ntotal of N(N\n1) audio streams would need to be sent into the network to support the\naudio conference. To reduce this bandwidth usage, Skype employs a clever distribution\n-\n-\n622\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nCallee\npeer\nCaller\npeer\nRelay\npeer\nSuper\npeer\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nSkype\nFigure 7.10 \u0002 Skype peers"
    },
    {
      "chunk_id": "45952407-81a0-4111-8bd3-3689cfd236bb",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.4 Protocols for Real-Time Conversational Applications",
      "original_titles": [
        "7.4 Protocols for Real-Time Conversational Applications"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.4 Protocols for Real-Time Conversational Applications",
      "start_page": 650,
      "end_page": 650,
      "token_count": 674,
      "text": "technique. Specifically, each user sends its audio stream to the conference initiator. The\nconference initiator combines the audio streams into one stream (basically by adding\nall the audio signals together) and then sends a copy of each combined stream to each\nof the other N\n1 participants. In this manner, the number of streams is reduced to \n2(N\n1). For ordinary two-person video conversations, Skype routes the call peer-to-\npeer, unless NAT traversal is required, in which case the call is relayed through a non-\nNATed peer, as described earlier. For a video conference call involving N > 2\nparticipants, due to the nature of the video medium, Skype does not combine the call\ninto one stream at one location and then redistribute the stream to all the participants,\nas it does for voice calls. Instead, each participant's video stream is routed to a server\ncluster (located in Estonia as of 2011), which in turn relays to each participant the \nN\n1 streams of the N\n1 other participants [Zhang X 2012]. You may be wonder-\ning why each participant sends a copy to a server rather than directly sending a copy of\nits video stream to each of the other N\n1 participants? Indeed, for both approaches,\nN(N\n1) video streams are being collectively received by the N participants in the\nconference.  The reason is, because  upstream link bandwidths are  significantly lower\nthan downstream link bandwidths in most access links, the upstream links may not be\nable to support the N\n1 streams with the P2P approach. \nVoIP systems such as Skype, QQ, and Google Talk introduce new privacy \nconcerns. Specifically, when Alice and Bob communicate over VoIP, Alice can sniff\nBob’s IP address and then use geo-location services [MaxMind 2012; Quova 2012] \nto determine Bob’s current location and ISP (for example, his work or home ISP). In\nfact, with Skype it is possible for Alice to block the transmission of certain packets \nduring call establishment so that she obtains Bob’s current IP address, say every hour,\nwithout Bob knowing that he is being tracked and without being on Bob’s contact\nlist. Furthermore, the IP address discovered from Skype can be correlated with IP\naddresses found in BitTorrent, so that Alice can determine the files that Bob is down-\nloading [LeBlond 2011]. Moreover, it is possible to partially decrypt a Skype call by\ndoing a traffic analysis of the packet sizes in a stream [White 2011].\n7.4 Protocols for Real-Time Conversational\nApplications\nReal-time conversational applications, including VoIP and video conferencing, are\ncompelling and very popular. It is therefore not surprising that standards bodies,\nsuch as the IETF and ITU, have been busy for many years (and continue to be\nbusy!) at hammering out standards for this class of applications. With the appropri-\nate standards in place for real-time conversational applications, independent compa-\nnies are creating new products that interoperate with each other. In this section we\nexamine RTP and SIP for real-time conversational applications. Both standards are\nenjoying widespread implementation in industry products.\n-\n-\n-\n-\n-\n-\n-\n7.4\n•\nPROTOCOLS FOR REAL-TIME CONVERSATIONAL APPLICATIONS\n623"
    },
    {
      "chunk_id": "c8017ecb-50c6-40d2-ad3a-9b354cb2749a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.4.1 RTP",
      "original_titles": [
        "7.4.1 RTP"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.4 Protocols for Real-Time Conversational Applications > 7.4.1 RTP",
      "start_page": 651,
      "end_page": 653,
      "token_count": 1621,
      "text": "7.4.1 RTP\nIn the previous section, we learned that the sender side of a VoIP application appends\nheader fields to the audio chunks before passing them to the transport layer. These\nheader fields include sequence numbers and timestamps. Since most multimedia\nnetworking applications can make use of sequence numbers and timestamps, it is con-\nvenient to have a standardized packet structure that includes fields for audio/video data,\nsequence number, and timestamp, as well as other potentially useful fields. RTP,\ndefined in RFC 3550, is such a standard. RTP can be used for transporting common\nformats such as PCM, ACC, and MP3 for sound and MPEG and H.263 for video. It can\nalso be used for transporting proprietary sound and video formats. Today, RTP enjoys\nwidespread implementation in many products and research prototypes. It is also com-\nplementary to other important real-time interactive protocols, such as SIP.\nIn this section, we provide an introduction to RTP. We also encourage you to\nvisit Henning Schulzrinne’s RTP site [Schulzrinne-RTP 2012], which provides a\nwealth of information on the subject. Also, you may want to visit the RAT site\n[RAT 2012], which documents VoIP application that uses RTP.\nRTP Basics\nRTP typically runs on top of UDP. The sending side encapsulates a media chunk\nwithin an RTP packet, then encapsulates the packet in a UDP segment, and then\nhands the segment to IP. The receiving side extracts the RTP packet from the UDP\nsegment, then extracts the media chunk from the RTP packet, and then passes the\nchunk to the media player for decoding and rendering.\nAs an example, consider the use of RTP to transport voice. Suppose the voice\nsource is PCM-encoded (that is, sampled, quantized, and digitized) at 64 kbps. Fur-\nther suppose that the application collects the encoded data in 20-msec chunks, that is,\n160 bytes in a chunk. The sending side precedes each chunk of the audio data with\nan RTP header that includes the type of audio encoding, a sequence number, and a\ntimestamp. The RTP header is normally 12 bytes. The audio chunk along with the\nRTP header form the RTP packet. The RTP packet is then sent into the UDP socket\ninterface. At the receiver side, the application receives the RTP packet from its socket\ninterface. The application extracts the audio chunk from the RTP packet and uses the\nheader fields of the RTP packet to properly decode and play back the audio chunk.\nIf an application incorporates RTP—instead of a proprietary scheme to provide\npayload type, sequence numbers, or timestamps—then the application will more easily\ninteroperate with other networked multimedia applications. For example, if two differ-\nent companies develop VoIP software and they both incorporate RTP into their product,\nthere may be some hope that a user using one of the VoIP products will be able to com-\nmunicate with a user using the other VoIP product. In Section 7.4.2, we’ll see that RTP\nis often used in conjunction with SIP, an important standard for Internet telephony.\nIt should be emphasized that RTP does not provide any mechanism to ensure\ntimely delivery of data or provide other quality-of-service (QoS) guarantees; it\n624\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\ndoes not even guarantee delivery of packets or prevent out-of-order delivery of\npackets. Indeed, RTP encapsulation is seen only at the end systems. Routers do\nnot distinguish between IP datagrams that carry RTP packets and IP datagrams\nthat don’t.\nRTP allows each source (for example, a camera or a microphone) to be assigned\nits own independent RTP stream of packets. For example, for a video conference\nbetween two participants, four RTP streams could be opened—two streams for\ntransmitting the audio (one in each direction) and two streams for transmitting the\nvideo (again, one in each direction). However, many popular encoding techniques—\nincluding MPEG 1 and MPEG 2—bundle the audio and video into a single stream\nduring the encoding process. When the audio and video are bundled by the encoder,\nthen only one RTP stream is generated in each direction.\nRTP packets are not limited to unicast applications. They can also be sent over\none-to-many and many-to-many multicast trees. For a many-to-many multicast\nsession, all of the session’s senders and sources typically use the same multicast\ngroup for sending their RTP streams. RTP multicast streams belonging together,\nsuch as audio and video streams emanating from multiple senders in a video confer-\nence application, belong to an RTP session.\nRTP Packet Header Fields\nAs shown in Figure 7.11, the four main RTP packet header fields are the payload\ntype, sequence number, timestamp, and source identifier fields.\nThe payload type field in the RTP packet is 7 bits long. For an audio stream, the\npayload type field is used to indicate the type of audio encoding (for example, PCM,\nadaptive delta modulation, linear predictive encoding) that is being used. If a sender\ndecides to change the encoding in the middle of a session, the sender can inform the\nreceiver of the change through this payload type field. The sender may want to change\nthe encoding in order to increase the audio quality or to decrease the RTP stream bit\nrate. Table 7.2 lists some of the audio payload types currently supported by RTP.\nFor a video stream, the payload type is used to indicate the type of video encoding\n(for example, motion JPEG, MPEG 1, MPEG 2, H.261). Again, the sender can change\nvideo encoding on the fly during a session. Table 7.3 lists some of the video payload\ntypes currently supported by RTP. The other important fields are the following:\n•\nSequence number field. The sequence number field is 16 bits long. The sequence\nnumber increments by one for each RTP packet sent, and may be used by the\nPayload\ntype\nSequence\nnumber\nSynchronization\nsource identifier\nMiscellaneous\nfields\nTimestamp\nFigure 7.11 \u0002 RTP header fields\n7.4\n•\nPROTOCOLS FOR REAL-TIME CONVERSATIONAL APPLICATIONS\n625\n\nreceiver to detect packet loss and to restore packet sequence. For example, if the\nreceiver side of the application receives a stream of RTP packets with a gap\nbetween sequence numbers 86 and 89, then the receiver knows that packets 87\nand 88 are missing. The receiver can then attempt to conceal the lost data.\n•\nTimestamp field. The timestamp field is 32 bits long. It reflects the sampling\ninstant of the first byte in the RTP data packet. As we saw in the preceding\nsection, the receiver can use timestamps to remove packet jitter introduced in\nthe network and to provide synchronous playout at the receiver. The time-\nstamp is derived from a sampling clock at the sender. As an example, for\naudio the timestamp clock increments by one for each sampling period (for\nexample, each 125 \u0003sec for an 8 kHz sampling clock); if the audio applica-\ntion generates chunks consisting of 160 encoded samples, then the timestamp\nincreases by 160 for each RTP packet when the source is active. The \n626\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nPayload-Type Number\nAudio Format\nSampling Rate\nRate\n0\nPCM \u0003-law\n8 kHz\n64 kbps\n1\n1016\n8 kHz\n4.8 kbps\n3\nGSM\n8 kHz\n13 kbps\n7\nLPC\n8 kHz\n2.4 kbps\n9\nG.722\n16 kHz\n48–64 kbps\n14\nMPEG Audio\n90 kHz\n—\n15\nG.728\n8 kHz\n16 kbps\nTable 7.2 \u0002 Audio payload types supported by RTP\nPayload-Type Number\nVideo Format\n26\nMotion JPEG\n31\nH.261\n32\nMPEG 1 video\n33\nMPEG 2 video\nTable 7.3 \u0002 Some video payload types supported by RTP"
    },
    {
      "chunk_id": "abf0286c-7ff9-41aa-8890-d0a9190f1813",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.4.2 SIP",
      "original_titles": [
        "7.4.2 SIP"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.4 Protocols for Real-Time Conversational Applications > 7.4.2 SIP",
      "start_page": 654,
      "end_page": 658,
      "token_count": 2602,
      "text": "timestamp clock continues to increase at a constant rate even if the source is\ninactive.\n•\nSynchronization source identifier (SSRC). The SSRC field is 32 bits long. It iden-\ntifies the source of the RTP stream. Typically, each stream in an RTP session has\na distinct SSRC. The SSRC is not the IP address of the sender, but instead is a\nnumber that the source assigns randomly when the new stream is started. The\nprobability that two streams get assigned the same SSRC is very small. Should\nthis happen, the two sources pick a new SSRC value.\n7.4.2 SIP\nThe Session Initiation Protocol (SIP), defined in [RFC 3261; RFC 5411], is an\nopen and lightweight protocol that does the following:\n•\nIt provides mechanisms for establishing calls between a caller and a callee over\nan IP network. It allows the caller to notify the callee that it wants to start a call.\nIt allows the participants to agree on media encodings. It also allows participants\nto end calls.\n•\nIt provides mechanisms for the caller to determine the current IP address of the\ncallee. Users do not have a single, fixed IP address because they may be assigned\naddresses dynamically (using DHCP) and because they may have multiple IP\ndevices, each with a different IP address.\n•\nIt provides mechanisms for call management, such as adding new media streams\nduring the call, changing the encoding during the call, inviting new participants\nduring the call, call transfer, and call holding.\nSetting Up a Call to a Known IP Address\nTo understand the essence of SIP, it is best to take a look at a concrete example. In\nthis example, Alice is at her PC and she wants to call Bob, who is also working at\nhis PC. Alice’s and Bob’s PCs are both equipped with SIP-based software for mak-\ning and receiving phone calls. In this initial example, we’ll assume that Alice knows\nthe IP address of Bob’s PC. Figure 7.12 illustrates the SIP call-establishment\nprocess.\nIn Figure 7.12, we see that an SIP session begins when Alice sends Bob an\nINVITE message, which resembles an HTTP request message. This INVITE mes-\nsage is sent over UDP to the well-known port 5060 for SIP. (SIP messages can also\nbe sent over TCP.) The INVITE message includes an identifier for Bob\n(bob@193.64.210.89), an indication of Alice’s current IP address, an indication that\nAlice desires to receive audio, which is to be encoded in format AVP 0 (PCM\nencoded \u0003-law) and encapsulated in RTP, and an indication that she wants to receive\n7.4\n•\nPROTOCOLS FOR REAL-TIME CONVERSATIONAL APPLICATIONS\n627\n\nthe RTP packets on port 38060. After receiving Alice’s INVITE message, Bob sends\nan SIP response message, which resembles an HTTP response message. This\nresponse SIP message is also sent to the SIP port 5060. Bob’s response includes a\n200 OK as well as an indication of his IP address, his desired encoding and packeti-\nzation for reception, and his port number to which the audio packets should be sent.\nNote that in this example Alice and Bob are going to use different audio-encoding\nmechanisms: Alice is asked to encode her audio with GSM whereas Bob is asked to\nencode his audio with PCM \u0003-law. After receiving Bob’s response, Alice sends Bob\nan SIP acknowledgment message. After this SIP transaction, Bob and Alice can talk.\n(For visual convenience, Figure 7.12 shows Alice talking after Bob, but in truth they\n628\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nTime\nTime\n167.180.112.24\nINVITE bob@193.64.210.89\nc=IN IP4 167.180.112.24\nm=audio 38060 RTP/AVP 0\n200 OK\nc=In IP4 193.64.210.89\nm=audio 48753 RTP/AVP 3\nBob’s\nterminal rings\n193.64.210.89\nμ Law audio\nport 5060\nport 5060\nport 38060\nAlice\nBob\nport 5060\nport 48753\nACK\nGSM\nFigure 7.12 \u0002 SIP call establishment when Alice knows Bob’s IP address\n\nwould normally talk at the same time.) Bob will encode and packetize the audio as\nrequested and send the audio packets to port number 38060 at IP address\n167.180.112.24. Alice will also encode and packetize the audio as requested and\nsend the audio packets to port number 48753 at IP address 193.64.210.89.\nFrom this simple example, we have learned a number of key characteristics\nof SIP. First, SIP is an out-of-band protocol: The SIP messages are sent and\nreceived in sockets that are different from those used for sending and receiving\nthe media data. Second, the SIP messages themselves are ASCII-readable and\nresemble HTTP messages. Third, SIP requires all messages to be acknowledged,\nso it can run over UDP or TCP.\nIn this example, let’s consider what would happen if Bob does not have a\nPCM \u0003-law codec for encoding audio. In this case, instead of responding with 200\nOK, Bob would likely respond with a 600 Not Acceptable and list in the message\nall the codecs he can use. Alice would then choose one of the listed codecs and\nsend another INVITE message, this time advertising the chosen codec. Bob could\nalso simply reject the call by sending one of many possible rejection reply codes.\n(There are many such codes, including “busy,” “gone,” “payment required,” and\n“forbidden.”)\nSIP Addresses\nIn the previous example, Bob’s SIP address is sip:bob@193.64.210.89. However,\nwe expect many—if not most—SIP addresses to resemble e-mail addresses. For\nexample, Bob’s address might be sip:bob@domain.com. When Alice’s SIP device\nsends an INVITE message, the message would include this e-mail-like address;\nthe SIP infrastructure would then route the message to the IP device that Bob is\ncurrently using (as we’ll discuss below). Other possible forms for the SIP address\ncould be Bob’s legacy phone number or simply Bob’s first/middle/last name\n(assuming it is unique).\nAn interesting feature of SIP addresses is that they can be included in Web\npages, just as people’s e-mail addresses are included in Web pages with the mailto\nURL. For example, suppose Bob has a personal homepage, and he wants to pro-\nvide a means for visitors to the homepage to call him. He could then simply\ninclude the URL sip:bob@domain.com. When the visitor clicks on the URL, the\nSIP application in the visitor’s device is launched and an INVITE message is sent\nto Bob.\nSIP Messages\nIn this short introduction to SIP, we’ll not cover all SIP message types and headers.\nInstead, we’ll take a brief look at the SIP INVITE message, along with a few com-\nmon header lines. Let us again suppose that Alice wants to initiate a VoIP call to\nBob, and this time Alice knows only Bob’s SIP address, bob@domain.com, and\n7.4\n•\nPROTOCOLS FOR REAL-TIME CONVERSATIONAL APPLICATIONS\n629\n\ndoes not know the IP address of the device that Bob is currently using. Then her\nmessage might look something like this:\nINVITE sip:bob@domain.com SIP/2.0\nVia: SIP/2.0/UDP 167.180.112.24\nFrom: sip:alice@hereway.com\nTo: sip:bob@domain.com\nCall-ID: a2e3a@pigeon.hereway.com\nContent-Type: application/sdp\nContent-Length: 885\nc=IN IP4 167.180.112.24\nm=audio 38060 RTP/AVP 0\nThe INVITE line includes the SIP version, as does an HTTP request message.\nWhenever an SIP message passes through an SIP device (including the device that orig-\ninates the message), it attaches a Via header, which indicates the IP address of the\ndevice. (We’ll see soon that the typical INVITE message passes through many SIP\ndevices before reaching the callee’s SIP application.) Similar to an e-mail message, the\nSIP message includes a From header line and a To header line. The message includes a\nCall-ID, which uniquely identifies the call (similar to the message-ID in e-mail). It\nincludes a Content-Type header line, which defines the format used to describe the con-\ntent contained in the SIP message. It also includes a Content-Length header line, which\nprovides the length in bytes of the content in the message. Finally, after a carriage return\nand line feed, the message contains the content. In this case, the content provides infor-\nmation about Alice’s IP address and how Alice wants to receive the audio.\nName Translation and User Location\nIn the example in Figure 7.12, we assumed that Alice’s SIP device knew the IP\naddress where Bob could be contacted. But this assumption is quite unrealistic, not\nonly because IP addresses are often dynamically assigned with DHCP, but also\nbecause Bob may have multiple IP devices (for example, different devices for his\nhome, work, and car). So now let us suppose that Alice knows only Bob’s e-mail\naddress, bob@domain.com, and that this same address is used for SIP-based calls.\nIn this case, Alice needs to obtain the IP address of the device that the user\nbob@domain.com is currently using. To find this out, Alice creates an INVITE mes-\nsage that begins with INVITE bob@domain.com SIP/2.0 and sends this message to\nan SIP proxy. The proxy will respond with an SIP reply that might include the IP\naddress of the device that bob@domain.com is currently using. Alternatively, the\nreply might include the IP address of Bob’s voicemail box, or it might include a\nURL of a Web page (that says “Bob is sleeping. Leave me alone!”). Also, the result\nreturned by the proxy might depend on the caller: If the call is from Bob’s wife, he\n630\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nmight accept the call and supply his IP address; if the call is from Bob’s mother-in-\nlaw, he might respond with the URL that points to the I-am-sleeping Web page!\nNow, you are probably wondering, how can the proxy server determine the cur-\nrent IP address for bob@domain.com? To answer this question, we need to say a few\nwords about another SIP device, the SIP registrar. Every SIP user has an associated\nregistrar. Whenever a user launches an SIP application on a device, the application\nsends an SIP register message to the registrar, informing the registrar of its current\nIP address. For example, when Bob launches his SIP application on his PDA, the\napplication would send a message along the lines of:\nREGISTER sip:domain.com SIP/2.0\nVia: SIP/2.0/UDP 193.64.210.89\nFrom: sip:bob@domain.com\nTo: sip:bob@domain.com\nExpires: 3600\nBob’s registrar keeps track of Bob’s current IP address. Whenever Bob switches\nto a new SIP device, the new device sends a new register message, indicating the\nnew IP address. Also, if Bob remains at the same device for an extended period of\ntime, the device will send refresh register messages, indicating that the most\nrecently sent IP address is still valid. (In the example above, refresh messages need\nto be sent every 3600 seconds to maintain the address at the registrar server.) It is\nworth noting that the registrar is analogous to a DNS authoritative name server: The\nDNS server translates fixed host names to fixed IP addresses; the SIP registrar trans-\nlates fixed human identifiers (for example, bob@domain.com) to dynamic IP\naddresses. Often SIP registrars and SIP proxies are run on the same host.\nNow let’s examine how Alice’s SIP proxy server obtains Bob’s current IP\naddress. From the preceding discussion we see that the proxy server simply needs to\nforward Alice’s INVITE message to Bob’s registrar/proxy. The registrar/proxy\ncould then forward the message to Bob’s current SIP device. Finally, Bob, having\nnow received Alice’s INVITE message, could send an SIP response to Alice.\nAs an example, consider Figure 7.13, in which jim@umass.edu, currently\nworking on 217.123.56.89, wants to initiate a Voice-over-IP (VoIP) session with\nkeith@upenn.edu, currently working on 197.87.54.21. The following steps are\ntaken: (1) Jim sends an INVITE message to the umass SIP proxy. (2) The proxy\ndoes a DNS lookup on the SIP registrar upenn.edu (not shown in diagram) and then\nforwards the message to the registrar server. (3) Because keith@upenn.edu is no\nlonger registered at the upenn registrar, the upenn registrar sends a redirect response,\nindicating that it should try keith@eurecom.fr. (4) The umass proxy sends an\nINVITE message to the eurecom SIP registrar. (5) The eurecom registrar knows the\nIP address of keith@eurecom.fr and forwards the INVITE message to the host\n197.87.54.21, which is running Keith’s SIP client. (6–8) An SIP response is sent back\nthrough registrars/proxies to the SIP client on 217.123.56.89. (9) Media is sent\n7.4\n•\nPROTOCOLS FOR REAL-TIME CONVERSATIONAL APPLICATIONS\n631"
    },
    {
      "chunk_id": "21359975-1199-486a-b9c9-284fd5ed989d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.5 Network Support for Multimedia",
      "original_titles": [
        "7.5 Network Support for Multimedia"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.5 Network Support for Multimedia",
      "start_page": 659,
      "end_page": 660,
      "token_count": 629,
      "text": "directly between the two clients. (There is also an SIP acknowledgment message,\nwhich is not shown.)\nOur discussion of SIP has focused on call initiation for voice calls. SIP, being a\nsignaling protocol for initiating and ending calls in general, can be used for video\nconference calls as well as for text-based sessions. In fact, SIP has become a funda-\nmental component in many instant messaging applications. Readers desiring to\nlearn more about SIP are encouraged to visit Henning Schulzrinne’s SIP Web site\n[Schulzrinne-SIP 2012]. In particular, on this site you will find open source software\nfor SIP clients and servers [SIP Software 2012].\n7.5 Network Support for Multimedia\nIn Sections 7.2 through 7.4, we learned how application-level mechanisms such as\nclient buffering, prefetching, adapting media quality to available bandwidth, adap-\ntive playout, and loss mitigation techniques can be used by multimedia applications\n632\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n9\n5\n6\n4\n7\n2\n3\n1\n8\nSIP registrar\nupenn.edu\nSIP proxy\numass.edu\nSIP client\n217.123.56.89\nSIP client\n197.87.54.21\nSIP registrar\neurcom.fr\nFigure 7.13 \u0002 Session initiation, involving SIP proxies and registrars\n\nto improve a multimedia application’s performance. We also learned how content\ndistribution networks and P2P overlay networks can be used to provide a system-\nlevel approach for delivering multimedia content. These techniques and approaches\nare all designed to be used in today’s best-effort Internet. Indeed, they are in use\ntoday precisely because the Internet provides only a single, best-effort class of serv-\nice. But as designers of computer networks, we can’t help but ask whether the\nnetwork (rather than the applications or application-level infrastructure alone) might\nprovide mechanisms to support multimedia content delivery. As we’ll see shortly,\nthe answer is, of course, “yes”! But we’ll also see that a number of these new net-\nwork-level mechanisms have yet to be widely deployed. This may be due to their\ncomplexity and to the fact that application-level techniques together with best-effort\nservice and properly dimensioned network resources (for example, bandwidth) can\nindeed provide a “good-enough” (even if not-always-perfect) end-to-end multimedia\ndelivery service.\nTable 7.4 summarizes three broad approaches towards providing network-level\nsupport for multimedia applications.\n•\nMaking the best of best-effort service.\nThe application-level mechanisms and\ninfrastructure that we studied in Sections 7.2 through 7.4 can be successfully\nused in a well-dimensioned network where packet loss and excessive end-to-end\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n633\nApproach\nGranularity\nGuarantee\nMechanisms\nComplexity\nDeployment to date\nMaking the\nall traffic\nnone, or\napplication-\nminimal\neverywhere\nbest of best-\ntreated\nsoft\nlayer support,\neffort service.\nequally\nCDNs, overlays,\nnetwork-level\nresource\nprovisioning\nDifferentiated\ndifferent\nnone,\npacket marking,\nmedium\nsome\nservice\nclasses of\nor soft\npolicing,\ntraffic\nscheduling\ntreated\ndifferently\nPer-connection \neach\nsoft or hard,\npacket marking, \nlight\nlittle\nQuality-of-\nsource-\nonce flow \npolicing, scheduling; \nService (QoS) \ndestination\nis admitted\ncall admission and \nGuarantees\nflows treated \nsignaling\ndifferently\nTable 7.4 \u0002 Three network-level approaches to supporting multimedia\napplications"
    },
    {
      "chunk_id": "3d600443-6f7d-4b69-be8a-e2bc977849ca",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.5.1 Dimensioning Best-Effort Networks",
      "original_titles": [
        "7.5.1 Dimensioning Best-Effort Networks"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.5 Network Support for Multimedia > 7.5.1 Dimensioning Best-Effort Networks",
      "start_page": 661,
      "end_page": 662,
      "token_count": 1157,
      "text": "delay rarely occur. When demand increases are forecasted, the ISPs deploy addi-\ntional bandwidth and switching capacity to continue to ensure satisfactory delay\nand packet-loss performance [Huang 2005]. We’ll discuss such network dimen-\nsioning further in Section 7.5.1.\n•\nDifferentiated service.\nSince the early days of the Internet, it’s been envisioned\nthat different types of traffic (for example, as indicated in the Type-of-Service\nfield in the IP4v packet header) could be provided with different classes of serv-\nice, rather than a single one-size-fits-all best-effort service. With differentiated\nservice, one type of traffic might be given strict priority over another class of\ntraffic when both types of traffic are queued at a router. For example, packets\nbelonging to a real-time conversational application might be given priority over\nother packets due to their stringent delay constraints. Introducing differentiated\nservice into the network will require new mechanisms for packet marking (indi-\ncating a packet’s class of service), packet scheduling, and more. We’ll cover dif-\nferentiated service, and new network mechanisms needed to implement this\nservice, in Section 7.5.2.\n•\nPer-connection Quality-of-Service (QoS) Guarantees.\nWith per-connection\nQoS guarantees, each instance of an application explicitly reserves end-to-end\nbandwidth and thus has a guaranteed end-to-end performance. A hard guarantee\nmeans the application will receive its requested quality of service (QoS) with\ncertainty. A soft guarantee means the application will receive its requested\nquality of service with high probability. For example, if a user wants to make a\nVoIP call from Host A to Host B, the user’s VoIP application reserves band-\nwidth explicitly in each link along a route between the two hosts. But permit-\nting applications to make reservations and requiring the network to honor the\nreservations requires some big changes. First, we need a protocol that, on\nbehalf of the applications, reserves link bandwidth on the paths from the\nsenders to their receivers. Second, we’ll need new scheduling policies in the\nrouter queues so that per-connection bandwidth reservations can be honored.\nFinally, in order to make a reservation, the applications must give the network\na description of the traffic that they intend to send into the network and the net-\nwork will need to police each application’s traffic to make sure that it abides\nby that description. These mechanisms, when combined, require new and com-\nplex software in hosts and routers. Because per-connection QoS guaranteed\nservice has not seen significant deployment, we’ll cover these mechanisms\nonly briefly in Section 7.5.3.\n7.5.1 Dimensioning Best-Effort Networks \nFundamentally, the difficulty in supporting multimedia applications arises from\ntheir stringent performance requirements––low end-to-end packet delay, delay\n634\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\njitter, and loss—and the fact that packet delay, delay jitter, and loss occur when-\never the network becomes congested. A first approach to improving the quality\nof multimedia applications—an approach that can often be used to solve just\nabout any problem where resources are constrained—is simply to “throw money\nat the problem” and thus simply avoid resource contention. In the case of net-\nworked multimedia, this means providing enough link capacity throughout the\nnetwork so that network congestion, and its consequent packet delay and loss,\nnever (or only very rarely) occurs. With enough link capacity, packets could zip\nthrough today’s Internet without queuing delay or loss. From many perspectives\nthis is an ideal situation—multimedia applications would perform perfectly, users\nwould be happy, and this could all be achieved with no changes to Internet’s best-\neffort architecture.\nThe question, of course, is how much capacity is “enough” to achieve this\nnirvana, and whether the costs of providing “enough” bandwidth are practical\nfrom a business standpoint to the ISPs. The question of how much capacity to\nprovide at network links in a given topology to achieve a given level of perform-\nance is often known as bandwidth provisioning. The even more complicated\nproblem of how to design a network topology (where to place routers, how to\ninterconnect routers with links, and what capacity to assign to links) to achieve a\ngiven level of end-to-end performance is a network design problem often referred\nto as network dimensioning. Both bandwidth provisioning and network dimen-\nsioning are complex topics, well beyond the scope of this textbook. We note here,\nhowever, that the following issues must be addressed in order to predict applica-\ntion-level performance between two network end points, and thus provision\nenough capacity to meet an application’s performance requirements.\n•\nModels of traffic demand between network end points.\nModels may need to be\nspecified at both the call level (for example, users “arriving” to the network and\nstarting up end-to-end applications) and at the packet level (for example, packets\nbeing generated by ongoing applications). Note that workload may change over\ntime.\n•\nWell-defined performance requirements.\nFor example, a performance require-\nment for supporting delay-sensitive traffic, such as a conversational multimedia\napplication, might be that the probability that the end-to-end delay of the packet\nis greater than a maximum tolerable delay be less than some small value\n[Fraleigh 2003].\n•\nModels to predict end-to-end performance for a given workload model, and tech-\nniques to find a minimal cost bandwidth allocation that will result in all user\nrequirements being met.\nHere, researchers are busy developing performance\nmodels that can quantify performance for a given workload, and optimization\ntechniques to find minimal-cost bandwidth allocations meeting performance\nrequirements.\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n635"
    },
    {
      "chunk_id": "b8319903-f9e3-492c-93c6-fdfe3a32000b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.5.2 Providing Multiple Classes of Service",
      "original_titles": [
        "7.5.2 Providing Multiple Classes of Service"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.5 Network Support for Multimedia > 7.5.2 Providing Multiple Classes of Service",
      "start_page": 663,
      "end_page": 674,
      "token_count": 5301,
      "text": "Given that today’s best-effort Internet could (from a technology standpoint)\nsupport multimedia traffic at an appropriate performance level if it were dimen-\nsioned to do so, the natural question is why today’s Internet doesn’t do so. The\nanswers are primarily economic and organizational. From an economic standpoint,\nwould users be willing to pay their ISPs enough for the ISPs to install sufficient\nbandwidth to support multimedia applications over a best-effort Internet? The orga-\nnizational issues are perhaps even more daunting. Note that an end-to-end path\nbetween two multimedia end points will pass through the networks of multiple ISPs.\nFrom an organizational standpoint, would these ISPs be willing to cooperate \n(perhaps with revenue sharing) to ensure that the end-to-end path is properly dimen-\nsioned to support multimedia applications? For a perspective on these economic and\norganizational issues, see [Davies 2005]. For a perspective on provisioning tier-1\nbackbone networks to support delay-sensitive traffic, see [Fraleigh 2003].\n7.5.2 Providing Multiple Classes of Service\nPerhaps the simplest enhancement to the one-size-fits-all best-effort service in\ntoday’s Internet is to divide traffic into classes, and provide different levels of serv-\nice to these different classes of traffic. For example, an ISP might well want to pro-\nvide a higher class of service to delay-sensitive Voice-over-IP or teleconferencing\ntraffic (and charge more for this service!) than to elastic traffic such as email or\nHTTP. Alternatively, an ISP may simply want to provide a higher quality of service\nto customers willing to pay more for this improved service. A number of residential\nwired-access ISPs and cellular wireless-access ISPs have adopted such tiered levels\nof service—with platinum-service subscribers receiving better performance than\ngold- or silver-service subscribers.\nWe’re all familiar with different classes of service from our everyday lives—\nfirst-class airline passengers get better service than business-class passengers, who\nin turn get better service than those of us who fly economy class; VIPs are provided\nimmediate entry to events while everyone else waits in line; elders are revered in\nsome countries and provided seats of honor and the finest food at a table. It’s impor-\ntant to note that such differential service is provided among aggregates of traffic,\nthat is, among classes of traffic, not among individual connections. For example, all\nfirst-class passengers are handled the same (with no first-class passenger receiving\nany better treatment than any other first-class passenger), just as all VoIP packets\nwould receive the same treatment within the network, independent of the particular\nend-to-end connection to which they belong. As we will see, by dealing with a small\nnumber of traffic aggregates, rather than a large number of individual connections,\nthe new network mechanisms required to provide better-than-best service can be\nkept relatively simple.\nThe early Internet designers clearly had this notion of multiple classes of serv-\nice in mind. Recall the type-of-service (ToS) field in the IPv4 header in Figure 4.13.\n636\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nIEN123 [ISI 1979] describes the ToS field also present in an ancestor of the IPv4\ndatagram as follows: “The Type of Service [field] provides an indication of the\nabstract parameters of the quality of service desired. These parameters are to be used\nto guide the selection of the actual service parameters when transmitting a datagram\nthrough a particular network. Several networks offer service precedence, which\nsomehow treats high precedence traffic as more important that other traffic.” More\nthan four decades ago, the vision of providing different levels of service to different\nclasses of traffic was clear! However, it’s taken us an equally long period of time to\nrealize this vision.\nMotivating Scenarios \nLet’s begin our discussion of network mechanisms for providing multiple classes of\nservice with a few motivating scenarios.\nFigure 7.14 shows a simple network scenario in which two application packet\nflows originate on Hosts H1 and H2 on one LAN and are destined for Hosts H3 and\nH4 on another LAN. The routers on the two LANs are connected by a 1.5 Mbps\nlink. Let’s assume the LAN speeds are significantly higher than 1.5 Mbps, and focus\non the output queue of router R1; it is here that packet delay and packet loss will\noccur if the aggregate sending rate of H1 and H2 exceeds 1.5 Mbps. Let’s further\nsuppose that a 1 Mbps audio application (for example, a CD-quality audio call)\nshares the 1.5 Mbps link between R1 and R2 with an HTTP Web-browsing applica-\ntion that is downloading a Web page from H2 to H4.\nR1\n1.5 Mbps link\nR2\nH2\nH1\nH4\nH3\nFigure 7.14 \u0002 Competing audio and HTTP applications\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n637\n\nIn the best-effort Internet, the audio and HTTP packets are mixed in the out-\nput queue at R1 and (typically) transmitted in a first-in-first-out (FIFO) order. In\nthis scenario, a burst of packets from the Web server could potentially fill up the\nqueue, causing IP audio packets to be excessively delayed or lost due to buffer\noverflow at R1. How should we solve this potential problem? Given that the\nHTTP Web-browsing application does not have time constraints, our intuition\nmight be to give strict priority to audio packets at R1. Under a strict priority\nscheduling discipline, an audio packet in the R1 output buffer would always be\ntransmitted before any HTTP packet in the R1 output buffer. The link from R1 to\nR2 would look like a dedicated link of 1.5 Mbps to the audio traffic, with HTTP\ntraffic using the R1-to-R2 link only when no audio traffic is queued. In order for\nR1 to distinguish between the audio and HTTP packets in its queue, each packet\nmust be marked as belonging to one of these two classes of traffic. This was the\noriginal goal of the type-of-service (ToS) field in IPv4. As obvious as this might\nseem, this then is our first insight into mechanisms needed to provide multiple\nclasses of traffic:\nInsight 1: Packet marking allows a router to distinguish among packets\nbelonging to different classes of traffic.\nNote that although our example considers a competing multimedia and elastic\nflow, the same insight applies to the case that platinum, gold, and silver classes of\nservice are implemented—a packet-marking mechanism is still needed to indicate\nthat class of service to which a packet belongs.\nNow suppose that the router is configured to give priority to packets marked as\nbelonging to the 1 Mbps audio application. Since the outgoing link speed is \n1.5 Mbps, even though the HTTP packets receive lower priority, they can still, on\naverage, receive 0.5 Mbps of transmission service. But what happens if the audio\napplication starts sending packets at a rate of 1.5 Mbps or higher (either maliciously\nor due to an error in the application)? In this case, the HTTP packets will starve, that\nis, they will not receive any service on the R1-to-R2 link. Similar problems would\noccur if multiple applications (for example, multiple audio calls), all with the same\nclass of service as the audio application, were sharing the link’s bandwidth; they too\ncould collectively starve the FTP session. Ideally, one wants a degree of isolation\namong classes of traffic so that one class of traffic can be protected from the other.\nThis protection could be implemented at different places in the network—at each\nand every router, at first entry to the network, or at inter-domain network bound-\naries. This then is our second insight:\nInsight 2: It is desirable to provide a degree of traffic isolation among classes\nso that one class is not adversely affected by another class of traffic that misbe-\nhaves.\n638\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nWe’ll examine several specific mechanisms for providing such isolation\namong traffic classes. We note here that two broad approaches can be taken.\nFirst, it is possible to perform traffic policing, as shown in Figure 7.15. If a traf-\nfic class or flow must meet certain criteria (for example, that the audio flow not\nexceed a peak rate of 1 Mbps), then a policing mechanism can be put into place\nto ensure that these criteria are indeed observed. If the policed application mis-\nbehaves, the policing mechanism will take some action (for example, drop or\ndelay packets that are in violation of the criteria) so that the traffic actually enter-\ning the network conforms to the criteria. The leaky bucket mechanism that we’ll\nexamine shortly is perhaps the most widely used policing mechanism. In Figure\n7.15, the packet classification and marking mechanism (Insight 1) and the polic-\ning mechanism (Insight 2) are both implemented together at the network’s edge,\neither in the end system or at an edge router.\nA complementary approach for providing isolation among traffic classes is\nfor the link-level packet-scheduling mechanism to explicitly allocate a fixed\nR1\n1.5 Mbps link\nPacket marking\nand policing\nMetering and policing\nMarks\nR2\nH2\nH1\nKey:\nH4\nH3\nFigure 7.15 \u0002 Policing (and marking) the audio and HTTP traffic classes\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n639\n\namount of link bandwidth to each class. For example, the audio class could be\nallocated 1 Mbps at R1, and the HTTP class could be allocated 0.5 Mbps. In this\ncase, the audio and HTTP flows see a logical link with capacity 1.0 and 0.5\nMbps, respectively, as shown in Figure 7.16. With strict enforcement of the link-\nlevel allocation of bandwidth, a class can use only the amount of bandwidth that\nhas been allocated; in particular, it cannot utilize bandwidth that is not currently\nbeing used by others. For example, if the audio flow goes silent (for example, if\nthe speaker pauses and generates no audio packets), the HTTP flow would still\nnot be able to transmit more than 0.5 Mbps over the R1-to-R2 link, even though\nthe audio flow’s 1 Mbps bandwidth allocation is not being used at that moment.\nSince bandwidth is a “use-it-or-lose-it” resource, there is no reason to prevent\nHTTP traffic from using bandwidth not used by the audio traffic. We’d like to use\nbandwidth as efficiently as possible, never wasting it when it could be otherwise\nused. This gives rise to our third insight:\nInsight 3: While providing isolation among classes or flows, it is desirable\nto use resources (for example, link bandwidth and buffers) as efficiently as\npossible.\nScheduling Mechanisms\nRecall from our discussion in Section 1.3 and Section 4.3 that packets belonging\nto various network flows are multiplexed and queued for transmission at the \n640\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nR1\n1.5 Mbps link\n1.0 Mbps\nlogical link\n0.5 Mbps\nlogical link\nR2\nH2\nH1\nH4\nH3\nFigure 7.16 \u0002 Logical isolation of audio and HTTP traffic classes\n\noutput buffers associated with a link. The manner in which queued packets are\nselected for transmission on the link is known as the link-scheduling discipline.\nLet us now consider several of the most important link-scheduling disciplines in\nmore detail.\nFirst-In-First-Out (FIFO)\nFigure 7.17 shows the queuing model abstractions for the FIFO link-scheduling dis-\ncipline. Packets arriving at the link output queue wait for transmission if the link is\ncurrently busy transmitting another packet. If there is not sufficient buffering space\nto hold the arriving packet, the queue’s packet-discarding policy then determines\nwhether the packet will be dropped (lost) or whether other packets will be removed\nfrom the queue to make space for the arriving packet. In our discussion below, we\nwill ignore packet discard. When a packet is completely transmitted over the out-\ngoing link (that is, receives service) it is removed from the queue.\nThe FIFO (also known as first-come-first-served, or FCFS) scheduling disci-\npline selects packets for link transmission in the same order in which they arrived at\nthe output link queue. We’re all familiar with FIFO queuing from bus stops (partic-\nularly in England, where queuing seems to have been perfected) or other service\ncenters, where arriving customers join the back of the single waiting line, remain in\norder, and are then served when they reach the front of the line.\nFigure 7.18 shows the FIFO queue in operation. Packet arrivals are indicated\nby numbered arrows above the upper timeline, with the number indicating the order\nR1\n1.5 Mbps link\nR1 output\ninterface queue\nR2\nH2\nH1\nH4\nH3\nFigure 7.17 \u0002 FIFO queuing abstraction\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n641\n\nin which the packet arrived. Individual packet departures are shown below the lower\ntimeline. The time that a packet spends in service (being transmitted) is indicated by\nthe shaded rectangle between the two timelines. Because of the FIFO discipline,\npackets leave in the same order in which they arrived. Note that after the departure\nof packet 4, the link remains idle (since packets 1 through 4 have been transmitted\nand removed from the queue) until the arrival of packet 5.\nPriority Queuing\nUnder priority queuing, packets arriving at the output link are classified into priority\nclasses at the output queue, as shown in Figure 7.19. As discussed in the previous sec-\ntion, a packet’s priority class may depend on an explicit marking that it carries in its\npacket header (for example, the value of the ToS bits in an IPv4 packet), its source or\ndestination IP address, its destination port number, or other criteria. Each priority class\ntypically has its own queue. When choosing a packet to transmit, the priority queuing\ndiscipline will transmit a packet from the highest priority class that has a nonempty\nqueue (that is, has packets waiting for transmission). The choice among packets in the\nsame priority class is typically done in a FIFO manner.\nFigure 7.20 illustrates the operation of a priority queue with two priority\nclasses. Packets 1, 3, and 4 belong to the high-priority class, and packets 2 and 5\nbelong to the low-priority class. Packet 1 arrives and, finding the link idle, begins\ntransmission. During the transmission of packet 1, packets 2 and 3 arrive and are\nqueued in the low- and high-priority queues, respectively. After the transmission\nof packet 1, packet 3 (a high-priority packet) is selected for transmission over\npacket 2 (which, even though it arrived earlier, is a low-priority packet). At the end\nof the transmission of packet 3, packet 2 then begins transmission. Packet 4 (a\nhigh-priority packet) arrives during the transmission of packet 2 (a low-priority\npacket). Under a nonpreemptive priority queuing discipline, the transmission of\n642\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nTime\nArrivals\nDepartures\nPacket\nin service\nTime\n1\n1\n2\n3\n4\n5\n2\n3\n1\nt = 0\nt = 2\nt = 4\nt = 6\nt = 8\nt = 10\nt = 12\nt = 14\n2\n3\n4\n5\n4\n5\nFigure 7.18 \u0002 The FIFO queue in operation\n\na packet is not interrupted once it has begun. In this case, packet 4 queues for\ntransmission and begins being transmitted after the transmission of packet 2 is\ncompleted.\nRound Robin and Weighted Fair Queuing (WFQ)\nUnder the round robin queuing discipline, packets are sorted into classes as\nwith priority queuing. However, rather than there being a strict priority of service\namong classes, a round robin scheduler alternates service among the classes. In\nthe simplest form of round robin scheduling, a class 1 packet is transmitted, fol-\nlowed by a class 2 packet, followed by a class 1 packet, followed by a class 2\npacket, and so on. A so-called work-conserving queuing discipline will never\nallow the link to remain idle whenever there are packets (of any class) queued for\nArrivals\nDepartures\nPacket\nin service\n1\n1\n2\n3\n4\n5\n2\n3\n1\n2\n3\n4\n5\n4\n5\nTime\nTime\nt = 0\nt = 2\nt = 4\nt = 6\nt = 8\nt = 10\nt = 12\nt = 14\nFigure 7.20 \u0002 Operation of the priority queue\nArrivals\nDepartures\nLow-priority queue\n(waiting area)\nClassify\nHigh-priority queue\n(waiting area)\nLink\n(server)\nFigure 7.19 \u0002 Priority queuing model\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n643\n\ntransmission. A work-conserving round robin discipline that looks for a packet\nof a given class but finds none will immediately check the next class in the round\nrobin sequence.\nFigure 7.21 illustrates the operation of a two-class round robin queue. In\nthis example, packets 1, 2, and 4 belong to class 1, and packets 3 and 5 belong to the\nsecond class. Packet 1 begins transmission immediately upon arrival at the output\nqueue. Packets 2 and 3 arrive during the transmission of packet 1 and thus queue for\ntransmission. After the transmission of packet 1, the link scheduler looks for a class\n2 packet and thus transmits packet 3. After the transmission of packet 3, the sched-\nuler looks for a class 1 packet and thus transmits packet 2. After the transmission of\npacket 2, packet 4 is the only queued packet; it is thus transmitted immediately after\npacket 2.\nA generalized abstraction of round robin queuing that has found considerable\nuse in QoS architectures is the so-called weighted fair queuing (WFQ) discipline\n[Demers 1990; Parekh 1993]. WFQ is illustrated in Figure 7.22. Arriving packets\nare classified and queued in the appropriate per-class waiting area. As in round robin\nscheduling, a WFQ scheduler will serve classes in a circular manner—first serving\nclass 1, then serving class 2, then serving class 3, and then (assuming there are three\nclasses) repeating the service pattern. WFQ is also a work-conserving queuing\ndiscipline and thus will immediately move on to the next class in the service\nsequence when it finds an empty class queue.\nWFQ differs from round robin in that each class may receive a differential\namount of service in any interval of time. Specifically, each class, i, is assigned a\nweight, wi. Under WFQ, during any interval of time during which there are class i\npackets to send, class i will then be guaranteed to receive a fraction of service equal\nto wi/(∑wj), where the sum in the denominator is taken over all classes that also have\npackets queued for transmission. In the worst case, even if all classes have queued\npackets, class i will still be guaranteed to receive a fraction wi/(∑wj) of the\n644\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nArrivals\nPacket\nin service\n1\n1\n2\n3\n4\n5\n2\n3\n1\n2\n3\n4\n5\n4\n5\nDepartures\nTime\nTime\nt = 0\nt = 2\nt = 4\nt = 6\nt = 8\nt = 10\nt = 12\nt = 14\nFigure 7.21 \u0002 Operation of the two-class round robin queue\n\nbandwidth. Thus, for a link with transmission rate R, class i will always achieve a\nthroughput of at least R · wi/(∑wj). Our description of WFQ has been an idealized\none, as we have not considered the fact that packets are discrete units of data and a\npacket’s transmission will not be interrupted to begin transmission of another\npacket; [Demers 1990] and [Parekh 1993] discuss this packetization issue. As we\nwill see in the following sections, WFQ plays a central role in QoS architectures. It\nis also available in today’s router products [Cisco QoS 2012].\nPolicing: The Leaky Bucket\nOne of our earlier insights was that policing, the regulation of the rate at which a\nclass or flow (we will assume the unit of policing is a flow in our discussion below)\nis allowed to inject packets into the network, is an important QoS mechanism. But\nwhat aspects of a flow’s packet rate should be policed? We can identify three impor-\ntant policing criteria, each differing from the other according to the time scale over\nwhich the packet flow is policed:\n•\nAverage rate. The network may wish to limit the long-term average rate (packets\nper time interval) at which a flow’s packets can be sent into the network. A\ncrucial issue here is the interval of time over which the average rate will be\npoliced. A flow whose average rate is limited to 100 packets per second is\nmore constrained than a source that is limited to 6,000 packets per minute, even\nthough both have the same average rate over a long enough interval of time. For\nexample, the latter constraint would allow a flow to send 1,000 packets in a given\nsecond-long interval of time, while the former constraint would disallow this\nsending behavior.\nClassify\nArrivals\nDepartures\nw1\nw2\nw3\nLink\nFigure 7.22 \u0002 Weighted fair queuing (WFQ)\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n645\n\n•\nPeak rate. While the average-rate constraint limits the amount of traffic that can\nbe sent into the network over a relatively long period of time, a peak-rate con-\nstraint limits the maximum number of packets that can be sent over a shorter\nperiod of time. Using our example above, the network may police a flow at an\naverage rate of 6,000 packets per minute, while limiting the flow’s peak rate to\n1,500 packets per second.\n•\nBurst size. The network may also wish to limit the maximum number of packets\n(the “burst” of packets) that can be sent into the network over an extremely short\ninterval of time. In the limit, as the interval length approaches zero, the burst size\nlimits the number of packets that can be instantaneously sent into the network.\nEven though it is physically impossible to instantaneously send multiple packets\ninto the network (after all, every link has a physical transmission rate that cannot\nbe exceeded!), the abstraction of a maximum burst size is a useful one.\nThe leaky bucket mechanism is an abstraction that can be used to characterize\nthese policing limits. As shown in Figure 7.23, a leaky bucket consists of a bucket\nthat can hold up to b tokens. Tokens are added to this bucket as follows. New tokens,\nwhich may potentially be added to the bucket, are always being generated at a rate\nof r tokens per second. (We assume here for simplicity that the unit of time is a sec-\nond.) If the bucket is filled with less than b tokens when a token is generated, the\nnewly generated token is added to the bucket; otherwise the newly generated token\nis ignored, and the token bucket remains full with b tokens.\nLet us now consider how the leaky bucket can be used to police a packet flow.\nSuppose that before a packet is transmitted into the network, it must first remove a\n646\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nTo network\nPackets\nRemove\ntoken\nToken\nwait area\nBucket holds\nup to\nb tokens\nr tokens/sec\nFigure 7.23 \u0002 The leaky bucket policer\n\ntoken from the token bucket. If the token bucket is empty, the packet must wait for\na token. (An alternative is for the packet to be dropped, although we will not consider\nthat option here.) Let us now consider how this behavior polices a traffic flow. Because\nthere can be at most b tokens in the bucket, the maximum burst size for a leaky-bucket-\npoliced flow is b packets. Furthermore, because the token generation rate is r, the max-\nimum number of packets that can enter the network of any interval of time of length t\nis rt + b. Thus, the token-generation rate, r, serves to limit the long-term average rate\nat which packets can enter the network. It is also possible to use leaky buckets (specif-\nically, two leaky buckets in series) to police a flow’s peak rate in addition to the long-\nterm average rate; see the homework problems at the end of this chapter.\nLeaky Bucket + Weighted Fair Queuing = Provable Maximum Delay in a\nQueue\nLet’s close our discussion of scheduling and policing by showing how the two can\nbe combined to provide a bound on the delay through a router’s queue. Let’s con-\nsider a router’s output link that multiplexes n flows, each policed by a leaky bucket\nwith parameters bi and ri, i = 1, . . . , n, using WFQ scheduling. We use the term flow\nhere loosely to refer to the set of packets that are not distinguished from each other\nby the scheduler. In practice, a flow might be comprised of traffic from a single end-\nto-end connection or a collection of many such connections, see Figure 7.24.\nRecall from our discussion of WFQ that each flow, i, is guaranteed to receive a\nshare of the link bandwidth equal to at least R · wi/(∑wj), where R is the transmission\nb1\nr1\nw1\nwn\nbn\nrn\nFigure 7.24 \u0002 n multiplexed leaky bucket flows with WFQ scheduling\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n647"
    },
    {
      "chunk_id": "db9364e8-249a-4a59-a369-e922f5b9b307",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.5.3 Diffserv",
      "original_titles": [
        "7.5.3 Diffserv"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.5 Network Support for Multimedia > 7.5.3 Diffserv",
      "start_page": 675,
      "end_page": 678,
      "token_count": 2198,
      "text": "rate of the link in packets/sec. What then is the maximum delay that a packet will\nexperience while waiting for service in the WFQ (that is, after passing through the\nleaky bucket)? Let us focus on flow 1. Suppose that flow 1’s token bucket is initially\nfull. A burst of b1 packets then arrives to the leaky bucket policer for flow 1. These\npackets remove all of the tokens (without wait) from the leaky bucket and then join\nthe WFQ waiting area for flow 1. Since these b1 packets are served at a rate of at least\nR · wi/(∑wj) packet/sec, the last of these packets will then have a maximum delay,\ndmax, until its transmission is completed, where\nThe rationale behind this formula is that if there are b1 packets in the queue and\npackets are being serviced (removed) from the queue at a rate of at least R · w1/\n(∑wj) packets per second, then the amount of time until the last bit of the last packet\nis transmitted cannot be more than b1/(R · w1/(∑wj)). A homework problem asks you\nto prove that as long as r1 < R · w1/(∑wj), then dmax is indeed the maximum delay\nthat any packet in flow 1 will ever experience in the WFQ queue.\n7.5.3 Diffserv \nHaving seen the motivation, insights, and specific mechanisms for providing multi-\nple classes of service, let’s wrap up our study of approaches toward proving multi-\nple classes of service with an example—the Internet Diffserv architecture [RFC\n2475; RFC Kilkki 1999]. Diffserv provides service differentiation—that is, the abil-\nity to handle different classes of traffic in different ways within the Internet in a scal-\nable manner. The need for scalability arises from the fact that millions of\nsimultaneous source-destination traffic flows may be present at a backbone router.\nWe’ll see shortly that this need is met by placing only simple functionality within\nthe network core, with more complex control operations being implemented at the\nnetwork’s edge.\nLet’s begin with the simple network shown in Figure 7.25. We’ll describe one\npossible use of Diffserv here; other variations are possible, as described in RFC\n2475. The Diffserv architecture consists of two sets of functional elements:\n•\nEdge functions: packet classification and traffic conditioning.\nAt the incom-\ning edge of the network (that is, at either a Diffserv-capable host that generates\ntraffic or at the first Diffserv-capable router that the traffic passes through), arriv-\ning packets are marked. More specifically, the differentiated service (DS) field in\nthe IPv4 or IPv6 packet header is set to some value [RFC 3260]. The definition\nof the DS field is intended to supersede the earlier definitions of the IPv4 type-\nof-service field and the IPv6 traffic class fields that we discussed in Chapter 4.\nFor example, in Figure 7.25, packets being sent from H1 to H3 might be marked\ndmax =\nb1\nR \u0004 w1> gwj\n648\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nat R1, while packets being sent from H2 to H4 might be marked at R2. The mark\nthat a packet receives identifies the class of traffic to which it belongs. Different\nclasses of traffic will then receive different service within the core network.\n•\nCore function: forwarding.\nWhen a DS-marked packet arrives at a Diffserv-\ncapable router, the packet is forwarded onto its next hop according to the so-called\nper-hop behavior (PHB) associated with that packet’s class. The per-hop behavior\ninfluences how a router’s buffers and link bandwidth are shared among the compet-\ning classes of traffic. Acrucial tenet of the Diffserv architecture is that a router’s per-\nhop behavior will be based only on packet markings, that is, the class of traffic to\nwhich a packet belongs. Thus, if packets being sent from H1 to H3 in Figure 7.25\nreceive the same marking as packets being sent from H2 to H4, then the network\nrouters treat these packets as an aggregate, without distinguishing whether the pack-\nets originated at H1 or H2. For example, R3 would not distinguish between packets\nfrom H1 and H2 when forwarding these packets on to R4. Thus, the Diffserv archi-\ntecture obviates the need to keep router state for individual source-destination\npairs—a critical consideration in making Diffserv scalable.\nAn analogy might prove useful here. At many large-scale social events (for example, a\nlarge public reception, a large dance club or discothèque, a concert, or a football game),\npeople entering the event receive a pass of one type or another: VIP passes for Very\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n649\nR4\nLeaf router\nKey:\nCore router\nR2\nR1\nR6\nR7\nR3\nR5\nH1\nH2\nH4\nH3\nR2\nR3\nFigure 7.25 \u0002 A simple Diffserv network example\n\nImportant People; over-21 passes for people who are 21 years old or older (for exam-\nple, if alcoholic drinks are to be served); backstage passes at concerts; press passes for\nreporters; even an ordinary pass for the Ordinary Person. These passes are typically dis-\ntributed upon entry to the event, that is, at the edge of the event. It is here at the edge\nwhere computationally intensive operations, such as paying for entry, checking for the\nappropriate type of invitation, and matching an invitation against a piece of identifica-\ntion, are performed. Furthermore, there may be a limit on the number of people of a\ngiven type that are allowed into an event. If there is such a limit, people may have to\nwait before entering the event. Once inside the event, one’s pass allows one to receive\ndifferentiated service at many locations around the event—a VIP is provided with free\ndrinks, a better table, free food, entry to exclusive rooms, and fawning service. Con-\nversely, an ordinary person is excluded from certain areas, pays for drinks, and receives\nonly basic service. In both cases, the service received within the event depends solely\non the type of one’s pass. Moreover, all people within a class are treated alike.\nFigure 7.26 provides a logical view of the classification and marking functions\nwithin the edge router. Packets arriving to the edge router are first classified. The\nclassifier selects packets based on the values of one or more packet header fields\n(for example, source address, destination address, source port, destination port, and\nprotocol ID) and steers the packet to the appropriate marking function. As noted\nabove, a packet’s marking is carried in the DS field in the packet header.\nIn some cases, an end user may have agreed to limit its packet-sending rate to con-\nform to a declared traffic profile. The traffic profile might contain a limit on the peak\nrate, as well as the burstiness of the packet flow, as we saw previously with the leaky\nbucket mechanism. As long as the user sends packets into the network in a way that\nconforms to the negotiated traffic profile, the packets receive their priority marking and\nare forwarded along their route to the destination. On the other hand, if the traffic pro-\nfile is violated, out-of-profile packets might be marked differently, might be shaped (for\nexample, delayed so that a maximum rate constraint would be observed), or might be\ndropped at the network edge. The role of the metering function, shown in Figure 7.26,\nis to compare the incoming packet flow with the negotiated traffic profile and to deter-\nmine whether a packet is within the negotiated traffic profile. The actual decision about\nwhether to immediately remark, forward, delay, or drop a packet is a policy issue deter-\nmined by the network administrator and is not specified in the Diffserv architecture.\nSo far, we have focused on the marking and policing functions in the Diffserv\narchitecture. The second key component of the Diffserv architecture involves the\nper-hop behavior (PHB) performed by Diffserv-capable routers. PHB is rather cryp-\ntically, but carefully, defined as “a description of the externally observable forward-\ning behavior of a Diffserv node applied to a particular Diffserv behavior aggregate”\n[RFC 2475]. Digging a little deeper into this definition, we can see several impor-\ntant considerations embedded within:\n•\nA PHB can result in different classes of traffic receiving different performance\n(that is, different externally observable forwarding behaviors).\n650\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\n•\nWhile a PHB defines differences in performance (behavior) among classes, it\ndoes not mandate any particular mechanism for achieving these behaviors. As\nlong as the externally observable performance criteria are met, any implementa-\ntion mechanism and any buffer/bandwidth allocation policy can be used. For\nexample, a PHB would not require that a particular packet-queuing discipline\n(for example, a priority queue versus a WFQ queue versus a FCFS queue) be\nused to achieve a particular behavior. The PHB is the end, to which resource allo-\ncation and implementation mechanisms are the means.\n•\nDifferences in performance must be observable and hence measurable.\nTwo PHBs have been defined: an expedited forwarding (EF) PHB [RFC 3246] and\nan assured forwarding (AF) PHB [RFC 2597]. The expedited forwarding PHB\nspecifies that the departure rate of a class of traffic from a router must equal or\nexceed a configured rate. The assured forwarding PHB divides traffic into four\nclasses, where each AF class is guaranteed to be provided with some minimum\namount of bandwidth and buffering.\nLet’s close our discussion of Diffserv with a few observations regarding its\nservice model. First, we have implicitly assumed that Diffserv is deployed within a\nsingle administrative domain, but typically an end-to-end service must be fashioned\nfrom multiple ISPs sitting between communicating end systems. In order to provide\nend-to-end Diffserv service, all the ISPs between the end systems must not only pro-\nvide this service, but most also cooperate and make settlements in order to offer end\ncustomers true end-to-end service. Without this kind of cooperation, ISPs directly\nselling Diffserv service to customers will find themselves repeatedly saying: “Yes,\nwe know you paid extra, but we don’t have a service agreement with the ISP that\ndropped and delayed your traffic. I’m sorry that there were so many gaps in your\nPackets\nForward\nClassifier\nMarker\nDrop\nShaper/\nDropper\nMeter\nFigure 7.26 \u0002 A simple Diffserv network example\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n651"
    },
    {
      "chunk_id": "4df1d3c4-cb40-484a-bfac-f604360b9c0b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.5.4 Per-Connection Quality-of-Service (QoS) Guarantees: Resource Reservation and Call Admission",
      "original_titles": [
        "7.5.4 Per-Connection Quality-of-Service (QoS) Guarantees: Resource Reservation and Call Admission"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.5 Network Support for Multimedia > 7.5.4 Per-Connection Quality-of-Service (QoS) Guarantees: Resource Reservation and Call Admission",
      "start_page": 679,
      "end_page": 681,
      "token_count": 1766,
      "text": "VoIP call!” Second, if Diffserv were actually in place and the network ran at only\nmoderate load, most of the time there would be no perceived difference between a\nbest-effort service and a Diffserv service. Indeed, end-to-end delay is usually domi-\nnated by access rates and router hops rather than by queuing delays in the routers.\nImagine the unhappy Diffserv customer who has paid more for premium service but\nfinds that the best-effort service being provided to others almost always has the\nsame performance as premium service!\n7.5.4 Per-Connection Quality-of-Service (QoS) Guarantees:\nResource Reservation and Call Admission\nIn the previous section, we have seen that packet marking and policing, traffic isola-\ntion, and link-level scheduling can provide one class of service with better perform-\nance than another. Under certain scheduling disciplines, such as priority scheduling,\nthe lower classes of traffic are essentially “invisible” to the highest-priority class of\ntraffic. With proper network dimensioning, the highest class of service can indeed\nachieve extremely low packet loss and delay—essentially circuit-like performance.\nBut can the network guarantee that an ongoing flow in a high-priority traffic class\nwill continue to receive such service throughout the flow’s duration using only the\nmechanisms that we have described so far? It cannot. In this section, we’ll see why\nyet additional network mechanisms and protocols are required when a hard service\nguarantee is provided to individual connections.\nLet’s return to our scenario from Section 7.5.2 and consider two 1 Mbps\naudio applications transmitting their packets over the 1.5 Mbps link, as shown in\nFigure 7.27. The combined data rate of the two flows (2 Mbps) exceeds the link\n652\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nR1\n1.5 Mbps link\n1 Mbps\naudio\n1 Mbps\naudio\nR2\nH2\nH1\nH4\nH3\nFigure 7.27 \u0002 Two competing audio applications overloading \nthe R1-to-R2 link\n\ncapacity. Even with classification and marking, isolation of flows, and sharing of\nunused bandwidth (of which there is none), this is clearly a losing proposition.\nThere is simply not enough bandwidth to accommodate the needs of both appli-\ncations at the same time. If the two applications equally share the bandwidth,\neach application would lose 25 percent of its transmitted packets. This is such an\nunacceptably low QoS that both audio applications are completely unusable;\nthere’s no need even to transmit any audio packets in the first place.\nGiven that the two applications in Figure 7.27 cannot both be satisfied simulta-\nneously, what should the network do? Allowing both to proceed with an unusable\nQoS wastes network resources on application flows that ultimately provide no util-\nity to the end user. The answer is hopefully clear—one of the application flows\nshould be blocked (that is, denied access to the network), while the other should be\nallowed to proceed on, using the full 1 Mbps needed by the application. The tele-\nphone network is an example of a network that performs such call blocking—if the\nrequired resources (an end-to-end circuit in the case of the telephone network) can-\nnot be allocated to the call, the call is blocked (prevented from entering the network)\nand a busy signal is returned to the user. In our example, there is no gain in allowing\na flow into the network if it will not receive a sufficient QoS to be considered\nusable. Indeed, there is a cost to admitting a flow that does not receive its needed\nQoS, as network resources are being used to support a flow that provides no utility\nto the end user.\nBy explicitly admitting or blocking flows based on their resource requirements,\nand the source requirements of already-admitted flows, the network can guarantee\nthat admitted flows will be able to receive their requested QoS. Implicit in the need\nto provide a guaranteed QoS to a flow is the need for the flow to declare its QoS\nrequirements. This process of having a flow declare its QoS requirement, and then\nhaving the network either accept the flow (at the required QoS) or block the flow is\nreferred to as the call admission process. This then is our fourth insight (in addition\nto the three earlier insights from Section 7.5.2) into the mechanisms needed to pro-\nvide QoS.\nInsight 4: If sufficient resources will not always be available, and QoS is to be\nguaranteed, a call admission process is needed in which flows declare their\nQoS requirements and are then either admitted to the network (at the required\nQoS) or blocked from the network (if the required QoS cannot be provided by\nthe network).\nOur motivating example in Figure 7.27 highlights the need for several new network\nmechanisms and protocols if a call (an end-to-end flow) is to be guaranteed a given\nquality of service once it begins:\n•\nResource reservation.\nThe only way to guarantee that a call will have the\nresources (link bandwidth, buffers) needed to meet its desired QoS is to explicitly\n7.5\n•\nNETWORK SUPPORT FOR MULTIMEDIA\n653\n\nallocate those resources to the call—a process known in networking parlance as\nresource reservation. Once resources are reserved, the call has on-demand access\nto these resources throughout its duration, regardless of the demands of all other\ncalls. If a call reserves and receives a guarantee of x Mbps of link bandwidth, and\nnever transmits at a rate greater than x, the call will see loss- and delay-free per-\nformance.\n•\nCall admission.\nIf resources are to be reserved, then the network must have a\nmechanism for calls to request and reserve resources. Since resources are not\ninfinite, a call making a call admission request will be denied admission, that is,\nbe blocked, if the requested resources are not available. Such a call admission is\nperformed by the telephone network—we request resources when we dial a num-\nber. If the circuits (TDMA slots) needed to complete the call are available, the\ncircuits are allocated and the call is completed. If the circuits are not available,\nthen the call is blocked, and we receive a busy signal. A blocked call can try\nagain to gain admission to the network, but it is not allowed to send traffic into\nthe network until it has successfully completed the call admission process. Of\ncourse, a router that allocates link bandwidth should not allocate more than is\navailable at that link. Typically, a call may reserve only a fraction of the link’s\nbandwidth, and so a router may allocate link bandwidth to more than one call.\nHowever, the sum of the allocated bandwidth to all calls should be less than the\nlink capacity if hard quality of service guarantees are to be provided.\n•\nCall setup signaling.\nThe call admission process described above requires\nthat a call be able to reserve sufficient resources at each and every network\nrouter on its source-to-destination path to ensure that its end-to-end QoS\nrequirement is met. Each router must determine the local resources required by\nthe session, consider the amounts of its resources that are already committed to\nother ongoing sessions, and determine whether it has sufficient resources to\nsatisfy the per-hop QoS requirement of the session at this router without vio-\nlating local QoS guarantees made to an already-admitted session. A signaling\nprotocol is needed to coordinate these various activities—the per-hop alloca-\ntion of local resources, as well as the overall end-to-end decision of whether or\nnot the call has been able to reserve sufficient resources at each and every\nrouter on the end-to-end path. This is the job of the call setup protocol, as\nshown in Figure 7.28. The RSVP protocol [Zhang 1993, RFC 2210] was \nproposed for this purpose within an Internet architecture for providing quality-\nof-service guarantees. In ATM networks, the Q2931b protocol [Black 1995]\ncarries this information among the ATM network’s switches and end point.\nDespite a tremendous amount of research and development, and even prod-\nucts that provide for per-connection quality of service guarantees, there has been\nalmost no extended deployment of such services. There are many possible rea-\nsons. First and foremost, it may well be the case that the simple application-level\nmechanisms that we studied in Sections 7.2 through 7.4, combined with proper\n654\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING"
    },
    {
      "chunk_id": "9299f532-22b2-4fc0-b318-c4eb1128f80d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "7.6 Summary",
      "original_titles": [
        "7.6 Summary",
        "Homework Problems and Questions"
      ],
      "path": "Chapter 7 Multimedia Networking > 7.6 Summary",
      "start_page": 682,
      "end_page": 692,
      "token_count": 5364,
      "text": "network dimensioning (Section 7.5.1) provide “good enough” best-effort network\nservice for multimedia applications. In addition, the added complexity and cost of\ndeploying and managing a network that provides per-connection quality of serv-\nice guarantees may be judged by ISPs to be simply too high given predicted cus-\ntomer revenues for that service.\n7.6 Summary\nMultimedia networking is one of the most exciting developments in the Internet\ntoday. People throughout the world are spending less time in front of their radios\nand televisions, and are instead turning to the Internet to receive audio and video\ntransmissions, both live and prerecorded. This trend will certainly continue as high-\nspeed wireless Internet access becomes more and more prevalent. Moreover, with\nsites like YouTube, users have become producers as well as consumers of multime-\ndia Internet content. In addition to video distribution, the Internet is also being used\nto transport phone calls. In fact, over the next 10 years, the Internet, along with wire-\nless Internet access, may make the traditional circuit-switched telephone system a\nthing of the past. VoIP not only provides phone service inexpensively, but also pro-\nvides numerous value-added services, such as video conferencing, online directory\nservices, voice messaging, and integration into social networks such as Facebook\nand Google+.\n7.6\n•\nSUMMARY\n655\nQoS call signaling setup\nRequest/reply\nFigure 7.28 \u0002 The call setup process\nIn Section 7.1, we described the intrinsic characteristics of video and voice, and\nthen classified multimedia applications into three categories: (i) streaming stored\naudio/video, (ii) conversational voice/video-over-IP, and (iii) streaming live audio/\nvideo.\nIn Section 7.2, we studied streaming stored video in some depth. For streaming\nvideo applications, prerecorded videos are placed on servers, and users send\nrequests to these servers to view the videos on demand. We saw that streaming video\nsystems can be classified into three categories: UDP streaming, HTTP streaming,\nand adaptive HTTP streaming. Although all three types of systems are used in prac-\ntice, the majority of today’s systems employ HTTP streaming and adaptive HTTP\nstreaming. We observed that the most important performance measure for streaming\nvideo is average throughput. In Section 7.2 we also investigated CDNs, which help\ndistribute massive amounts of video data to users around the world. We also sur-\nveyed the technology behind three major Internet video-streaming companies: Net-\nflix, YouTube, and Kankan.\nIn Section 7.3, we examined how conversational multimedia applications, such as\nVoIP, can be designed to run over a best-effort network. For conversational multimedia,\ntiming considerations are important because conversational applications are highly\ndelay-sensitive. On the other hand, conversational multimedia applications are loss-\ntolerant—occasional loss only causes occasional glitches in audio/video playback, and\nthese losses can often be partially or fully concealed. We saw how a combination of\nclient buffers, packet sequence numbers, and timestamps can greatly alleviate the\neffects of network-induced jitter. We also surveyed the technology behind Skype, one\nof the leading voice- and video-over-IP companies. In Section 7.4, we examined two of\nthe most important standardized protocols for VoIP, namely, RTP and SIP.\nIn Section 7.5, we introduced how several network mechanisms (link-level\nscheduling disciplines and traffic policing) can be used to provide differentiated\nservice among several classes of traffic.\nHomework Problems and Questions\nChapter 7 Review Questions\nSECTION 7.1\nR1. Reconstruct Table 7.1 for when Victor Video is watching a 4 Mbps video,\nFacebook Frank is looking at a new 100 Kbyte image every 20 seconds, and\nMartha Music is listening to 200 kbps audio stream.\nR2. There are two types of redundancy in video. Describe them, and discuss how\nthey can be exploited for efficient compression.\nR3. Suppose an analog audio signal is sampled 16,000 times per second, and each\nsample is quantized into one of 1024 levels. What would be the resulting bit\nrate of the PCM digital audio signal?\n656\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nR4. Multimedia applications can be classified into three categories. Name and\ndescribe each category.\nSECTION 7.2\nR5. Streaming video systems can be classified into three categories. Name and\nbriefly describe each of these categories.\nR6. List three disadvantages of UDP streaming.\nR7. With HTTP streaming, are the TCP receive buffer and the client’s application\nbuffer the same thing? If not, how do they interact?\nR8. Consider the simple model for HTTP streaming. Suppose the server sends\nbits at a constant rate of 2 Mbps and playback begins when 8 million bits\nhave been received. What is the initial buffering delay ?\nR9. CDNs typically adopt one of two different server placement philosophies.\nName and briefly describe these two philosophies.\nR10. Several cluster selection strategies were described in Section 7.2.4. Which of\nthese strategies finds a good cluster with respect to the client’s LDNS? Which \nof these strategies finds a good cluster with respect to the client itself?\nR11. Besides network-related considerations such as delay, loss, and bandwidth\nperformance, there are many additional important factors that go into design-\ning a cluster selection strategy. What are they?\nSECTION 7.3\nR12. What is the difference between end-to-end delay and packet jitter? What are\nthe causes of packet jitter?\nR13. Why is a packet that is received after its scheduled playout time considered\nlost?\nR14. Section 7.3 describes two FEC schemes. Briefly summarize them. Both\nschemes increase the transmission rate of the stream by adding overhead.\nDoes interleaving also increase the transmission rate?\nSECTION 7.4\nR15. How are different RTP streams in different sessions identified by a receiver?\nHow are different streams from within the same session identified?\nR16. What is the role of a SIP registrar? How is the role of an SIP registrar different\nfrom that of a home agent in Mobile IP?\nSECTION 7.5\nR17. In Section 7.5, we discussed non-preemptive priority queuing. What would\nbe preemptive priority queuing? Does preemptive priority queuing make\nsense for computer networks?\nR18. Give an example of a scheduling discipline that is not work conserving.\ntp\nHOMEWORK PROBLEMS AND QUESTIONS\n657\n\nR19. Give an example from queues you experience in your everyday life of FIFO,\npriority, RR, and WFQ.\nProblems\nP1. Consider the figure below. Similar to our discussion of Figure 7.1, suppose\nthat video is encoded at a fixed bit rate, and thus each video block contains\nvideo frames that are to be played out over the same fixed amount of \ntime, \n. The server transmits the first video block at , the second block\nat \n, the third block at \n, and so on. Once the client begins\nplayout, each block should be played out \ntime units after the previous\nblock.\na. Suppose that the client begins playout as soon as the first block arrives at\n. In the figure below, how many blocks of video (including the first\nblock) will have arrived at the client in time for their playout? Explain\nhow you arrived at your answer.\nb. Suppose that the client begins playout now at \n. How many blocks\nof video (including the first block) will have arrived at the client in time\nfor their playout? Explain how you arrived at your answer.\nc. In the same scenario at (b) above, what is the largest number of blocks\nthat is ever stored in the client buffer, awaiting playout? Explain how you\narrived at your answer.\nd. What is the smallest playout delay at the client, such that every video\nblock has arrived in time for its playout? Explain how you arrived at your\nanswer.\nt1 + \u0002\nt1\n\u0002\nt0 + 2\u0002\nt0 + \u0002\nt0\n\u0002\n658\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\nConstant bit\nrate video\ntransmission\nby server\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTime\nΔ Δ\nΔ Δ Δ Δ Δ Δ Δ Δ Δ\nVideo block number\nt0\nt1\nVideo\nreception\nat client\n\nP2. Recall the simple model for HTTP streaming shown in Figure 7.3. Recall that\nB denotes the size of the client’s application buffer, and Q denotes the num-\nber of bits that must be buffered before the client application begins playout.\nAlso r denotes the video consumption rate. Assume that the server sends bits\nat a constant rate x whenever the client buffer is not full.\na. Suppose that x < r. As discussed in the text, in this case playout will alter-\nnate between periods of continuous playout and periods of freezing.\nDetermine the length of each continuous playout and freezing period as a\nfunction of Q, r, and x.\nb. Now suppose that x > r. At what time \ndoes the client application\nbuffer become full?\nP3. Recall the simple model for HTTP streaming shown in Figure 7.3. Suppose\nthe buffer size is infinite but the server sends bits at variable rate x(t). Specifi-\ncally, suppose x(t) has the following saw-tooth shape. The rate is initially\nzero at time t = 0 and linearly climbs to H at time t = T. It then repeats this\npattern again and again, as shown in the figure below.\na. What is the server’s average send rate?\nb. Suppose that Q = 0, so that the client starts playback as soon as it receives\na video frame. What will happen?\nc. Now suppose Q > 0. Determine as a function of Q, H, and T the time at\nwhich playback first begins.\nd. Suppose H > 2r and Q = HT/2. Prove there will be no freezing after the\ninitial playout delay.\ne. Suppose H > 2r. Find the smallest value of Q such that there will be no\nfreezing after the initial playback delay.\nf. Now suppose that the buffer size B is finite. Suppose H > 2r. As a func-\ntion of Q, B, T, and H, determine the time \nwhen the client applica-\ntion buffer first becomes full.\nt = tf\nt = tf\nH\nTime\nT\n2T\n3T\n4T\nBit rate x(t)\nPROBLEMS\n659\n\nP4. Recall the simple model for HTTP streaming shown in Figure 7.3. Suppose\nthe client application buffer is infinite, the server sends at the constant rate x,\nand the video consumption rate is r with r < x. Also suppose playback begins\nimmediately. Suppose that the user terminates the video early at time t = E.\nAt the time of termination, the server stops sending bits (if it hasn’t already\nsent all the bits in the video).\na. Suppose the video is infinitely long. How many bits are wasted (that is,\nsent but not viewed)?\nb. Suppose the video is T seconds long with T > E. How many bits are\nwasted (that is, sent but not viewed)?\nP5. Consider a DASH system for which there are N video versions (at N different\nrates and qualities) and N audio versions (at N different rates and versions).\nSuppose we want to allow the player to choose at any time any of the N video\nversions and any of the N audio versions.\na. If we create files so that the audio is mixed in with the video, so server\nsends only one media stream at given time, how many files will the server\nneed to store (each a different URL)?\nb. If the server instead sends the audio and video streams separately and has\nthe client synchronize the streams, how many files will the server need to\nstore?\nP6. In the VoIP example in Section 7.3, let h be the total number of header bytes\nadded to each chunk, including UDP and IP header.\na. Assuming an IP datagram is emitted every 20 msecs, find the transmission\nrate in bits per second for the datagrams generated by one side of this appli-\ncation.\nb. What is a typical value of h when RTP is used?\nP7. Consider the procedure described in Section 7.3 for estimating average delay\ndi. Suppose that u = 0.1. Let r1 – t1 be the most recent sample delay, let r2 – t2\nbe the next most recent sample delay, and so on.\na. For a given audio application suppose four packets have arrived at the\nreceiver with sample delays r4 – t4, r3 – t3, r2 – t2, and r1 – t1. Express the\nestimate of delay d in terms of the four samples.\nb. Generalize your formula for n sample delays.\nc. For the formula in Part b, let n approach infinity and give the resulting\nformula. Comment on why this averaging procedure is called an exponen-\ntial moving average.\nP8. Repeat Parts a and b in Question P7 for the estimate of average delay deviation.\nP9. For the VoIP example in Section 7.3, we introduced an online procedure\n(exponential moving average) for estimating delay. In this problem we will\n660\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nexamine an alternative procedure. Let ti be the timestamp of the ith packet\nreceived; let ri be the time at which the ith packet is received. Let dn be our\nestimate of average delay after receiving the nth packet. After the first packet\nis received, we set the delay estimate equal to d1 = r1 – t1.\na. Suppose that we would like dn = (r1 – t1 + r2 – t2 + . . . + rn – tn)/n for all\nn. Give a recursive formula for dn in terms of dn–1, rn, and tn.\nb. Describe why for Internet telephony, the delay estimate described in Sec-\ntion 7.3 is more appropriate than the delay estimate outlined in Part a.\nP10. Compare the procedure described in Section 7.3 for estimating average delay\nwith the procedure in Section 3.5 for estimating round-trip time. What do the\nprocedures have in common? How are they different?\nP11. Consider the figure below (which is similar to Figure 7.7). A sender begins\nsending packetized audio periodically at t = 1. The first packet arrives at the\nreceiver at t = 8.\nPackets\ngenerated\nTime\nPackets\n1\n8\nPackets\nreceived\na. What are the delays (from sender to receiver, ignoring any playout delays)\nof packets 2 through 8? Note that each vertical and horizontal line seg-\nment in the figure has a length of 1, 2, or 3 time units.\nb. If audio playout begins as soon as the first packet arrives at the receiver at\nt = 8, which of the first eight packets sent will not arrive in time for playout?\nc. If audio playout begins at t = 9, which of the first eight packets sent will\nnot arrive in time for playout?\nd. What is the minimum playout delay at the receiver that results in all of the\nfirst eight packets arriving in time for their playout?\nPROBLEMS\n661\n\nP12. Consider again the figure in P11, showing packet audio transmission and\nreception times.\na. Compute the estimated delay for packets 2 through 8, using the formula\nfor di from Section 7.3.2. Use a value of u = 0.1.\nb. Compute the estimated deviation of the delay from the estimated average\nfor packets 2 through 8, using the formula for vi from Section 7.3.2. Use a\nvalue of u = 0.1.\nP13. Recall the two FEC schemes for VoIP described in Section 7.3. Suppose the\nfirst scheme generates a redundant chunk for every four original chunks.\nSuppose the second scheme uses a low-bit rate encoding whose transmission\nrate is 25 percent of the transmission rate of the nominal stream.\na. How much additional bandwidth does each scheme require? How much\nplayback delay does each scheme add?\nb. How do the two schemes perform if the first packet is lost in every group\nof five packets? Which scheme will have better audio quality?\nc. How do the two schemes perform if the first packet is lost in every group\nof two packets? Which scheme will have better audio quality?\nP14. a. Consider an audio conference call in Skype with N > 2 participants. \nSuppose each participant generates a constant stream of rate r bps. How\nmany bits per second will the call initiator need to send? How many bits\nper second will each of the other N – 1 participants need to send?  What \nis the total send rate, aggregated over all participants?\nb. Repeat part (a) for a Skype video conference call using a central server.\nc. Repeat part (b), but now for when each peer sends a copy of its video\nstream to each of the N – 1 other peers.\nP15. a. Suppose we send into the Internet two IP datagrams, each carrying a differ-\nent UDP segment. The first datagram has source IP address A1, destination\nIP address B, source port P1, and destination port T. The second datagram\nhas source IP address A2, destination IP address B, source port P2, and des-\ntination port T. Suppose that A1 is different from A2 and that P1 is different\nfrom P2. Assuming that both datagrams reach their final destination, will\nthe two UDP datagrams be received by the same socket? Why or why not?\nb. Suppose Alice, Bob, and Claire want to have an audio conference call\nusing SIP and RTP. For Alice to send and receive RTP packets to and from\nBob and Claire, is only one UDP socket sufficient (in addition to the\nsocket needed for the SIP messages)? If yes, then how does Alice’s SIP\nclient distinguish between the RTP packets received from Bob and Claire?\nP16. True or false:\na. If stored video is streamed directly from a Web server to a media player, then\nthe application is using TCP as the underlying transport protocol.\n662\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nb. When using RTP, it is possible for a sender to change encoding in the mid-\ndle of a session.\nc. All applications that use RTP must use port 87.\nd. If an RTP session has a separate audio and video stream for each sender,\nthen the audio and video streams use the same SSRC.\ne. In differentiated services, while per-hop behavior defines differences in\nperformance among classes, it does not mandate any particular mecha-\nnism for achieving these performances.\nf. Suppose Alice wants to establish an SIP session with Bob. In her INVITE\nmessage she includes the line: m=audio 48753 RTP/AVP 3 (AVP 3\ndenotes GSM audio). Alice has therefore indicated in this message that\nshe wishes to send GSM audio.\ng. Referring to the preceding statement, Alice has indicated in her INVITE\nmessage that she will send audio to port 48753.\nh. SIP messages are typically sent between SIP entities using a default SIP\nport number.\ni. In order to maintain registration, SIP clients must periodically send\nREGISTER messages.\nj. SIP mandates that all SIP clients support G.711 audio encoding.\nP17. Suppose that the WFQ scheduling policy is applied to a buffer that supports three\nclasses, and suppose the weights are 0.5, 0.25, and 0.25 for the three classes.\na. Suppose that each class has a large number of packets in the buffer. In what\nsequence might the three classes be served in order to achieve the WFQ\nweights? (For round robin scheduling, a natural sequence is 123123123 . . .).\nb. Suppose that classes 1 and 2 have a large number of packets in the buffer,\nand there are no class 3 packets in the buffer. In what sequence might the\nthree classes be served in to achieve the WFQ weights?\nP18. Consider the figure below. Answer the following questions:\nTime\nArrivals\nDepartures\nPacket\nin service\nTime\n1\n6\n5\n9\n3\n2\n8\n10\n11\n7\n4\n12\nt = 0\n1\nt = 2\nt = 4\nt = 6\nt = 8\nt = 10\nt = 12\nt = 14\n1\nPROBLEMS\n663\n\na. Assuming FIFO service, indicate the time at which packets 2 through 12\neach leave the queue. For each packet, what is the delay between its\narrival and the beginning of the slot in which it is transmitted? What is the\naverage of this delay over all 12 packets?\nb. Now assume a priority service, and assume that odd-numbered packets\nare high priority, and even-numbered packets are low priority. Indicate\nthe time at which packets 2 through 12 each leave the queue. For each\npacket, what is the delay between its arrival and the beginning of the\nslot in which it is transmitted? What is the average of this delay over all\n12 packets?\nc. Now assume round robin service. Assume that packets 1, 2, 3, 6, 11, and\n12 are from class 1, and packets 4, 5, 7, 8, 9, and 10 are from class 2. Indi-\ncate the time at which packets 2 through 12 each leave the queue. For\neach packet, what is the delay between its arrival and its departure? What\nis the average delay over all 12 packets?\nd. Now assume weighted fair queueing (WFQ) service. Assume that odd-\nnumbered packets are from class 1, and even-numbered packets are from\nclass 2. Class 1 has a WFQ weight of 2, while class 2 has a WFQ weight\nof 1. Note that it may not be possible to achieve an idealized WFQ sched-\nule as described in the text, so indicate why you have chosen the particu-\nlar packet to go into service at each time slot. For each packet what is the\ndelay between its arrival and its departure? What is the average delay over\nall 12 packets?\ne. What do you notice about the average delay in all four cases (FIFO, RR,\npriority, and WFQ)?\nP19. Consider again the figure for P18.\na. Assume a priority service, with packets 1, 4, 5, 6, and 11 being high-\npriority packets. The remaining packets are low priority. Indicate the slots\nin which packets 2 through 12 each leave the queue.\nb. Now suppose that round robin service is used, with packets 1, 4, 5, 6, and\n11 belonging to one class of traffic, and the remaining packets belonging\nto the second class of traffic. Indicate the slots in which packets 2 through\n12 each leave the queue.\nc. Now suppose that WFQ service is used, with packets 1, 4, 5, 6, and 11\nbelonging to one class of traffic, and the remaining packets belonging to\nthe second class of traffic. Class 1 has a WFQ weight of 1, while class 2\nhas a WFQ weight of 2 (note that these weights are different than in the\nprevious question). Indicate the slots in which packets 2 through 12 each\nleave the queue. See also the caveat in the question above regarding WFQ\nservice.\n664\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nP20. Consider the figure below, which shows a leaky bucket policer being fed by a\nstream of packets. The token buffer can hold at most two tokens, and is ini-\ntially full at t = 0. New tokens arrive at a rate of one token per slot. The out-\nput link speed is such that if two packets obtain tokens at the beginning of a\ntime slot, they can both go to the output link in the same slot. The timing\ndetails of the system are as follows:\nPROBLEMS\n665\nArrivals\nPacket queue\n(wait for tokens)\n9\n10\n7\n6\n4\n8\n5\n1\n3\n2\nt = 8\nt = 6\nt = 4\nt = 2\nt = 0\nt = 4\nt = 2\nt = 0\nr = 1 token/slot\nb = 2 tokens\n1. Packets (if any) arrive at the beginning of the slot. Thus in the figure,\npackets 1, 2, and 3 arrive in slot 0. If there are already packets in the\nqueue, then the arriving packets join the end of the queue. Packets \nproceed towards the front of the queue in a FIFO manner.\n2. After the arrivals have been added to the queue, if there are any queued\npackets, one or two of those packets (depending on the number of avail-\nable tokens) will each remove a token from the token buffer and go to the\noutput link during that slot. Thus, packets 1 and 2 each remove a token\nfrom the buffer (since there are initially two tokens) and go to the output\nlink during slot 0.\n3. A new token is added to the token buffer if it is not full, since the token\ngeneration rate is r = 1 token/slot.\n4. Time then advances to the next time slot, and these steps repeat.\nAnswer the following questions:\na. For each time slot, identify the packets that are in the queue and the num-\nber of tokens in the bucket, immediately after the arrivals have been\nprocessed (step 1 above) but before any of the packets have passed\nthrough the queue and removed a token. Thus, for the t = 0 time slot in the\nexample above, packets 1, 2 and 3 are in the queue, and there are two\ntokens in the buffer."
    },
    {
      "chunk_id": "a46f6463-0702-4ef1-83f5-1e7fa5264a5b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Programming Assignment",
      "original_titles": [
        "Programming Assignment"
      ],
      "path": "Chapter 7 Multimedia Networking > Programming Assignment",
      "start_page": 693,
      "end_page": 694,
      "token_count": 715,
      "text": "b. For each time slot indicate which packets appear on the output after the\ntoken(s) have been removed from the queue. Thus, for the t = 0 time\nslot in the example above, packets 1 and 2 appear on the output link\nfrom the leaky buffer during slot 0.\nP21. Repeat P20 but assume that r = 2. Assume again that the bucket is initially\nfull.\nP22. Consider P21 and suppose now that r = 3, and that b = 2 as before. Will your\nanswer to the question above change?\nP23. Consider the leaky-bucket policer that polices the average rate and burst size\nof a packet flow. We now want to police the peak rate, p, as well. Show how\nthe output of this leaky-bucket policer can be fed into a second leaky bucket\npolicer so that the two leaky buckets in series police the average rate, peak\nrate, and burst size. Be sure to give the bucket size and token generation rate\nfor the second policer.\nP24. A packet flow is said to conform to a leaky-bucket specification (r,b) with\nburst size b and average rate r if the number of packets that arrive to the leaky\nbucket is less than rt + b packets in every interval of time of length t for all t.\nWill a packet flow that conforms to a leaky-bucket specification (r,b) ever\nhave to wait at a leaky bucket policer with parameters r and b? Justify your\nanswer.\nP25. Show that as long as r1 < R w1/(∑wj), then dmax is indeed the maximum delay\nthat any packet in flow 1 will ever experience in the WFQ queue.\nProgramming Assignment\nIn this lab, you will implement a streaming video server and client. The client will\nuse the real-time streaming protocol (RTSP) to control the actions of the server. The\nserver will use the real-time protocol (RTP) to packetize the video for transport over\nUDP. You will be given Python code that partially implements RTSP and RTP at the\nclient and server. Your job will be to complete both the client and server code. When\nyou are finished, you will have created a client-server application that does the fol-\nlowing:\n•\nThe client sends SETUP, PLAY, PAUSE, and TEARDOWN RTSP commands,\nand the server responds to the commands.\n•\nWhen the server is in the playing state, it periodically grabs a stored JPEG frame,\npacketizes the frame with RTP, and sends the RTP packet into a UDP socket.\n•\nThe client receives the RTP packets, removes the JPEG frames, decompresses\nthe frames, and renders the frames on the client’s monitor.\n666\nCHAPTER 7\n•\nMULTIMEDIA NETWORKING\n\nThe code you will be given implements the RTSP protocol in the server and the\nRTP depacketization in the client. The code also takes care of displaying the trans-\nmitted video. You will need to implement RTSP in the client and RTP server. This\nprogramming assignment will significantly enhance the student’s understanding of\nRTP, RTSP, and streaming video. It is highly recommended. The assignment also\nsuggests a number of optional exercises, including implementing the RTSP\nDESCRIBE command at both client and server. You can find full details of \nthe assignment, as well as an overview of the RTSP protocol, at the Web site \nhttp://www.awl.com/kurose-ross.\nPROGRAMMING ASSIGNMENT\n667"
    },
    {
      "chunk_id": "742bca4a-31e9-44d9-a669-2207c85a7f85",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Interview: Henning Schulzrinne",
      "original_titles": [
        "Interview: Henning Schulzrinne"
      ],
      "path": "Chapter 7 Multimedia Networking > Interview: Henning Schulzrinne",
      "start_page": 695,
      "end_page": 697,
      "token_count": 1315,
      "text": "What made you decide to specialize in multimedia networking?\nThis happened almost by accident. As a PhD student, I got involved with DARTnet, an\nexperimental network spanning the United States with T1 lines. DARTnet was used as \na proving ground for multicast and Internet real-time tools. That led me to write my first\naudio tool, NeVoT. Through some of the DARTnet participants, I became involved in the\nIETF, in the then-nascent Audio Video Transport working group. This group later ended up\nstandardizing RTP.\nWhat was your first job in the computer industry? What did it entail?\nMy first job in the computer industry was soldering together an Altair computer kit when I\nwas a high school student in Livermore, California. Back in Germany, I started a little con-\nsulting company that devised an address management program for a travel agency—storing\ndata on cassette tapes for our TRS-80 and using an IBM Selectric typewriter with a home-\nbrew hardware interface as a printer.\nMy first real job was with AT&T Bell Laboratories, developing a network emulator for\nconstructing experimental networks in a lab environment.\nWhat are the goals of the Internet Real-Time Lab?\nOur goal is to provide components and building blocks for the Internet as the single future\ncommunications infrastructure. This includes developing new protocols, such as GIST\n(for network-layer signaling) and LoST (for finding resources by location), or enhancing\nprotocols that we have worked on earlier, such as SIP, through work on rich presence,\npeer-to-peer systems, next-generation emergency calling, and service creation tools.\nRecently, we have also looked extensively at wireless systems for VoIP, as 802.11b and\n802.11n networks and maybe WiMax networks are likely to become important last-mile\ntechnologies for telephony. We are also trying to greatly improve the ability of users to\ndiagnose faults in the complicated tangle of providers and equipment, using a peer-to-peer\nfault diagnosis system called DYSWIS (Do You See What I See).\n668\nHenning Schulzrinne\nHenning Schulzrinne is a professor, chair of the Department of\nComputer Science, and head of the Internet Real-Time Laboratory at\nColumbia University. He is the co-author of RTP, RTSP, SIP, and\nGIST—key protocols for audio and video communications over the\nInternet. Henning received his BS in electrical and industrial engineer-\ning at TU Darmstadt in Germany, his MS in electrical and computer\nengineering at the University of Cincinnati, and his PhD in electrical\nengineering at the University of Massachusetts, Amherst.\nAN INTERVIEW WITH...\n\nWe try to do practically relevant work, by building prototypes and open source sys-\ntems, by measuring performance of real systems, and by contributing to IETF standards.\nWhat is your vision for the future of multimedia networking?\nWe are now in a transition phase; just a few years shy of when IP will be the universal plat-\nform for multimedia services, from IPTV to VoIP. We expect radio, telephone, and TV to be\navailable even during snowstorms and earthquakes, so when the Internet takes over the role\nof these dedicated networks, users will expect the same level of reliability.\nWe will have to learn to design network technologies for an ecosystem of competing\ncarriers, service and content providers, serving lots of technically untrained users and\ndefending them against a small, but destructive, set of malicious and criminal users.\nChanging protocols is becoming increasingly hard. They are also becoming more complex,\nas they need to take into account competing business interests, security, privacy, and the\nlack of transparency of networks caused by firewalls and network address translators.\nSince multimedia networking is becoming the foundation for almost all of consumer\nentertainment, there will be an emphasis on managing very large networks, at low cost.\nUsers will expect ease of use, such as finding the same content on all of their devices.\nWhy does SIP have a promising future?\nAs the current wireless network upgrade to 3G networks proceeds, there is the hope of a\nsingle multimedia signaling mechanism spanning all types of networks, from cable\nmodems, to corporate telephone networks and public wireless networks. Together with\nsoftware radios, this will make it possible in the future that a single device can be used on\na home network, as a cordless BlueTooth phone, in a corporate network via 802.11 and in\nthe wide area via 3G networks. Even before we have such a single universal wireless\ndevice, the personal mobility mechanisms make it possible to hide the differences between\nnetworks. One identifier becomes the universal means of reaching a person, rather than\nremembering or passing around half a dozen technology- or location-specific telephone\nnumbers.\nSIP also breaks apart the provision of voice (bit) transport from voice services. It now\nbecomes technically possible to break apart the local telephone monopoly, where one\ncompany provides neutral bit transport, while others provide IP “dial tone” and the classical\ntelephone services, such as gateways, call forwarding, and caller ID.\nBeyond multimedia signaling, SIP offers a new service that has been missing in the\nInternet: event notification. We have approximated such services with HTTP kludges and\ne-mail, but this was never very satisfactory. Since events are a common abstraction for\ndistributed systems, this may simplify the construction of new services.\n669\n\nDo you have any advice for students entering the networking field?\nNetworking bridges disciplines. It draws from electrical engineering, all aspects of com-\nputer science, operations research, statistics, economics, and other disciplines. Thus,\nnetworking researchers have to be familiar with subjects well beyond protocols and rout-\ning algorithms.\nGiven that networks are becoming such an important part of everyday life, students\nwanting to make a difference in the field should think of the new resource constraints in\nnetworks: human time and effort, rather than just bandwidth or storage.\nWork in networking research can be immensely satisfying since it is about allowing\npeople to communicate and exchange ideas, one of the essentials of being human. The\nInternet has become the third major global infrastructure, next to the transportation system\nand energy distribution. Almost no part of the economy can work without high-performance\nnetworks, so there should be plenty of opportunities for the foreseeable future.\n670"
    },
    {
      "chunk_id": "19e5476f-f2c6-49ec-9026-a60d9db242b4",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Chapter 8 Security in Computer Networks",
      "original_titles": [
        "Chapter 8 Security in Computer Networks"
      ],
      "path": "Chapter 8 Security in Computer Networks",
      "start_page": 698,
      "end_page": 698,
      "token_count": 317,
      "text": "CHAPTER 8\nSecurity in\nComputer\nNetworks\n671\nWay back in Section 1.6 we described some of the more prevalent and damaging\nclasses of Internet attacks, including malware attacks, denial of service, sniffing,\nsource masquerading, and message modification and deletion. Although we have\nsince learned a tremendous amount about computer networks, we still haven’t\nexamined how to secure networks from those attacks. Equipped with our newly\nacquired expertise in computer networking and Internet protocols, we’ll now study\nin-depth secure communication and, in particular, how computer networks can be\ndefended from those nasty bad guys.\nLet us introduce Alice and Bob, two people who want to communicate and\nwish to do so “securely.” This being a networking text, we should remark that Alice\nand Bob could be two routers that want to exchange routing tables securely, a client\nand server that want to establish a secure transport connection, or two e-mail appli-\ncations that want to exchange secure e-mail—all case studies that we will consider\nlater in this chapter. Alice and Bob are well-known fixtures in the security commu-\nnity, perhaps because their names are more fun than a generic entity named “A”\nthat wants to communicate securely with a generic entity named “B.” Love affairs,\nwartime communication, and business transactions are the commonly cited human\nneeds for secure communications; preferring the first to the latter two, we’re happy\nto use Alice and Bob as our sender and receiver, and imagine them in this first \nscenario."
    },
    {
      "chunk_id": "f7bdce95-25c3-4dac-8c56-4a74c87ff3d2",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.1 What Is Network Security?",
      "original_titles": [
        "8.1 What Is Network Security?"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.1 What Is Network Security?",
      "start_page": 699,
      "end_page": 701,
      "token_count": 1617,
      "text": "We said that Alice and Bob want to communicate and wish to do so “securely,”\nbut what precisely does this mean? As we will see, security (like love) is a many-\nsplendored thing; that is, there are many facets to security. Certainly, Alice and\nBob would like for the contents of their communication to remain secret from an\neavesdropper. They probably would also like to make sure that when they are\ncommunicating, they are indeed communicating with each other, and that if their\ncommunication is tampered with by an eavesdropper, that this tampering is\ndetected. In the first part of this chapter, we’ll cover the fundamental cryptography\ntechniques that allow for encrypting communication, authenticating the party with\nwhom one is communicating, and ensuring message integrity.\nIn the second part of this chapter, we’ll examine how the fundamental crypto-\ngraphy principles can be used to create secure networking protocols. Once again\ntaking a top-down approach, we’ll examine secure protocols in each of the (top\nfour) layers, beginning with the application layer. We’ll examine how to secure e-\nmail, how to secure a TCP connection, how to provide blanket security at the net-\nwork layer, and how to secure a wireless LAN. In the third part of this chapter we’ll\nconsider operational security, which is about protecting organizational networks\nfrom attacks. In particular, we’ll take a careful look at how firewalls and intrusion\ndetection systems can enhance the security of an organizational network.\n8.1 What Is Network Security?\nLet’s begin our study of network security by returning to our lovers, Alice and Bob,\nwho want to communicate “securely.” What precisely does this mean? Certainly,\nAlice wants only Bob to be able to understand a message that she has sent, even\nthough they are communicating over an insecure medium where an intruder\n(Trudy, the intruder) may intercept whatever is transmitted from Alice to Bob. Bob\nalso wants to be sure that the message he receives from Alice was indeed sent by\nAlice, and Alice wants to make sure that the person with whom she is communicat-\ning is indeed Bob. Alice and Bob also want to make sure that the contents of their\nmessages have not been altered in transit. They also want to be assured that they\ncan communicate in the first place (i.e., that no one denies them access to the\nresources needed to communicate). Given these considerations, we can identify the\nfollowing desirable properties of secure communication.\n•\nConfidentiality. Only the sender and intended receiver should be able to under-\nstand the contents of the transmitted message. Because eavesdroppers may inter-\ncept the message, this necessarily requires that the message be somehow\nencrypted so that an intercepted message cannot be understood by an intercep-\ntor. This aspect of confidentiality is probably the most commonly perceived\n672\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nmeaning of the term secure communication. We’ll study cryptographic tech-\nniques for encrypting and decrypting data in Section 8.2.\n•\nMessage integrity. Alice and Bob want to ensure that the content of their com-\nmunication is not altered, either maliciously or by accident, in transit. Extensions\nto the checksumming techniques that we encountered in reliable transport and\ndata link protocols can be used to provide such message integrity. We will study\nmessage integrity in Section 8.3.\n•\nEnd-point authentication. Both the sender and receiver should be able to \nconfirm the identity of the other party involved in the communication—\nto confirm that the other party is indeed who or what they claim to be. \nFace-to-face human communication solves this problem easily by visual\nrecognition. When communicating entities exchange messages over a\nmedium where they cannot see the other party, authentication is not so\nsimple. When a user wants to access an inbox, how does the mail server ver-\nify that the user is the person he or she claims to be? We study end-point\nauthentication in Section 8.4.\n•\nOperational security. Almost all organizations (companies, universities, and\nso on) today have networks that are attached to the public Internet. These net-\nworks therefore can potentially be compromised. Attackers can attempt \nto deposit worms into the hosts in the network, obtain corporate secrets, map\nthe internal network configurations, and launch DoS attacks. We’ll see in \nSection 8.9 that operational devices such as firewalls and intrusion detection\nsystems are used to counter attacks against an organization’s network. A\nfirewall sits between the organization’s network and the public network, \ncontrolling packet access to and from the network. An intrusion detection sys-\ntem performs “deep packet inspection,” alerting the network administrators\nabout suspicious activity.\nHaving established what we mean by network security, let’s next consider\nexactly what information an intruder may have access to, and what actions can be\ntaken by the intruder. Figure 8.1 illustrates the scenario. Alice, the sender, wants to\nsend data to Bob, the receiver. In order to exchange data securely, while meeting the\nrequirements of confidentiality, end-point authentication, and message integrity,\nAlice and Bob will exchange control messages and data messages (in much the\nsame way that TCP senders and receivers exchange control segments and data seg-\nments). All or some of these messages will typically be encrypted. As discussed in\nSection 1.6, an intruder can potentially perform\n•\neavesdropping—sniffing and recording control and data messages on the\nchannel.\n•\nmodification, insertion, or deletion of messages or message content.\n8.1\n•\nWHAT IS NETWORK SECURITY?\n673\n\nAs we’ll see, unless appropriate countermeasures are taken, these capabilities\nallow an intruder to mount a wide variety of security attacks: snooping on commu-\nnication (possibly stealing passwords and data), impersonating another entitity,\nhijacking an ongoing session, denying service to legitimate network users by over-\nloading system resources, and so on. A summary of reported attacks is maintained at\nthe CERT Coordination Center [CERT 2012].\nHaving established that there are indeed real threats loose in the Internet,\nwhat are the Internet equivalents of Alice and Bob, our friends who need to com-\nmunicate securely? Certainly, Bob and Alice might be human users at two end\nsystems, for example, a real Alice and a real Bob who really do want to exchange\nsecure e-mail. They might also be participants in an electronic commerce transac-\ntion. For example, a real Bob might want to transfer his credit card number\nsecurely to a Web server to purchase an item online. Similarly, a real Alice might\nwant to interact with her bank online. The parties needing secure communication\nmight themselves also be part of the network infrastructure. Recall that the\ndomain name system (DNS, see Section 2.5) or routing daemons that exchange\nrouting information (see Section 4.6) require secure communication between two\nparties. The same is true for network management applications, a topic we exam-\nine in Chapter 9. An intruder that could actively interfere with DNS lookups (as\ndiscussed in Section 2.5), routing computations [RFC 4272], or network manage-\nment functions [RFC 3414] could wreak havoc in the Internet.\nHaving now established the framework, a few of the most important defi-\nnitions, and the need for network security, let us next delve into cryptography.\nWhile the use of cryptography in providing confidentiality is self-evident, we’ll see\nshortly that it is also central to providing end-point authentication and message\nintegrity—making cryptography a cornerstone of network security.\n674\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nSecure\nsender\nAlice\nTrudy\nChannel\nControl, data messages\nSecure\nreceiver\nBob\nData\nData\nFigure 8.1 \u0002 Sender, receiver, and intruder (Alice, Bob, and Trudy)"
    },
    {
      "chunk_id": "f2582ab1-7b55-420b-8b3f-9ff2870169ac",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.2 Principles of Cryptography",
      "original_titles": [
        "8.2 Principles of Cryptography"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.2 Principles of Cryptography",
      "start_page": 702,
      "end_page": 702,
      "token_count": 383,
      "text": "8.2 Principles of Cryptography\nAlthough cryptography has a long history dating back at least as far as Julius Caesar,\nmodern cryptographic techniques, including many of those used in the Internet,\nare based on advances made in the past 30 years. Kahn’s book, The Codebreakers\n[Kahn 1967], and Singh’s book, The Code Book: The Science of Secrecy from\nAncient Egypt to Quantum Cryptography [Singh 1999], provide a fascinating look\nat the long history of cryptography. A complete discussion of cryptography itself\nrequires a complete book [Kaufman 1995; Schneier 1995] and so we only touch\non the essential aspects of cryptography, particularly as they are practiced on the\nInternet. We also note that while our focus in this section will be on the use of\ncryptography for confidentiality, we’ll see shortly that cryptographic techniques\nare inextricably woven into authentication, message integrity, nonrepudiation,\nand more.\nCryptographic techniques allow a sender to disguise data so that an intruder can\ngain no information from the intercepted data. The receiver, of course, must be able\nto recover the original data from the disguised data. Figure 8.2 illustrates some of\nthe important terminology.\nSuppose now that Alice wants to send a message to Bob. Alice’s message in\nits original form (for example, “Bob, I love you. Alice”) is known as\nplaintext, or cleartext. Alice encrypts her plaintext message using an encryption\nalgorithm so that the encrypted message, known as ciphertext, looks unintelligible\nto any intruder. Interestingly, in many modern cryptographic systems, including\nthose used in the Internet, the encryption technique itself is known—published, stan-\ndardized, and available to everyone (for example, [RFC 1321; RFC 3447; RFC\n8.2\n•\nPRINCIPLES OF CRYPTOGRAPHY\n675\nEncryption\nalgorithm\nCiphertext\nChannel\nTrudy\nAlice\nBob\nDecryption\nalgorithm\nPlaintext\nKey:\nKey\nPlaintext\nKA\nKB\nFigure 8.2 \u0002 Cryptographic components"
    },
    {
      "chunk_id": "4dcbb9dd-1c14-4e72-b428-898d6db80769",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.2.1 Symmetric Key Cryptography",
      "original_titles": [
        "8.2.1 Symmetric Key Cryptography"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.2 Principles of Cryptography > 8.2.1 Symmetric Key Cryptography",
      "start_page": 703,
      "end_page": 709,
      "token_count": 4470,
      "text": "2420; NIST 2001]), even a potential intruder! Clearly, if everyone knows the\nmethod for encoding data, then there must be some secret information that prevents\nan intruder from decrypting the transmitted data. This is where keys come in.\nIn Figure 8.2, Alice provides a key, KA, a string of numbers or characters, as\ninput to the encryption algorithm. The encryption algorithm takes the key and the\nplaintext message, m, as input and produces ciphertext as output. The notation\nKA(m) refers to the ciphertext form (encrypted using the key KA) of the plaintext\nmessage, m. The actual encryption algorithm that uses key KA will be evident from\nthe context. Similarly, Bob will provide a key, KB, to the decryption algorithm\nthat takes the ciphertext and Bob’s key as input and produces the original plain-\ntext as output. That is, if Bob receives an encrypted message KA(m), he decrypts it\nby computing KB(KA(m)) = m. In symmetric key systems, Alice’s and Bob’s keys\nare identical and are secret. In public key systems, a pair of keys is used. One of\nthe keys is known to both Bob and Alice (indeed, it is known to the whole world).\nThe other key is known only by either Bob or Alice (but not both). In the follow-\ning two subsections, we consider symmetric key and public key systems in more\ndetail.\n8.2.1 Symmetric Key Cryptography\nAll cryptographic algorithms involve substituting one thing for another, for exam-\nple, taking a piece of plaintext and then computing and substituting the appropriate\nciphertext to create the encrypted message. Before studying a modern key-based\ncryptographic system, let us first get our feet wet by studying a very old, very sim-\nple symmetric key algorithm attributed to Julius Caesar, known as the Caesar\ncipher (a cipher is a method for encrypting data).\nFor English text, the Caesar cipher would work by taking each letter in the\nplaintext message and substituting the letter that is k letters later (allowing wrap-\naround; that is, having the letter z followed by the letter a) in the alphabet. For\nexample if k = 3, then the letter a in plaintext becomes d in ciphertext; b in plaintext\nbecomes e in ciphertext, and so on. Here, the value of k serves as the key. As an\nexample, the plaintext message “bob, i love you. alice” becomes “ere,\nl oryh brx. dolfh” in ciphertext. While the ciphertext does indeed look like\ngibberish, it wouldn’t take long to break the code if you knew that the Caesar cipher\nwas being used, as there are only 25 possible key values.\nAn improvement on the Caesar cipher is the monoalphabetic cipher, which\nalso substitutes one letter of the alphabet with another letter of the alphabet. How-\never, rather than substituting according to a regular pattern (for example, substitu-\ntion with an offset of k for all letters), any letter can be substituted for any other\nletter, as long as each letter has a unique substitute letter, and vice versa. The substi-\ntution rule in Figure 8.3 shows one possible rule for encoding plaintext.\nThe plaintext message “bob, i love you. alice” becomes “nkn, s\ngktc wky. mgsbc.” Thus, as in the case of the Caesar cipher, this looks like\n676\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\ngibberish. A monoalphabetic cipher would also appear to be better than the Caesar\ncipher in that there are 26! (on the order of 1026) possible pairings of letters\nrather than 25 possible pairings. A brute-force approach of trying all 1026 possible\npairings would require far too much work to be a feasible way of breaking the\nencryption algorithm and decoding the message. However, by statistical analysis\nof the plaintext language, for example, knowing that the letters e and t are the most\nfrequently occurring letters in typical English text (accounting for 13 percent and 9\npercent of letter occurrences), and knowing that particular two- and three-letter\noccurrences of letters appear quite often together (for example, “in,” “it,” “the,”\n“ion,” “ing,” and so forth) make it relatively easy to break this code. If the intruder\nhas some knowledge about the possible contents of the message, then it is even eas-\nier to break the code. For example, if Trudy the intruder is Bob’s wife and suspects\nBob of having an affair with Alice, then she might suspect that the names “bob”\nand “alice” appear in the text. If Trudy knew for certain that those two names\nappeared in the ciphertext and had a copy of the example ciphertext message\nabove, then she could immediately determine seven of the 26 letter pairings,\nrequiring 109 fewer possibilities to be checked by a brute-force method. Indeed, if\nTrudy suspected Bob of having an affair, she might well expect to find some other\nchoice words in the message as well.\nWhen considering how easy it might be for Trudy to break Bob and Alice’s\nencryption scheme, one can distinguish three different scenarios, depending on what\ninformation the intruder has.\n•\nCiphertext-only attack. In some cases, the intruder may have access only to the\nintercepted ciphertext, with no certain information about the contents of the\nplaintext message. We have seen how statistical analysis can help in a cipher-\ntext-only attack on an encryption scheme.\n•\nKnown-plaintext attack. We saw above that if Trudy somehow knew for sure that\n“bob” and “alice” appeared in the ciphertext message, then she could have deter-\nmined the (plaintext, ciphertext) pairings for the letters a, l, i, c, e, b, and o.\nTrudy might also have been fortunate enough to have recorded all of the cipher-\ntext transmissions and then found Bob’s own decrypted version of one of the\ntransmissions scribbled on a piece of paper. When an intruder knows some of the\n(plaintext, ciphertext) pairings, we refer to this as a known-plaintext attack on\nthe encryption scheme.\n8.2\n•\nPRINCIPLES OF CRYPTOGRAPHY\n677\nPlaintext letter:\na b c d e f g h i j k l m n o p q r s t u v w x y z\nCiphertext letter:\nm n b v c x z a s d f g h j k l p o i u y t r e w q\nFigure 8.3 \u0002 A monoalphabetic cipher\n\n•\nChosen-plaintext attack. In a chosen-plaintext attack, the intruder is able to\nchoose the plaintext message and obtain its corresponding ciphertext form. For\nthe simple encryption algorithms we’ve seen so far, if Trudy could get Alice to\nsend the message, “The quick brown fox jumps over the lazy\ndog,” she could completely break the encryption scheme. We’ll see shortly that\nfor more sophisticated encryption techniques, a chosen-plaintext attack does not\nnecessarily mean that the encryption technique can be broken.\nFive hundred years ago, techniques improving on monoalphabetic encryption,\nknown as polyalphabetic encryption, were invented. The idea behind polyalpha-\nbetic encryption is to use multiple monoalphabetic ciphers, with a specific monoal-\nphabetic cipher to encode a letter in a specific position in the plaintext message.\nThus, the same letter, appearing in different positions in the plaintext message,\nmight be encoded differently. An example of a polyalphabetic encryption scheme is\nshown in Figure 8.4. It has two Caesar ciphers (with k = 5 and k = 19), shown as\nrows. We might choose to use these two Caesar ciphers, C1 and C2, in the repeating\npattern C1, C2, C2, C1, C2. That is, the first letter of plaintext is to be encoded using\nC1, the second and third using C2, the fourth using C1, and the fifth using C2. The\npattern then repeats, with the sixth letter being encoded using C1, the seventh with\nC2, and so on. The plaintext message “bob, i love you.” is thus encrypted\n“ghu, n etox dhz.” Note that the first b in the plaintext message is encrypted\nusing C1, while the second b is encrypted using C2. In this example, the encryption\nand decryption “key” is the knowledge of the two Caesar keys (k = 5, k = 19) and\nthe pattern C1, C2, C2, C1, C2.\nBlock Ciphers\nLet us now move forward to modern times and examine how symmetric key encryp-\ntion is done today. There are two broad classes of symmetric encryption techniques:\nstream ciphers and block ciphers. We’ll briefly examine stream ciphers in Section\n8.7 when we investigate security for wireless LANs. In this section, we focus on\nblock ciphers, which are used in many secure Internet protocols, including PGP\n(for secure e-mail), SSL (for securing TCP connections), and IPsec (for securing the\nnetwork-layer transport).\n678\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nPlaintext letter:\na b c d e f g h i j k l m n o p q r s t u v w x y z\nC1(k = 5): \nC2(k = 19): \nf g h i j k l m n o p q r s t u v w x y z a b c d e\nt u v w x y z a b c d e f g h i j k l m n o p q r s\nFigure 8.4 \u0002 A polyalphabetic cipher using two Caesar ciphers\n\n8.2\n•\nPRINCIPLES OF CRYPTOGRAPHY\n679\nIn a block cipher, the message to be encrypted is processed in blocks of k bits.\nFor example, if k = 64, then the message is broken into 64-bit blocks, and each block\nis encrypted independently. To encode a block, the cipher uses a one-to-one map-\nping to map the k-bit block of cleartext to a k-bit block of ciphertext. Let’s look at\nan example. Suppose that k = 3, so that the block cipher maps 3-bit inputs (clear-\ntext) to 3-bit outputs (ciphertext). One possible mapping is given in Table 8.1.\nNotice that this is a one-to-one mapping; that is, there is a different output for each\ninput. This block cipher breaks the message up into 3-bit blocks and encrypts each\nblock according to the above mapping. You should verify that the message\n010110001111 gets encrypted into 101000111001.\nContinuing with this 3-bit block example, note that the mapping in Table 8.1 is\njust one mapping of many possible mappings. How many possible mappings are there?\nTo answer this question, observe that a mapping is nothing more than a permuta-\ntion of all the possible inputs. There are 23 (= 8) possible inputs (listed under the\ninput columns). These eight inputs can be permuted in 8! = 40,320 different ways.\nSince each of these permutations specifies a mapping, there are 40,320 possible\nmappings. We can view each of these mappings as a key—if Alice and Bob both\nknow the mapping (the key), they can encrypt and decrypt the messages sent\nbetween them.\nThe brute-force attack for this cipher is to try to decrypt ciphtertext by using all\nmappings. With only 40,320 mappings (when k = 3), this can quickly be accom-\nplished on a desktop PC. To thwart brute-force attacks, block ciphers typically use\nmuch larger blocks, consisting of k = 64 bits or even larger. Note that the number of\npossible mappings for a general k-block cipher is 2k!, which is astronomical for even\nmoderate values of k (such as k = 64).\nAlthough full-table block ciphers, as just described, with moderate values of\nk can produce robust symmetric key encryption schemes, they are unfortunately\ndifficult to implement. For k = 64 and for a given mapping, Alice and Bob\nwould need to maintain a table with 264 input values, which is an infeasible task.\nMoreover, if Alice and Bob were to change keys, they would have to each regenerate\nTable 8.1 \u0002 A specific 3-bit block cipher\ninput\noutput\ninput\noutput\n000\n110\n100\n011\n001\n111\n101\n010\n010\n101\n110\n000\n011\n100\n111\n001\n\nthe table. Thus, a full-table block cipher, providing predetermined mappings\nbetween all inputs and outputs (as in the example above), is simply out of the\nquestion.\nInstead, block ciphers typically use functions that simulate randomly permuted\ntables. An example (adapted from [Kaufman 1995]) of such a function for k = 64\nbits is shown in Figure 8.5. The function first breaks a 64-bit block into 8 chunks,\nwith each chunk consisting of 8 bits. Each 8-bit chunk is processed by an 8-bit to 8-\nbit table, which is of manageable size. For example, the first chunk is processed by\nthe table denoted by T1. Next, the 8 output chunks are reassembled into a 64-bit\nblock. The positions of the 64 bits in the block are then scrambled (permuted) to\nproduce a 64-bit output. This output is fed back to the 64-bit input, where another\ncycle begins. After n such cycles, the function provides a 64-bit block of ciphertext.\nThe purpose of the rounds is to make each input bit affect most (if not all) of the\nfinal output bits. (If only one round were used, a given input bit would affect only 8\nof the 64 output bits.) The key for this block cipher algorithm would be the eight\npermutation tables (assuming the scramble function is publicly known).\nToday there are a number of popular block ciphers, including DES (standing for\nData Encryption Standard), 3DES, and AES (standing for Advanced Encryption\nStandard). Each of these standards uses functions, rather than predetermined tables,\nalong the lines of Figure 8.5 (albeit more complicated and specific to each cipher).\nEach of these algorithms also uses a string of bits for a key. For example, DES uses\n64-bit blocks with a 56-bit key. AES uses 128-bit blocks and can operate with keys\nthat are 128, 192, and 256 bits long. An algorithm’s key determines the specific\n680\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n64-bit output\nLoop\nfor n\nrounds\n8 bits\n8 bits\nT1\n8 bits\n8 bits\nT2\n8 bits\n8 bits\nT3\n8 bits\n64-bit input\n8 bits\nT4\n8 bits\n8 bits\nT5\n8 bits\n8 bits\nT6\n8 bits\n8 bits\nT7\n8 bits\n8 bits\nT8\n64-bit scrambler\nFigure 8.5 \u0002 An example of a block cipher\n\n8.2\n•\nPRINCIPLES OF CRYPTOGRAPHY\n681\n“mini-table” mappings and permutations within the algorithm’s internals. The brute-\nforce attack for each of these ciphers is to cycle through all the keys, applying\nthe decryption algorithm with each key. Observe that with a key length of n, there\nare 2n possible keys. NIST [NIST 2001] estimates that a machine that could crack\n56-bit DES in one second (that is, try all 256 keys in one second) would take approx-\nimately 149 trillion years to crack a 128-bit AES key.\nCipher-Block Chaining\nIn computer networking applications, we typically need to encrypt long messages\n(or long streams of data). If we apply a block cipher as described by simply chop-\nping up the message into k-bit blocks and independently encrypting each block, a\nsubtle but important problem occurs. To see this, observe that two or more of the clear-\ntext blocks can be identical. For example, the cleartext in two or more blocks could\nbe “HTTP/1.1”. For these identical blocks, a block cipher would, of course, produce\nthe same ciphertext. An attacker could potentially guess the cleartext when it sees\nidentical ciphertext blocks and may even be able to decrypt the entire message by\nidentifying identical ciphtertext blocks and using knowledge about the underlying\nprotocol structure [Kaufman 1995].\nTo address this problem, we can mix some randomness into the ciphertext so\nthat identical plaintext blocks produce different ciphertext blocks. To explain this\nidea, let m(i) denote the ith plaintext block, c(i) denote the ith ciphertext block,\nand a \u0002 b denote the exclusive-or (XOR) of two bit strings, a and b. (Recall that\nthe 0 \u0002 0 = 1 \u0002 1 = 0 and 0 \u0002 1 = 1 \u0002 0 = 1, and the XOR of two bit strings is done\non a bit-by-bit basis. So, for example, 10101010 \u0002 11110000 = 01011010.) Also,\ndenote the block-cipher encryption algorithm with key S as KS. The basic idea is\nas follows. The sender creates a random k-bit number r(i) for the ith block and cal-\nculates c(i) = KS(m(i)\u0002 r(i)). Note that a new k-bit random number is chosen for\neach block. The sender then sends c(1), r(1), c(2), r(2), c(3), r(3), and so on. Since\nthe receiver receives c(i) and r(i), it can recover each block of the plaintext by\ncomputing m(i) = KS(c(i)) \u0002 r(i). It is important to note that, although r(i) is sent\nin the clear and thus can be sniffed by Trudy, she cannot obtain the plaintext m(i),\nsince she does not know the key KS. Also note that if two plaintext blocks m(i) and\nm( j) are the same, the corresponding ciphertext blocks c(i) and c( j) will be differ-\nent (as long as the random numbers r(i) and r( j) are different, which occurs with\nvery high probability).\nAs an example, consider the 3-bit block cipher in Table 8.1. Suppose the plain-\ntext is 010010010. If Alice encrypts this directly, without including the randomness,\nthe resulting ciphertext becomes 101101101. If Trudy sniffs this ciphertext, because\neach of the three cipher blocks is the same, she can correctly surmise that each of\nthe three plaintext blocks are the same. Now suppose instead Alice generates the\nrandom blocks r(1) = 001, r(2) =111, and r(3) = 100 and uses the above technique\nto generate the ciphertext c(1) = 100, c(2) = 010, and c(3) = 000. Note that the three\n\nciphertext blocks are different even though the plaintext blocks are the same. Alice\nthen sends c(1), r(1), c(2), and r(2). You should verify that Bob can obtain the origi-\nnal plaintext using the shared key KS.\nThe astute reader will note that introducing randomness solves one problem but\ncreates another: namely, Alice must transmit twice as many bits as before. Indeed,\nfor each cipher bit, she must now also send a random bit, doubling the required\nbandwidth. In order to have our cake and eat it too, block ciphers typically use a\ntechnique called Cipher Block Chaining (CBC). The basic idea is to send only one\nrandom value along with the very first message, and then have the sender and\nreceiver use the computed coded blocks in place of the subsequent random number.\nSpecifically, CBC operates as follows:\n1. Before encrypting the message (or the stream of data), the sender generates a\nrandom k-bit string, called the Initialization Vector (IV). Denote this initial-\nization vector by c(0). The sender sends the IV to the receiver in cleartext.\n2. For the first block, the sender calculates m(1) \u0002 c(0), that is, calculates the\nexclusive-or of the first block of cleartext with the IV. It then runs the result\nthrough the block-cipher algorithm to get the corresponding ciphertext block;\nthat is, c(1) = KS(m(1) \u0002 c(0)). The sender sends the encrypted block c(1) to\nthe receiver.\n3. For the ith block, the sender generates the ith ciphertext block from c(i) =\nKS(m(i) \u0002 c(i \u0002 1)).\nLet’s now examine some of the consequences of this approach. First, the\nreceiver will still be able to recover the original message. Indeed, when the receiver\nreceives c(i), it decrypts it with KS to obtain s(i) = m(i) \u0002 c(i – 1); since the receiver\nalso knows c(i – 1), it then obtains the cleartext block from m(i) = s(i) \u0002 c(i – 1). Sec-\nond, even if two cleartext blocks are identical, the corresponding ciphtertexts\n(almost always) will be different. Third, although the sender sends the IV in the\nclear, an intruder will still not be able to decrypt the ciphertext blocks, since the\nintruder does not know the secret key, S. Finally, the sender only sends one over-\nhead block (the IV), thereby negligibly increasing the bandwidth usage for long\nmessages (consisting of hundreds of blocks).\nAs an example, let’s now determine the ciphertext for the 3-bit block cipher in\nTable 8.1 with plaintext 010010010 and IV = c(0) = 001. The sender first uses the\nIV to calculate c(1) = KS(m(1) \u0002 c(0)) = 100. The sender then calculates c(2) =\nKS(m(2) \u0002 c(1)) = KS(010 \u0002 100) = 000, and c(3) = KS(m(3) \u0002 c(2)) = KS(010 \u0002\n000) = 101. The reader should verify that the receiver, knowing the IV and KS can\nrecover the original plaintext.\nCBC has an important consequence when designing secure network protocols:\nwe’ll need to provide a mechanism within the protocol to distribute the IV from\nsender to receiver. We’ll see how this is done for several protocols later in this\nchapter.\n682\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS"
    },
    {
      "chunk_id": "f726e637-f6af-41cb-b34f-59ee871f6981",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.2.2 Public Key Encryption",
      "original_titles": [
        "8.2.2 Public Key Encryption"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.2 Principles of Cryptography > 8.2.2 Public Key Encryption",
      "start_page": 710,
      "end_page": 714,
      "token_count": 2896,
      "text": "8.2.2 Public Key Encryption\nFor more than 2,000 years (since the time of the Caesar cipher and up to the\n1970s), encrypted communication required that the two communicating parties\nshare a common secret—the symmetric key used for encryption and decryption.\nOne difficulty with this approach is that the two parties must somehow agree on\nthe shared key; but to do so requires (presumably secure) communication! Perhaps\nthe parties could first meet and agree on the key in person (for example, two of\nCaesar’s centurions might meet at the Roman baths) and thereafter communicate\nwith encryption. In a networked world, however, communicating parties may never\nmeet and may never converse except over the network. Is it possible for two par-\nties to communicate with encryption without having a shared secret key that is\nknown in advance? In 1976, Diffie and Hellman [Diffie 1976] demonstrated an\nalgorithm (known now as Diffie-Hellman Key Exchange) to do just that—a radi-\ncally different and marvelously elegant approach toward secure communication\nthat has led to the development of today’s public key cryptography systems. We’ll\nsee shortly that public key cryptography systems also have several wonderful\nproperties that make them useful not only for encryption, but for authentication\nand digital signatures as well. Interestingly, it has recently come to light that\nideas similar to those in [Diffie 1976] and [RSA 1978] had been independently\ndeveloped in the early 1970s in a series of secret reports by researchers at the\nCommunications-Electronics Security Group in the United Kingdom [Ellis\n1987]. As is often the case, great ideas can spring up independently in many\nplaces; fortunately, public key advances took place not only in private, but also\nin the public view, as well.\nThe use of public key cryptography is conceptually quite simple. Suppose Alice\nwants to communicate with Bob. As shown in Figure 8.6, rather than Bob and Alice\n8.2\n•\nPRINCIPLES OF CRYPTOGRAPHY\n683\nEncryption\nalgorithm\nCiphertext\nDecryption\nalgorithm\nPlaintext\nmessage, m\nPlaintext\nmessage, m\nPrivate decryption key\nm = KB\n–(KB\n+(m))\nKB\n–\nKB\n+(m)\nPublic encryption key\nKB\n+\nFigure 8.6 \u0002 Public key cryptography\n\nsharing a single secret key (as in the case of symmetric key systems), Bob (the recip-\nient of Alice’s messages) instead has two keys—a public key that is available to\neveryone in the world (including Trudy the intruder) and a private key that is known\nonly to Bob. We will use the notation KB\n+ and KB\n– to refer to Bob’s public and private\nkeys, respectively. In order to communicate with Bob, Alice first fetches Bob’s pub-\nlic key. Alice then encrypts her message, m, to Bob using Bob’s public key and a\nknown (for example, standardized) encryption algorithm; that is, Alice computes\nKB\n+(m). Bob receives Alice’s encrypted message and uses his private key and a known\n(for example, standardized) decryption algorithm to decrypt Alice’s encrypted mes-\nsage. That is, Bob computes KB\n–(KB\n+(m)). We will see below that there are encryp-\ntion/decryption algorithms and techniques for choosing public and private keys such\nthat KB\n–(KB\n+(m)) = m; that is, applying Bob’s public key, KB\n+, to a message, m (to get\nKB\n+(m)), and then applying Bob’s private key, KB\n–, to the encrypted version of m (that\nis, computing KB\n–(KB\n+(m))) gives back m. This is a remarkable result! In this manner,\nAlice can use Bob’s publicly available key to send a secret message to Bob without\neither of them having to distribute any secret keys! We will see shortly that we can\ninterchange the public key and private key encryption and get the same remarkable\nresult––that is, KB\n– (B\n+(m)) = KB\n+ (KB\n–(m)) = m.\nThe use of public key cryptography is thus conceptually simple. But two imme-\ndiate worries may spring to mind. A first concern is that although an intruder inter-\ncepting Alice’s encrypted message will see only gibberish, the intruder knows both\nthe key (Bob’s public key, which is available for all the world to see) and the algo-\nrithm that Alice used for encryption. Trudy can thus mount a chosen-plaintext\nattack, using the known standardized encryption algorithm and Bob’s publicly avail-\nable encryption key to encode any message she chooses! Trudy might well try, for\nexample, to encode messages, or parts of messages, that she suspects that Alice\nmight send. Clearly, if public key cryptography is to work, key selection and encryp-\ntion/decryption must be done in such a way that it is impossible (or at least so hard\nas to be nearly impossible) for an intruder to either determine Bob’s private key or\nsomehow otherwise decrypt or guess Alice’s message to Bob. A second concern is\nthat since Bob’s encryption key is public, anyone can send an encrypted message to\nBob, including Alice or someone claiming to be Alice. In the case of a single shared\nsecret key, the fact that the sender knows the secret key implicitly identifies the\nsender to the receiver. In the case of public key cryptography, however, this is no\nlonger the case since anyone can send an encrypted message to Bob using Bob’s\npublicly available key. A digital signature, a topic we will study in Section 8.3, is\nneeded to bind a sender to a message.\nRSA\nWhile there may be many algorithms that address these concerns, the RSA algo-\nrithm (named after its founders, Ron Rivest, Adi Shamir, and Leonard Adleman)\nhas become almost synonymous with public key cryptography. Let’s first see how\nRSA works and then examine why it works.\n684\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nRSA makes extensive use of arithmetic operations using modulo-n arithmetic.\nSo let’s briefly review modular arithmetic. Recall that x mod n simply means the\nremainder of x when divided by n; so, for example, 19 mod 5 = 4. In modular arith-\nmetic, one performs the usual operations of addition, multiplication, and exponenti-\nation. However, the result of each operation is replaced by the integer remainder that\nis left when the result is divided by n. Adding and multiplying with modular arith-\nmetic is facilitated with the following handy facts:\n[(a mod n) + (b mod n)] mod n = (a + b) mod n\n[(a mod n) – (b mod n)] mod n = (a – b) mod n\n[(a mod n) • (b mod n)] mod n = (a • b) mod n\nIt follows from the third fact that (a mod n)d mod n = ad mod n, which is an identity\nthat we will soon find very useful.\nNow suppose that Alice wants to send to Bob an RSA-encrypted message, as\nshown in Figure 8.6. In our discussion of RSA, let’s always keep in mind that a mes-\nsage is nothing but a bit pattern, and every bit pattern can be uniquely represented\nby an integer number (along with the length of the bit pattern). For example, suppose\na message is the bit pattern 1001; this message can be represented by the decimal\ninteger 9. Thus, when encrypting a message with RSA, it is equivalent to encrypting\nthe unique integer number that represents the message.\nThere are two interrelated components of RSA:\n•\nThe choice of the public key and the private key\n•\nThe encryption and decryption algorithm\nTo generate the public and private RSA keys, Bob performs the following steps:\n1. Choose two large prime numbers, p and q. How large should p and q be? The\nlarger the values, the more difficult it is to break RSA, but the longer it takes to\nperform the encoding and decoding. RSA Laboratories recommends that the\nproduct of p and q be on the order of 1,024 bits. For a discussion of how to\nfind large prime numbers, see [Caldwell 2012].\n2. Compute n = pq and z = (p – 1)(q – 1).\n3. Choose a number, e, less than n, that has no common factors (other than 1)\nwith z. (In this case, e and z are said to be relatively prime.) The letter e is used\nsince this value will be used in encryption.\n4. Find a number, d, such that ed – 1 is exactly divisible (that is, with no remain-\nder) by z. The letter d is used because this value will be used in decryption. Put\nanother way, given e, we choose d such that\ned mod z = 1\n5. The public key that Bob makes available to the world, KB\n+, is the pair of num-\nbers (n, e); his private key, KB\n–, is the pair of numbers (n, d).\n8.2\n•\nPRINCIPLES OF CRYPTOGRAPHY\n685\n\nThe encryption by Alice and the decryption by Bob are done as follows:\n•\nSuppose Alice wants to send Bob a bit pattern represented by the integer number m\n(with m < n). To encode, Alice performs the exponentiation me, and then computes\nthe integer remainder when me is divided by n. In other words, the encrypted\nvalue, c, of Alice’s plaintext message, m, is\nc = me mod n\nThe bit pattern corresponding to this ciphertext c is sent to Bob.\n•\nTo decrypt the received ciphertext message, c, Bob computes\nm = cd mod n\nwhich requires the use of his private key (n,d).\nAs a simple example of RSA, suppose Bob chooses p = 5 and q = 7. (Admit-\ntedly, these values are far too small to be secure.) Then n = 35 and z = 24. Bob\nchooses e = 5, since 5 and 24 have no common factors. Finally, Bob chooses d = 29,\nsince 5 \u0003 29 – 1 (that is, ed – 1) is exactly divisible by 24. Bob makes the two val-\nues, n = 35 and e = 5, public and keeps the value d = 29 secret. Observing these two\npublic values, suppose Alice now wants to send the letters l, o, v, and e to Bob. Inter-\npreting each letter as a number between 1 and 26 (with a being 1, and z being 26),\nAlice and Bob perform the encryption and decryption shown in Tables 8.2 and 8.3,\nrespectively. Note that in this example, we consider each of the four letters as a dis-\ntinct message. A more realistic example would be to convert the four letters into\ntheir 8-bit ASCII representations and then encrypt the integer corresponding to the\nresulting 32-bit bit pattern. (Such a realistic example generates numbers that are\nmuch too long to print in a textbook!)\nGiven that the “toy” example in Tables 8.2 and 8.3 has already produced some\nextremely large numbers, and given that we saw earlier that p and q should each be\nseveral hundred bits long, several practical issues regarding RSA come to mind.\n686\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nTable 8.2 \u0002 Alice’s RSA encryption, e = 5, n = 35\nPlaintext Letter\nm: numeric representation\nm e\nCiphertext c = m e mod n\nl\n12\n248832\n17\no\n15\n759375\n15\nv\n22\n5153632\n22\ne\n5\n3125\n10\n\nHow does one choose large prime numbers? How does one then choose e and d?\nHow does one perform exponentiation with large numbers? A discussion of these\nimportant issues is beyond the scope of this book; see [Kaufman 1995] and the ref-\nerences therein for details.\nSession Keys\nWe note here that the exponentiation required by RSA is a rather time-consuming\nprocess. By contrast, DES is at least 100 times faster in software and between 1,000\nand 10,000 times faster in hardware [RSA Fast 2012]. As a result, RSA is often used\nin practice in combination with symmetric key cryptography. For example, if Alice\nwants to send Bob a large amount of encrypted data, she could do the following.\nFirst Alice chooses a key that will be used to encode the data itself; this key is\nreferred to as a session key, and is denoted by KS. Alice must inform Bob of the ses-\nsion key, since this is the shared symmetric key they will use with a symmetric key\ncipher (e.g., with DES or AES). Alice encrypts the session key using Bob’s public\nkey, that is, computes c = (KS)e mod n. Bob receives the RSA-encrypted session key,\nc, and decrypts it to obtain the session key, KS. Bob now knows the session key that\nAlice will use for her encrypted data transfer.\nWhy Does RSA Work?\nRSA encryption/decryption appears rather magical. Why should it be that by apply-\ning the encryption algorithm and then the decryption algorithm, one recovers the\noriginal message? In order to understand why RSA works, again denote n = pq,\nwhere p and q are the large prime numbers used in the RSA algorithm.\nRecall that, under RSA encryption, a message (uniquely represented by an inte-\nger), m, is exponentiated to the power e using modulo-n arithmetic, that is,\nc = me mod n\nDecryption is performed by raising this value to the power d, again using modulo-n\narithmetic. The result of an encryption step followed by a decryption step is thus\n8.2\n•\nPRINCIPLES OF CRYPTOGRAPHY\n687\nTable 8.3 \u0002 Bob’s RSA decryption, d = 29, n = 35\nCiphertext c\ncd\nm = cd mod n\nPlaintext Letter\n17\n4819685721067509150915091411825223071697\n12\nl\n15\n127834039403948858939111232757568359375\n15\no\n22\n851643319086537701956194499721106030592\n22\nv\n10\n1000000000000000000000000000000\n5\ne"
    },
    {
      "chunk_id": "23710dbe-d994-49e9-9c4e-60efd03732f7",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.3 Message Integrity and Digital Signatures",
      "original_titles": [
        "8.3 Message Integrity and Digital Signatures"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.3 Message Integrity and Digital Signatures",
      "start_page": 715,
      "end_page": 715,
      "token_count": 631,
      "text": "(me mod n)d mod n . Let’s now see what we can say about this quantity. As mentioned\nearlier, one important property of modulo arithmetic is (a mod n)d mod n = ad mod\nn for any values a, n, and d. Thus, using a = me in this property, we have\n(me mod n)d mod n = med mod n\nIt therefore remains to show that med mod n = m. Although we’re trying to remove\nsome of the magic about why RSA works, to establish this, we’ll need to use a rather\nmagical result from number theory here. Specifically, we’ll need the result that says if\np and q are prime, n = pq, and z = (p – 1)(q – 1), then xy mod n is the same as x(y mod z)\nmod n [Kaufman 1995]. Applying this result with x = m and y = ed we have\nmed mod n = m(ed mod z) mod n\nBut remember that we have chosen e and d such that ed mod z = 1. This gives us\nmed mod n = m1 mod n = m\nwhich is exactly the result we are looking for! By first exponentiating to the power\nof e (that is, encrypting) and then exponentiating to the power of d (that is, decrypt-\ning), we obtain the original value, m. Even more wonderful is the fact that if we first\nexponentiate to the power of d and then exponentiate to the power of e—that is, we\nreverse the order of encryption and decryption, performing the decryption operation\nfirst and then applying the encryption operation—we also obtain the original value,\nm. This wonderful result follows immediately from the modular arithmetic:\n(md mod n)e mod n = mde mod n = med mod n = (me mod n)d mod n\n688\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nThe security of RSA relies on the fact that there are no known algorithms for\nquickly factoring a number, in this case the public value n, into the primes p and q. If\none knew p and q, then given the public value e, one could easily compute the secret\nkey, d. On the other hand, it is not known whether or not there exist fast algorithms for\nfactoring a number, and in this sense, the security of RSA is not guaranteed.\nAnother popular public-key encryption algorithm is the Diffie-Hellman algo-\nrithm, which we will briefly explore in the homework problems. Diffie-Hellman is\nnot as versatile as RSA in that it cannot be used to encrypt messages of arbitrary\nlength; it can be used, however, to establish a symmetric session key, which is in\nturn used to encrypt messages.\n8.3\nMessage Integrity and Digital Signatures\nIn the previous section we saw how encryption can be used to provide confidential-\nity to two communicating entities. In this section we turn to the equally important"
    },
    {
      "chunk_id": "df3fcdec-8beb-458c-8edf-14d056671ff3",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.3.1 Cryptographic Hash Functions",
      "original_titles": [
        "8.3.1 Cryptographic Hash Functions"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.3 Message Integrity and Digital Signatures > 8.3.1 Cryptographic Hash Functions",
      "start_page": 716,
      "end_page": 717,
      "token_count": 1002,
      "text": "cryptography topic of providing message integrity (also known as message authen-\ntication). Along with message integrity, we will discuss two related topics in this\nsection: digital signatures and end-point authentication.\nWe define the message integrity problem using, once again, Alice and Bob.\nSuppose Bob receives a message (which may be encrypted or may be in plaintext)\nand he believes this message was sent by Alice. To authenticate this message, Bob\nneeds to verify:\n1. The message indeed originated from Alice.\n2. The message was not tampered with on its way to Bob.\nWe’ll see in Sections 8.4 through 8.7 that this problem of message integrity is a\ncritical concern in just about all secure networking protocols.\nAs a specific example, consider a computer network using a link-state routing\nalgorithm (such as OSPF) for determining routes between each pair of routers in\nthe network (see Chapter 4). In a link-state algorithm, each router needs to broad-\ncast a link-state message to all other routers in the network. A router’s link-state\nmessage includes a list of its directly connected neighbors and the direct costs to\nthese neighbors. Once a router receives link-state messages from all of the other\nrouters, it can create a complete map of the network, run its least-cost routing\nalgorithm, and configure its forwarding table. One relatively easy attack on the\nrouting algorithm is for Trudy to distribute bogus link-state messages with incor-\nrect link-state information. Thus the need for message integrity—when router B\nreceives a link-state message from router A, router B should verify that router A\nactually created the message and, further, that no one tampered with the message\nin transit.\nIn this section, we describe a popular message integrity technique that is used\nby many secure networking protocols. But before doing so, we need to cover\nanother important topic in cryptography—cryptographic hash functions.\n8.3.1 Cryptographic Hash Functions\nAs shown in Figure 8.7, a hash function takes an input, m, and computes a fixed-\nsize string H(m) known as a hash. The Internet checksum (Chapter 3) and CRCs\n(Chapter 4) meet this definition. A cryptographic hash function is required to have\nthe following additional property:\n•\nIt is computationally infeasible to find any two different messages x and y such\nthat H(x) = H(y).\nInformally, this property means that it is computationally infeasible for an\nintruder to substitute one message for another message that is protected by the hash\nfunction. That is, if (m, H(m)) are the message and the hash of the message created\n8.3\n•\nMESSAGE INTEGRITY AND DIGITAL SIGNATURES\n689\n\nby the sender, then an intruder cannot forge the contents of another message, y, that\nhas the same hash value as the original message.\nLet’s convince ourselves that a simple checksum, such as the Internet check-\nsum, would make a poor cryptographic hash function. Rather than performing 1s\ncomplement arithmetic (as in the Internet checksum), let us compute a checksum\nby treating each character as a byte and adding the bytes together using 4-byte\nchunks at a time. Suppose Bob owes Alice $100.99 and sends an IOU to Alice\nconsisting of the text string “IOU100.99BOB.” The ASCII representation (in\nhexadecimal notation) for these letters is 49, 4F, 55, 31, 30, 30, 2E, 39, 39,\n42, 4F, 42.\nFigure 8.8 (top) shows that the 4-byte checksum for this message is B2 C1\nD2 AC. A slightly different message (and a much more costly one for Bob) is\nshown in the bottom half of Figure 8.8. The messages “IOU100.99BOB” and\n“IOU900.19BOB” have the same checksum. Thus, this simple checksum algo-\nrithm violates the requirement above. Given the original data, it is simple to find\nanother set of data with the same checksum. Clearly, for security purposes, we are\ngoing to need a more powerful hash function than a checksum.\nThe MD5 hash algorithm of Ron Rivest [RFC 1321] is in wide use today. It\ncomputes a 128-bit hash in a four-step process consisting of a padding step\n(adding a one followed by enough zeros so that the length of the message satisfies\ncertain conditions), an append step (appending a 64-bit representation of the mes-\nsage length before padding), an initialization of an accumulator, and a final loop-\ning step in which the message’s 16-word blocks are processed (mangled) in four\nrounds. For a description of MD5 (including a C source code implementation) see\n[RFC 1321].\n690\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nMany-to-one\nhash function\nLong message: m\nDear Alice:\nThis is a VERY long letter\nsince there is so much to\nsay.....\n..........\n..........\nBob\nFixed-length\nhash: H(m)\nOpgmdvboijrtnsd\ngghPPdogm;lcvkb\nFigure 8.7 \u0002 Hash functions"
    },
    {
      "chunk_id": "8ad77ede-963c-4d5d-87da-7516f32748ee",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.3.2 Message Authentication Code",
      "original_titles": [
        "8.3.2 Message Authentication Code"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.3 Message Integrity and Digital Signatures > 8.3.2 Message Authentication Code",
      "start_page": 718,
      "end_page": 719,
      "token_count": 767,
      "text": "The second major hash algorithm in use today is the Secure Hash Algorithm\n(SHA-1) [FIPS 1995]. This algorithm is based on principles similar to those used in\nthe design of MD4 [RFC 1320], the predecessor to MD5. SHA-1, a US federal\nstandard, is required for use whenever a cryptographic hash algorithm is needed for\nfederal applications. It produces a 160-bit message digest. The longer output length\nmakes SHA-1 more secure.\n8.3.2 Message Authentication Code\nLet’s now return to the problem of message integrity. Now that we understand hash\nfunctions, let’s take a first stab at how we might perform message integrity:\n1. Alice creates message m and calculates the hash H(m) (for example with\nSHA-1).\n2. Alice then appends H(m) to the message m, creating an extended message\n(m, H(m)), and sends the extended message to Bob.\n3. Bob receives an extended message (m, h) and calculates H(m). If H(m) = h,\nBob concludes that everything is fine.\nThis approach is obviously flawed. Trudy can create a bogus message m´ in which\nshe says she is Alice, calculate H(m´), and send Bob (m´, H(m´)). When Bob receives\nthe message, everything checks out in step 3, so Bob doesn’t suspect any funny\nbusiness.\nFigure 8.8 \u0002 Initial message and fraudulent message have the same \nchecksum!\nMessage\nI O U 1\n0 0 . 9\n9 B O B\nASCII\nRepresentation\n49\n4F\n55\n31\n30\n30\n2E\n39\n39\n42\n4F\n42\nB2\nC1\nD2\nAC\nChecksum\nMessage\nI O U 9\n0 0 . 1\n9 B O B\nASCII\nRepresentation\n49\n4F\n55\n39\n30\n30\n2E\n31\n39\n42\n4F\n42\nB2\nC1\nD2\nAC\nChecksum\n8.3\n•\nMESSAGE INTEGRITY AND DIGITAL SIGNATURES\n691\n\nTo perform message integrity, in addition to using cryptographic hash func-\ntions, Alice and Bob will need a shared secret s. This shared secret, which is nothing\nmore than a string of bits, is called the authentication key. Using this shared secret,\nmessage integrity can be performed as follows:\n1. Alice creates message m, concatenates s with m to create m + s, and calculates\nthe hash H(m + s) (for example with SHA-1). H(m + s) is called the message\nauthentication code (MAC).\n2. Alice then appends the MAC to the message m, creating an extended message\n(m, H(m + s)), and sends the extended message to Bob.\n3. Bob receives an extended message (m, h) and knowing s, calculates the MAC\nH(m + s). If H(m + s) = h, Bob concludes that everything is fine.\nA summary of the procedure is shown in Figure 8.9. Readers should note that the\nMAC here (standing for “message authentication code”) is not the same MAC used\nin link-layer protocols (standing for “medium access control”)!\nOne nice feature of a MAC is that it does not require an encryption algorithm.\nIndeed, in many applications, including the link-state routing algorithm described\nearlier, communicating entities are only concerned with message integrity and are\nnot concerned with message confidentiality. Using a MAC, the entities can authenti-\ncate the messages they send to each other without having to integrate complex\nencryption algorithms into the integrity process.\nAs you might expect, a number of different standards for MACs have been pro-\nposed over the years. The most popular standard today is HMAC, which can be\n692\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nH(.)\nH(.)\nm\nm\nm\nm\ns\ns\ns\n+\nInternet\nCompare\nKey:\n= Message\n= Shared secret\nH(m+s)\nH(m+s)\nFigure 8.9 \u0002 Message authentication code (MAC)"
    },
    {
      "chunk_id": "cd26f741-126e-416c-90a6-a16f2a8af987",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.3.3 Digital Signatures",
      "original_titles": [
        "8.3.3 Digital Signatures"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.3 Message Integrity and Digital Signatures > 8.3.3 Digital Signatures",
      "start_page": 720,
      "end_page": 726,
      "token_count": 3160,
      "text": "used either with MD5 or SHA-1. HMAC actually runs data and the authentication\nkey through the hash function twice [Kaufman 1995; RFC 2104].\nThere still remains an important issue. How do we distribute the shared authen-\ntication key to the communicating entities? For example, in the link-state routing\nalgorithm, we would somehow need to distribute the secret authentication key to\neach of the routers in the autonomous system. (Note that the routers can all use\nthe same authentication key.) A network administrator could actually accomplish\nthis by physically visiting each of the routers. Or, if the network administrator is\na lazy guy, and if each router has its own public key, the network administrator\ncould distribute the authentication key to any one of the routers by encrypting it\nwith the router’s public key and then sending the encrypted key over the network\nto the router.\n8.3.3 Digital Signatures\nThink of the number of the times you’ve signed your name to a piece of paper dur-\ning the last week. You sign checks, credit card receipts, legal documents, and letters.\nYour signature attests to the fact that you (as opposed to someone else) have\nacknowledged and/or agreed with the document’s contents. In a digital world, one\noften wants to indicate the owner or creator of a document, or to signify one’s agree-\nment with a document’s content. A digital signature is a cryptographic technique\nfor achieving these goals in a digital world.\nJust as with handwritten signatures, digital signing should be done in a way that\nis verifiable and nonforgeable. That is, it must be possible to prove that a document\nsigned by an individual was indeed signed by that individual (the signature must be\nverifiable) and that only that individual could have signed the document (the signa-\nture cannot be forged).\nLet’s now consider how we might design a digital signature scheme. Observe\nthat when Bob signs a message, Bob must put something on the message that is\nunique to him. Bob could consider attaching a MAC for the signature, where the\nMAC is created by appending his key (unique to him) to the message, and then taking\nthe hash. But for Alice to verify the signature, she must also have a copy of the key,\nin which case the key would not be unique to Bob. Thus, MACs are not going to get\nthe job done here.\nRecall that with public-key cryptography, Bob has both a public and private\nkey, with both of these keys being unique to Bob. Thus, public-key cryptography is\nan excellent candidate for providing digital signatures. Let us now examine how it\nis done.\nSuppose that Bob wants to digitally sign a document, m. We can think of the\ndocument as a file or a message that Bob is going to sign and send. As shown in\nFigure 8.10, to sign this document, Bob simply uses his private key, KB\n–, to com-\npute KB\n–(m). At first, it might seem odd that Bob is using his private key (which, as\nwe saw in Section 8.2, was used to decrypt a message that had been encrypted\n8.3\n•\nMESSAGE INTEGRITY AND DIGITAL SIGNATURES\n693\n\nwith his public key) to sign a document. But recall that encryption and decryption\nare nothing more than mathematical operations (exponentiation to the power of e\nor d in RSA; see Section 8.2) and recall that Bob’s goal is not to scramble or\nobscure the contents of the document, but rather to sign the document in a man-\nner that is verifiable and nonforgeable. Bob’s digital signature of the document is\nKB\n–(m).\nDoes the digital signature KB\n–(m) meet our requirements of being verifiable and\nnonforgeable? Suppose Alice has m and KB\n–(m). She wants to prove in court (being\nlitigious) that Bob had indeed signed the document and was the only person who\ncould have possibly signed the document. Alice takes Bob’s public key, KB\n+, and\napplies it to the digital signature, KB\n–(m), associated with the document, m. That is,\nshe computes KB\n+(KB\n–(m)), and voilà, with a dramatic flurry, she produces m, which\nexactly matches the original document! Alice then argues that only Bob could have\nsigned the document, for the following reasons:\n•\nWhoever signed the message must have used the private key, KB\n–, in computing\nthe signature KB\n–(m), such that KB\n+(KB\n–(m)) = m.\n•\nThe only person who could have known the private key, KB\n–, is Bob. Recall from\nour discussion of RSA in Section 8.2 that knowing the public key, KB\n+, is of no\nhelp in learning the private key, KB\n–. Therefore, the only person who could know\nKB\n– is the person who generated the pair of keys, (KB\n+, KB\n–), in the first place, Bob.\n(Note that this assumes, though, that Bob has not given KB\n– to anyone, nor has\nanyone stolen KB\n– from Bob.)\n694\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nEncryption\nalgorithm\nMessage: m\nBob’s private\nkey, KB\n–\nDear Alice:\nSorry I have been unable\nto write for so long. Since\nwe.....\n..........\n..........\nBob\nSigned message:\nKB\n– (m)\nfadfg54986fgnzmcnv\nT98734ngldskg02j\nser09tugkjdflg\n..........\nFigure 8.10 \u0002 Creating a digital signature for a document\n\nIt is also important to note that if the original document, m, is ever modified to\nsome alternate form, m´, the signature that Bob created for m will not be valid for m´,\nsince KB\n+(KB\n–(m)) does not equal m´. Thus we see that digital signatures also provide\nmessage integrity, allowing the receiver to verify that the message was unaltered as\nwell as the source of the message.\nOne concern with signing data by encryption is that encryption and decryption\nare computationally expensive. Given the overheads of encryption and decryption,\nsigning data via complete encryption/decryption can be overkill. A more efficient\napproach is to introduce hash functions into the digital signature. Recall from Sec-\ntion 8.3.2 that a hash algorithm takes a message, m, of arbitrary length and computes\na fixed-length “fingerprint” of the message, denoted by H(m). Using a hash func-\ntion, Bob signs the hash of a message rather than the message itself, that is, Bob cal-\nculates KB\n–(H(m)). Since H(m) is generally much smaller than the original message\nm, the computational effort required to create the digital signature is substantially\nreduced.\nIn the context of Bob sending a message to Alice, Figure 8.11 provides a sum-\nmary of the operational procedure of creating a digital signature. Bob puts his origi-\nnal long message through a hash function. He then digitally signs the resulting hash\nBob’s private\nkey, KB\n–\nMany-to-one\nhash function\nLong message\nDear Alice: \nThis is a VERY long letter \nsince there is so much to \nsay..... \n..........\n..........\nBob\nFixed-length\nhash\nOpgmdvboijrtnsd\ngghPPdogm;lcvkb\nSigned\nhash\nPackage to send\nto Alice\nFgkopdgoo69cmxw\n54psdterma[asofmz\nEncryption\nalgorithm\nFigure 8.11 \u0002 Sending a digitally signed message\n8.3\n•\nMESSAGE INTEGRITY AND DIGITAL SIGNATURES\n695\n\nwith his private key. The original message (in cleartext) along with the digitally\nsigned message digest (henceforth referred to as the digital signature) is then sent to\nAlice. Figure 8.12 provides a summary of the operational procedure of the signa-\nture. Alice applies the sender’s public key to the message to obtain a hash result.\nAlice also applies the hash function to the cleartext message to obtain a second hash\nresult. If the two hashes match, then Alice can be sure about the integrity and author\nof the message.\nBefore moving on, let’s briefly compare digital signatures with MACs, since\nthey have parallels, but also have important subtle differences. Both digital signatures\nand MACs start with a message (or a document). To create a MAC out of the mes-\nsage, we append an authentication key to the message, and then take the hash of the\nresult. Note that neither public key nor symmetric key encryption is involved in cre-\nating the MAC. To create a digital signature, we first take the hash of the message\nand then encrypt the message with our private key (using public key cryptography).\n696\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nBob’s public\nkey, KB\n+\nLong message\nDear Alice: \nThis is a VERY long letter \nsince there is so much to \nsay..... \n..........\n..........\nBob\nFixed-length\nhash\nOpgmdvboijrtnsd\ngghPPdogm;lcvkb\nSigned\nhash\nFgkopdgoo69cmxw\n54psdterma[asofmz\nMany-to-one\nhash function\nCompare\nFixed-length\nhash\nOpgmdvboijrtnsd\ngghPPdogm;lcvkb\nEncryption\nalgorithm\nFigure 8.12 \u0002 Verifying a signed message\n\nThus, a digital signature is a “heavier” technique, since it requires an underlying \nPublic Key Infrastructure (PKI) with certification authorities as described below.\nWe’ll see in Section 8.4 that PGP—a popular secure e-mail system—uses digital \nsignatures for message integrity. We’ve seen already that OSPF uses MACs for \nmessage integrity. We’ll see in Sections 8.5 and 8.6 that MACs are also used for pop-\nular transport-layer and network-layer security protocols.\nPublic Key Certification\nAn important application of digital signatures is public key certification, that is,\ncertifying that a public key belongs to a specific entity. Public key certification is\nused in many popular secure networking protocols, including IPsec and SSL.\nTo gain insight into this problem, let’s consider an Internet-commerce version\nof the classic “pizza prank.” Alice is in the pizza delivery business and accepts\norders over the Internet. Bob, a pizza lover, sends Alice a plaintext message that\nincludes his home address and the type of pizza he wants. In this message, Bob also\nincludes a digital signature (that is, a signed hash of the original plaintext message)\nto prove to Alice that he is the true source of the message. To verify the signature,\nAlice obtains Bob’s public key (perhaps from a public key server or from the e-mail\nmessage) and checks the digital signature. In this manner she makes sure that Bob,\nrather than some adolescent prankster, placed the order.\nThis all sounds fine until clever Trudy comes along. As shown in Figure 8.13,\nTrudy is indulging in a prank. She sends a message to Alice in which she says she is\nBob, gives Bob’s home address, and orders a pizza. In this message she also\nincludes her (Trudy’s) public key, although Alice naturally assumes it is Bob’s pub-\nlic key. Trudy also attaches a digital signature, which was created with her own\n(Trudy’s) private key. After receiving the message, Alice applies Trudy’s public key\n(thinking that it is Bob’s) to the digital signature and concludes that the plaintext\nmessage was indeed created by Bob. Bob will be very surprised when the delivery\nperson brings a pizza with pepperoni and anchovies to his home!\nWe see from this example that for public key cryptography to be useful, you\nneed to be able to verify that you have the actual public key of the entity (person,\nrouter, browser, and so on) with whom you want to communicate. For example, when\nAlice wants to communicate with Bob using public key cryptography, she needs to\nverify that the public key that is supposed to be Bob’s is indeed Bob’s.\nBinding a public key to a particular entity is typically done by a Certification\nAuthority (CA), whose job is to validate identities and issue certificates. A CA has\nthe following roles:\n1. A CA verifies that an entity (a person, a router, and so on) is who it says it is.\nThere are no mandated procedures for how certification is done. When dealing\nwith a CA, one must trust the CA to have performed a suitably rigorous identity\nverification. For example, if Trudy were able to walk into the Fly-by-Night CA\n8.3\n•\nMESSAGE INTEGRITY AND DIGITAL SIGNATURES\n697\n\nand simply announce “I am Alice” and receive certificates associated with the\nidentity of Alice, then one shouldn’t put much faith in public keys certified by\nthe Fly-by-Night CA. On the other hand, one might (or might not!) be more\nwilling to trust a CA that is part of a federal or state program. You can trust the\nidentity associated with a public key only to the extent to which you can trust a\nCA and its identity verification techniques. What a tangled web of trust we spin!\n2. Once the CA verifies the identity of the entity, the CA creates a certificate that\nbinds the public key of the entity to the identity. The certificate contains the\npublic key and globally unique identifying information about the owner of the\npublic key (for example, a human name or an IP address). The certificate is\ndigitally signed by the CA. These steps are shown in Figure 8.14.\nLet us now see how certificates can be used to combat pizza-ordering\npranksters, like Trudy, and other undesirables. When Bob places his order he also\nsends his CA-signed certificate. Alice uses the CA’s public key to check the validity\nof Bob’s certificate and extract Bob’s public key.\n698\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nTrudy’s private\nkey, KT\n–\nTrudy’s public\nkey, KT\n+\nSigned (using\nTrudy's private key)\nmessage digest\nFgkopdgoo69cmxw\n54psdterma[asofmz\nMessage\nAlice,\nDeliver a pizza to me.\n                               Bob\nMany-to-one\nhash function\nAlice uses Trudy’s\npublic key, thinking\nit is Bob’s, and\nconcludes the\nmessage is from Bob\nPIZZA\nEncryption\nalgorithm\nFigure 8.13 \u0002 Trudy masquerades as Bob using public key cryptography\n\nBoth the International Telecommunication Union (ITU) and the IETF have\ndeveloped standards for CAs. ITU X.509 [ITU 2005a] specifies an authentication\nservice as well as a specific syntax for certificates. [RFC 1422] describes CA-based\nkey management for use with secure Internet e-mail. It is compatible with X.509 but\ngoes beyond X.509 by establishing procedures and conventions for a key manage-\nment architecture. Table 8.4 describes some of the important fields in a certificate.\nBob’s CA-signed\ncertificate containing\nhis public key, KB\n+\nCertification\nAuthority (CA)\n(KB\n+, B)\nCA’s private\nkey, KCA\n–\nEncryption\nalgorithm\nFigure 8.14 \u0002 Bob has his public key certified by the CA\nField Name\nDescription\nVersion\nVersion number of X.509 specification\nSerial number\nCA-issued unique identifier for a certificate\nSignature\nSpecifies the algorithm used by CA to sign this certificate\nIssuer name\nIdentity of CA issuing this certificate, in distinguished name (DN)[RFC 4514] format\nValidity period\nStart and end of period of validity for certificate\nSubject name\nIdentity of entity whose public key is associated with this certificate, in DN format\nSubject public key\nThe subject’s public key as well indication of the public key algorithm (and algorithm\nparameters) to be used with this key\nTable 8.4 \u0002 Selected fields in an X.509 and RFC 1422 public key\n8.3\n•\nMESSAGE INTEGRITY AND DIGITAL SIGNATURES\n699"
    },
    {
      "chunk_id": "0fa48a00-57d4-4102-9057-ee409f4da9c9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.4 End-Point Authentication",
      "original_titles": [
        "8.4 End-Point Authentication"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.4 End-Point Authentication",
      "start_page": 727,
      "end_page": 727,
      "token_count": 565,
      "text": "8.4\nEnd-Point Authentication\nEnd-point authentication is the process of one entity proving its identity to\nanother entity over a computer network, for example, a user proving its identity \nto an email server. As humans, we authenticate each other in many ways: We rec-\nognize each other’s faces when we meet, we recognize each other’s voices on the\ntelephone, we are authenticated by the customs official who checks us against the\npicture on our passport.\nIn this section, we consider how one party can authenticate another \nparty when the two are communicating over a network. We focus here on authen-\nticating a “live” party, at the point in time when communication is actually occur-\nring. A concrete example is a user authenticating him or herself to an e-mail\nserver. This is a subtly different problem from proving that a message received at\nsome point in the past did indeed come from that claimed sender, as studied in\nSection 8.3.\nWhen performing authentication over the network, the communicating par-\nties cannot rely on biometric information, such as a visual appearance or a voice-\nprint. Indeed, we will see in our later case studies that it is often network\nelements such as routers and client/server processes that must authenticate each\nother. Here, authentication must be done solely on the basis of messages and data\nexchanged as part of an authentication protocol. Typically, an authentication\nprotocol would run before the two communicating parties run some other proto-\ncol (for example, a reliable data transfer protocol, a routing information\nexchange protocol, or an e-mail protocol). The authentication protocol first\nestablishes the identities of the parties to each other’s satisfaction; only after\nauthentication do the parties get down to the work at hand.\nAs in the case of our development of a reliable data transfer (rdt) protocol in\nChapter 3, we will find it instructive here to develop various versions of an authen-\ntication protocol, which we will call ap (authentication protocol), and poke holes in\neach version as we proceed. (If you enjoy this stepwise evolution of a design, you\nmight also enjoy [Bryant 1988], which recounts a fictitious narrative between\ndesigners of an open-network authentication system, and their discovery of the\nmany subtle issues involved.)\nLet’s assume that Alice needs to authenticate herself to Bob.\n8.4.1 Authentication Protocol ap1.0\nPerhaps the simplest authentication protocol we can imagine is one where Alice\nsimply sends a message to Bob saying she is Alice. This protocol is shown in\nFigure 8.15. The flaw here is obvious—there is no way for Bob actually to know\n700\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS"
    },
    {
      "chunk_id": "53c61911-72d1-4f91-821b-87c06a103f67",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.4.1 Authentication Protocol ap1.0",
      "original_titles": [
        "8.4.1 Authentication Protocol ap1.0"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.4 End-Point Authentication > 8.4.1 Authentication Protocol ap1.0",
      "start_page": 727,
      "end_page": 727,
      "token_count": 565,
      "text": "8.4\nEnd-Point Authentication\nEnd-point authentication is the process of one entity proving its identity to\nanother entity over a computer network, for example, a user proving its identity \nto an email server. As humans, we authenticate each other in many ways: We rec-\nognize each other’s faces when we meet, we recognize each other’s voices on the\ntelephone, we are authenticated by the customs official who checks us against the\npicture on our passport.\nIn this section, we consider how one party can authenticate another \nparty when the two are communicating over a network. We focus here on authen-\nticating a “live” party, at the point in time when communication is actually occur-\nring. A concrete example is a user authenticating him or herself to an e-mail\nserver. This is a subtly different problem from proving that a message received at\nsome point in the past did indeed come from that claimed sender, as studied in\nSection 8.3.\nWhen performing authentication over the network, the communicating par-\nties cannot rely on biometric information, such as a visual appearance or a voice-\nprint. Indeed, we will see in our later case studies that it is often network\nelements such as routers and client/server processes that must authenticate each\nother. Here, authentication must be done solely on the basis of messages and data\nexchanged as part of an authentication protocol. Typically, an authentication\nprotocol would run before the two communicating parties run some other proto-\ncol (for example, a reliable data transfer protocol, a routing information\nexchange protocol, or an e-mail protocol). The authentication protocol first\nestablishes the identities of the parties to each other’s satisfaction; only after\nauthentication do the parties get down to the work at hand.\nAs in the case of our development of a reliable data transfer (rdt) protocol in\nChapter 3, we will find it instructive here to develop various versions of an authen-\ntication protocol, which we will call ap (authentication protocol), and poke holes in\neach version as we proceed. (If you enjoy this stepwise evolution of a design, you\nmight also enjoy [Bryant 1988], which recounts a fictitious narrative between\ndesigners of an open-network authentication system, and their discovery of the\nmany subtle issues involved.)\nLet’s assume that Alice needs to authenticate herself to Bob.\n8.4.1 Authentication Protocol ap1.0\nPerhaps the simplest authentication protocol we can imagine is one where Alice\nsimply sends a message to Bob saying she is Alice. This protocol is shown in\nFigure 8.15. The flaw here is obvious—there is no way for Bob actually to know\n700\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS"
    },
    {
      "chunk_id": "40e4b521-c20f-498e-9fcc-44300cc07855",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.4.2 Authentication Protocol ap2.0",
      "original_titles": [
        "8.4.2 Authentication Protocol ap2.0"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.4 End-Point Authentication > 8.4.2 Authentication Protocol ap2.0",
      "start_page": 728,
      "end_page": 728,
      "token_count": 382,
      "text": "Alice\nI am Alice\nBob\nTrudy\nTrudy\nAlice\nI am Alice\nBob\nFigure 8.15 \u0002 Protocol ap1.0 and a failure scenario\n8.4\n•\nEND-POINT AUTHENTICATION\n701\nthat the person sending the message “I am Alice” is indeed Alice. For example,\nTrudy (the intruder) could just as well send such a message.\n8.4.2 Authentication Protocol ap2.0\nIf Alice has a well-known network address (e.g., an IP address) from which she\nalways communicates, Bob could attempt to authenticate Alice by verifying that the\nsource address on the IP datagram carrying the authentication message matches\nAlice’s well-known address. In this case, Alice would be authenticated. This might\nstop a very network-naive intruder from impersonating Alice, but it wouldn’t stop\nthe determined student studying this book, or many others!\nFrom our study of the network and data link layers, we know that it is not that\nhard (for example, if one had access to the operating system code and could build\none’s own operating system kernel, as is the case with Linux and several other\nfreely available operating systems) to create an IP datagram, put whatever IP source\naddress we want (for example, Alice’s well-known IP address) into the IP datagram,\nand send the datagram over the link-layer protocol to the first-hop router. From then\non, the incorrectly source-addressed datagram would be dutifully forwarded to Bob.\nThis approach, shown in Figure 8.16, is a form of IP spoofing. IP spoofing can be\navoided if Trudy’s first-hop router is configured to forward only datagrams contain-\ning Trudy’s IP source address [RFC 2827]. However, this capability is not univer-\nsally deployed or enforced. Bob would thus be foolish to assume that Trudy’s\nnetwork manager (who might be Trudy herself) had configured Trudy’s first-hop\nrouter to forward only appropriately addressed datagrams."
    },
    {
      "chunk_id": "173df0fd-f066-4e2e-aa8d-4981b69f9856",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.4.3 Authentication Protocol ap3.0",
      "original_titles": [
        "8.4.3 Authentication Protocol ap3.0",
        "8.4.4 Authentication Protocol ap3.1"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.4 End-Point Authentication > 8.4.3 Authentication Protocol ap3.0",
      "start_page": 729,
      "end_page": 731,
      "token_count": 1142,
      "text": "8.4.3 Authentication Protocol ap3.0\nOne classic approach to authentication is to use a secret password. The password is\na shared secret between the authenticator and the person being authenticated. Gmail,\nFacebook, telnet, FTP, and many other services use password authentication. In pro-\ntocol ap3.0, Alice thus sends her secret password to Bob, as shown in Figure 8.17.\n702\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nAlice\nI am Alice,\npassword\nOK\nBob\nTrudy\nAlice\nI am Alice,\npassword\nOK\nBob\nTrudy\nTape recorder\nKey:\nFigure 8.17 \u0002 Protocol ap3.0 and a failure scenario\nAlice\nI am Alice\nAlice’s IP addr.\nBob\nTrudy\nAlice\nI am Alice\nAlice’s IP addr.\nBob\nTrudy\nFigure 8.16 \u0002 Protocol ap2.0 and a failure scenario\nSince passwords are so widely used, we might suspect that protocol ap3.0\nis fairly secure. If so, we’d be wrong! The security flaw here is clear. If Trudy\neavesdrops on Alice’s communication, then she can learn Alice’s password. Lest\nyou think this is unlikely, consider the fact that when you Telnet to another\nmachine and log in, the login password is sent unencrypted to the Telnet server.\nSomeone connected to the Telnet client or server’s LAN can possibly sniff \n(read and store) all packets transmitted on the LAN and thus steal the login pass-\nword. In fact, this is a well-known approach for stealing passwords (see, for\nexample, [Jimenez 1997]). Such a threat is obviously very real, so ap3.0 clearly\nwon’t do.\n8.4.4 Authentication Protocol ap3.1\nOur next idea for fixing ap3.0 is naturally to encrypt the password. By encrypting the\npassword, we can prevent Trudy from learning Alice’s password. If we assume that\nAlice and Bob share a symmetric secret key,\nthen Alice can encrypt \nthe password and send her identification message, “I am Alice,” and her encrypted\npassword to Bob. Bob then decrypts the password and, assuming the password is \ncorrect, authenticates Alice. Bob feels comfortable in authenticating Alice since Alice\nnot only knows the password, but also knows the shared secret key value needed to\nencrypt the password. Let’s call this protocol ap3.1.\nWhile it is true that ap3.1 prevents Trudy from learning Alice’s password,\nthe use of cryptography here does not solve the authentication problem. Bob is\nsubject to a playback attack: Trudy need only eavesdrop on Alice’s communica-\ntion, record the encrypted version of the password, and play back the encrypted\nversion of the password to Bob to pretend that she is Alice. The use of an\nencrypted password in ap3.1 doesn’t make the situation manifestly different from\nthat of protocol ap3.0 in Figure 8.17.\n8.4.5 Authentication Protocol ap4.0\nThe failure scenario in Figure 8.17 resulted from the fact that Bob could not dis-\ntinguish between the original authentication of Alice and the later playback of\nAlice’s original authentication. That is, Bob could not tell if Alice was live (that\nis, was currently really on the other end of the connection) or whether the mes-\nsages he was receiving were a recorded playback of a previous authentication of\nAlice. The very (very) observant reader will recall that the three-way TCP hand-\nshake protocol needed to address the same problem—the server side of a TCP\nconnection did not want to accept a connection if the received SYN segment was\nan old copy (retransmission) of a SYN segment from an earlier connection. How\nKA-B,\n8.4\n•\nEND-POINT AUTHENTICATION\n703\n\ndid the TCP server side solve the problem of determining whether the client was\nreally live? It chose an initial sequence number that had not been used in a very\nlong time, sent that number to the client, and then waited for the client to respond\nwith an ACK segment containing that number. We can adopt the same idea here\nfor authentication purposes.\nA nonce is a number that a protocol will use only once in a lifetime. That is,\nonce a protocol uses a nonce, it will never use that number again. Our ap4.0\nprotocol uses a nonce as follows:\n1. Alice sends the message “I am Alice” to Bob.\n2. Bob chooses a nonce, R, and sends it to Alice.\n3. Alice encrypts the nonce using Alice and Bob’s symmetric secret key, \nand sends the encrypted nonce, \n(R), back to Bob. As in protocol ap3.1,\nit is the fact that Alice knows \nand uses it to encrypt a value that lets \nBob know that the message he receives was generated by Alice. The nonce \nis used to ensure that Alice is live.\n4. Bob decrypts the received message. If the decrypted nonce equals the nonce he\nsent Alice, then Alice is authenticated.\nProtocol ap4.0 is illustrated in Figure 8.18. By using the once-in-a-lifetime\nvalue, R, and then checking the returned value,\n(R), Bob can be sure \nthat Alice is both who she says she is (since she knows the secret key value\nneeded to encrypt R) and live (since she has encrypted the nonce, R, that Bob just\ncreated).\nThe use of a nonce and symmetric key cryptography forms the basis of ap4.0.\nA natural question is whether we can use a nonce and public key cryptography\nKA-B\nKA-B\nKA-B\nKA-B,\n704\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nAlice\nR\nKA–B(R)\nI am Alice\nBob\nFigure 8.18 \u0002 Protocol ap4.0 and a failure scenario"
    },
    {
      "chunk_id": "b68bd12a-83dd-4b72-8403-c52761788acc",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.4.5 Authentication Protocol ap4.0",
      "original_titles": [
        "8.4.5 Authentication Protocol ap4.0"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.4 End-Point Authentication > 8.4.5 Authentication Protocol ap4.0",
      "start_page": 730,
      "end_page": 731,
      "token_count": 986,
      "text": "Since passwords are so widely used, we might suspect that protocol ap3.0\nis fairly secure. If so, we’d be wrong! The security flaw here is clear. If Trudy\neavesdrops on Alice’s communication, then she can learn Alice’s password. Lest\nyou think this is unlikely, consider the fact that when you Telnet to another\nmachine and log in, the login password is sent unencrypted to the Telnet server.\nSomeone connected to the Telnet client or server’s LAN can possibly sniff \n(read and store) all packets transmitted on the LAN and thus steal the login pass-\nword. In fact, this is a well-known approach for stealing passwords (see, for\nexample, [Jimenez 1997]). Such a threat is obviously very real, so ap3.0 clearly\nwon’t do.\n8.4.4 Authentication Protocol ap3.1\nOur next idea for fixing ap3.0 is naturally to encrypt the password. By encrypting the\npassword, we can prevent Trudy from learning Alice’s password. If we assume that\nAlice and Bob share a symmetric secret key,\nthen Alice can encrypt \nthe password and send her identification message, “I am Alice,” and her encrypted\npassword to Bob. Bob then decrypts the password and, assuming the password is \ncorrect, authenticates Alice. Bob feels comfortable in authenticating Alice since Alice\nnot only knows the password, but also knows the shared secret key value needed to\nencrypt the password. Let’s call this protocol ap3.1.\nWhile it is true that ap3.1 prevents Trudy from learning Alice’s password,\nthe use of cryptography here does not solve the authentication problem. Bob is\nsubject to a playback attack: Trudy need only eavesdrop on Alice’s communica-\ntion, record the encrypted version of the password, and play back the encrypted\nversion of the password to Bob to pretend that she is Alice. The use of an\nencrypted password in ap3.1 doesn’t make the situation manifestly different from\nthat of protocol ap3.0 in Figure 8.17.\n8.4.5 Authentication Protocol ap4.0\nThe failure scenario in Figure 8.17 resulted from the fact that Bob could not dis-\ntinguish between the original authentication of Alice and the later playback of\nAlice’s original authentication. That is, Bob could not tell if Alice was live (that\nis, was currently really on the other end of the connection) or whether the mes-\nsages he was receiving were a recorded playback of a previous authentication of\nAlice. The very (very) observant reader will recall that the three-way TCP hand-\nshake protocol needed to address the same problem—the server side of a TCP\nconnection did not want to accept a connection if the received SYN segment was\nan old copy (retransmission) of a SYN segment from an earlier connection. How\nKA-B,\n8.4\n•\nEND-POINT AUTHENTICATION\n703\n\ndid the TCP server side solve the problem of determining whether the client was\nreally live? It chose an initial sequence number that had not been used in a very\nlong time, sent that number to the client, and then waited for the client to respond\nwith an ACK segment containing that number. We can adopt the same idea here\nfor authentication purposes.\nA nonce is a number that a protocol will use only once in a lifetime. That is,\nonce a protocol uses a nonce, it will never use that number again. Our ap4.0\nprotocol uses a nonce as follows:\n1. Alice sends the message “I am Alice” to Bob.\n2. Bob chooses a nonce, R, and sends it to Alice.\n3. Alice encrypts the nonce using Alice and Bob’s symmetric secret key, \nand sends the encrypted nonce, \n(R), back to Bob. As in protocol ap3.1,\nit is the fact that Alice knows \nand uses it to encrypt a value that lets \nBob know that the message he receives was generated by Alice. The nonce \nis used to ensure that Alice is live.\n4. Bob decrypts the received message. If the decrypted nonce equals the nonce he\nsent Alice, then Alice is authenticated.\nProtocol ap4.0 is illustrated in Figure 8.18. By using the once-in-a-lifetime\nvalue, R, and then checking the returned value,\n(R), Bob can be sure \nthat Alice is both who she says she is (since she knows the secret key value\nneeded to encrypt R) and live (since she has encrypted the nonce, R, that Bob just\ncreated).\nThe use of a nonce and symmetric key cryptography forms the basis of ap4.0.\nA natural question is whether we can use a nonce and public key cryptography\nKA-B\nKA-B\nKA-B\nKA-B,\n704\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nAlice\nR\nKA–B(R)\nI am Alice\nBob\nFigure 8.18 \u0002 Protocol ap4.0 and a failure scenario"
    },
    {
      "chunk_id": "43d4f23d-b95b-4522-9ca4-e3180eea1401",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.5 Securing E-Mail",
      "original_titles": [
        "8.5 Securing E-Mail"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.5 Securing E-Mail",
      "start_page": 732,
      "end_page": 732,
      "token_count": 608,
      "text": "(rather than symmetric key cryptography) to solve the authentication problem. This\nissue is explored in the problems at the end of the chapter.\n8.5 Securing E-Mail\nIn previous sections, we examined fundamental issues in network security,\nincluding symmetric key and public key cryptography, end-point authentication,\nkey distribution, message integrity, and digital signatures. We are now going to\nexamine how these tools are being used to provide security in the Internet.\nInterestingly, it is possible to provide security services in any of the top four\nlayers of the Internet protocol stack. When security is provided for a specific appli-\ncation-layer protocol, the application using the protocol will enjoy one or more\nsecurity services, such as confidentiality, authentication, or integrity. When security\nis provided for a transport-layer protocol, all applications that use that protocol\nenjoy the security services of the transport protocol. When security is provided at\nthe network layer on a host-to-host basis, all transport-layer segments (and hence all\napplication-layer data) enjoy the security services of the network layer. When secu-\nrity is provided on a link basis, then the data in all frames traveling over the link\nreceive the security services of the link.\nIn Sections 8.5 through 8.8, we examine how security tools are being used in\nthe application, transport, network, and link layers. Being consistent with the gen-\neral structure of this book, we begin at the top of the protocol stack and discuss\nsecurity at the application layer. Our approach is to use a specific application, \ne-mail, as a case study for application-layer security. We then move down the proto-\ncol stack. We’ll examine the SSL protocol (which provides security at the transport\nlayer), IPsec (which provides security at the network layer), and the security of the\nIEEE 802.11 wireless LAN protocol.\nYou might be wondering why security functionality is being provided at\nmore than one layer in the Internet. Wouldn’t it suffice simply to provide the\nsecurity functionality at the network layer and be done with it? There are two\nanswers to this question. First, although security at the network layer can offer\n“blanket coverage” by encrypting all the data in the datagrams (that is, all the\ntransport-layer segments) and by authenticating all the source IP addresses, it\ncan’t provide user-level security. For example, a commerce site cannot rely on\nIP-layer security to authenticate a customer who is purchasing goods at the com-\nmerce site. Thus, there is a need for security functionality at higher layers as well\nas blanket coverage at lower layers. Second, it is generally easier to deploy new\nInternet services, including security services, at the higher layers of the protocol\nstack. While waiting for security to be broadly deployed at the network layer,\nwhich is probably still many years in the future, many application developers\n8.5\n•\nSECURING E-MAIL\n705"
    },
    {
      "chunk_id": "57c9129f-9e27-4cd2-95fe-4fe8701aa347",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.5.1 Secure E-Mail",
      "original_titles": [
        "8.5.1 Secure E-Mail"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.5 Securing E-Mail > 8.5.1 Secure E-Mail",
      "start_page": 733,
      "end_page": 736,
      "token_count": 1992,
      "text": "“just do it” and introduce security functionality into their favorite applications. A\nclassic example is Pretty Good Privacy (PGP), which provides secure e-mail\n(discussed later in this section). Requiring only client and server application\ncode, PGP was one of the first security technologies to be broadly used in the\nInternet.\n8.5.1 Secure E-Mail\nWe now use the cryptographic principles of Sections 8.2 through 8.3 to create a\nsecure e-mail system. We create this high-level design in an incremental manner, at\neach step introducing new security services. When designing a secure e-mail sys-\ntem, let us keep in mind the racy example introduced in Section 8.1—the love affair\nbetween Alice and Bob. Imagine that Alice wants to send an e-mail message to Bob,\nand Trudy wants to intrude.\nBefore plowing ahead and designing a secure e-mail system for Alice and\nBob, we should consider which security features would be most desirable for\nthem. First and foremost is confidentiality. As discussed in Section 8.1, neither\nAlice nor Bob wants Trudy to read Alice’s e-mail message. The second feature\nthat Alice and Bob would most likely want to see in the secure e-mail system is\nsender authentication. In particular, when Bob receives the message “I don’t\nlove you anymore. I never want to see you again. For-\nmerly yours, Alice,” he would naturally want to be sure that the message\ncame from Alice and not from Trudy. Another feature that the two lovers would\nappreciate is message integrity, that is, assurance that the message Alice sends is\nnot modified while en route to Bob. Finally, the e-mail system should provide\nreceiver authentication; that is, Alice wants to make sure that she is indeed send-\ning the letter to Bob and not to someone else (for example, Trudy) who is imper-\nsonating Bob.\nSo let’s begin by addressing the foremost concern, confidentiality. The most\nstraightforward way to provide confidentiality is for Alice to encrypt the message\nwith symmetric key technology (such as DES or AES) and for Bob to decrypt the\nmessage on receipt. As discussed in Section 8.2, if the symmetric key is long\nenough, and if only Alice and Bob have the key, then it is extremely difficult for\nanyone else (including Trudy) to read the message. Although this approach is\nstraightforward, it has the fundamental difficulty that we discussed in Section\n8.2—distributing a symmetric key so that only Alice and Bob have copies of it. So\nwe naturally consider an alternative approach—public key cryptography (using,\nfor example, RSA). In the public key approach, Bob makes his public key pub-\nlicly available (e.g., in a public key server or on his personal Web page), Alice\nencrypts her message with Bob’s public key, and she sends the encrypted message\nto Bob’s e-mail address. When Bob receives the message, he simply decrypts it\nwith his private key. Assuming that Alice knows for sure that the public key is\n706\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nBob’s public key, this approach is an excellent means to provide the desired confi-\ndentiality. One problem, however, is that public key encryption is relatively inef-\nficient, particularly for long messages.\nTo overcome the efficiency problem, let’s make use of a session key (dis-\ncussed in Section 8.2.2). In particular, Alice (1) selects a random symmetric ses-\nsion key, KS, (2) encrypts her message, m, with the symmetric key, (3) encrypts\nthe symmetric key with Bob’s public key, KB\n+, (4) concatenates the encrypted\nmessage and the encrypted symmetric key to form a “package,” and (5) sends the\npackage to Bob’s e-mail address. The steps are illustrated in Figure 8.19. (In this\nand the subsequent figures, the circled “+” represents concatenation and the cir-\ncled “–” represents deconcatenation.) When Bob receives the package, he (1)\nuses his private key, KB\n–, to obtain the symmetric key, KS, and (2) uses the sym-\nmetric key KS to decrypt the message m.\nHaving designed a secure e-mail system that provides confidentiality, let’s now\ndesign another system that provides both sender authentication and message\nintegrity. We’ll suppose, for the moment, that Alice and Bob are no longer concerned\nwith confidentiality (they want to share their feelings with everyone!), and are\nconcerned only about sender authentication and message integrity. To accomplish\nthis task, we use digital signatures and message digests, as described in Section 8.3.\nSpecifically, Alice (1) applies a hash function, H (for example, MD5), to her\nmessage, m, to obtain a message digest, (2) signs the result of the hash function \nwith her private key, KA\n–, to create a digital signature, (3) concatenates the original\n(unencrypted) message with the signature to create a package, and (4) sends the\npackage to Bob’s e-mail address. When Bob receives the package, he (1) applies\nAlice’s public key, KA\n+, to the signed message digest and (2) compares the result of\nthis operation with his own hash, H, of the message. The steps are illustrated in\n8.5\n•\nSECURING E-MAIL\n707\nKS(.)\nKS(.)\nKS(m)\nKS(m)\nKS\nKS\nKB\n+(.)\nKB\n+(KS)\nKB\n+(KS)\nm\nm\n+\n–\nInternet\nKB\n–(.)\nAlice sends e-mail message m\nBob receives e-mail message m\nFigure 8.19 \u0002 Alice used a symmetric session key, KS, to send a secret \ne-mail to Bob\n\nFigure 8.20. As discussed in Section 8.3, if the two results are the same, Bob can be\npretty confident that the message came from Alice and is unaltered.\nNow let’s consider designing an e-mail system that provides confidentiality,\nsender authentication, and message integrity. This can be done by combining the\nprocedures in Figures 8.19 and 8.20. Alice first creates a preliminary package,\nexactly as in Figure 8.20, that consists of her original message along with a digi-\ntally signed hash of the message. She then treats this preliminary package as a\nmessage in itself and sends this new message through the sender steps in Figure 8.19,\ncreating a new package that is sent to Bob. The steps applied by Alice are shown\nin Figure 8.21. When Bob receives the package, he first applies his side of Figure 8.19\nand then his side of Figure 8.20. It should be clear that this design achieves the\ngoal of providing confidentiality, sender authentication, and message integrity.\nNote that, in this scheme, Alice uses public key cryptography twice: once \nwith her own private key and once with Bob’s public key. Similarly, Bob also\nuses public key cryptography twice—once with his private key and once with\nAlice’s public key.\nThe secure e-mail design outlined in Figure 8.21 probably provides satisfac-\ntory security for most e-mail users for most occasions. But there is still one\nimportant issue that remains to be addressed. The design in Figure 8.21 requires\nAlice to obtain Bob’s public key, and requires Bob to obtain Alice’s public key.\nThe distribution of these public keys is a nontrivial problem. For example, Trudy\nmight masquerade as Bob and give Alice her own public key while saying that it\nis Bob’s public key, enabling her to receive the message meant for Bob. As we\nlearned in Section 8.3, a popular approach for securely distributing public keys is\nto certify the public keys using a CA.\n708\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nH(.)\nKA\n–(.)\nKA\n+(.)\nKA\n–(H(m))\nKA\n–(H(m))\nm\nm\nm\n+\n–\nInternet\nAlice sends e-mail message m\nBob receives e-mail message m\nH(.)\nCompare\nFigure 8.20 \u0002 Using hash functions and digital signatures to provide\nsender authentication and message integrity\n\n8.5\n•\nSECURING E-MAIL\n709\nPHIL ZIMMERMANN AND PGP\nPhilip R. Zimmermann is the creator of Pretty Good Privacy (PGP). For that, he was\nthe target of a three-year criminal investigation because the government held that US\nexport restrictions for cryptographic software were violated when PGP spread all\naround the world following its 1991 publication as freeware. After releasing PGP as\nshareware, someone else put it on the Internet and foreign citizens downloaded it.\nCryptography programs in the United States are classified as munitions under federal\nlaw and may not be exported.\nDespite the lack of funding, the lack of any paid staff, and the lack of a company\nto stand behind it, and despite government interventions, PGP nonetheless became\nthe most widely used e-mail encryption software in the world. Oddly enough, the US\ngovernment may have inadvertently contributed to PGP’s spread because of the\nZimmermann case.\nThe US government dropped the case in early 1996. The announcement was met\nwith celebration by Internet activists. The Zimmermann case had become the story of\nan innocent person fighting for his rights against the abuses of big government. The\ngovernment’s giving in was welcome news, in part because of the campaign for\nInternet censorship in Congress and the push by the FBI to allow increased govern-\nment snooping.\nAfter the government dropped its case, Zimmermann founded PGP Inc., which\nwas acquired by Network Associates in December 1997. Zimmermann is now an\nindependent consultant in matters cryptographic.\nCASE HISTORY\nH(.)\nKA\n–(.)\nKS (.)\nKS\nKA\n–(H(m))\nm\nm\n+\n+\nto Internet\nKB\n+(.)\nFigure 8.21 \u0002 Alice uses symmetric key cyptography, public key \ncryptography, a hash function, and a digital signature to\nprovide secrecy, sender authentication, and message integrity"
    },
    {
      "chunk_id": "30f8d152-3ea5-41b8-91ef-c276ab086d69",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.5.2 PGP",
      "original_titles": [
        "8.5.2 PGP"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.5 Securing E-Mail > 8.5.2 PGP",
      "start_page": 737,
      "end_page": 737,
      "token_count": 533,
      "text": "8.5.2 PGP\nWritten by Phil Zimmermann in 1991, Pretty Good Privacy (PGP) is an e-mail\nencryption scheme that has become a de facto standard. Its Web site serves more than\na million pages a month to users in 166 countries [PGPI 2012]. Versions of PGP are\navailable in the public domain; for example, you can find the PGP software for your\nfavorite platform as well as lots of interesting reading at the International PGP\nHome Page [PGPI 2012]. (A particularly interesting essay by the author of PGP is\n[Zimmermann 2012].) The PGP design is, in essence, the same as the design shown\nin Figure 8.21. Depending on the version, the PGP software uses MD5 or SHA for\ncalculating the message digest; CAST, triple-DES, or IDEA for symmetric key\nencryption; and RSA for the public key encryption.\nWhen PGP is installed, the software creates a public key pair for the user. The\npublic key can be posted on the user’s Web site or placed in a public key server. The\nprivate key is protected by the use of a password. The password has to be entered\nevery time the user accesses the private key. PGP gives the user the option of digi-\ntally signing the message, encrypting the message, or both digitally signing and\nencrypting. Figure 8.22 shows a PGP signed message. This message appears after\nthe MIME header. The encoded data in the message is KA\n– (H(m)), that is, the digi-\ntally signed message digest. As we discussed above, in order for Bob to verify the\nintegrity of the message, he needs to have access to Alice’s public key.\nFigure 8.23 shows a secret PGP message. This message also appears after the\nMIME header. Of course, the plaintext message is not included within the secret\ne-mail message. When a sender (such as Alice) wants both confidentiality and\nintegrity, PGP contains a message like that of Figure 8.23 within the message of\nFigure 8.22.\nPGP also provides a mechanism for public key certification, but the mechanism\nis quite different from the more conventional CA. PGP public keys are certified by a\nweb of trust. Alice herself can certify any key/username pair when she believes the\n710\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nFigure 8.22 \u0002 A PGP signed message\n-----BEGIN PGP SIGNED MESSAGE-----\nHash:  SHA1\nBob:\nCan I see you tonight?\nPassionately yours, Alice\n-----BEGIN PGP SIGNATURE-----\nVersion: PGP for Personal Privacy 5.0\nCharset:\nnoconv\nyhHJRHhGJGhgg/12EpJ+lo8gE4vB3mqJhFEvZP9t6n7G6m5Gw2\n-----END PGP SIGNATURE-----"
    },
    {
      "chunk_id": "0ebeb8d2-4d69-4953-8c30-0b8fe636edc5",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.6 Securing TCP Connections: SSL",
      "original_titles": [
        "8.6 Securing TCP Connections: SSL"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.6 Securing TCP Connections: SSL",
      "start_page": 738,
      "end_page": 739,
      "token_count": 887,
      "text": "pair really belong together. In addition, PGP permits Alice to say that she trusts\nanother user to vouch for the authenticity of more keys. Some PGP users sign each\nother’s keys by holding key-signing parties. Users physically gather, exchange\npublic keys, and certify each other’s keys by signing them with their private keys.\n8.6 Securing TCP Connections: SSL\nIn the previous section, we saw how cryptographic techniques can provide confi-\ndentiality, data integrity, and end-point authentication to a specific application,\nnamely, e-mail. In this section, we’ll drop down a layer in the protocol stack and\nexamine how cryptography can enhance TCP with security services, including con-\nfidentiality, data integrity, and end-point authentication. This enhanced version of\nTCP is commonly known as Secure Sockets Layer (SSL). A slightly modified ver-\nsion of SSL version 3, called Transport Layer Security (TLS), has been standard-\nized by the IETF [RFC 4346].\nThe SSL protocol was originally designed by Netscape, but the basic ideas\nbehind securing TCP had predated Netscape’s work (for example, see Woo [Woo\n1994]). Since its inception, SSL has enjoyed broad deployment. SSL is supported\nby all popular Web browsers and Web servers, and it is used by essentially all\nInternet commerce sites (including Amazon, eBay, Yahoo!, MSN, and so on). Tens\nof billions of dollars are spent over SSL every year. In fact, if you have ever pur-\nchased anything over the Internet with your credit card, the communication\nbetween your browser and the server for this purchase almost certainly went over\nSSL. (You can identify that SSL is being used by your browser when the URL\nbegins with https: rather than http.)\nTo understand the need for SSL, let’s walk through a typical Internet com-\nmerce scenario. Bob is surfing the Web and arrives at the Alice Incorporated site,\nwhich is selling perfume. The Alice Incorporated site displays a form in which\nBob is supposed to enter the type of perfume and quantity desired, his address,\nand his payment card number. Bob enters this information, clicks on Submit, and\n8.6\n•\nSECURING TCP CONNECTIONS: SSL\n711\nFigure 8.23 \u0002 A secret PGP message\n-----BEGIN PGP MESSAGE-----\nVersion: PGP for Personal Privacy 5.0\nu2R4d+/jKmn8Bc5+hgDsqAewsDfrGdszX68liKm5F6Gc4sDfcXyt\nRfdS10juHgbcfDssWe7/K=lKhnMikLo0+1/BvcX4t==Ujk9PbcD4\nThdf2awQfgHbnmKlok8iy6gThlp\n-----END PGP MESSAGE\n\nexpects to receive (via ordinary postal mail) the purchased perfumes; he also\nexpects to receive a charge for his order in his next payment card statement. This\nall sounds good, but if no security measures are taken, Bob could be in for a few\nsurprises.\n•\nIf no confidentiality (encryption) is used, an intruder could intercept Bob’s order\nand obtain his payment card information. The intruder could then make pur-\nchases at Bob’s expense.\n•\nIf no data integrity is used, an intruder could modify Bob’s order, having him\npurchase ten times more bottles of perfume than desired.\n•\nFinally, if no server authentication is used, a server could display Alice Incorpo-\nrated’s famous logo when in actuality the site maintained by Trudy, who is mas-\nquerading as Alice Incorporated. After receiving Bob’s order, Trudy could take\nBob’s money and run. Or Trudy could carry out an identity theft by collecting\nBob’s name, address, and credit card number.\nSSL addresses these issues by enhancing TCP with confidentiality, data integrity,\nserver authentication, and client authentication.\nSSL is often used to provide security to transactions that take place over HTTP.\nHowever, because SSL secures TCP, it can be employed by any application that\nruns over TCP. SSL provides a simple Application Programmer Interface (API)\nwith sockets, which is similar and analogous to TCP’s API. When an application\nwants to employ SSL, the application includes SSL classes/libraries. As shown in\nFigure 8.24, although SSL technically resides in the application layer, from the\ndeveloper’s perspective it is a transport protocol that provides TCP’s services\nenhanced with security services.\n712\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nFigure 8.24 \u0002 Although SSL technically resides in the application layer,\nfrom the developer’s perspective it is a transport-layer \nprotocol\nTCP\nSSL sublayer\nIP\nApplication\nApplication\nlayer\nTCP enhanced with SSL\nSSL socket\nTCP socket\nTCP\nIP\nApplication\nTCP API\nTCP socket"
    },
    {
      "chunk_id": "eab7a104-6f2d-4209-986d-b0baf14b58e2",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.6.1 The Big Picture",
      "original_titles": [
        "8.6.1 The Big Picture"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.6 Securing TCP Connections: SSL > 8.6.1 The Big Picture",
      "start_page": 740,
      "end_page": 742,
      "token_count": 1493,
      "text": "8.6.1 The Big Picture\nWe begin by describing a simplified version of SSL, one that will allow us to get a\nbig-picture understanding of the why and how of SSL. We will refer to this simpli-\nfied version of SSL as “almost-SSL.” After describing almost-SSL, in the next sub-\nsection we’ll then describe the real SSL, filling in the details. Almost-SSL (and\nSSL) has three phases: handshake, key derivation, and data transfer. We now\ndescribe these three phases for a communication session between a client (Bob) and\na server (Alice), with Alice having a private/public key pair and a certificate that\nbinds her identity to her public key.\nHandshake\nDuring the handshake phase, Bob needs to (a) establish a TCP connection with\nAlice, (b) verify that Alice is really Alice, and (c) send Alice a master secret key,\nwhich will be used by both Alice and Bob to generate all the symmetric keys they\nneed for the SSL session. These three steps are shown in Figure 8.25. Note that once\nthe TCP connection is established, Bob sends Alice a hello message. Alice then\nresponds with her certificate, which contains her public key. As discussed in Section\n8.3, because the certificate has been certified by a CA, Bob knows for sure that the\n8.6\n•\nSECURING TCP CONNECTIONS: SSL\n713\nTCP SYN\nTCP/SYNACK\nDecrypts EMS with\nKA\n– to get MS\nEMS = KA\n+(MS)\nTCP ACK\nSSL hello\ncertificate\n(b)\n(a)\n(c)\nCreate Master\nSecret (MS)\nFigure 8.25 \u0002 The almost-SSL handshake, beginning with a TCP \nconnection\n\npublic key in the certificate belongs to Alice. Bob then generates a Master Secret\n(MS) (which will only be used for this SSL session), encrypts the MS with Alice’s\npublic key to create the Encyrpted Master Secret (EMS), and sends the EMS to\nAlice. Alice decrypts the EMS with her private key to get the MS. After this phase,\nboth Bob and Alice (and no one else) know the master secret for this SSL session.\nKey Derivation\nIn principle, the MS, now shared by Bob and Alice, could be used as the symmetric\nsession key for all subsequent encryption and data integrity checking. It is, however,\ngenerally considered safer for Alice and Bob to each use different cryptographic\nkeys, and also to use different keys for encryption and integrity checking. Thus, both\nAlice and Bob use the MS to generate four keys:\n•\nEB = session encryption key for data sent from Bob to Alice\n•\nMB = session MAC key for data sent from Bob to Alice\n•\nEA = session encryption key for data sent from Alice to Bob\n•\nMA = session MAC key for data sent from Alice to Bob\nAlice and Bob each generate the four keys from the MS. This could be done by sim-\nply slicing the MS into four keys. (But in real SSL it is a little more complicated, as\nwe’ll see.) At the end of the key derivation phase, both Alice and Bob have all four\nkeys. The two encryption keys will be used to encrypt data; the two MAC keys will\nbe used to verify the integrity of the data.\nData Transfer\nNow that Alice and Bob share the same four session keys (EB, MB, EA, and MA),\nthey can start to send secured data to each other over the TCP connection. Since\nTCP is a byte-stream protocol, a natural approach would be for SSL to encrypt\napplication data on the fly and then pass the encrypted data on the fly to TCP. But if\nwe were to do this, where would we put the MAC for the integrity check? We cer-\ntainly do not want to wait until the end of the TCP session to verify the integrity of\nall of Bob’s data that was sent over the entire session! To address this issue, SSL\nbreaks the data stream into records, appends a MAC to each record for integrity\nchecking, and then encrypts the record+MAC. To create the MAC, Bob inputs the\nrecord data along with the key MB into a hash function, as discussed in Section 8.3.\nTo encrypt the package record+MAC, Bob uses his session encryption key EB. This\nencrypted package is then passed to TCP for transport over the Internet.\nAlthough this approach goes a long way, it still isn’t bullet-proof when it comes\nto providing data integrity for the entire message stream. In particular, suppose Trudy\nis a woman-in-the-middle and has the ability to insert, delete, and replace segments\n714\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nin the stream of TCP segments sent between Alice and Bob. Trudy, for example,\ncould capture two segments sent by Bob, reverse the order of the segments, adjust\nthe TCP sequence numbers (which are not encrypted), and then send the two reverse-\nordered segments to Alice. Assuming that each TCP segment encapsulates exactly\none record, let’s now take a look at how Alice would process these segments.\n1. TCP running in Alice would think everything is fine and pass the two records\nto the SSL sublayer.\n2. SSL in Alice would decrypt the two records.\n3. SSL in Alice would use the MAC in each record to verify the data integrity of\nthe two records.\n4. SSL would then pass the decrypted byte streams of the two records to the\napplication layer; but the complete byte stream received by Alice would not be\nin the correct order due to reversal of the records!\nYou are encouraged to walk through similar scenarios for when Trudy removes seg-\nments or when Trudy replays segments.\nThe solution to this problem, as you probably guessed, is to use sequence num-\nbers. SSL does this as follows. Bob maintains a sequence number counter, which\nbegins at zero and is incremented for each SSL record he sends. Bob doesn’t actu-\nally include a sequence number in the record itself, but when he calculates the\nMAC, he includes the sequence number in the MAC calculation. Thus, the MAC is\nnow a hash of the data plus the MAC key MB plus the current sequence number.\nAlice tracks Bob’s sequence numbers, allowing her to verify the data integrity of a\nrecord by including the appropriate sequence number in the MAC calculation. This\nuse of SSL sequence numbers prevents Trudy from carrying out a woman-in-the-\nmiddle attack, such as reordering or replaying segments. (Why?)\nSSL Record\nThe SSL record (as well as the almost-SSL record) is shown in Figure 8.26. The\nrecord consists of a type field, version field, length field, data field, and MAC field.\nNote that the first three fields are not encrypted. The type field indicates whether the\nrecord is a handshake message or a message that contains application data. It is also\n8.6\n•\nSECURING TCP CONNECTIONS: SSL\n715\nFigure 8.26 \u0002 Record format for SSL\nVersion\nLength\nType\nData\nMAC\nEncrypted with EB"
    },
    {
      "chunk_id": "a465d5b2-e5c8-4af6-8112-f7c5b0dfc50d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.6.2 A More Complete Picture",
      "original_titles": [
        "8.6.2 A More Complete Picture"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.6 Securing TCP Connections: SSL > 8.6.2 A More Complete Picture",
      "start_page": 743,
      "end_page": 744,
      "token_count": 1293,
      "text": "used to close the SSL connection, as discussed below. SSL at the receiving end uses\nthe length field to extract the SSL records out of the incoming TCP byte stream. The\nversion field is self-explanatory.\n8.6.2 A More Complete Picture\nThe previous subsection covered the almost-SSL protocol; it served to give us a basic\nunderstanding of the why and how of SSL. Now that we have a basic understanding\nof SSL, we can dig a little deeper and examine the essentials of the actual SSL proto-\ncol. In parallel to reading this description of the SSL protocol, you are encouraged to\ncomplete the Wireshark SSL lab, available at the textbook’s companion Web site.\nSSL Handshake\nSSL does not mandate that Alice and Bob use a specific symmetric key algorithm, a\nspecific public-key algorithm, or a specific MAC. Instead, SSL allows Alice and\nBob to agree on the cryptographic algorithms at the beginning of the SSL session,\nduring the handshake phase. Additionally, during the handshake phase, Alice and\nBob send nonces to each other, which are used in the creation of the session keys\n(EB, MB, EA, and MA). The steps of the real SSL handshake are as follows:\n1. The client sends a list of cryptographic algorithms it supports, along with a\nclient nonce.\n2. From the list, the server chooses a symmetric algorithm (for example, AES), a\npublic key algorithm (for example, RSA with a specific key length), and a\nMAC algorithm. It sends back to the client its choices, as well as a certificate\nand a server nonce.\n3. The client verifies the certificate, extracts the server’s public key, generates a\nPre-Master Secret (PMS), encrypts the PMS with the server’s public key, and\nsends the encrypted PMS to the server.\n4. Using the same key derivation function (as specified by the SSL standard),\nthe client and server independently compute the Master Secret (MS) from\nthe PMS and nonces. The MS is then sliced up to generate the two encryption\nand two MAC keys. Furthermore, when the chosen symmetric cipher employs\nCBC (such as 3DES or AES), then two Initialization Vectors (IVs)—one for\neach side of the connection—are also obtained from the MS. Henceforth, all\nmessages sent between client and server are encrypted and authenticated\n(with the MAC).\n5. The client sends a MAC of all the handshake messages.\n6. The server sends a MAC of all the handshake messages.\nThe last two steps protect the handshake from tampering. To see this, observe\nthat in step 1, the client typically offers a list of algorithms—some strong, some\n716\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nweak. This list of algorithms is sent in cleartext, since the encryption algorithms and\nkeys have not yet been agreed upon. Trudy, as a woman-in-the-middle, could delete\nthe stronger algorithms from the list, forcing the client to select a weak algorithm.\nTo prevent such a tampering attack, in step 5 the client sends a MAC of the concate-\nnation of all the handshake messages it sent and received. The server can compare\nthis MAC with the MAC of the handshake messages it received and sent. If there is\nan inconsistency, the server can terminate the connection. Similarly, the server sends\na MAC of the handshake messages it has seen, allowing the client to check for\ninconsistencies.\nYou may be wondering why there are nonces in steps 1 and 2. Don’t sequence\nnumbers suffice for preventing the segment replay attack? The answer is yes, but they\ndon’t alone prevent the “connection replay attack.” Consider the following connection\nreplay attack. Suppose Trudy sniffs all messages between Alice and Bob. The next\nday, Trudy masquerades as Bob and sends to Alice exactly the same sequence of mes-\nsages that Bob sent to Alice on the previous day. If Alice doesn’t use nonces, she will\nrespond with exactly the same sequence of messages she sent the previous day. Alice\nwill not suspect any funny business, as each message she receives will pass the\nintegrity check. If Alice is an e-commerce server, she will think that Bob is placing a\nsecond order (for exactly the same thing). On the other hand, by including a nonce in\nthe protocol, Alice will send different nonces for each TCP session, causing the\nencryption keys to be different on the two days. Therefore, when Alice receives\nplayed-back SSL records from Trudy, the records will fail the integrity checks, and the\nbogus e-commerce transaction will not succeed. In summary, in SSL, nonces are used\nto defend against the “connection replay attack” and sequence numbers are used to\ndefend against replaying individual packets during an ongoing session.\nConnection Closure\nAt some point, either Bob or Alice will want to end the SSL session. One approach\nwould be to let Bob end the SSL session by simply terminating the underlying TCP\nconnection—that is, by having Bob send a TCP FIN segment to Alice. But such a\nnaive design sets the stage for the truncation attack whereby Trudy once again gets\nin the middle of an ongoing SSL session and ends the session early with a TCP FIN.\nIf Trudy were to do this, Alice would think she received all of Bob’s data when actu-\nality she only received a portion of it. The solution to this problem is to indicate in\nthe type field whether the record serves to terminate the SSL session. (Although the\nSSL type is sent in the clear, it is authenticated at the receiver using the record’s\nMAC.) By including such a field, if Alice were to receive a TCP FIN before receiv-\ning a closure SSL record, she would know that something funny was going on.\nThis completes our introduction to SSL. We’ve seen that it uses many of the\ncryptography principles discussed in Sections 8.2 and 8.3. Readers who want to\nexplore SSL on yet a deeper level can read Rescorla’s highly readable book on SSL\n[Rescorla 2001].\n8.6\n•\nSECURING TCP CONNECTIONS: SSL\n717"
    },
    {
      "chunk_id": "c2a98e2d-5d6b-49b9-8403-b3ad5486cf5a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.7 Network-Layer Security: IPsec and Virtual Private Networks",
      "original_titles": [
        "8.7 Network-Layer Security: IPsec and Virtual Private Networks"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.7 Network-Layer Security: IPsec and Virtual Private Networks",
      "start_page": 745,
      "end_page": 746,
      "token_count": 1058,
      "text": "8.7 Network-Layer Security: IPsec and \nVirtual Private Networks\nThe IP security protocol, more commonly known as IPsec, provides security at the\nnetwork layer. IPsec secures IP datagrams between any two network-layer entities,\nincluding hosts and routers. As we will soon describe, many institutions (corpora-\ntions, government branches, non-profit organizations, and so on) use IPsec to create\nvirtual private networks (VPNs) that run over the public Internet.\nBefore getting into the specifics of IPsec, let’s step back and consider what it\nmeans to provide confidentiality at the network layer. With network-layer confiden-\ntiality between a pair of network entities (for example, between two routers, between\ntwo hosts, or between a router and a host), the sending entity encrypts the payloads\nof all the datagrams it sends to the receiving entity. The encrypted payload could be a\nTCP segment, a UDP segment, an ICMP message, and so on. If such a network-layer\nservice were in place, all data sent from one entity to the other—including e-mail,\nWeb pages, TCP handshake messages, and management messages (such as ICMP\nand SNMP)—would be hidden from any third party that might be sniffing the net-\nwork. For this reason, network-layer security is said to provide “blanket coverage”.\nIn addition to confidentiality, a network-layer security protocol could potentially\nprovide other security services. For example, it could provide source authentication, so\nthat the receiving entity can verify the source of the secured datagram. A network-layer\nsecurity protocol could provide data integrity, so that the receiving entity can check for\nany tampering of the datagram that may have occurred while the datagram was in tran-\nsit. A network-layer security service could also provide replay-attack prevention, mean-\ning that Bob could detect any duplicate datagrams that an attacker might insert. We will\nsoon see that IPsec indeed provides mechanisms for all these security services, that is,\nfor confidentiality, source authentication, data integrity, and replay-attack prevention.\n8.7.1 IPsec and Virtual Private Networks (VPNs)\nAn institution that extends over multiple geographical regions often desires its own\nIP network, so that its hosts and servers can send data to each other in a secure and\nconfidential manner. To achieve this goal, the institution could actually deploy a\nstand-alone physical network—including routers, links, and a DNS infrastructure—that\nis completely separate from the public Internet. Such a disjoint network, dedicated\nto a particular institution, is called a private network. Not surprisingly, a private\nnetwork can be very costly, as the institution needs to purchase, install, and main-\ntain its own physical network infrastructure.\nInstead of deploying and maintaining a private network, many institutions today\ncreate VPNs over the existing public Internet. With a VPN, the institution’s inter-\noffice traffic is sent over the public Internet rather than over a physically independent\nnetwork. But to provide confidentiality, the inter-office traffic is encrypted before it\nenters the public Internet. A simple example of a VPN is shown in Figure 8.27. Here\n718\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nthe institution consists of a headquarters, a branch office, and traveling salespersons\nthat typically access the Internet from their hotel rooms. (There is only one salesper-\nson shown in the figure.) In this VPN, whenever two hosts within headquarters send\nIP datagrams to each other or whenever two hosts within the branch office want to\ncommunicate, they use good-old vanilla IPv4 (that is, without IPsec services). How-\never, when two of the institution’s hosts communicate over a path that traverses the\npublic Internet, the traffic is encrypted before it enters the Internet.\nTo get a feel for how a VPN works, let’s walk through a simple example in the\ncontext of Figure 8.27. When a host in headquarters sends an IP datagram to a sales-\nperson in a hotel, the gateway router in headquarters converts the vanilla IPv4 data-\ngram into an IPsec datagram and then forwards this IPsec datagram into the Internet.\nThis IPsec datagram actually has a traditional IPv4 header, so that the routers in the\npublic Internet process the datagram as if it were an ordinary IPv4 datagram—to\nthem, the datagram is a perfectly ordinary datagram. But, as shown Figure 8.27, the\npayload of the IPsec datagram includes an IPsec header, which is used for IPsec proc-\nessing; furthermore, the payload of the IPsec datagram is encrypted. When the IPsec\ndatagram arrives at the salesperson’s laptop, the OS in the laptop decrypts the pay-\nload (and provides other security services, such as verifying data integrity) and passes\nthe unencrypted payload to the upper-layer protocol (for example, to TCP or UDP).\n8.7\n•\nNETWORK-LAYER SECURITY: IPSEC AND VIRTUAL PRIVATE NETWORKS\n719\nFigure 8.27 \u0002 Virtual Private Network (VPN)\nIP\nheader\nIPsec\nheader\nSecure\npayload\nIP\nheader\nIPsec\nheader\nSecure\npayload\nIP\nheader\nIPsec\nheader\nSecure\npayload\nIP\nheader\nPayload\nIP\nheader\nPayload\nLaptop w/IPsec\nRouter\nw/IPv4 and\nIPsec\nRouter\nw/IPv4 and\nIPsec\nBranch Office\nHeadquarters\nSalesperson\nin Hotel\nPublic\nInternet"
    },
    {
      "chunk_id": "af8a517d-79de-4300-a477-26c1480a6476",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.7.1 IPsec and Virtual Private Networks (VPNs)",
      "original_titles": [
        "8.7.1 IPsec and Virtual Private Networks (VPNs)"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.7 Network-Layer Security: IPsec and Virtual Private Networks > 8.7.1 IPsec and Virtual Private Networks (VPNs)",
      "start_page": 745,
      "end_page": 746,
      "token_count": 1058,
      "text": "8.7 Network-Layer Security: IPsec and \nVirtual Private Networks\nThe IP security protocol, more commonly known as IPsec, provides security at the\nnetwork layer. IPsec secures IP datagrams between any two network-layer entities,\nincluding hosts and routers. As we will soon describe, many institutions (corpora-\ntions, government branches, non-profit organizations, and so on) use IPsec to create\nvirtual private networks (VPNs) that run over the public Internet.\nBefore getting into the specifics of IPsec, let’s step back and consider what it\nmeans to provide confidentiality at the network layer. With network-layer confiden-\ntiality between a pair of network entities (for example, between two routers, between\ntwo hosts, or between a router and a host), the sending entity encrypts the payloads\nof all the datagrams it sends to the receiving entity. The encrypted payload could be a\nTCP segment, a UDP segment, an ICMP message, and so on. If such a network-layer\nservice were in place, all data sent from one entity to the other—including e-mail,\nWeb pages, TCP handshake messages, and management messages (such as ICMP\nand SNMP)—would be hidden from any third party that might be sniffing the net-\nwork. For this reason, network-layer security is said to provide “blanket coverage”.\nIn addition to confidentiality, a network-layer security protocol could potentially\nprovide other security services. For example, it could provide source authentication, so\nthat the receiving entity can verify the source of the secured datagram. A network-layer\nsecurity protocol could provide data integrity, so that the receiving entity can check for\nany tampering of the datagram that may have occurred while the datagram was in tran-\nsit. A network-layer security service could also provide replay-attack prevention, mean-\ning that Bob could detect any duplicate datagrams that an attacker might insert. We will\nsoon see that IPsec indeed provides mechanisms for all these security services, that is,\nfor confidentiality, source authentication, data integrity, and replay-attack prevention.\n8.7.1 IPsec and Virtual Private Networks (VPNs)\nAn institution that extends over multiple geographical regions often desires its own\nIP network, so that its hosts and servers can send data to each other in a secure and\nconfidential manner. To achieve this goal, the institution could actually deploy a\nstand-alone physical network—including routers, links, and a DNS infrastructure—that\nis completely separate from the public Internet. Such a disjoint network, dedicated\nto a particular institution, is called a private network. Not surprisingly, a private\nnetwork can be very costly, as the institution needs to purchase, install, and main-\ntain its own physical network infrastructure.\nInstead of deploying and maintaining a private network, many institutions today\ncreate VPNs over the existing public Internet. With a VPN, the institution’s inter-\noffice traffic is sent over the public Internet rather than over a physically independent\nnetwork. But to provide confidentiality, the inter-office traffic is encrypted before it\nenters the public Internet. A simple example of a VPN is shown in Figure 8.27. Here\n718\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nthe institution consists of a headquarters, a branch office, and traveling salespersons\nthat typically access the Internet from their hotel rooms. (There is only one salesper-\nson shown in the figure.) In this VPN, whenever two hosts within headquarters send\nIP datagrams to each other or whenever two hosts within the branch office want to\ncommunicate, they use good-old vanilla IPv4 (that is, without IPsec services). How-\never, when two of the institution’s hosts communicate over a path that traverses the\npublic Internet, the traffic is encrypted before it enters the Internet.\nTo get a feel for how a VPN works, let’s walk through a simple example in the\ncontext of Figure 8.27. When a host in headquarters sends an IP datagram to a sales-\nperson in a hotel, the gateway router in headquarters converts the vanilla IPv4 data-\ngram into an IPsec datagram and then forwards this IPsec datagram into the Internet.\nThis IPsec datagram actually has a traditional IPv4 header, so that the routers in the\npublic Internet process the datagram as if it were an ordinary IPv4 datagram—to\nthem, the datagram is a perfectly ordinary datagram. But, as shown Figure 8.27, the\npayload of the IPsec datagram includes an IPsec header, which is used for IPsec proc-\nessing; furthermore, the payload of the IPsec datagram is encrypted. When the IPsec\ndatagram arrives at the salesperson’s laptop, the OS in the laptop decrypts the pay-\nload (and provides other security services, such as verifying data integrity) and passes\nthe unencrypted payload to the upper-layer protocol (for example, to TCP or UDP).\n8.7\n•\nNETWORK-LAYER SECURITY: IPSEC AND VIRTUAL PRIVATE NETWORKS\n719\nFigure 8.27 \u0002 Virtual Private Network (VPN)\nIP\nheader\nIPsec\nheader\nSecure\npayload\nIP\nheader\nIPsec\nheader\nSecure\npayload\nIP\nheader\nIPsec\nheader\nSecure\npayload\nIP\nheader\nPayload\nIP\nheader\nPayload\nLaptop w/IPsec\nRouter\nw/IPv4 and\nIPsec\nRouter\nw/IPv4 and\nIPsec\nBranch Office\nHeadquarters\nSalesperson\nin Hotel\nPublic\nInternet"
    },
    {
      "chunk_id": "aedf51a1-8d56-4af1-bb17-95390e6a8d78",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.7.2 The AH and ESP Protocols",
      "original_titles": [
        "8.7.2 The AH and ESP Protocols"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.7 Network-Layer Security: IPsec and Virtual Private Networks > 8.7.2 The AH and ESP Protocols",
      "start_page": 747,
      "end_page": 747,
      "token_count": 712,
      "text": "We have just given a high-level overview of how an institution can employ\nIPsec to create a VPN. To see the forest through the trees, we have brushed aside\nmany important details. Let’s now take a closer look.\n8.7.2 The AH and ESP Protocols\nIPsec is a rather complex animal—it is defined in more than a dozen RFCs. Two\nimportant RFCs are RFC 4301, which describes the overall IP security architecture,\nand RFC 6071, which provides an overview of the IPsec protocol suite. Our goal in\nthis textbook, as usual, is not simply to re-hash the dry and arcane RFCs, but instead\ntake a more operational and pedagogic approach to describing the protocols.\nIn the IPsec protocol suite, there are two principal protocols: the Authentication\nHeader (AH) protocol and the Encapsulation Security Payload (ESP) protocol.\nWhen a source IPsec entity (typically a host or a router) sends secure datagrams to a\ndestination entity (also a host or a router), it does so with either the AH protocol or the\nESP protocol. The AH protocol provides source authentication and data integrity but\ndoes not provide confidentiality. The ESP protocol provides source authentication,\ndata integrity, and confidentiality. Because confidentiality is often critical for VPNs\nand other IPsec applications, the ESP protocol is much more widely used than the AH\nprotocol. In order to de-mystify IPsec and avoid much of its complication, we will\nhenceforth focus exclusively on the ESP protocol. Readers wanting to learn also about\nthe AH protocol are encouraged to explore the RFCs and other online resources.\n8.7.3 Security Associations\nIPsec datagrams are sent between pairs of network entities, such as between two hosts,\nbetween two routers, or between a host and router. Before sending IPsec datagrams\nfrom source entity to destination entity, the source and destination entities create a net-\nwork-layer logical connection. This logical connection is called a security association\n(SA). An SA is a simplex logical connection; that is, it is unidirectional from source to\ndestination. If both entities want to send secure datagrams to each other, then two SAs\n(that is, two logical connections) need to be established, one in each direction.\nFor example, consider once again the institutional VPN in Figure 8.27. This insti-\ntution consists of a headquarters office, a branch office and, say, n traveling salesper-\nsons. For the sake of example, let’s suppose that there is bi-directional IPsec traffic\nbetween headquarters and the branch office and bi-directional IPsec traffic between\nheadquarters and the salespersons. In this VPN, how many SAs are there? To answer\nthis question, note that there are two SAs between the headquarters gateway router and\nthe branch-office gateway router (one in each direction); for each salesperson’s laptop,\nthere are two SAs between the headquarters gateway router and the laptop (again, one\nin each direction). So, in total, there are (2 + 2n) SAs. Keep in mind, however, that not\nall traffic sent into the Internet by the gateway routers or by the laptops will be IPsec\nsecured. For example, a host in headquarters may want to access a Web server (such as\nAmazon or Google) in the public Internet. Thus, the gateway router (and the laptops)\nwill emit into the Internet both vanilla IPv4 datagrams and secured IPsec datagrams.\n720\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS"
    },
    {
      "chunk_id": "fd36013d-772d-45e6-ab4a-863552db7182",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.7.3 Security Associations",
      "original_titles": [
        "8.7.3 Security Associations"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.7 Network-Layer Security: IPsec and Virtual Private Networks > 8.7.3 Security Associations",
      "start_page": 747,
      "end_page": 747,
      "token_count": 712,
      "text": "We have just given a high-level overview of how an institution can employ\nIPsec to create a VPN. To see the forest through the trees, we have brushed aside\nmany important details. Let’s now take a closer look.\n8.7.2 The AH and ESP Protocols\nIPsec is a rather complex animal—it is defined in more than a dozen RFCs. Two\nimportant RFCs are RFC 4301, which describes the overall IP security architecture,\nand RFC 6071, which provides an overview of the IPsec protocol suite. Our goal in\nthis textbook, as usual, is not simply to re-hash the dry and arcane RFCs, but instead\ntake a more operational and pedagogic approach to describing the protocols.\nIn the IPsec protocol suite, there are two principal protocols: the Authentication\nHeader (AH) protocol and the Encapsulation Security Payload (ESP) protocol.\nWhen a source IPsec entity (typically a host or a router) sends secure datagrams to a\ndestination entity (also a host or a router), it does so with either the AH protocol or the\nESP protocol. The AH protocol provides source authentication and data integrity but\ndoes not provide confidentiality. The ESP protocol provides source authentication,\ndata integrity, and confidentiality. Because confidentiality is often critical for VPNs\nand other IPsec applications, the ESP protocol is much more widely used than the AH\nprotocol. In order to de-mystify IPsec and avoid much of its complication, we will\nhenceforth focus exclusively on the ESP protocol. Readers wanting to learn also about\nthe AH protocol are encouraged to explore the RFCs and other online resources.\n8.7.3 Security Associations\nIPsec datagrams are sent between pairs of network entities, such as between two hosts,\nbetween two routers, or between a host and router. Before sending IPsec datagrams\nfrom source entity to destination entity, the source and destination entities create a net-\nwork-layer logical connection. This logical connection is called a security association\n(SA). An SA is a simplex logical connection; that is, it is unidirectional from source to\ndestination. If both entities want to send secure datagrams to each other, then two SAs\n(that is, two logical connections) need to be established, one in each direction.\nFor example, consider once again the institutional VPN in Figure 8.27. This insti-\ntution consists of a headquarters office, a branch office and, say, n traveling salesper-\nsons. For the sake of example, let’s suppose that there is bi-directional IPsec traffic\nbetween headquarters and the branch office and bi-directional IPsec traffic between\nheadquarters and the salespersons. In this VPN, how many SAs are there? To answer\nthis question, note that there are two SAs between the headquarters gateway router and\nthe branch-office gateway router (one in each direction); for each salesperson’s laptop,\nthere are two SAs between the headquarters gateway router and the laptop (again, one\nin each direction). So, in total, there are (2 + 2n) SAs. Keep in mind, however, that not\nall traffic sent into the Internet by the gateway routers or by the laptops will be IPsec\nsecured. For example, a host in headquarters may want to access a Web server (such as\nAmazon or Google) in the public Internet. Thus, the gateway router (and the laptops)\nwill emit into the Internet both vanilla IPv4 datagrams and secured IPsec datagrams.\n720\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS"
    },
    {
      "chunk_id": "0eb4f90a-99f7-4a4c-8f80-5a4edf4dffc3",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.7.4 The IPsec Datagram",
      "original_titles": [
        "8.7.4 The IPsec Datagram"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.7 Network-Layer Security: IPsec and Virtual Private Networks > 8.7.4 The IPsec Datagram",
      "start_page": 748,
      "end_page": 751,
      "token_count": 2399,
      "text": "Let’s now take a look “inside” an SA. To make the discussion tangible and con-\ncrete, let’s do this in the context of an SA from router R1 to router R2 in Figure 8.28.\n(You can think of Router R1 as the headquarters gateway router and Router R2 as\nthe branch office gateway router from Figure 8.27.) Router R1 will maintain state\ninformation about this SA, which will include:\n•\nA 32-bit identifier for the SA, called the Security Parameter Index (SPI)\n•\nThe origin interface of the SA (in this case 200.168.1.100) and the destination\ninterface of the SA (in this case 193.68.2.23)\n•\nThe type of encryption to be used (for example, 3DES with CBC)\n•\nThe encryption key\n•\nThe type of integrity check (for example, HMAC with MD5)\n•\nThe authentication key\nWhenever router R1 needs to construct an IPsec datagram for forwarding over\nthis SA, it accesses this state information to determine how it should authenticate\nand encrypt the datagram. Similarly, router R2 will maintain the same state infor-\nmation for this SA and will use this information to authenticate and decrypt any\nIPsec datagram that arrives from the SA.\nAn IPsec entity (router or host) often maintains state information for many SAs.\nFor example, in the VPN example in Figure 8.27 with n salespersons, the headquar-\nters gateway router maintains state information for (2 + 2n) SAs. An IPsec entity\nstores the state information for all of its SAs in its Security Association Database\n(SAD), which is a data structure in the entity’s OS kernel.\n8.7.4 The IPsec Datagram\nHaving now described SAs, we can now describe the actual IPsec datagram. IPsec\nhas two different packet forms, one for the so-called tunnel mode and the other for\nthe so-called transport mode. The tunnel mode, being more appropriate for VPNs,\nis more widely deployed than the transport mode. In order to further de-mystify\n8.7\n•\nNETWORK-LAYER SECURITY: IPSEC AND VIRTUAL PRIVATE NETWORKS\n721\nFigure 8.28 \u0002 Security Association (SA) from R1 to R2\nInternet\nSA\nR1\n172.16.1/24\nHeadquarters\nBranch Office\n200.168.1.100\n193.68.2.23\n172.16.2/24\nR2\n\nIPsec and avoid much of its complication, we henceforth focus exclusively on the\ntunnel mode. Once you have a solid grip on the tunnel mode, you should be able to\neasily learn about the transport mode on your own.\nThe packet format of the IPsec datagram is shown in Figure 8.29. You might\nthink that packet formats are boring and insipid, but we will soon see that the IPsec\ndatagram actually looks and tastes like a popular Tex-Mex delicacy! Let’s examine\nthe IPsec fields in the context of Figure 8.28. Suppose router R1 receives an ordi-\nnary IPv4 datagram from host 172.16.1.17 (in the headquarters network) which is\ndestined to host 172.16.2.48 (in the branch-office network). Router R1 uses the fol-\nlowing recipe to convert this “original IPv4 datagram” into an IPsec datagram:\n•\nAppends to the back of the original IPv4 datagram (which includes the original\nheader fields!) an “ESP trailer” field\n•\nEncrypts the result using the algorithm and key specified by the SA\n•\nAppends to the front of this encrypted quantity a field called “ESP header”; the\nresulting package is called the “enchilada”\n•\nCreates an authentication MAC over the whole enchilada using the algorithm\nand key specified in the SA\n•\nAppends the MAC to the back of the enchilada forming the payload\n•\nFinally, creates a brand new IP header with all the classic IPv4 header fields\n(together normally 20 bytes long), which it appends before the payload\nNote that the resulting IPsec datagram is a bona fide IPv4 datagram, with the\ntraditional IPv4 header fields followed by a payload. But in this case, the payload\ncontains an ESP header, the original IP datagram, an ESP trailer, and an ESP authen-\ntication field (with the original datagram and ESP trailer encrypted). The original IP\ndatagram has 172.16.1.17 for the source IP address and 172.16.2.48 for the destina-\ntion IP address. Because the IPsec datagram includes the original IP datagram, these\naddresses are included (and encrypted) as part of the payload of the IPsec packet.\nBut what about the source and destination IP addresses that are in the new IP header,\nthat is, in the left-most header of the IPsec datagram? As you might expect, they are\nset to the source and destination router interfaces at the two ends of the tunnels,\nnamely, 200.168.1.100 and 193.68.2.23. Also, the protocol number in this new IPv4\nheader field is not set to that of TCP, UDP, or SMTP, but instead to 50, designating\nthat this is an IPsec datagram using the ESP protocol.\nAfter R1 sends the IPsec datagram into the public Internet, it will pass through\nmany routers before reaching R2. Each of these routers will process the datagram as\nif it were an ordinary datagram—they are completely oblivious to the fact that the\ndatagram is carrying IPsec-encrypted data. For these public Internet routers, because\nthe destination IP address in the outer header is R2, the ultimate destination of the\ndatagram is R2.\nHaving walked through an example of how an IPsec datagram is constructed, let’s\nnow take a closer look at the ingredients in the enchilada. We see in Figure 8.29\n722\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nthat the ESP trailer consists of three fields: padding; pad length; and next header.\nRecall that block ciphers require the message to be encrypted to be an integer multi-\nple of the block length. Padding (consisting of meaningless bytes) is used so that\nwhen added to the original datagram (along with the pad length and next header\nfields), the resulting “message” is an integer number of blocks. The pad-length field\nindicates to the receiving entity how much padding was inserted (and thus needs to\nbe removed). The next header identifies the type (e.g., UDP) of data contained in the\npayload-data field. The payload data (typically the original IP datagram) and the\nESP trailer are concatenated and then encrypted.\nAppended to the front of this encrypted unit is the ESP header, which is sent in\nthe clear and consists of two fields: the SPI and the sequence number field. The SPI\nindicates to the receiving entity the SA to which the datagram belongs; the receiving\nentity can then index its SAD with the SPI to determine the appropriate authentica-\ntion/decryption algorithms and keys. The sequence number field is used to defend\nagainst replay attacks.\nThe sending entity also appends an authentication MAC. As stated earlier, the\nsending entity calculates a MAC over the whole enchilada (consisting of the ESP\nheader, the original IP datagram, and the ESP trailer—with the datagram and trailer\nbeing encrypted). Recall that to calculate a MAC, the sender appends a secret MAC\nkey to the enchilada and then calculates a fixed-length hash of the result.\nWhen R2 receives the IPsec datagram, R2 observes that the destination IP\naddress of the datagram is R2 itself. R2 therefore processes the datagram. Because\nthe protocol field (in the left-most IP header) is 50, R2 sees that it should apply\nIPsec ESP processing to the datagram. First, peering into the enchilada, R2 uses the\nSPI to determine to which SA the datagram belongs. Second, it calculates the MAC\nof the enchilada and verifies that the MAC is consistent with the value in the ESP\nMAC field. If it is, it knows that the enchilada comes from R1 and has not been tam-\npered with. Third, it checks the sequence-number field to verify that the datagram is\nfresh (and not a replayed datagram). Fourth, it decrypts the encrypted unit using the\n8.7\n•\nNETWORK-LAYER SECURITY: IPSEC AND VIRTUAL PRIVATE NETWORKS\n723\nFigure 8.29 \u0002 IPsec datagram format\nNew IP\nheader\nESP\nheader\nESP\ntrailer\nESP\nMAC\nOriginal\nIP header\nOriginal IP\ndatagram payload\nEncrypted\n“Enchilada” authenticated\nPad\nlength\nPadding\nNext\nheader\nSPI\nSeq #\n\ndecryption algorithm and key associated with the SA. Fifth, it removes padding and\nextracts the original, vanilla IP datagram. And finally, sixth, it forwards the original\ndatagram into the branch office network towards its ultimate destination. Whew,\nwhat a complicated recipe, huh? Well no one ever said that preparing and unravel-\ning an enchilada was easy!\nThere is actually another important subtlety that needs to be addressed. It cen-\nters on the following question: When R1 receives an (unsecured) datagram from a\nhost in the headquarters network, and that datagram is destined to some destina-\ntion IP address outside of headquarters, how does R1 know whether it should be\nconverted to an IPsec datagram? And if it is to be processed by IPsec, how does\nR1 know which SA (of many SAs in its SAD) should be used to construct the\nIPsec datagram? The problem is solved as follows. Along with a SAD, the IPsec\nentity also maintains another data structure called the Security Policy Database\n(SPD). The SPD indicates what types of datagrams (as a function of source IP\naddress, destination IP address, and protocol type) are to be IPsec processed; and\nfor those that are to be IPsec processed, which SA should be used. In a sense, the\ninformation in a SPD indicates “what” to do with an arriving datagram; the infor-\nmation in the SAD indicates “how” to do it.\nSummary of IPsec Services\nSo what services does IPsec provide, exactly? Let us examine these services from\nthe perspective of an attacker, say Trudy, who is a woman-in-the-middle, sitting\nsomewhere on the path between R1 and R2 in Figure 8.28. Assume throughout this\ndiscussion that Trudy does not know the authentication and encryption keys used by\nthe SA. What can and cannot Trudy do? First, Trudy cannot see the original data-\ngram. If fact, not only is the data in the original datagram hidden from Trudy, but so\nis the protocol number, the source IP address, and the destination IP address. For\ndatagrams sent over the SA, Trudy only knows that the datagram originated from\nsome host in 172.16.1.0/24 and is destined to some host in 172.16.2.0/24. She does\nnot know if it is carrying TCP, UDP, or ICMP data; she does not know if it is carry-\ning HTTP, SMTP, or some other type of application data. This confidentiality thus\ngoes a lot farther than SSL. Second, suppose Trudy tries to tamper with a datagram\nin the SA by flipping some of its bits. When this tampered datagram arrives at R2, it\nwill fail the integrity check (using the MAC), thwarting Trudy’s vicious attempts\nonce again. Third, suppose Trudy tries to masquerade as R1, creating a IPsec data-\ngram with source 200.168.1.100 and destination 193.68.2.23. Trudy’s attack will be\nfutile, as this datagram will again fail the integrity check at R2. Finally, because\nIPsec includes sequence numbers, Trudy will not be able create a successful replay\nattack. In summary, as claimed at the beginning of this section, IPsec provides—\nbetween any pair of devices that process packets through the network layer—\nconfidentiality, source authentication, data integrity, and replay-attack prevention.\n724\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS"
    },
    {
      "chunk_id": "5d7f466d-2500-4117-950e-71239dbb9158",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.7.5 IKE: Key Management in IPsec",
      "original_titles": [
        "8.7.5 IKE: Key Management in IPsec"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.7 Network-Layer Security: IPsec and Virtual Private Networks > 8.7.5 IKE: Key Management in IPsec",
      "start_page": 752,
      "end_page": 752,
      "token_count": 630,
      "text": "8.7.5 IKE: Key Management in IPsec\nWhen a VPN has a small number of end points (for example, just two routers as in\nFigure 8.28), the network administrator can manually enter the SA information\n(encryption/authentication algorithms and keys, and the SPIs) into the SADs of the\nendpoints. Such “manual keying” is clearly impractical for a large VPN, which may\nconsist of hundreds or even thousands of IPsec routers and hosts. Large, geographi-\ncally distributed deployments require an automated mechanism for creating the\nSAs. IPsec does this with the Internet Key Exchange (IKE) protocol, specified in\nRFC 5996.\nIKE has some similarities with the handshake in SSL (see Section 8.6). Each\nIPsec entity has a certificate, which includes the entity’s public key. As with SSL, the\nIKE protocol has the two entities exchange certificates, negotiate authentication and\nencryption algorithms, and securely exchange key material for creating session keys\nin the IPsec SAs. Unlike SSL, IKE employs two phases to carry out these tasks.\nLet’s investigate these two phases in the context of two routers, R1 and R2, in\nFigure 8.28. The first phase consists of two exchanges of message pairs between R1\nand R2:\n•\nDuring the first exchange of messages, the two sides use Diffie-Hellman (see\nHomework Problems) to create a bi-directional IKE SA between the routers. To\nkeep us all confused, this bi-directional IKE SA is entirely different from the\nIPsec SAs discussed in Sections 8.6.3 and 8.6.4. The IKE SA provides an authen-\nticated and encrypted channel between the two routers. During this first mes-\nsage-pair exchange, keys are established for encryption and authentication for\nthe IKE SA. Also established is a master secret that will be used to compute\nIPSec SA keys later in phase 2. Observe that during this first step, RSA public\nand private keys are not used. In particular, neither R1 nor R2 reveals its identity\nby signing a message with its private key.\n•\nDuring the second exchange of messages, both sides reveal their identity to each\nother by signing their messages. However, the identities are not revealed to a\npassive sniffer, since the messages are sent over the secured IKE SA channel.\nAlso during this phase, the two sides negotiate the IPsec encryption and authen-\ntication algorithms to be employed by the IPsec SAs.\nIn phase 2 of IKE, the two sides create an SA in each direction. At the end of\nphase 2, the encryption and authentication session keys are established on both sides\nfor the two SAs. The two sides can then use the SAs to send secured datagrams, as\ndescribed in Sections 8.7.3 and 8.7.4. The primary motivation for having two phases\nin IKE is computational cost—since the second phase doesn’t involve any public-\nkey cryptography, IKE can generate a large number of SAs between the two IPsec\nentities with relatively little computational cost.\n8.7\n•\nNETWORK-LAYER SECURITY: IPSEC AND VIRTUAL PRIVATE NETWORKS\n725"
    },
    {
      "chunk_id": "a956902c-b9d2-414f-8f24-ab1976df80c3",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.8 Securing Wireless LANs",
      "original_titles": [
        "8.8 Securing Wireless LANs"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.8 Securing Wireless LANs",
      "start_page": 753,
      "end_page": 754,
      "token_count": 1095,
      "text": "8.8 Securing Wireless LANs\nSecurity is a particularly important concern in wireless networks, where radio waves\ncarrying frames can propagate far beyond the building containing the wireless base\nstation and hosts. In this section we present a brief introduction to wireless security.\nFor a more in-depth treatment, see the highly readable book by Edney and Arbaugh\n[Edney 2003].\nThe issue of security in 802.11 has attracted considerable attention in both techni-\ncal circles and in the media. While there has been considerable discussion, there has\nbeen little debate—there seems to be universal agreement that the original 802.11\nspecification contains a number of serious security flaws. Indeed, public domain soft-\nware can now be downloaded that exploits these holes, making those who use the\nvanilla 802.11 security mechanisms as open to security attacks as users who use no\nsecurity features at all.\nIn the following section, we discuss the security mechanisms initially standard-\nized in the 802.11 specification, known collectively as Wired Equivalent Privacy\n(WEP). As the name suggests, WEP is meant to provide a level of security similar\nto that found in wired networks. We’ll then discuss a few of the security holes in\nWEP and discuss the 802.11i standard, a fundamentally more secure version of\n802.11 adopted in 2004.\n8.8.1 Wired Equivalent Privacy (WEP)\nThe IEEE 802.11 WEP protocol was designed in 1999 to provide authentication and\ndata encryption between a host and a wireless access point (that is, base station)\nusing a symmetric shared key approach. WEP does not specify a key management\nalgorithm, so it is assumed that the host and wireless access point have somehow\nagreed on the key via an out-of-band method. Authentication is carried out as follows:\n1. A wireless host requests authentication by an access point.\n2. The access point responds to the authentication request with a 128-byte nonce\nvalue.\n3. The wireless host encrypts the nonce using the symmetric key that it shares\nwith the access point.\n4. The access point decrypts the host-encrypted nonce.\nIf the decrypted nonce matches the nonce value originally sent to the host, then the\nhost is authenticated by the access point.\nThe WEP data encryption algorithm is illustrated in Figure 8.30. A secret 40-bit\nsymmetric key, KS, is assumed to be known by both a host and the access point. In\naddition, a 24-bit Initialization Vector (IV) is appended to the 40-bit key to create a\n64-bit key that will be used to encrypt a single frame. The IV will change from one\n726\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nframe to another, and hence each frame will be encrypted with a different 64-bit key.\nEncryption is performed as follows. First a 4-byte CRC value (see Section 5.2) is\ncomputed for the data payload. The payload and the four CRC bytes are then\nencrypted using the RC4 stream cipher. We will not cover the details of RC4 here\n(see [Schneier 1995] and [Edney 2003] for details). For our purposes, it is enough to\nknow that when presented with a key value (in this case, the 64-bit (KS, IV) key), the\nRC4 algorithm produces a stream of key values, k1\nIV, k2\nIV, k3\nIV, . . . that are used to\nencrypt the data and CRC value in a frame. For practical purposes, we can think of\nthese operations being performed a byte at a time. Encryption is performed by\nXOR-ing the ith byte of data, di, with the ith key, ki\nIV, in the stream of key values\ngenerated by the (KS,IV) pair to produce the ith byte of ciphertext, ci:\nci = di \u0002 ki\nIV\nThe IV value changes from one frame to the next and is included in plaintext in\nthe header of each WEP-encrypted 802.11 frame, as shown in Figure 8.30. The\nreceiver takes the secret 40-bit symmetric key that it shares with the sender, appends\nthe IV, and uses the resulting 64-bit key (which is identical to the key used by the\nsender to perform encryption) to decrypt the frame:\ndi = ci \u0002 ki\nIV\nProper use of the RC4 algorithm requires that the same 64-bit key value never\nbe used more than once. Recall that the WEP key changes on a frame-by-frame\nbasis. For a given KS (which changes rarely, if ever), this means that there are only\n224 unique keys. If these keys are chosen randomly, we can show [Walker 2000;\nEdney 2003] that the probability of having chosen the same IV value (and hence\nused the same 64-bit key) is more than 99 percent after only 12,000 frames. With 1\nKbyte frame sizes and a data transmission rate of 11 Mbps, only a few seconds are\n8.8\n•\nSECURING WIRELESS LANS\n727\nFigure 8.30 \u0002 802.11 WEP protocol\nKey sequence generator\n(for given Ks, IV)\nk1\nIV\nd1\nc1\nk2\nIV k3\nIV\nkN\nIV\nIV\nkN+1\nIV\nkN+4\nKs: 40-bit secret symmetric\nPlaintext frame data plus CRC\nIV (per frame)\n802.11\nheader\nIV\nWEP-encrypted data\nplus CRC\nd2\nc2\nd3\nc3\ndN\ncN\nCRC1\ncN+1\ncN+4\nCRC4"
    },
    {
      "chunk_id": "c83ca5f1-1478-4da9-94ad-28b7b1d28736",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.8.1 Wired Equivalent Privacy (WEP)",
      "original_titles": [
        "8.8.1 Wired Equivalent Privacy (WEP)"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.8 Securing Wireless LANs > 8.8.1 Wired Equivalent Privacy (WEP)",
      "start_page": 753,
      "end_page": 754,
      "token_count": 1095,
      "text": "8.8 Securing Wireless LANs\nSecurity is a particularly important concern in wireless networks, where radio waves\ncarrying frames can propagate far beyond the building containing the wireless base\nstation and hosts. In this section we present a brief introduction to wireless security.\nFor a more in-depth treatment, see the highly readable book by Edney and Arbaugh\n[Edney 2003].\nThe issue of security in 802.11 has attracted considerable attention in both techni-\ncal circles and in the media. While there has been considerable discussion, there has\nbeen little debate—there seems to be universal agreement that the original 802.11\nspecification contains a number of serious security flaws. Indeed, public domain soft-\nware can now be downloaded that exploits these holes, making those who use the\nvanilla 802.11 security mechanisms as open to security attacks as users who use no\nsecurity features at all.\nIn the following section, we discuss the security mechanisms initially standard-\nized in the 802.11 specification, known collectively as Wired Equivalent Privacy\n(WEP). As the name suggests, WEP is meant to provide a level of security similar\nto that found in wired networks. We’ll then discuss a few of the security holes in\nWEP and discuss the 802.11i standard, a fundamentally more secure version of\n802.11 adopted in 2004.\n8.8.1 Wired Equivalent Privacy (WEP)\nThe IEEE 802.11 WEP protocol was designed in 1999 to provide authentication and\ndata encryption between a host and a wireless access point (that is, base station)\nusing a symmetric shared key approach. WEP does not specify a key management\nalgorithm, so it is assumed that the host and wireless access point have somehow\nagreed on the key via an out-of-band method. Authentication is carried out as follows:\n1. A wireless host requests authentication by an access point.\n2. The access point responds to the authentication request with a 128-byte nonce\nvalue.\n3. The wireless host encrypts the nonce using the symmetric key that it shares\nwith the access point.\n4. The access point decrypts the host-encrypted nonce.\nIf the decrypted nonce matches the nonce value originally sent to the host, then the\nhost is authenticated by the access point.\nThe WEP data encryption algorithm is illustrated in Figure 8.30. A secret 40-bit\nsymmetric key, KS, is assumed to be known by both a host and the access point. In\naddition, a 24-bit Initialization Vector (IV) is appended to the 40-bit key to create a\n64-bit key that will be used to encrypt a single frame. The IV will change from one\n726\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nframe to another, and hence each frame will be encrypted with a different 64-bit key.\nEncryption is performed as follows. First a 4-byte CRC value (see Section 5.2) is\ncomputed for the data payload. The payload and the four CRC bytes are then\nencrypted using the RC4 stream cipher. We will not cover the details of RC4 here\n(see [Schneier 1995] and [Edney 2003] for details). For our purposes, it is enough to\nknow that when presented with a key value (in this case, the 64-bit (KS, IV) key), the\nRC4 algorithm produces a stream of key values, k1\nIV, k2\nIV, k3\nIV, . . . that are used to\nencrypt the data and CRC value in a frame. For practical purposes, we can think of\nthese operations being performed a byte at a time. Encryption is performed by\nXOR-ing the ith byte of data, di, with the ith key, ki\nIV, in the stream of key values\ngenerated by the (KS,IV) pair to produce the ith byte of ciphertext, ci:\nci = di \u0002 ki\nIV\nThe IV value changes from one frame to the next and is included in plaintext in\nthe header of each WEP-encrypted 802.11 frame, as shown in Figure 8.30. The\nreceiver takes the secret 40-bit symmetric key that it shares with the sender, appends\nthe IV, and uses the resulting 64-bit key (which is identical to the key used by the\nsender to perform encryption) to decrypt the frame:\ndi = ci \u0002 ki\nIV\nProper use of the RC4 algorithm requires that the same 64-bit key value never\nbe used more than once. Recall that the WEP key changes on a frame-by-frame\nbasis. For a given KS (which changes rarely, if ever), this means that there are only\n224 unique keys. If these keys are chosen randomly, we can show [Walker 2000;\nEdney 2003] that the probability of having chosen the same IV value (and hence\nused the same 64-bit key) is more than 99 percent after only 12,000 frames. With 1\nKbyte frame sizes and a data transmission rate of 11 Mbps, only a few seconds are\n8.8\n•\nSECURING WIRELESS LANS\n727\nFigure 8.30 \u0002 802.11 WEP protocol\nKey sequence generator\n(for given Ks, IV)\nk1\nIV\nd1\nc1\nk2\nIV k3\nIV\nkN\nIV\nIV\nkN+1\nIV\nkN+4\nKs: 40-bit secret symmetric\nPlaintext frame data plus CRC\nIV (per frame)\n802.11\nheader\nIV\nWEP-encrypted data\nplus CRC\nd2\nc2\nd3\nc3\ndN\ncN\nCRC1\ncN+1\ncN+4\nCRC4"
    },
    {
      "chunk_id": "05823d28-be3e-498b-a4f6-452760b38718",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.8.2 IEEE 802.11i",
      "original_titles": [
        "8.8.2 IEEE 802.11i"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.8 Securing Wireless LANs > 8.8.2 IEEE 802.11i",
      "start_page": 755,
      "end_page": 757,
      "token_count": 1446,
      "text": "needed before 12,000 frames are transmitted. Furthermore, since the IV is transmit-\nted in plaintext in the frame, an eavesdropper will know whenever a duplicate IV\nvalue is used.\nTo see one of the several problems that occur when a duplicate key is used, con-\nsider the following chosen-plaintext attack taken by Trudy against Alice. Suppose\nthat Trudy (possibly using IP spoofing) sends a request (for example, an HTTP or\nFTP request) to Alice to transmit a file with known content, d1, d2, d3, d4,. . . . Trudy\nalso observes the encrypted data c1, c2, c3, c4. . . . Since di = ci \u0002 ki\nIV, if we XOR ci\nwith each side of this equality we have\ndi \u0002 ci = ki\nIV\nWith this relationship, Trudy can use the known values of di and ci to compute ki\nIV.\nThe next time Trudy sees the same value of IV being used, she will know the key\nsequence k1\nIV, k2\nIV, k3\nIV, . . . and will thus be able to decrypt the encrypted message.\nThere are several additional security concerns with WEP as well. [Fluhrer\n2001] described an attack exploiting a known weakness in RC4 when certain weak\nkeys are chosen. [Stubblefield 2002] discusses efficient ways to implement and\nexploit this attack. Another concern with WEP involves the CRC bits shown in Fig-\nure 8.30 and transmitted in the 802.11 frame to detect altered bits in the payload.\nHowever, an attacker who changes the encrypted content (e.g., substituting gibber-\nish for the original encrypted data), computes a CRC over the substituted gibberish,\nand places the CRC into a WEP frame can produce an 802.11 frame that will be\naccepted by the receiver. What is needed here are message integrity techniques such\nas those we studied in Section 8.3 to detect content tampering or substitution. For\nmore details of WEP security, see [Edney 2003; Walker 2000; Weatherspoon 2000]\nand the references therein.\n8.8.2 IEEE 802.11i\nSoon after the 1999 release of IEEE 802.11, work began on developing a new and\nimproved version of 802.11 with stronger security mechanisms. The new standard,\nknown as 802.11i, underwent final ratification in 2004. As we’ll see, while WEP\nprovided relatively weak encryption, only a single way to perform authentication,\nand no key distribution mechanisms, IEEE 802.11i provides for much stronger\nforms of encryption, an extensible set of authentication mechanisms, and a key dis-\ntribution mechanism. In the following, we present an overview of 802.11i; an excel-\nlent (streaming audio) technical overview of 802.11i is [TechOnline 2012].\nFigure 8.31 overviews the 802.11i framework. In addition to the wireless client\nand access point, 802.11i defines an authentication server with which the AP can\ncommunicate. Separating the authentication server from the AP allows one authenti-\ncation server to serve many APs, centralizing the (often sensitive) decisions\n728\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nregarding authentication and access within the single server, and keeping AP costs\nand complexity low. 802.11i operates in four phases:\n1. Discovery. In the discovery phase, the AP advertises its presence and the forms\nof authentication and encryption that can be provided to the wireless client\nnode. The client then requests the specific forms of authentication and encryp-\ntion that it desires. Although the client and AP are already exchanging mes-\nsages, the client has not yet been authenticated nor does it have an encryption\nkey, and so several more steps will be required before the client can communi-\ncate with an arbitrary remote host over the wireless channel.\n2. Mutual authentication and Master Key (MK) generation. Authentication takes\nplace between the wireless client and the authentication server. In this phase,\nthe access point acts essentially as a relay, forwarding messages between the\nclient and the authentication server. The Extensible Authentication Protocol\n(EAP) [RFC 3748] defines the end-to-end message formats used in a simple\nrequest/response mode of interaction between the client and authentication\nserver. As shown in Figure 8.32 EAP messages are encapsulated using\nEAPoL (EAP over LAN, [IEEE 802.1X]) and sent over the 802.11 wireless\nlink. These EAP messages are then decapsulated at the access point, and then\n8.8\n•\nSECURING WIRELESS LANS\n729\nFigure 8.31 \u0002 802.11i: four phases of operation\nSTA:\nclient station\nAP:\naccess point\nWired\nnetwork\nAS:\nauthentication\nserver\n1\nDiscovery of\nsecurity capabilities\n4\nSTA, AP use PMK to derive\nTemporal Key (TK) used for\nmessage encryption, integrity\nAS derives same PMK,\nsends to AP\nSTA derives Pairwise \nMaster Key (PMK)\n2\n3\n3\nSTA and AS mutually authenticate, together generate\nMaster Key (MK). AP serves as “pass through”\n\nre-encapsulated using the RADIUS protocol for transmission over UDP/IP to\nthe authentication server. While the RADIUS server and protocol [RFC 2865]\nare not required by the 802.11i protocol, they are de facto standard compo-\nnents for 802.11i. The recently standardized DIAMETER protocol [RFC\n3588] is likely to replace RADIUS in the near future.\nWith EAP, the authentication server can choose one of a number of ways to\nperform authentication. While 802.11i does not mandate a particular authenti-\ncation method, the EAP-TLS authentication scheme [RFC 5216] is often used.\nEAP-TLS uses public key techniques (including nonce encryption and message\ndigests) similar to those we studied in Section 8.3 to allow the client and the\nauthentication server to mutually authenticate each other, and to derive a \nMaster Key (MK) that is known to both parties.\n3. Pairwise Master Key (PMK) generation. The MK is a shared secret known\nonly to the client and the authentication server, which they each use to generate\na second key, the Pairwise Master Key (PMK). The authentication server then\nsends the PMK to the AP. This is where we wanted to be! The client and AP\nnow have a shared key (recall that in WEP, the problem of key distribution was\nnot addressed at all) and have mutually authenticated each other. They’re just\nabout ready to get down to business.\n4. Temporal Key (TK) generation. With the PMK, the wireless client and AP can\nnow generate additional keys that will be used for communication. Of particular\ninterest is the Temporal Key (TK), which will be used to perform the link-level\nencryption of data sent over the wireless link and to an arbitrary remote host.\n730\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nFigure 8.32 \u0002 EAP is an end-to-end protocol. EAP messages are encapsu-\nlated using EAPoL over the wireless link between the client\nand the access point, and using RADIUS over UDP/IP\nbetween the access point and the authentication server\nSTA:\nclient station\nAP:\naccess point\nWired\nnetwork\nAS:\nauthentication\nserver\nEAP TLS\nEAP\nEAP over LAN (EAPoL)\nRADIUS\nIEEE 802.11\nUDP/IP"
    },
    {
      "chunk_id": "4910acfa-1070-44e2-bd90-79b8d6e98792",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.9 Operational Security: Firewalls and Intrusion Detection Systems",
      "original_titles": [
        "8.9 Operational Security: Firewalls and Intrusion Detection Systems"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.9 Operational Security: Firewalls and Intrusion Detection Systems",
      "start_page": 758,
      "end_page": 765,
      "token_count": 3980,
      "text": "802.11i provides several forms of encryption, including an AES-based encryption\nscheme and a strengthened version of WEP encryption.\n8.9 Operational Security: Firewalls and \nIntrusion Detection Systems\nWe’ve seen throughout this chapter that the Internet is not a very safe place—bad\nguys are out there, wreaking all sorts of havoc. Given the hostile nature of the\nInternet, let’s now consider an organization’s network and the network administra-\ntor who administers it. From a network administrator’s point of view, the world\ndivides quite neatly into two camps—the good guys (who belong to the organiza-\ntion’s network, and who should be able to access resources inside the organi-\nzation’s network in a relatively unconstrained manner) and the bad guys (everyone\nelse, whose access to network resources must be carefully scrutinized). In many\norganizations, ranging from medieval castles to modern corporate office buildings,\nthere is a single point of entry/exit where both good guys and bad guys entering\nand leaving the organization are security-checked. In a castle, this was done at a\ngate at one end of the drawbridge; in a corporate building, this is done at the secu-\nrity desk. In a computer network, when traffic entering/leaving a network is secu-\nrity-checked, logged, dropped, or forwarded, it is done by operational devices\nknown as firewalls, intrusion detection systems (IDSs), and intrusion prevention\nsystems (IPSs).\n8.9.1 Firewalls\nA firewall is a combination of hardware and software that isolates an organization’s\ninternal network from the Internet at large, allowing some packets to pass and block-\ning others. A firewall allows a network administrator to control access between the\noutside world and resources within the administered network by managing the traf-\nfic flow to and from these resources. A firewall has three goals:\n•\nAll traffic from outside to inside, and vice versa, passes through the firewall.\nFigure 8.33 shows a firewall, sitting squarely at the boundary between the admin-\nistered network and the rest of the Internet. While large organizations may use\nmultiple levels of firewalls or distributed firewalls [Skoudis 2006], locating a\nfirewall at a single access point to the network, as shown in Figure 8.33, makes\nit easier to manage and enforce a security-access policy.\n•\nOnly authorized traffic, as defined by the local security policy, will be allowed to\npass. With all traffic entering and leaving the institutional network passing\nthrough the firewall, the firewall can restrict access to authorized traffic.\n8.9\n•\nOPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS\n731\n\n•\nThe firewall itself is immune to penetration. The firewall itself is a device con-\nnected to the network. If not designed or installed properly, it can be compro-\nmised, in which case it provides only a false sense of security (which is worse\nthan no firewall at all!).\nCisco and Check Point are two of the leading firewall vendors today. You can also\neasily create a firewall (packet filter) from a Linux box using iptables (public-\ndomain software that is normally shipped with Linux).\nFirewalls can be classified in three categories: traditional packet filters,\nstateful filters, and application gateways. We’ll cover each of these in turn in the\nfollowing subsections.\nTraditional Packet Filters\nAs shown in Figure 8.33, an organization typically has a gateway router connecting\nits internal network to its ISP (and hence to the larger public Internet). All traffic leav-\ning and entering the internal network passes through this router, and it is at this router\nwhere packet filtering occurs. A packet filter examines each datagram in isolation,\ndetermining whether the datagram should be allowed to pass or should be dropped\nbased on administrator-specific rules. Filtering decisions are typically based on:\n732\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nFigure 8.33 \u0002 Firewall placement between the administered network and\nthe outside world\nAdministered\nnetwork\nFirewall\nPublic\nInternet\n\n•\nIP source or destination address\n•\nProtocol type in IP datagram field: TCP, UDP, ICMP, OSPF, and so on\n•\nTCP or UDP source and destination port\n•\nTCP flag bits: SYN, ACK, and so on\n•\nICMP message type\n•\nDifferent rules for datagrams leaving and entering the network\n•\nDifferent rules for the different router interfaces\nA network administrator configures the firewall based on the policy of the organ-\nization. The policy may take user productivity and bandwidth usage into account as\nwell as the security concerns of an organization. Table 8.5 lists a number of possible\npolices an organization may have, and how they would be addressed with a packet\nfilter. For example, if the organization doesn’t want any incoming TCP connections\nexcept those for its public Web server, it can block all incoming TCP SYN segments\nexcept TCP SYN segments with destination port 80 and the destination IP address\ncorresponding to the Web server. If the organization doesn’t want its users to monop-\nolize access bandwidth with Internet radio applications, it can block all not-critical\nUDP traffic (since Internet radio is often sent over UDP). If the organization doesn’t\nwant its internal network to be mapped (tracerouted) by an outsider, it can block all\nICMP TTL expired messages leaving the organization’s network.\nA filtering policy can be based on a combination of addresses and port numbers.\nFor example, a filtering router could forward all Telnet datagrams (those with a \nport number of 23) except those going to and coming from a list of specific IP\naddresses. This policy permits Telnet connections to and from hosts on the allowed\n8.9\n•\nOPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS\n733\nPolicy\nFirewall Setting\nNo outside Web access.\nDrop all outgoing packets to any IP address, port 80\nTable 8.5 \u0002 Policies and corresponding filtering rules for an organization’s\nnetwork 130.27/16 with Web server at 130.207.244.203\nNo incoming TCP connections, except \nthose for organization’s public Web server only.\nDrop all incoming TCP SYN packets to any IP except\n130.207.244.203, port 80\nPrevent Web-radios from eating up the \navailable bandwidth.\nDrop all incoming UDP packets—except DNS packets.\nPrevent your network from being used \nfor a smurf DoS attack.\nDrop all ICMP ping packets going to a “broadcast”\naddress (eg 130.207.255.255).\nPrevent your network from being tracerouted\nDrop all outgoing ICMP TTL expired traffic\n\nlist. Unfortunately, basing the policy on external addresses provides no protection\nagainst datagrams that have had their source addresses spoofed.\nFiltering can also be based on whether or not the TCP ACK bit is set. This trick\nis quite useful if an organization wants to let its internal clients connect to external\nservers but wants to prevent external clients from connecting to internal servers.\nRecall from Section 3.5 that the first segment in every TCP connection has the ACK\nbit set to 0, whereas all the other segments in the connection have the ACK bit set to\n1. Thus, if an organization wants to prevent external clients from initiating connec-\ntions to internal servers, it simply filters all incoming segments with the ACK bit set\nto 0. This policy kills all TCP connections originating from the outside, but permits\nconnections originating internally.\nFirewall rules are implemented in routers with access control lists, with each\nrouter interface having its own list. An example of an access control list for an\norganization 222.22/16 is shown in Table 8.6. This access control list is for an\ninterface that connects the router to the organization’s external ISPs. Rules are\napplied to each datagram that passes through the interface from top to bottom. The\nfirst two rules together allow internal users to surf the Web: The first rule allows any\nTCP packet with destination port 80 to leave the organization’s network; the sec-\nond rule allows any TCP packet with source port 80 and the ACK bit set to enter\nthe organization’s network. Note that if an external source attempts to establish a\nTCP connection with an internal host, the connection will be blocked, even if the\nsource or destination port is 80. The second two rules together allow DNS packets\nto enter and leave the organization’s network. In summary, this rather restrictive\naccess control list blocks all traffic except Web traffic initiated from within the\norganization and DNS traffic. [CERT Filtering 2012] provides a list of recom-\nmended port/protocol packet filterings to avoid a number of well-known security\nholes in existing network applications.\n734\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\naction\nsource address\ndest address\nprotocol\nsource port\ndest port\nflag bit\nTable 8.6 \u0002 An access control list for a router interface\nallow\n222.22/16\noutside of\n222.22/16\nTCP\n> 1023\n80\nany\nallow\noutside of\n222.22/16\n222.22/16\nTCP\n80\n> 1023\nACK\nallow\n222.22/16\noutside of\n222.22/16\nUDP\n> 1023\n53\n—\nallow\noutside of\n222.22/16\n222.22/16\nUDP\n53\n> 1023\n—\ndeny\nall\nall\nall\nall\nall\nall\n\nStateful Packet Filters\nIn a traditional packet filter, filtering decisions are made on each packet in isolation.\nStateful filters actually track TCP connections, and use this knowledge to make fil-\ntering decisions.\nTo understand stateful filters, let’s reexamine the access control list in Table 8.6.\nAlthough rather restrictive, the access control list in Table 8.6 nevertheless allows\nany packet arriving from the outside with ACK = 1 and source port 80 to get through\nthe filter. Such packets could be used by attackers in attempts to crash internal sys-\ntems with malformed packets, carry out denial-of-service attacks, or map the inter-\nnal network. The naive solution is to block TCP ACK packets as well, but such an\napproach would prevent the organization’s internal users from surfing the Web.\nStateful filters solve this problem by tracking all ongoing TCP connections in a\nconnection table. This is possible because the firewall can observe the beginning of\na new connection by observing a three-way handshake (SYN, SYNACK, and\nACK); and it can observe the end of a connection when it sees a FIN packet for the\nconnection. The firewall can also (conservatively) assume that the connection is\nover when it hasn’t seen any activity over the connection for, say, 60 seconds. An\nexample connection table for a firewall is shown in Table 8.7. This connection table\nindicates that there are currently three ongoing TCP connections, all of which have\nbeen initiated from within the organization. Additionally, the stateful filter includes\na new column, “check connection,” in its access control list, as shown in Table 8.8.\nNote that Table 8.8 is identical to the access control list in Table 8.6, except now it\nindicates that the connection should be checked for two of the rules.\nLet’s walk through some examples to see how the connection table and the\nextended access control list work hand-in-hand. Suppose an attacker attempts to\nsend a malformed packet into the organization’s network by sending a datagram\nwith TCP source port 80 and with the ACK flag set. Further suppose that this packet\nhas source port number 12543 and source IP address 150.23.23.155. When this\npacket reaches the firewall, the firewall checks the access control list in Table 8.7,\nwhich indicates that the connection table must also be checked before permitting\nthis packet to enter the organization’s network. The firewall duly checks the connec-\ntion table, sees that this packet is not part of an ongoing TCP connection, and rejects\n8.9\n•\nOPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS\n735\nsource address\ndest address\nsource port\ndest port\n222.22.1.7\n37.96.87.123\n12699\n80\n222.22.93.2\n199.1.205.23\n37654\n80\n222.22.65.143\n203.77.240.43\n48712\n80\nTable 8.7 \u0002 Connection table for stateful filter\n\nthe packet. As a second example, suppose that an internal user wants to surf an\nexternal Web site. Because this user first sends a TCP SYN segment, the user’s TCP\nconnection gets recorded in the connection table. When the Web server sends back\npackets (with the ACK bit necessarily set), the firewall checks the table and sees that\na corresponding connection is in progress. The firewall will thus let these packets\npass, thereby not interfering with the internal user’s Web surfing activity.\nApplication Gateway\nIn the examples above, we have seen that packet-level filtering allows an organiza-\ntion to perform coarse-grain filtering on the basis of the contents of IP and\nTCP/UDP headers, including IP addresses, port numbers, and acknowledgment bits.\nBut what if an organization wants to provide a Telnet service to a restricted set of\ninternal users (as opposed to IP addresses)? And what if the organization wants such\nprivileged users to authenticate themselves first before being allowed to create Telnet\nsessions to the outside world? Such tasks are beyond the capabilities of traditional\nand stateful filters. Indeed, information about the identity of the internal users is\napplication-layer data and is not included in the IP/TCP/UDP headers.\nTo have finer-level security, firewalls must combine packet filters with applica-\ntion gateways. Application gateways look beyond the IP/TCP/UDP headers and\nmake policy decisions based on application data. An application gateway is an\napplication-specific server through which all application data (inbound and out-\nbound) must pass. Multiple application gateways can run on the same host, but each\ngateway is a separate server with its own processes.\n736\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nTable 8.8 \u0002 Access control list for stateful filter\naction\nsource\naddress\ndest\naddress\nprotocol\nsource port\ndest port\nflag bit\nallow\n222.22/16\noutside of\n222.22/16\nTCP\n>1023\n80\nany\nallow\noutside of\n222.22/16\n222.22/16\nTCP\n80\n>1023\nACK\nallow\n222.22/16\noutside of\n222.22/16\nUDP\n>1023\n53\n—\nallow\noutside of\n222.22/16\n222.22/16\nUDP\n53\n>1023\n—\ndeny\nall\nall\nall\nall\nall\nall\ncheck\nconxion\nX\nX\n\nTo get some insight into application gateways, let’s design a firewall that allows\nonly a restricted set of internal users to Telnet outside and prevents all external\nclients from Telneting inside. Such a policy can be accomplished by implementing a\ncombination of a packet filter (in a router) and a Telnet application gateway, as\nshown in Figure 8.34. The router’s filter is configured to block all Telnet connec-\ntions except those that originate from the IP address of the application gateway.\nSuch a filter configuration forces all outbound Telnet connections to pass through\nthe application gateway. Consider now an internal user who wants to Telnet to the\noutside world. The user must first set up a Telnet session with the application gate-\nway. An application running in the gateway, which listens for incoming Telnet ses-\nsions, prompts the user for a user ID and password. When the user supplies this\ninformation, the application gateway checks to see if the user has permission to\nTelnet to the outside world. If not, the Telnet connection from the internal user to the\ngateway is terminated by the gateway. If the user has permission, then the gateway\n(1) prompts the user for the host name of the external host to which the user wants\nto connect, (2) sets up a Telnet session between the gateway and the external host,\nand (3) relays to the external host all data arriving from the user, and relays to the\nuser all data arriving from the external host. Thus, the Telnet application gateway\nnot only performs user authorization but also acts as a Telnet server and a Telnet\nclient, relaying information between the user and the remote Telnet server. Note that\n8.9\n•\nOPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS\n737\nFigure 8.34 \u0002 Firewall consisting of an application gateway and a filter\nApplication\ngateway\nHost-to-gateway\nTelnet session\nGateway-to-remote\nhost Telnet session\nRouter\nand filter\n\nthe filter will permit step 2 because the gateway initiates the Telnet connection to\nthe outside world.\n738\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nANONYMITY AND PRIVACY\nSuppose you want to visit a controversial Web site (for example, a political activist\nsite) and you (1) don’t want to reveal your IP address to the Web site, (2) don’t want\nyour local ISP (which may be your home or office ISP) to know that you are visiting\nthe site, and (3) you don’t want your local ISP to see the data you are exchanging\nwith the site. If you use the traditional approach of connecting directly to the Web\nsite without any encryption, you fail on all three counts. Even if you use SSL, you fail\non the first two counts: Your source IP address is presented to the Web site in every\ndatagram you send; and the destination address of every packet you send can easily\nbe sniffed by your local ISP.\nTo obtain privacy and anonymity, you can instead use a combination of a trusted\nproxy server and SSL, as shown in Figure 8.35. With this approach, you first make\nan SSL connection to the trusted proxy. You then send, into this SSL connection, an\nHTTP request for a page at the desired site. When the proxy receives the SSL-encrypt-\ned HTTP request, it decrypts the request and forwards the cleartext HTTP request to\nthe Web site. The Web site then responds to the proxy, which in turn forwards the\nresponse to you over SSL. Because the Web site only sees the IP address of the\nproxy, and not of your client’s address, you are indeed obtaining anonymous access\nto the Web site. And because all traffic between you and the proxy is encrypted,\nyour local ISP cannot invade your privacy by logging the site you visited or recording\nthe data you are exchanging. Many companies today (such as proxify.com) make\navailable such proxy services.\nOf course, in this solution, your proxy knows everything: It knows your IP address\nand the IP address of the site you’re surfing; and it can see all the traffic in cleartext\nexchanged between you and the Web site. Such a solution, therefore, is only as\ngood as the trustworthiness of the proxy. A more robust approach, taken by the TOR\nanonymizing and privacy service, is to route your traffic through a series of non-\ncolluding proxy servers [TOR 2012]. In particular, TOR allows independent individu-\nals to contribute proxies to its proxy pool. When a user connects to a server using\nTOR, TOR randomly chooses (from its proxy pool) a chain of three proxies and\nroutes all traffic between client and server over the chain. In this manner, assuming\nthe proxies do not collude, no one knows that communication took place between\nyour IP address and the target Web site. Furthermore, although cleartext is sent\nbetween the last proxy and the server, the last proxy doesn’t know what IP address is\nsending and receiving the cleartext.\nCASE HISTORY"
    },
    {
      "chunk_id": "6abc862b-cc63-4cbb-868a-77ee289b8a41",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.9.1 Firewalls",
      "original_titles": [
        "8.9.1 Firewalls"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.9 Operational Security: Firewalls and Intrusion Detection Systems > 8.9.1 Firewalls",
      "start_page": 758,
      "end_page": 765,
      "token_count": 3980,
      "text": "802.11i provides several forms of encryption, including an AES-based encryption\nscheme and a strengthened version of WEP encryption.\n8.9 Operational Security: Firewalls and \nIntrusion Detection Systems\nWe’ve seen throughout this chapter that the Internet is not a very safe place—bad\nguys are out there, wreaking all sorts of havoc. Given the hostile nature of the\nInternet, let’s now consider an organization’s network and the network administra-\ntor who administers it. From a network administrator’s point of view, the world\ndivides quite neatly into two camps—the good guys (who belong to the organiza-\ntion’s network, and who should be able to access resources inside the organi-\nzation’s network in a relatively unconstrained manner) and the bad guys (everyone\nelse, whose access to network resources must be carefully scrutinized). In many\norganizations, ranging from medieval castles to modern corporate office buildings,\nthere is a single point of entry/exit where both good guys and bad guys entering\nand leaving the organization are security-checked. In a castle, this was done at a\ngate at one end of the drawbridge; in a corporate building, this is done at the secu-\nrity desk. In a computer network, when traffic entering/leaving a network is secu-\nrity-checked, logged, dropped, or forwarded, it is done by operational devices\nknown as firewalls, intrusion detection systems (IDSs), and intrusion prevention\nsystems (IPSs).\n8.9.1 Firewalls\nA firewall is a combination of hardware and software that isolates an organization’s\ninternal network from the Internet at large, allowing some packets to pass and block-\ning others. A firewall allows a network administrator to control access between the\noutside world and resources within the administered network by managing the traf-\nfic flow to and from these resources. A firewall has three goals:\n•\nAll traffic from outside to inside, and vice versa, passes through the firewall.\nFigure 8.33 shows a firewall, sitting squarely at the boundary between the admin-\nistered network and the rest of the Internet. While large organizations may use\nmultiple levels of firewalls or distributed firewalls [Skoudis 2006], locating a\nfirewall at a single access point to the network, as shown in Figure 8.33, makes\nit easier to manage and enforce a security-access policy.\n•\nOnly authorized traffic, as defined by the local security policy, will be allowed to\npass. With all traffic entering and leaving the institutional network passing\nthrough the firewall, the firewall can restrict access to authorized traffic.\n8.9\n•\nOPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS\n731\n\n•\nThe firewall itself is immune to penetration. The firewall itself is a device con-\nnected to the network. If not designed or installed properly, it can be compro-\nmised, in which case it provides only a false sense of security (which is worse\nthan no firewall at all!).\nCisco and Check Point are two of the leading firewall vendors today. You can also\neasily create a firewall (packet filter) from a Linux box using iptables (public-\ndomain software that is normally shipped with Linux).\nFirewalls can be classified in three categories: traditional packet filters,\nstateful filters, and application gateways. We’ll cover each of these in turn in the\nfollowing subsections.\nTraditional Packet Filters\nAs shown in Figure 8.33, an organization typically has a gateway router connecting\nits internal network to its ISP (and hence to the larger public Internet). All traffic leav-\ning and entering the internal network passes through this router, and it is at this router\nwhere packet filtering occurs. A packet filter examines each datagram in isolation,\ndetermining whether the datagram should be allowed to pass or should be dropped\nbased on administrator-specific rules. Filtering decisions are typically based on:\n732\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nFigure 8.33 \u0002 Firewall placement between the administered network and\nthe outside world\nAdministered\nnetwork\nFirewall\nPublic\nInternet\n\n•\nIP source or destination address\n•\nProtocol type in IP datagram field: TCP, UDP, ICMP, OSPF, and so on\n•\nTCP or UDP source and destination port\n•\nTCP flag bits: SYN, ACK, and so on\n•\nICMP message type\n•\nDifferent rules for datagrams leaving and entering the network\n•\nDifferent rules for the different router interfaces\nA network administrator configures the firewall based on the policy of the organ-\nization. The policy may take user productivity and bandwidth usage into account as\nwell as the security concerns of an organization. Table 8.5 lists a number of possible\npolices an organization may have, and how they would be addressed with a packet\nfilter. For example, if the organization doesn’t want any incoming TCP connections\nexcept those for its public Web server, it can block all incoming TCP SYN segments\nexcept TCP SYN segments with destination port 80 and the destination IP address\ncorresponding to the Web server. If the organization doesn’t want its users to monop-\nolize access bandwidth with Internet radio applications, it can block all not-critical\nUDP traffic (since Internet radio is often sent over UDP). If the organization doesn’t\nwant its internal network to be mapped (tracerouted) by an outsider, it can block all\nICMP TTL expired messages leaving the organization’s network.\nA filtering policy can be based on a combination of addresses and port numbers.\nFor example, a filtering router could forward all Telnet datagrams (those with a \nport number of 23) except those going to and coming from a list of specific IP\naddresses. This policy permits Telnet connections to and from hosts on the allowed\n8.9\n•\nOPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS\n733\nPolicy\nFirewall Setting\nNo outside Web access.\nDrop all outgoing packets to any IP address, port 80\nTable 8.5 \u0002 Policies and corresponding filtering rules for an organization’s\nnetwork 130.27/16 with Web server at 130.207.244.203\nNo incoming TCP connections, except \nthose for organization’s public Web server only.\nDrop all incoming TCP SYN packets to any IP except\n130.207.244.203, port 80\nPrevent Web-radios from eating up the \navailable bandwidth.\nDrop all incoming UDP packets—except DNS packets.\nPrevent your network from being used \nfor a smurf DoS attack.\nDrop all ICMP ping packets going to a “broadcast”\naddress (eg 130.207.255.255).\nPrevent your network from being tracerouted\nDrop all outgoing ICMP TTL expired traffic\n\nlist. Unfortunately, basing the policy on external addresses provides no protection\nagainst datagrams that have had their source addresses spoofed.\nFiltering can also be based on whether or not the TCP ACK bit is set. This trick\nis quite useful if an organization wants to let its internal clients connect to external\nservers but wants to prevent external clients from connecting to internal servers.\nRecall from Section 3.5 that the first segment in every TCP connection has the ACK\nbit set to 0, whereas all the other segments in the connection have the ACK bit set to\n1. Thus, if an organization wants to prevent external clients from initiating connec-\ntions to internal servers, it simply filters all incoming segments with the ACK bit set\nto 0. This policy kills all TCP connections originating from the outside, but permits\nconnections originating internally.\nFirewall rules are implemented in routers with access control lists, with each\nrouter interface having its own list. An example of an access control list for an\norganization 222.22/16 is shown in Table 8.6. This access control list is for an\ninterface that connects the router to the organization’s external ISPs. Rules are\napplied to each datagram that passes through the interface from top to bottom. The\nfirst two rules together allow internal users to surf the Web: The first rule allows any\nTCP packet with destination port 80 to leave the organization’s network; the sec-\nond rule allows any TCP packet with source port 80 and the ACK bit set to enter\nthe organization’s network. Note that if an external source attempts to establish a\nTCP connection with an internal host, the connection will be blocked, even if the\nsource or destination port is 80. The second two rules together allow DNS packets\nto enter and leave the organization’s network. In summary, this rather restrictive\naccess control list blocks all traffic except Web traffic initiated from within the\norganization and DNS traffic. [CERT Filtering 2012] provides a list of recom-\nmended port/protocol packet filterings to avoid a number of well-known security\nholes in existing network applications.\n734\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\naction\nsource address\ndest address\nprotocol\nsource port\ndest port\nflag bit\nTable 8.6 \u0002 An access control list for a router interface\nallow\n222.22/16\noutside of\n222.22/16\nTCP\n> 1023\n80\nany\nallow\noutside of\n222.22/16\n222.22/16\nTCP\n80\n> 1023\nACK\nallow\n222.22/16\noutside of\n222.22/16\nUDP\n> 1023\n53\n—\nallow\noutside of\n222.22/16\n222.22/16\nUDP\n53\n> 1023\n—\ndeny\nall\nall\nall\nall\nall\nall\n\nStateful Packet Filters\nIn a traditional packet filter, filtering decisions are made on each packet in isolation.\nStateful filters actually track TCP connections, and use this knowledge to make fil-\ntering decisions.\nTo understand stateful filters, let’s reexamine the access control list in Table 8.6.\nAlthough rather restrictive, the access control list in Table 8.6 nevertheless allows\nany packet arriving from the outside with ACK = 1 and source port 80 to get through\nthe filter. Such packets could be used by attackers in attempts to crash internal sys-\ntems with malformed packets, carry out denial-of-service attacks, or map the inter-\nnal network. The naive solution is to block TCP ACK packets as well, but such an\napproach would prevent the organization’s internal users from surfing the Web.\nStateful filters solve this problem by tracking all ongoing TCP connections in a\nconnection table. This is possible because the firewall can observe the beginning of\na new connection by observing a three-way handshake (SYN, SYNACK, and\nACK); and it can observe the end of a connection when it sees a FIN packet for the\nconnection. The firewall can also (conservatively) assume that the connection is\nover when it hasn’t seen any activity over the connection for, say, 60 seconds. An\nexample connection table for a firewall is shown in Table 8.7. This connection table\nindicates that there are currently three ongoing TCP connections, all of which have\nbeen initiated from within the organization. Additionally, the stateful filter includes\na new column, “check connection,” in its access control list, as shown in Table 8.8.\nNote that Table 8.8 is identical to the access control list in Table 8.6, except now it\nindicates that the connection should be checked for two of the rules.\nLet’s walk through some examples to see how the connection table and the\nextended access control list work hand-in-hand. Suppose an attacker attempts to\nsend a malformed packet into the organization’s network by sending a datagram\nwith TCP source port 80 and with the ACK flag set. Further suppose that this packet\nhas source port number 12543 and source IP address 150.23.23.155. When this\npacket reaches the firewall, the firewall checks the access control list in Table 8.7,\nwhich indicates that the connection table must also be checked before permitting\nthis packet to enter the organization’s network. The firewall duly checks the connec-\ntion table, sees that this packet is not part of an ongoing TCP connection, and rejects\n8.9\n•\nOPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS\n735\nsource address\ndest address\nsource port\ndest port\n222.22.1.7\n37.96.87.123\n12699\n80\n222.22.93.2\n199.1.205.23\n37654\n80\n222.22.65.143\n203.77.240.43\n48712\n80\nTable 8.7 \u0002 Connection table for stateful filter\n\nthe packet. As a second example, suppose that an internal user wants to surf an\nexternal Web site. Because this user first sends a TCP SYN segment, the user’s TCP\nconnection gets recorded in the connection table. When the Web server sends back\npackets (with the ACK bit necessarily set), the firewall checks the table and sees that\na corresponding connection is in progress. The firewall will thus let these packets\npass, thereby not interfering with the internal user’s Web surfing activity.\nApplication Gateway\nIn the examples above, we have seen that packet-level filtering allows an organiza-\ntion to perform coarse-grain filtering on the basis of the contents of IP and\nTCP/UDP headers, including IP addresses, port numbers, and acknowledgment bits.\nBut what if an organization wants to provide a Telnet service to a restricted set of\ninternal users (as opposed to IP addresses)? And what if the organization wants such\nprivileged users to authenticate themselves first before being allowed to create Telnet\nsessions to the outside world? Such tasks are beyond the capabilities of traditional\nand stateful filters. Indeed, information about the identity of the internal users is\napplication-layer data and is not included in the IP/TCP/UDP headers.\nTo have finer-level security, firewalls must combine packet filters with applica-\ntion gateways. Application gateways look beyond the IP/TCP/UDP headers and\nmake policy decisions based on application data. An application gateway is an\napplication-specific server through which all application data (inbound and out-\nbound) must pass. Multiple application gateways can run on the same host, but each\ngateway is a separate server with its own processes.\n736\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nTable 8.8 \u0002 Access control list for stateful filter\naction\nsource\naddress\ndest\naddress\nprotocol\nsource port\ndest port\nflag bit\nallow\n222.22/16\noutside of\n222.22/16\nTCP\n>1023\n80\nany\nallow\noutside of\n222.22/16\n222.22/16\nTCP\n80\n>1023\nACK\nallow\n222.22/16\noutside of\n222.22/16\nUDP\n>1023\n53\n—\nallow\noutside of\n222.22/16\n222.22/16\nUDP\n53\n>1023\n—\ndeny\nall\nall\nall\nall\nall\nall\ncheck\nconxion\nX\nX\n\nTo get some insight into application gateways, let’s design a firewall that allows\nonly a restricted set of internal users to Telnet outside and prevents all external\nclients from Telneting inside. Such a policy can be accomplished by implementing a\ncombination of a packet filter (in a router) and a Telnet application gateway, as\nshown in Figure 8.34. The router’s filter is configured to block all Telnet connec-\ntions except those that originate from the IP address of the application gateway.\nSuch a filter configuration forces all outbound Telnet connections to pass through\nthe application gateway. Consider now an internal user who wants to Telnet to the\noutside world. The user must first set up a Telnet session with the application gate-\nway. An application running in the gateway, which listens for incoming Telnet ses-\nsions, prompts the user for a user ID and password. When the user supplies this\ninformation, the application gateway checks to see if the user has permission to\nTelnet to the outside world. If not, the Telnet connection from the internal user to the\ngateway is terminated by the gateway. If the user has permission, then the gateway\n(1) prompts the user for the host name of the external host to which the user wants\nto connect, (2) sets up a Telnet session between the gateway and the external host,\nand (3) relays to the external host all data arriving from the user, and relays to the\nuser all data arriving from the external host. Thus, the Telnet application gateway\nnot only performs user authorization but also acts as a Telnet server and a Telnet\nclient, relaying information between the user and the remote Telnet server. Note that\n8.9\n•\nOPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS\n737\nFigure 8.34 \u0002 Firewall consisting of an application gateway and a filter\nApplication\ngateway\nHost-to-gateway\nTelnet session\nGateway-to-remote\nhost Telnet session\nRouter\nand filter\n\nthe filter will permit step 2 because the gateway initiates the Telnet connection to\nthe outside world.\n738\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nANONYMITY AND PRIVACY\nSuppose you want to visit a controversial Web site (for example, a political activist\nsite) and you (1) don’t want to reveal your IP address to the Web site, (2) don’t want\nyour local ISP (which may be your home or office ISP) to know that you are visiting\nthe site, and (3) you don’t want your local ISP to see the data you are exchanging\nwith the site. If you use the traditional approach of connecting directly to the Web\nsite without any encryption, you fail on all three counts. Even if you use SSL, you fail\non the first two counts: Your source IP address is presented to the Web site in every\ndatagram you send; and the destination address of every packet you send can easily\nbe sniffed by your local ISP.\nTo obtain privacy and anonymity, you can instead use a combination of a trusted\nproxy server and SSL, as shown in Figure 8.35. With this approach, you first make\nan SSL connection to the trusted proxy. You then send, into this SSL connection, an\nHTTP request for a page at the desired site. When the proxy receives the SSL-encrypt-\ned HTTP request, it decrypts the request and forwards the cleartext HTTP request to\nthe Web site. The Web site then responds to the proxy, which in turn forwards the\nresponse to you over SSL. Because the Web site only sees the IP address of the\nproxy, and not of your client’s address, you are indeed obtaining anonymous access\nto the Web site. And because all traffic between you and the proxy is encrypted,\nyour local ISP cannot invade your privacy by logging the site you visited or recording\nthe data you are exchanging. Many companies today (such as proxify.com) make\navailable such proxy services.\nOf course, in this solution, your proxy knows everything: It knows your IP address\nand the IP address of the site you’re surfing; and it can see all the traffic in cleartext\nexchanged between you and the Web site. Such a solution, therefore, is only as\ngood as the trustworthiness of the proxy. A more robust approach, taken by the TOR\nanonymizing and privacy service, is to route your traffic through a series of non-\ncolluding proxy servers [TOR 2012]. In particular, TOR allows independent individu-\nals to contribute proxies to its proxy pool. When a user connects to a server using\nTOR, TOR randomly chooses (from its proxy pool) a chain of three proxies and\nroutes all traffic between client and server over the chain. In this manner, assuming\nthe proxies do not collude, no one knows that communication took place between\nyour IP address and the target Web site. Furthermore, although cleartext is sent\nbetween the last proxy and the server, the last proxy doesn’t know what IP address is\nsending and receiving the cleartext.\nCASE HISTORY"
    },
    {
      "chunk_id": "d490e140-7632-4963-8abf-0604d4a7f1e0",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.9.2 Intrusion Detection Systems",
      "original_titles": [
        "8.9.2 Intrusion Detection Systems"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.9 Operational Security: Firewalls and Intrusion Detection Systems > 8.9.2 Intrusion Detection Systems",
      "start_page": 766,
      "end_page": 768,
      "token_count": 1423,
      "text": "Internal networks often have multiple application gateways, for example, gate-\nways for Telnet, HTTP, FTP, and e-mail. In fact, an organization’s mail server (see\nSection 2.4) and Web cache are application gateways.\nApplication gateways do not come without their disadvantages. First, a differ-\nent application gateway is needed for each application. Second, there is a perform-\nance penalty to be paid, since all data will be relayed via the gateway. This becomes\na concern particularly when multiple users or applications are using the same gate-\nway machine. Finally, the client software must know how to contact the gateway\nwhen the user makes a request, and must know how to tell the application gateway\nwhat external server to connect to.\n8.9.2 Intrusion Detection Systems\nWe’ve just seen that a packet filter (traditional and stateful) inspects IP, TCP, UDP,\nand ICMP header fields when deciding which packets to let pass through the firewall.\nHowever, to detect many attack types, we need to perform deep packet inspection,\nthat is, look beyond the header fields and into the actual application data that the\npackets carry. As we saw in Section 8.9.1, application gateways often do deep packet\ninspection. But an application gateway only does this for a specific application.\nClearly, there is a niche for yet another device—a device that not only examines\nthe headers of all packets passing through it (like a packet filter), but also performs\ndeep packet inspection (unlike a packet filter). When such a device observes a\nsuspicious packet, or a suspicious series of packets, it could prevent those packets\nfrom entering the organizational network. Or, because the activity is only deemed as\nsuspicious, the device could let the packets pass, but send alerts to a network\nadministrator, who can then take a closer look at the traffic and take appropriate\n8.9\n•\nOPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS\n739\nFigure 8.35 \u0002 Providing anonymity and privacy with a proxy\nAlice\nAnonymizing\nProxy\nSSL\nCleartext\n\nactions. A device that generates alerts when it observes potentially malicious traffic\nis called an intrusion detection system (IDS). A device that filters out suspicious\ntraffic is called an intrusion prevention system (IPS). In this section we study both\nsystems—IDS and IPS—together, since the most interesting technical aspect of\nthese systems is how they detect suspicious traffic (and not whether they send alerts or\ndrop packets). We will henceforth collectively refer to IDS systems and IPS systems\nas IDS systems.\nAn IDS can be used to detect a wide range of attacks, including network map-\nping (emanating, for example, from nmap), port scans, TCP stack scans, DoS band-\nwidth-flooding attacks, worms and viruses, OS vulnerability attacks, and application\nvulnerability attacks. (See Section 1.6 for a survey of network attacks.) Today, thou-\nsands of organizations employ IDS systems. Many of these deployed systems are\nproprietary, marketed by Cisco, Check Point, and other security equipment vendors.\nBut many of the deployed IDS systems are public-domain systems, such as the\nimmensely popular Snort IDS system (which we’ll discuss shortly).\n740\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\nFigure 8.36 \u0002 An organization deploying a filter, an application gateway,\nand IDS sensors\nInternet\nWeb\nserver\nFTP\nserver\nDNS\nserver\nInternal\nnetwork\nApplication\ngateway\nDemilitarized zone\nFilter\nKey:\n= IDS sensors\n\nAn organization may deploy one or more IDS sensors in its organizational net-\nwork. Figure 8.36 shows an organization that has three IDS sensors. When multiple\nsensors are deployed, they typically work in concert, sending information about sus-\npicious traffic activity to a central IDS processor, which collects and integrates the\ninformation and sends alarms to network administrators when deemed appropriate.\nIn Figure 8.36, the organization has partitioned its network into two regions: a high-\nsecurity region, protected by a packet filter and an application gateway and moni-\ntored by IDS sensors; and a lower-security region—referred to as the demilitarized\nzone (DMZ)—which is protected only by the packet filter, but also monitored by\nIDS sensors. Note that the DMZ includes the organization’s servers that need to com-\nmunicate with the outside world, such as its public Web server and its authoritative\nDNS server.\nYou may be wondering at this stage, why multiple IDS sensors? Why not just\nplace one IDS sensor just behind the packet filter (or even integrated with the packet\nfilter) in Figure 8.36? We will soon see that an IDS not only needs to do deep packet\ninspection, but must also compare each passing packet with tens of thousands of\n“signatures”; this can be a significant amount of processing, particularly if the\norganization receives gigabits/sec of traffic from the Internet. By placing the IDS\nsensors further downstream, each sensor sees only a fraction of the organization’s\ntraffic, and can more easily keep up. Nevertheless, high-performance IDS and IPS\nsystems are available today, and many organizations can actually get by with just\none sensor located near its access router.\nIDS systems are broadly classified as either signature-based systems or\nanomaly-based systems. A signature-based IDS maintains an extensive database\nof attack signatures. Each signature is a set of rules pertaining to an intrusion activ-\nity. A signature may simply be a list of characteristics about a single packet (e.g.,\nsource and destination port numbers, protocol type, and a specific string of bits in\nthe packet payload), or may relate to a series of packets. The signatures are nor-\nmally created by skilled network security engineers who research known attacks.\nAn organization’s network administrator can customize the signatures or add its\nown to the database.\nOperationally, a signature-based IDS sniffs every packet passing by it, compar-\ning each sniffed packet with the signatures in its database. If a packet (or series of\npackets) matches a signature in the database, the IDS generates an alert. The alert\ncould be sent to the network administrator in an e-mail message, could be sent to the\nnetwork management system, or could simply be logged for future inspection.\nSignature-based IDS systems, although widely deployed, have a number of limi-\ntations. Most importantly, they require previous knowledge of the attack to generate\nan accurate signature. In other words, a signature-based IDS is completely blind to\nnew attacks that have yet to be recorded. Another disadvantage is that even if a sig-\nnature is matched, it may not be the result of an attack, so that a false alarm is gener-\nated. Finally, because every packet must be compared with an extensive collection of\nsignatures, the IDS can become overwhelmed with processing and actually fail to\ndetect many malicious packets.\n8.9\n•\nOPERATIONAL SECURITY: FIREWALLS AND INTRUSION DETECTION SYSTEMS\n741"
    },
    {
      "chunk_id": "20d66570-db1f-432a-920a-0d3b521242c0",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "8.10 Summary",
      "original_titles": [
        "8.10 Summary"
      ],
      "path": "Chapter 8 Security in Computer Networks > 8.10 Summary",
      "start_page": 769,
      "end_page": 770,
      "token_count": 1237,
      "text": "An anomaly-based IDS creates a traffic profile as it observes traffic in normal\noperation. It then looks for packet streams that are statistically unusual, for exam-\nple, an inordinate percentage of ICMP packets or a sudden exponential growth in\nport scans and ping sweeps. The great thing about anomaly-based IDS systems is\nthat they don’t rely on previous knowledge about existing attacks—that is, they can\npotentially detect new, undocumented attacks. On the other hand, it is an extremely\nchallenging problem to distinguish between normal traffic and statistically unusual\ntraffic. To date, most IDS deployments are primarily signature-based, although\nsome include some anomaly-based features.\nSnort\nSnort is a public-domain, open source IDS with hundreds of thousands of existing\ndeployments [Snort 2012; Koziol 2003]. It can run on Linux, UNIX, and Windows\nplatforms. It uses the generic sniffing interface libpcap, which is also used by\nWireshark and many other packet sniffers. It can easily handle 100 Mbps of traffic;\nfor installations with gibabit/sec traffic rates, multiple Snort sensors may be needed.\nTo gain some insight into Snort, let’s take a look at an example of a Snort signature:\nalert icmp $EXTERNAL_NET any -> $HOME_NET any\n(msg:”ICMP PING NMAP”; dsize: 0; itype: 8;)\nThis signature is matched by any ICMP packet that enters the organization’s net-\nwork ($HOME_NET) from the outside ($EXTERNAL_NET), is of type 8 (ICMP\nping), and has an empty payload (dsize = 0). Since nmap (see Section 1.6) generates\nping packets with these specific characteristics, this signature is designed to detect\nnmap ping sweeps. When a packet matches this signature, Snort generates an alert\nthat includes the message “ICMP PING NMAP”.\nPerhaps what is most impressive about Snort is the vast community of users and\nsecurity experts that maintain its signature database. Typically within a few hours of\na new attack, the Snort community writes and releases an attack signature, which is\nthen downloaded by the hundreds of thousands of Snort deployments distributed\naround the world. Moreover, using the Snort signature syntax, network administra-\ntors can tailor the signatures to their own organization’s needs by either modifying\nexisting signatures or creating entirely new ones.\n8.10 Summary\nIn this chapter, we’ve examined the various mechanisms that our secret lovers, Bob\nand Alice, can use to communicate securely. We’ve seen that Bob and Alice are\ninterested in confidentiality (so they alone are able to understand the contents of a\ntransmitted message), end-point authentication (so they are sure that they are talking\n742\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nwith each other), and message integrity (so they are sure that their messages are\nnot altered in transit). Of course, the need for secure communication is not confined\nto secret lovers. Indeed, we saw in Sections 8.5 through 8.8 that security can be used\nin various layers in a network architecture to protect against bad guys who have a\nlarge arsenal of possible attacks at hand.\nThe first part of this chapter presented various principles underlying secure\ncommunication. In Section 8.2, we covered cryptographic techniques for encrypting\nand decrypting data, including symmetric key cryptography and public key cryptog-\nraphy. DES and RSA were examined as specific case studies of these two major\nclasses of cryptographic techniques in use in today’s networks.\nIn Section 8.3, we examined two approaches for providing message integrity:\nmessage authentication codes (MACs) and digital signatures. The two approaches\nhave a number of parallels. Both use cryptographic hash functions and both tech-\nniques enable us to verify the source of the message as well as the integrity of the\nmessage itself. One important difference is that MACs do not rely on encryption\nwhereas digital signatures require a public key infrastructure. Both techniques are\nextensively used in practice, as we saw in Sections 8.5 through 8.8. Furthermore, dig-\nital signatures are used to create digital certificates, which are important for verifying\nthe validity of public keys. In Section 8.4, we examined endpoint authentication and\nintroduced nonces to defend against the replay attack.\nIn Sections 8.5 through 8.8 we examined several security networking proto-\ncols that enjoy extensive use in practice. We saw that symmetric key cryptogra-\nphy is at the core of PGP, SSL, IPsec, and wireless security. We saw that public\nkey cryptography is crucial for both PGP and SSL. We saw that PGP uses digital\nsignatures for message integrity, whereas SSL and IPsec use MACs. Having now\nan understanding of the basic principles of cryptography, and having studied how\nthese principles are actually used, you are now in position to design your own\nsecure network protocols!\nArmed with the techniques covered in Sections 8.2 through 8.8, Bob and Alice\ncan communicate securely. (One can only hope that they are networking students\nwho have learned this material and can thus avoid having their tryst uncovered by\nTrudy!) But confidentiality is only a small part of the network security picture.\nAs we learned in Section 8.9, increasingly, the focus in network security has been\non securing the network infrastructure against a potential onslaught by the bad guys.\nIn the latter part of this chapter, we thus covered firewalls and IDS systems which\ninspect packets entering and leaving an organization’s network.\nThis chapter has covered a lot of ground, while focusing on the most impor-\ntant topics in modern network security. Readers who desire to dig deeper are\nencouraged to investigate the references cited in this chapter. In particular, we rec-\nommend [Skoudis 2006] for attacks and operational security, [Kaufman 1995] for\ncryptography and how it applies to network security, [Rescorla 2001] for an in-\ndepth but readable treatment of SSL, and [Edney 2003] for a thorough discussion\nof 802.11 security, including an insightful investigation into WEP and its flaws.\n8.10\n•\nSUMMARY\n743"
    },
    {
      "chunk_id": "58cc4199-529a-4eff-8b42-8620e3871f15",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Homework Problems and Questions",
      "original_titles": [
        "Homework Problems and Questions"
      ],
      "path": "Chapter 8 Security in Computer Networks > Homework Problems and Questions",
      "start_page": 771,
      "end_page": 778,
      "token_count": 4249,
      "text": "Homework Problems and Questions\nChapter 8 Review Problems\nSECTION 8.1\nR1. What are the differences between message confidentiality and message\nintegrity? Can you have confidentiality without integrity? Can you have\nintegrity without confidentiality? Justify your answer.\nR2. Internet entities (routers, switches, DNS servers, Web servers, user end sys-\ntems, and so on) often need to communicate securely. Give three specific\nexample pairs of Internet entities that may want secure communication.\nSECTION 8.2\nR3. From a service perspective, what is an important difference between a\nsymmetric-key system and a public-key system?\nR4. Suppose that an intruder has an encrypted message as well as the decrypted\nversion of that message. Can the intruder mount a ciphertext-only attack, a\nknown-plaintext attack, or a chosen-plaintext attack?\nR5. Consider an 8-block cipher. How many possible input blocks does this cipher\nhave? How many possible mappings are there? If we view each mapping as a\nkey, then how many possible keys does this cipher have?\nR6. Suppose N people want to communicate with each of N – 1 other people\nusing symmetric key encryption. All communication between any two peo-\nple, i and j, is visible to all other people in this group of N, and no other per-\nson in this group should be able to decode their communication. How many\nkeys are required in the system as a whole? Now suppose that public key\nencryption is used. How many keys are required in this case?\nR7. Suppose n = 10,000, a = 10,023, and b = 10,004. Use an identity of modular\narithmetic to calculate in your head (a • b) mod n.\nR8. Suppose you want to encrypt the message 10101111 by encrypting the deci-\nmal number that corresponds to the message. What is the decimal number?\nSECTIONS 8.3–8.4\nR9. In what way does a hash provide a better message integrity check than a\nchecksum (such as the Internet checksum)?\nR10. Can you “decrypt” a hash of a message to get the original message? Explain\nyour answer.\nR11. Consider a variation of the MAC algorithm (Figure 8.9) where the sender\nsends (m, H(m) + s), where H(m) + s is the concatenation of H(m) and s. Is\nthis variation flawed? Why or why not?\n744\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nR12. What does it mean for a signed document to be verifiable and non-forgeable?\nR13. In what way does the public-key encrypted message hash provide a better\ndigital signature than the public-key encrypted message?\nR14. Suppose certifier.com creates a certificate for foo.com. Typically, the entire\ncertificate would be encrypted with certifier.com’s public key. True or False?\nR15. Suppose Alice has a message that she is ready to send to anyone who asks.\nThousands of people want to obtain Alice’s message, but each wants to be sure\nof the integrity of the message. In this context, do you think a MAC-based or a\ndigital-signature-based integrity scheme is more suitable? Why?\nR16. What is the purpose of a nonce in an end-point authentication protocol?\nR17. What does it mean to say that a nonce is a once-in-a-lifetime value? In whose\nlifetime?\nR18. Is the message integrity scheme based on HMAC susceptible to playback\nattacks? If so, how can a nonce be incorporated into the scheme to remove\nthis susceptibility?\nSECTIONS 8.5–8.8\nR19. Suppose that Bob receives a PGP message from Alice. How does Bob know\nfor sure that Alice created the message (rather than, say, Trudy)? Does PGP\nuse a MAC for message integrity?\nR20. In the SSL record, there is a field for SSL sequence numbers. True or False?\nR21. What is the purpose of the random nonces in the SSL handshake?\nR22. Suppose an SSL session employs a block cipher with CBC. True or False:\nThe server sends to the client the IV in the clear?\nR23. Suppose Bob initiates a TCP connection to Trudy who is pretending to be\nAlice. During the handshake, Trudy sends Bob Alice’s certificate. In what\nstep of the SSL handshake algorithm will Bob discover that he is not commu-\nnicating with Alice?\nR24. Consider sending a stream of packets from Host A to Host B using IPsec.\nTypically, a new SA will be established for each packet sent in the stream.\nTrue or False?\nR25. Suppose that TCP is being run over IPsec between headquarters and the\nbranch office in Figure 8.28. If TCP retransmits the same packet, then the two\ncorresponding packets sent by R1 packets will have the same sequence num-\nber in the ESP header. True or False?\nR26. An IKE SA and an IPsec SA are the same thing. True or False?\nR27. Consider WEP for 802.11. Suppose that the data is 10101100 and the\nkeystream is 1111000. What is the resulting ciphertext?\nR28. In WEP, an IV is sent in the clear in every frame. True or False?\nHOMEWORK PROBLEMS AND QUESTIONS\n745\n\nSECTION 8.9\nR29. Stateful packet filters maintain two data structures. Name them and briefly\ndescribe what they do.\nR30. Consider a traditional (stateless) packet filter. This packet filter may filter\npackets based on TCP flag bits as well as other header fields. True or False?\nR31. In a traditional packet filter, each interface can have its own access control\nlist. True or False?\nR32. Why must an application gateway work in conjunction with a router filter to\nbe effective?\nR33. Signature-based IDSs and IPSs inspect into the payloads of TCP and UDP\nsegments. True or False?\nProblems\nP1. Using the monoalphabetic cipher in Figure 8.3, encode the message “This is\nan easy problem.” Decode the message “rmij’u uamu xyj.”\nP2. Show that Trudy’s known-plaintext attack, in which she knows the\n(ciphertext, plaintext) translation pairs for seven letters, reduces the num-\nber of possible substitutions to be checked in the example in Section 8.2.1\nby approximately 109.\nP3. Consider the polyalphabetic system shown in Figure 8.4. Will a chosen-\nplaintext attack that is able to get the plaintext encoding of the message “The\nquick brown fox jumps over the lazy dog.” be sufficient to decode all mes-\nsages? Why or why not?\nP4. Consider the block cipher in Figure 8.5. Suppose that each block cipher Ti\nsimply reverses the order of the eight input bits (so that, for example,\n11110000 becomes 00001111). Further suppose that the 64-bit scrambler\ndoes not modify any bits (so that the output value of the mth bit is equal to\nthe input value of the mth bit). (a) With n = 3 and the original 64-bit input\nequal to 10100000 repeated eight times, what is the value of the output?\n(b) Repeat part (a) but now change the last bit of the original 64-bit input\nfrom a 0 to a 1. (c) Repeat parts (a) and (b) but now suppose that the 64-bit\nscrambler inverses the order of the 64 bits.\nP5. Consider the block cipher in Figure 8.5. For a given “key” Alice and Bob\nwould need to keep eight tables, each 8 bits by 8 bits. For Alice (or Bob)\nto store all eight tables, how many bits of storage are necessary? How does\nthis number compare with the number of bits required for a full-table 64-\nbit block cipher?\nP6. Consider the 3-bit block cipher in Table 8.1. Suppose the plaintext is\n100100100. (a) Initially assume that CBC is not used. What is the resulting\n746\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nciphertext? (b) Suppose Trudy sniffs the ciphertext. Assuming she knows that\na 3-bit block cipher without CBC is being employed (but doesn’t know the\nspecific cipher), what can she surmise? (c) Now suppose that CBC is used\nwith IV = 111. What is the resulting ciphertext?\nP7. (a) Using RSA, choose p = 3 and q = 11, and encode the word “dog” by\nencrypting each letter separately. Apply the decryption algorithm to the\nencrypted version to recover the original plaintext message. (b) Repeat\npart (a) but now encrypt “dog” as one message m.\nP8. Consider RSA with p = 5 and q = 11.\na. What are n and z?\nb. Let e be 3. Why is this an acceptable choice for e?\nc. Find d such that de = 1 (mod z) and d < 160.\nd. Encrypt the message m = 8 using the key (n, e). Let c denote the corre-\nsponding ciphertext. Show all work. Hint: To simplify the calculations,\nuse the fact:\n[(a mod n) • (b mod n)] mod n = (a • b) mod n\nP9. In this problem, we explore the Diffie-Hellman (DH) public-key encryption\nalgorithm, which allows two entities to agree on a shared key. The \nDH algorithm makes use of a large prime number p and another large number\ng less than p. Both p and g are made public (so that an attacker would know\nthem). In DH, Alice and Bob each independently choose secret keys, SA and\nSB, respectively. Alice then computes her public key, TA, by raising g to SA and\nthen taking mod p. Bob similarly computes his own public key TB by raising g\nto SB and then taking mod p. Alice and Bob then exchange their public keys\nover the Internet. Alice then calculates the shared secret key S by raising TB to\nSA and then taking mod p. Similarly, Bob calculates the shared key S´ by rais-\ning TA to SB and then taking mod p.\na. Prove that, in general, Alice and Bob obtain the same symmetric key, that\nis, prove S = S´.\nb. With p = 11 and g = 2, suppose Alice and Bob choose private keys SA = 5\nand SB = 12, respectively. Calculate Alice’s and Bob’s public keys, TA and\nTB . Show all work.\nc. Following up on part (b), now calculate S as the shared symmetric key. Show\nall work.\nd. Provide a timing diagram that shows how Diffie-Hellman can be attacked by\na man-in-the-middle. The timing diagram should have three vertical lines, one\nfor Alice, one for Bob, and one for the attacker Trudy.\nP10. Suppose Alice wants to communicate with Bob using symmetric key cryp-\ntography using a session key KS. In Section 8.2, we learned how public-key\nPROBLEMS\n747\n\ncryptography can be used to distribute the session key from Alice to Bob. In\nthis problem, we explore how the session key can be distributed—without\npublic key cryptography—using a key distribution center (KDC). The KDC\nis a server that shares a unique secret symmetric key with each registered\nuser. For Alice and Bob, denote these keys by KA-KDC and KB-KDC. Design a\nscheme that uses the KDC to distribute KS to Alice and Bob. Your scheme\nshould use three messages to distribute the session key: a message from Alice\nto the KDC; a message from the KDC to Alice; and finally a message from\nAlice to Bob. The first message is KA-KDC (A, B). Using the notation, KA-KDC,\nKB-KDC, S, A, and B answer the following questions.\na. What is the second message?\nb. What is the third message?\nP11. Compute a third message, different from the two messages in Figure 8.8, that\nhas the same checksum as the messages in Figure 8.8.\nP12. Suppose Alice and Bob share two secret keys: an authentication key S1 and a\nsymmetric encryption key S2. Augment Figure 8.9 so that both integrity and\nconfidentiality are provided.\nP13. In the BitTorrent P2P file distribution protocol (see Chapter 2), the seed\nbreaks the file into blocks, and the peers redistribute the blocks to each other.\nWithout any protection, an attacker can easily wreak havoc in a torrent by\nmasquerading as a benevolent peer and sending bogus blocks to a small sub-\nset of peers in the torrent. These unsuspecting peers then redistribute the\nbogus blocks to other peers, which in turn redistribute the bogus blocks to\neven more peers. Thus, it is critical for BitTorrent to have a mechanism that\nallows a peer to verify the integrity of a block, so that it doesn’t redistribute\nbogus blocks. Assume that when a peer joins a torrent, it initially gets a\n.torrent file from a fully trusted source. Describe a simple scheme that\nallows peers to verify the integrity of blocks.\nP14. The OSPF routing protocol uses a MAC rather than digital signatures to\nprovide message integrity. Why do you think a MAC was chosen over dig-\nital signatures?\nP15. Consider our authentication protocol in Figure 8.18 in which Alice authenti-\ncates herself to Bob, which we saw works well (i.e., we found no flaws in it).\nNow suppose that while Alice is authenticating herself to Bob, Bob must\nauthenticate himself to Alice. Give a scenario by which Trudy, pretending to be\nAlice, can now authenticate herself to Bob as Alice. (Hint: Consider that the\nsequence of operations of the protocol, one with Trudy initiating and one with\nBob initiating, can be arbitrarily interleaved. Pay particular attention to the fact\nthat both Bob and Alice will use a nonce, and that if care is not taken, the same\nnonce can be used maliciously.)\nP16. A natural question is whether we can use a nonce and public key cryptogra-\nphy to solve the end-point authentication problem in Section 8.4. Consider\n748\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n\nthe following natural protocol: (1)  Alice sends the message “I am\nAlice” to Bob. (2) Bob chooses a nonce, R, and sends it to Alice. (3) Alice\nuses her private key to encrypt the nonce and sends the resulting value to\nBob. (4) Bob applies Alice's public key to the received message. Thus, Bob\ncomputes R and authenticates Alice. \na. Diagram this protocol, using the notation for public and private keys\nemployed in the textbook.\nb. Suppose that certificates are not used. Describe how Trudy can become a\n“woman-in-the-middle” by intercepting Alice’s messages and then pre-\ntending to be Alice to Bob.\nP17. Figure 8.19 shows the operations that Alice must perform with PGP to pro-\nvide confidentiality, authentication, and integrity. Diagram the corresponding\noperations that Bob must perform on the package received from Alice.\nP18. Suppose Alice wants to send an e-mail to Bob. Bob has a public-private key\npair (KB\n+,KB\n–), and Alice has Bob’s certificate. But Alice does not have a\npublic, private key pair. Alice and Bob (and the entire world) share the same\nhash function H(\u0003).\na. In this situation, is it possible to design a scheme so that Bob can verify\nthat Alice created the message? If so, show how with a block diagram for\nAlice and Bob.\nb. Is it possible to design a scheme that provides confidentiality for sending\nthe message from Alice to Bob? If so, show how with a block diagram for\nAlice and Bob.\nP19. Consider the Wireshark output below for a portion of an SSL session.\na. Is Wireshark packet 112 sent by the client or server?\nb. What is the server’s IP address and port number?\nc. Assuming no loss and no retransmissions, what will be the sequence num-\nber of the next TCP segment sent by the client?\nd. How many SSL records does Wireshark packet 112 contain?\ne. Does packet 112 contain a Master Secret or an Encrypted Master Secret or\nneither?\nf. Assuming that the handshake type field is 1 byte and each length field is\n3 bytes, what are the values of the first and last bytes of the Master Secret\n(or Encrypted Master Secret)?\ng. The client encrypted handshake message takes into account how many\nSSL records?\nh. The server encrypted handshake message takes into account how many\nSSL records?\nP20. In Section 8.6.1, it is shown that without sequence numbers, Trudy (a woman-\nin-the middle) can wreak havoc in an SSL session by interchanging TCP\nPROBLEMS\n749\n\nsegments. Can Trudy do something similar by deleting a TCP segment? What\ndoes she need to do to succeed at the deletion attack? What effect will it have?\nP21. Suppose Alice and Bob are communicating over an SSL session. Suppose an\nattacker, who does not have any of the shared keys, inserts a bogus TCP segment\ninto a packet stream with correct TCP checksum and sequence numbers (and cor-\nrect IP addresses and port numbers). Will SSL at the receiving side accept the\nbogus packet and pass the payload to the receiving application? Why or why not?\nP22. The following True/False questions pertain to Figure 8.28.\na. When a host in 172.16.1/24 sends a datagram to an Amazon.com server,\nthe router R1 will encrypt the datagram using IPsec.\nb. When a host in 172.16.1/24 sends a datagram to a host in 172.16.2/24, the\nrouter R1 will change the source and destination address of the IP datagram.\nc. Suppose a host in 172.16.1/24 initiates a TCP connection to a Web server\nin 172.16.2/24. As part of this connection, all datagrams sent by R1 will\nhave protocol number 50 in the left-most IPv4 header field.\n750\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS\n(Wireshark screenshot reprinted by permission of the Wireshark Foundation.)\n\nd. Consider sending a TCP segment from a host in 172.16.1/24 to a host in\n172.16.2/24. Suppose the acknowledgment for this segment gets lost, so\nthat TCP resends the segment. Because IPsec uses sequence numbers, R1\nwill not resend the TCP segment.\nP23. Consider the example in Figure 8.28. Suppose Trudy is a woman-in-the-\nmiddle, who can insert datagrams into the stream of datagrams going from\nR1 and R2. As part of a replay attack, Trudy sends a duplicate copy of one of\nthe datagrams sent from R1 to R2. Will R2 decrypt the duplicate datagram\nand forward it into the branch-office network? If not, describe in detail how\nR2 detects the duplicate datagram.\nP24. Consider the following pseudo-WEP protocol. The key is 4 bits and the IV\nis 2 bits. The IV is appended to the end of the key when generating the\nkeystream. Suppose that the shared secret key is 1010. The keystreams for\nthe four possible inputs are as follows:\n101000: 0010101101010101001011010100100 . . .\n101001: 1010011011001010110100100101101 . . .\n101010: 0001101000111100010100101001111 . . .\n101011: 1111101010000000101010100010111 . . .\nSuppose all messages are 8-bits long. Suppose the ICV (integrity check) is \n4-bits long, and is calculated by XOR-ing the first 4 bits of data with the last\n4 bits of data. Suppose the pseudo-WEP packet consists of three fields: first\nthe IV field, then the message field, and last the ICV field, with some of \nthese fields encrypted.\na. We want to send the message m = 10100000 using the IV = 11 and using\nWEP. What will be the values in the three WEP fields?\nb. Show that when the receiver decrypts the WEP packet, it recovers the\nmessage and the ICV.\nc. Suppose Trudy intercepts a WEP packet (not necessarily with the IV = 11)\nand wants to modify it before forwarding it to the receiver. Suppose Trudy\nflips the first ICV bit. Assuming that Trudy does not know the keystreams\nfor any of the IVs, what other bit(s) must Trudy also flip so that the\nreceived packet passes the ICV check?\nd. Justify your answer by modifying the bits in the WEP packet in part (a),\ndecrypting the resulting packet, and verifying the integrity check.\nP25. Provide a filter table and a connection table for a stateful firewall that is as\nrestrictive as possible but accomplishes the following:\na. Allows all internal users to establish Telnet sessions with external \nhosts.\nb. Allows external users to surf the company Web site at 222.22.0.12.\nc. But otherwise blocks all inbound and outbound traffic.\nPROBLEMS\n751"
    },
    {
      "chunk_id": "196e0636-97ca-4987-96a9-c9c4986cd80b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Wireshark Lab: SSL",
      "original_titles": [
        "Wireshark Lab: SSL"
      ],
      "path": "Chapter 8 Security in Computer Networks > Wireshark Lab: SSL",
      "start_page": 779,
      "end_page": 779,
      "token_count": 598,
      "text": "The internal network is 222.22/16. In your solution, suppose that the connec-\ntion table is currently caching three connections, all from inside to outside.\nYou’ll need to invent appropriate IP addresses and port numbers.\nP26. Suppose Alice wants to visit the Web site activist.com using a TOR-like \nservice. This service uses two non-colluding proxy servers, Proxy1 and\nProxy2. Alice first obtains the certificates (each containing a public key) for\nProxy1 and Proxy2 from some central server. Denote K1\n+( ), K2\n+( ), K1\n–( ), and\nK2\n–( ) for the encryption/decryption with public and private RSA keys.\na. Using a timing diagram, provide a protocol (as simple as possible) that\nenables Alice to establish a shared session key S1 with Proxy1. Denote\nS1(m) for encryption/decryption of data m with the shared key S1.\nb. Using a timing diagram, provide a protocol (as simple as possible) that\nallows Alice to establish a shared session key S2 with Proxy2 without\nrevealing her IP address to Proxy2.\nc. Assume now that shared keys S1 and S2 are now established. Using a tim-\ning diagram, provide a protocol (as simple as possible and not using \npublic-key cryptography) that allows Alice to request an html page from\nactivist.com without revealing her IP address to Proxy2 and without\nrevealing to Proxy1 which site she is visiting. Your diagram should end\nwith an HTTP request arriving at activist.com.\nWireshark Lab\nIn this lab (available from the companion Web site), we investigate the Secure Sockets\nLayer (SSL) protocol. Recall from Section 8.6 that SSL is used for securing a TCP\nconnection, and that it is extensively used in practice for secure Internet transactions.\nIn this lab, we will focus on the SSL records sent over the TCP connection. We will\nattempt to delineate and classify each of the records, with a goal of understanding\nthe why and how for each record. We investigate the various SSL record types as\nwell as the fields in the SSL messages. We do so by analyzing a trace of the SSL\nrecords sent between your host and an e-commerce server.\nIPsec Lab\nIn this lab (available from the companion Web site), we will explore how to create\nIPsec SAs between linux boxes. You can do the first part of the lab with two ordi-\nnary linux boxes, each with one Ethernet adapter. But for the second part of the lab,\nyou will need four linux boxes, two of which having two Ethernet adapters. In the\nsecond half of the lab, you will create IPsec SAs using the ESP protocol in the tun-\nnel mode. You will do this by first manually creating the SAs, and then by having\nIKE create the SAs.\n752\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS"
    },
    {
      "chunk_id": "84fc8e1f-b859-438e-a41a-f5b886531027",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "IPsec Lab",
      "original_titles": [
        "IPsec Lab"
      ],
      "path": "Chapter 8 Security in Computer Networks > IPsec Lab",
      "start_page": 779,
      "end_page": 779,
      "token_count": 598,
      "text": "The internal network is 222.22/16. In your solution, suppose that the connec-\ntion table is currently caching three connections, all from inside to outside.\nYou’ll need to invent appropriate IP addresses and port numbers.\nP26. Suppose Alice wants to visit the Web site activist.com using a TOR-like \nservice. This service uses two non-colluding proxy servers, Proxy1 and\nProxy2. Alice first obtains the certificates (each containing a public key) for\nProxy1 and Proxy2 from some central server. Denote K1\n+( ), K2\n+( ), K1\n–( ), and\nK2\n–( ) for the encryption/decryption with public and private RSA keys.\na. Using a timing diagram, provide a protocol (as simple as possible) that\nenables Alice to establish a shared session key S1 with Proxy1. Denote\nS1(m) for encryption/decryption of data m with the shared key S1.\nb. Using a timing diagram, provide a protocol (as simple as possible) that\nallows Alice to establish a shared session key S2 with Proxy2 without\nrevealing her IP address to Proxy2.\nc. Assume now that shared keys S1 and S2 are now established. Using a tim-\ning diagram, provide a protocol (as simple as possible and not using \npublic-key cryptography) that allows Alice to request an html page from\nactivist.com without revealing her IP address to Proxy2 and without\nrevealing to Proxy1 which site she is visiting. Your diagram should end\nwith an HTTP request arriving at activist.com.\nWireshark Lab\nIn this lab (available from the companion Web site), we investigate the Secure Sockets\nLayer (SSL) protocol. Recall from Section 8.6 that SSL is used for securing a TCP\nconnection, and that it is extensively used in practice for secure Internet transactions.\nIn this lab, we will focus on the SSL records sent over the TCP connection. We will\nattempt to delineate and classify each of the records, with a goal of understanding\nthe why and how for each record. We investigate the various SSL record types as\nwell as the fields in the SSL messages. We do so by analyzing a trace of the SSL\nrecords sent between your host and an e-commerce server.\nIPsec Lab\nIn this lab (available from the companion Web site), we will explore how to create\nIPsec SAs between linux boxes. You can do the first part of the lab with two ordi-\nnary linux boxes, each with one Ethernet adapter. But for the second part of the lab,\nyou will need four linux boxes, two of which having two Ethernet adapters. In the\nsecond half of the lab, you will create IPsec SAs using the ESP protocol in the tun-\nnel mode. You will do this by first manually creating the SAs, and then by having\nIKE create the SAs.\n752\nCHAPTER 8\n•\nSECURITY IN COMPUTER NETWORKS"
    },
    {
      "chunk_id": "9e1c132b-3e8d-479e-9432-bc2daf7bef4a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Interview: Steven M. Bellovin",
      "original_titles": [
        "Interview: Steven M. Bellovin"
      ],
      "path": "Chapter 8 Security in Computer Networks > Interview: Steven M. Bellovin",
      "start_page": 780,
      "end_page": 781,
      "token_count": 908,
      "text": "What led you to specialize in the networking security area?\nThis is going to sound odd, but the answer is simple: It was fun. My background was in sys-\ntems programming and systems administration, which leads fairly naturally to security. And\nI’ve always been interested in communications, ranging back to part-time systems program-\nming jobs when I was in college.\nMy work on security continues to be motivated by two things—a desire to keep com-\nputers useful, which means that their function can’t be corrupted by attackers, and a desire\nto protect privacy.\nWhat was your vision for Usenet at the time that you were developing it? And now?\nWe originally viewed it as a way to talk about computer science and computer programming\naround the country, with a lot of local use for administrative matters, for-sale ads, and so on.\nIn fact, my original prediction was one to two messages per day, from 50–100 sites at the\nmost—ever. But the real growth was in people-related topics, including—but not limited\nto—human interactions with computers. My favorite newsgroups, over the years, have been\nthings like rec.woodworking, as well as sci.crypt.\nTo some extent, netnews has been displaced by the Web. Were I to start designing it\ntoday, it would look very different. But it still excels as a way to reach a very broad audi-\nence that is interested in the topic, without having to rely on particular Web sites.\nHas anyone inspired you professionally? In what ways?\nProfessor Fred Brooks—the founder and original chair of the computer science department\nat the University of North Carolina at Chapel Hill, the manager of the team that developed\nthe IBM S/360 and OS/360, and the author of The Mythical Man-Month—was a tremendous\ninfluence on my career. More than anything else, he taught outlook and trade-offs—how to\n753\nSteven M. Bellovin joined the faculty at Columbia University after\nmany years at the Network Services Research Lab at AT&T Labs\nResearch in Florham Park, New Jersey. His focus is on networks,\nsecurity, and why the two are incompatible. In 1995, he was\nawarded the Usenix Lifetime Achievement Award for his work in the\ncreation of Usenet, the first newsgroup exchange network that linked\ntwo or more computers and allowed users to share information and\njoin in discussions. Steve is also an elected member of the National\nAcademy of Engineering. He received his BA from Columbia\nUniversity and his PhD from the University of North Carolina at\nChapel Hill.\nAN INTERVIEW WITH . . .\nSteven M. Bellovin\n\nlook at problems in the context of the real world (and how much messier the real world is\nthan a theorist would like), and how to balance competing interests in designing a solution.\nMost computer work is engineering—the art of making the right trade-offs to satisfy many\ncontradictory objectives.\nWhat is your vision for the future of networking and security?\nThus far, much of the security we have has come from isolation. A firewall, for example,\nworks by cutting off access to certain machines and services. But we’re in an era of increas-\ning connectivity—it’s gotten harder to isolate things. Worse yet, our production systems\nrequire far more separate pieces, interconnected by networks. Securing all that is one of our\nbiggest challenges.\nWhat would you say have been the greatest advances in security? How much further do\nwe have to go?\nAt least scientifically, we know how to do cryptography. That’s been a big help. But most\nsecurity problems are due to buggy code, and that’s a much harder problem. In fact, it’s the\noldest unsolved problem in computer science, and I think it will remain that way. The chal-\nlenge is figuring out how to secure systems when we have to build them out of insecure\ncomponents. We can already do that for reliability in the face of hardware failures; can we\ndo the same for security?\nDo you have any advice for students about the Internet and networking security?\nLearning the mechanisms is the easy part. Learning how to “think paranoid” is harder. You\nhave to remember that probability distributions don’t apply—the attackers can and will find\nimprobable conditions. And the details matter—a lot.\n754"
    },
    {
      "chunk_id": "90494d39-11aa-4de3-b2d5-0423ced61cee",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Chapter 9 Network Management",
      "original_titles": [
        "Chapter 9 Network Management",
        "9.1 What Is Network Management?"
      ],
      "path": "Chapter 9 Network Management",
      "start_page": 782,
      "end_page": 786,
      "token_count": 2364,
      "text": "CHAPTER 9\nNetwork\nManagement\n755\nHaving made our way through the first eight chapters of this text, we’re now well\naware that a network consists of many complex, interacting pieces of hardware and\nsoftware—from the links, switches, routers, hosts, and other devices that comprise\nthe physical components of the network to the many protocols (in both hardware\nand software) that control and coordinate these devices. When hundreds or thou-\nsands of such components are cobbled together by an organization to form a net-\nwork, it is not surprising that components will occasionally malfunction, that\nnetwork elements will be misconfigured, that network resources will be overuti-\nlized, or that network components will simply “break” (for example, a cable will be\ncut or a can of soda will be spilled on top of a router). The network administrator,\nwhose job it is to keep the network “up and running,” must be able to respond to\n(and better yet, avoid) such mishaps. With potentially thousands of network compo-\nnents spread out over a wide area, the network administrator in a network operations\ncenter (NOC) clearly needs tools to help monitor, manage, and control the network.\nIn this chapter, we’ll examine the architecture, protocols, and information base used\nby a network administrator in this task.\n9.1 What Is Network Management?\nBefore diving in to network management itself, let’s first consider a few illustrative\n“real-world” non-networking scenarios in which a complex system with many inter-\nacting components must be monitored, managed, and controlled by an administra-\ntor. Electrical power-generation plants have a control room where dials, gauges, and\nlights monitor the status (temperature, pressure, flow) of remote valves, pipes, ves-\nsels, and other plant components. These devices allow the operator to monitor the\nplant’s many components, and may alert the operator (with the famous flashing red\nwarning light) when trouble is imminent. Actions are taken by the plant operator to\ncontrol these components. Similarly, an airplane cockpit is instrumented to allow a\npilot to monitor and control the many components that make up an airplane. In these\ntwo examples, the “administrator” monitors remote devices and analyzes their data\nto ensure that they are operational and operating within prescribed limits (for exam-\nple, that a core meltdown of a nuclear power plant is not imminent, or that the plane\nis not about to run out of fuel), reactively controls the system by making adjustments\nin response to the changes within the system or its environment, and proactively\nmanages the system (for example, by detecting trends or anomalous behavior,\nallowing action to be taken before serious problems arise). In a similar sense, the\nnetwork administrator will actively monitor, manage, and control the system with\nwhich she or he is entrusted.\nIn the early days of networking, when computer networks were research arti-\nfacts rather than a critical infrastructure used by hundreds of millions of people a\nday, “network management” was unheard of. If one encountered a network problem,\none might run a few pings to locate the source of the problem and then modify sys-\ntem settings, reboot hardware or software, or call a remote colleague to do so. (A\nvery readable discussion of the first major “crash” of the ARPAnet on October 27,\n1980, long before network management tools were available, and the efforts taken\nto recover from and understand the crash is [RFC 789].) As the public Internet and\nprivate intranets have grown from small networks into a large global infrastructure,\nthe need to manage the huge number of hardware and software components within\nthese networks more systematically has grown more important as well.\nIn order to motivate our study of network management, let’s begin with a sim-\nple example. Figure 9.1 illustrates a small network consisting of three routers and a\nnumber of hosts and servers. Even in such a simple network, there are many scenar-\nios in which a network administrator might benefit tremendously from having\nappropriate network management tools:\n• Detecting failure of an interface card at a host or a router. With appropriate net-\nwork management tools, a network entity (for example, router A) may report to\nthe network administrator that one of its interfaces has gone down. (This is\ncertainly preferable to a phone call to the NOC from an irate user who says the\nnetwork connection is down!) A network administrator who actively monitors\n756\nCHAPTER 9\n•\nNETWORK MANAGEMENT\n\nand analyzes network traffic may be able to really impress the would-be irate\nuser by detecting problems in the interface ahead of time and replacing the inter-\nface card before it fails. This might be done, for example, if the administrator\nnoted an increase in checksum errors in frames being sent by the soon-to-die\ninterface.\n• Host monitoring. Here, the network administrator might periodically check to\nsee if all network hosts are up and operational. Once again, the network adminis-\ntrator may really be able to impress a network user by proactively responding to\na problem (host down) before it is reported by a user.\n• Monitoring traffic to aid in resource deployment. A network administrator might\nmonitor source-to-destination traffic patterns and notice, for example, that by\nswitching servers between LAN segments, the amount of traffic that crosses\nmultiple LANs could be significantly decreased. Imagine the happiness all\naround when better performance is achieved with no new equipment costs. Simi-\nlarly, by monitoring link utilization, a network administrator might determine\nthat a LAN segment or the external link to the outside world is overloaded and\n9.1\n•\nWHAT IS NETWORK MANAGEMENT?\n757\nHost\nH1\nA\nB\nC\nHost\nLink to\nexternal\nnetwork\nServer\nFigure 9.1 \u0002 A simple scenario illustrating the uses of network management\n\nthat a higher-bandwidth link should thus be provisioned (alas, at an increased\ncost). The network administrator might also want to be notified automatically\nwhen congestion levels on a link exceed a given threshold value, in order to pro-\nvision a higher-bandwidth link before congestion becomes serious.\n• Detecting rapid changes in routing tables. Route flapping—frequent changes in the\nrouting tables—may indicate instabilities in the routing or a misconfigured router.\nCertainly, the network administrator who has improperly configured a router would\nprefer to discover the error him- or herself, before the network goes down.\n• Monitoring for SLAs. Service Level Agreements (SLAs) are contracts that\ndefine specific performance metrics and acceptable levels of network-provider\nperformance with respect to these metrics [Huston 1999a]. Verizon and Sprint\nare just two of the many network providers that guarantee SLAs [AT&T SLA\n2012; Verizon SLA 2012] to their customers. These SLAs include service avail-\nability (outage), latency, throughput, and outage notification requirements.\nClearly, if performance criteria are to be part of a service agreement between a\nnetwork provider and its users, then measuring and managing performance will\nbe of great importance to the network administrator.\n• Intrusion detection. A network administrator may want to be notified when net-\nwork traffic arrives from, or is destined for, a suspicious source (for example,\nhost or port number). Similarly, a network administrator may want to detect (and\nin many cases filter) the existence of certain types of traffic (for example, source-\nrouted packets, or a large number of SYN packets directed to a given host) that\nare known to be characteristic of the types of security attacks that we considered\nin Chapter 8.\nThe International Organization for Standardization (ISO) has created a network\nmanagement model that is useful for placing the anecdotal scenarios above in a\nmore structured framework. Five areas of network management are defined:\n• Performance management. The goal of performance management is to quan-\ntify, measure, report, analyze, and control the performance (for example, uti-\nlization and throughput) of different network components. These components\ninclude individual devices (for example, links, routers, and hosts) as well as\nend-to-end abstractions such as a path through the network. We will see\nshortly that protocol standards such as the Simple Network Management Pro-\ntocol (SNMP) [RFC 3410] play a central role in Internet performance \nmanagement.\n• Fault management. The goal of fault management is to log, detect, and respond\nto fault conditions in the network. The line between fault management and per-\nformance management is rather blurred. We can think of fault management as\nthe immediate handling of transient network failures (for example, link, host,\nor router hardware or software outages), while performance management takes\n758\nCHAPTER 9\n•\nNETWORK MANAGEMENT\n\nthe longer-term view of providing acceptable levels of performance in the face\nof varying traffic demands and occasional network device failures. As with\nperformance management, the SNMP protocol plays a central role in fault\nmanagement.\n• Configuration management. Configuration management allows a network man-\nager to track which devices are on the managed network and the hardware and\nsoftware configurations of these devices. An overview of configuration manage-\nment and requirements for IP-based networks can be found in [RFC 3139].\n• Accounting management. Accounting management allows the network manager\nto specify, log, and control user and device access to network resources. Usage\nquotas, usage-based charging, and the allocation of resource-access privileges all\nfall under accounting management.\n• Security management. The goal of security management is to control access to\nnetwork resources according to some well-defined policy. The key distribution\ncenters that we studied in Section 8.3 are components of security management.\nThe use of firewalls to monitor and control external access points to one’s net-\nwork, a topic we studied in Section 8.9, is another crucial component.\nIn this chapter, we’ll cover only the rudiments of network management. Our\nfocus will be purposefully narrow—we’ll examine only the infrastructure for\nnetwork management—the overall architecture, network management protocols,\nand information base through which a network administrator keeps the network\nup and running. We’ll not cover the decision-making processes of the network\nadministrator, who must plan, analyze, and respond to the management informa-\ntion that is conveyed to the NOC. In this area, topics such as fault identification\nand management [Katzela 1995; Medhi 1997; Labovitz 1997; Steinder 2002;\nFeamster 2005; Wu 2005; Teixeira 2006], anomaly detection [Lakhina 2004;\nLakhina 2005; Barford 2009], and more come into consideration. Nor will we\ncover the broader topic of service management [Saydam 1996; RFC 3052]—the\nprovisioning of resources such as bandwidth, server capacity, and the other com-\nputational/communication resources needed to meet the mission-specific service\nrequirements of an enterprise.\nAn often-asked question is “What is network management?” Our discussion\nabove has motivated the need for, and illustrated a few of the uses of, network man-\nagement. We’ll conclude this section with a single-sentence (albeit a rather long run-\non sentence) definition of network management from [Saydam 1996]:\n“Network management includes the deployment, integration, and coordination\nof the hardware, software, and human elements to monitor, test, poll, configure,\nanalyze, evaluate, and control the network and element resources to meet the\nreal-time, operational performance, and Quality of Service requirements at a\nreasonable cost.”\n9.1\n•\nWHAT IS NETWORK MANAGEMENT?\n759"
    },
    {
      "chunk_id": "951567a2-e730-4ee5-bc69-264184b0b603",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "9.2 The Infrastructure for Network Management",
      "original_titles": [
        "9.2 The Infrastructure for Network Management"
      ],
      "path": "Chapter 9 Network Management > 9.2 The Infrastructure for Network Management",
      "start_page": 787,
      "end_page": 790,
      "token_count": 1888,
      "text": "It’s a mouthful, but it’s a good workable definition. In the following sections, we’ll\nadd some meat to this rather bare-bones definition of network management.\n9.2 The Infrastructure for \nNetwork Management\nWe’ve seen in the preceding section that network management requires the ability\nto “monitor, test, poll, configure, . . . and control” the hardware and software com-\nponents in a network. Because the network devices are distributed, this will, at a\nminimum, require that the network administrator be able to gather data (for exam-\nple, for monitoring purposes) from a remote entity and effect changes at that remote\nentity (for example, control it). A human analogy will prove useful here for under-\nstanding the infrastructure needed for network management.\nImagine that you’re the head of a large organization that has branch offices\naround the world. It’s your job to make sure that the pieces of your organization are\noperating smoothly. How will you do so? At a minimum, you’ll periodically gather\ndata from your branch offices in the form of reports and various quantitative meas-\nures of activity, productivity, and budget. You’ll occasionally (but not always) be\nexplicitly notified when there’s a problem in one of the branch offices; the branch\nmanager who wants to climb the corporate ladder (perhaps to get your job) may\nsend you unsolicited reports indicating how smoothly things are running at his or\nher branch. You’ll sift through the reports you receive, hoping to find smooth opera-\ntions everywhere but no doubt finding problems in need of your attention. You\nmight initiate a one-on-one dialogue with one of your problem branch offices,\ngather more data in order to understand the problem, and then pass down an execu-\ntive order (“Make this change!”) to the branch office manager.\nImplicit in this very common human scenario is an infrastructure for control-\nling the organization—the boss (you), the remote sites being controlled (the branch\noffices), your remote agents (the branch office managers), communication protocols\n(for transmitting standard reports and data, and for one-on-one dialogues), and data\n(the report contents and the quantitative measures of activity, productivity, and\nbudget). Each of these components in human organizational management has a\ncounterpart in network management.\nThe architecture of a network management system is conceptually identical to\nthis simple human organizational analogy. The network management field has its\nown specific terminology for the various components of a network management\narchitecture, and so we adopt that terminology here. As shown in Figure 9.2, there\nare three principal components of a network management architecture: a managing\nentity (the boss in our analogy above—you), the managed devices (the branch\noffice), and a network management protocol.\n760\nCHAPTER 9\n•\nNETWORK MANAGEMENT\n\nThe managing entity is an application, typically with a human in the loop,\nrunning in a centralized network management station in the NOC. The managing\nentity is the locus of activity for network management; it controls the collection,\nprocessing, analysis, and/or display of network management information. It is\nhere that actions are initiated to control network behavior and here that the\nhuman network administrator interacts with the network devices.\nA managed device is a piece of network equipment (including its software)\nthat resides on a managed network. This is the branch office in our human anal-\nogy. A managed device might be a host, router, bridge, hub, printer, or modem.\nWithin a managed device, there may be several so-called managed objects. These\nmanaged objects are the actual pieces of hardware within the managed device (for\nexample, a network interface card), and the sets of configuration parameters for\nthe pieces of hardware and software (for example, an intradomain routing proto-\ncol such as RIP). In our human analogy, the managed objects might be the depart-\nments within the branch office. These managed objects have pieces of information\nassociated with them that are collected into a Management Information Base\n9.2\n•\nTHE INFRASTRUCTURE FOR NETWORK MANAGEMENT\n761\nFigure 9.2 \u0002 Principal components of a network management architecture\nAgent\nData\nAgent\nData\nAgent\nData\nAgent\nData\nManaged\ndevice\nManaged\ndevice\nManaged\ndevice\nManaged\ndevice\nAgent\nData\nManaging\nentity\nData\n\n(MIB); we’ll see that the values of these pieces of information are available to\n(and in many cases able to be set by) the managing entity. In our human analogy,\nthe MIB corresponds to quantitative data (measures of activity, productivity, and\nbudget, with the latter being settable by the managing entity!) exchanged between\nthe branch office and the main office. We’ll study MIBs in detail in Section 9.3.\nFinally, also resident in each managed device is a network management agent, a\nprocess running in the managed device that communicates with the managing\nentity, taking local actions at the managed device under the command and control\nof the managing entity. The network management agent is the branch manager in\nour human analogy.\nThe third piece of a network management architecture is the network manage-\nment protocol. The protocol runs between the managing entity and the managed\ndevices, allowing the managing entity to query the status of managed devices and\nindirectly take actions at these devices via its agents. Agents can use the network\nmanagement protocol to inform the managing entity of exceptional events (for\nexample, component failures or violation of performance thresholds). It’s important\nto note that the network management protocol does not itself manage the network.\nInstead, it provides capabilities that a network administrator can use to manage\n(“monitor, test, poll, configure, analyze, evaluate, and control”) the network. This is\na subtle, but important, distinction.\nAlthough the infrastructure for network management is conceptually simple,\none can often get bogged down with the network-management-speak vocabulary of\n“managing entity,” “managed device,” “managing agent,” and “Management Infor-\nmation Base.” For example, in network-management-speak, in our simple host-\nmonitoring scenario, “managing agents” located at “managed devices” are\nperiodically queried by the “managing entity”—a simple idea, but a linguistic\nmouthful! With any luck, keeping in mind the human organizational analogy and its\nobvious parallels with network management will be of help as we continue through\nthis chapter.\nOur discussion of network management architecture above has been generic,\nand broadly applies to a number of the network management standards and\nefforts that have been proposed over the years. Network management standards\nbegan maturing in the late 1980s, with OSI CMISE/CMIP (the Common Man-\nagement Information Services Element/Common Management Information\nProtocol) [Piscatello 1993; Stallings 1993; Glitho 1998] and the Internet SNMP\n(Simple Network Management Protocol) [RFC 3410; Stallings 1999; Rose\n1996] emerging as the two most important standards [Subramanian 2000]. Both\nare designed to be independent of vendor-specific products or networks. Because\nSNMP was quickly designed and deployed at a time when the need for network\nmanagement was becoming painfully clear, SNMP found widespread use and\nacceptance. Today, SNMP has emerged as the most widely used and deployed\nnetwork management framework. We’ll cover SNMP in detail in the following\nsection.\n762\nCHAPTER 9\n•\nNETWORK MANAGEMENT\n\n9.2\n•\nTHE INFRASTRUCTURE FOR NETWORK MANAGEMENT\n763\nCOMCAST’S NETWORK OPERATIONS CENTER\nComcast’s world-class fiber-based IP network delivers converged products and services to\n49 million combined video, data and voice customers. Comcast’s network includes more\nthan 618,000 plant route miles, 138,000 fiber route miles, 30,000 backbone miles,\n122,000 optical nodes, and massive storage for the Comcast Content Delivery Network,\nwhich delivers a Video on Demand product of more than 134 Terabytes. Each part of\nComcast’s network, up to and including the customers’ homes or businesses, is monitored\nby one of the company’s Operations Centers.\nComcast operates two National Network Operations Centers that manage the national\nbackbone, regional area networks, national applications and specific platforms support-\ning voice, data and video infrastructure for residential, commercial and wholesale cus-\ntomers. In addition, Comcast has three Divisional Operations Centers that manage the\nlocal infrastructure that supports all of their customers. Both the National and Divisional\nOperations Centers are accountable for proactively monitoring all aspects of their network\nand product performance on a 7 x 24 x 365 basis, utilizing common processes and \nsystems. For example, various network events at the national and local levels have \ncommon pre-defined severity levels, recovery processes, and expected Mean Time to\nRestore objectives. The national and divisional centers can back up each other if a local\nissue impacts a site’s operation. In addition, the National and Divisional Operations\nCenters have an extensive Virtual Private Network that allows engineers to securely access\nthe network to remotely perform proactive or reactive network management activities.\nComcast’s approach to network management involves five key areas: Performance\nManagement, Fault Management, Configuration Management, Accounting Management\nand Security Management. Performance Management is focused on understanding\nThese screens show tools supporting correlation, threshold management, tick-\neting used by Comcast technicians (Courtesy of Comcast.)\n(continues)\nPRINCIPLES IN PRACTICE"
    },
    {
      "chunk_id": "a9a01ce8-82bd-4c48-adfd-7b100eaacc3e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "9.3 The Internet-Standard Management Framework",
      "original_titles": [
        "9.3 The Internet-Standard Management Framework"
      ],
      "path": "Chapter 9 Network Management > 9.3 The Internet-Standard Management Framework",
      "start_page": 791,
      "end_page": 792,
      "token_count": 1160,
      "text": "how the network/systems and applications (collectively referred to as the ecosystem) are\nperforming with respect to pre-defined measures specific to time of day, day of week, or\nspecial events (e.g., storm surges or pay events, such as a boxing match). These pre-\ndefined performance measures exist throughout the service path, from the customer’s resi-\ndence or business through the entire network, as well as the interface points to partners\nand peers. In addition, synthetic transactions are run to ensure the health of the ecosystem\non a continual basis. Fault Management is defined as the ability to detect, log and\nunderstand anomalies that may impact customers. Comcast utilizes correlation engines to\nproperly determine an event’s severity and act appropriately, eliminating or remediating\npotential issues before they affect customers. Configuration Management makes sure\nappropriate versions of hardware and software are in place across all elements of the\necosystem. Keeping these elements at their peak “golden” levels helps them avoid unin-\ntended consequences. Accounting Management ensures that the operations centers\nhave a clear understanding of the provisioning and utilization of the ecosystem. This is\nespecially important to ensure that at all times the operations centers have the ability to re-\nroute traffic effectively. Security Management ensures that the proper controls exist to\nensure the ecosystem is effectively protected against inappropriate access.\nNetwork Operations Centers and the ecosystem they support are not static. Engineering\nand Operations personnel are constantly re-evaluating the pre-defined performance \nmeasures and tools to ensure that the customers’ expectations for operational excellence\nare met.\n9.3 The Internet-Standard Management\nFramework\nContrary to what the name SNMP (Simple Network Management Protocol) might\nsuggest, network management in the Internet is much more than just a protocol for\nmoving management data between a management entity and its agents, and has\ngrown to be much more complex than the word “simple” might suggest. The current\nInternet-Standard Management Framework traces its roots back to the Simple Gate-\nway Monitoring Protocol, SGMP [RFC 1028]. SGMP was designed by a group of\nuniversity network researchers, users, and managers, whose experience with SGMP\nallowed them to design, implement, and deploy SNMP in just a few months [Lynch\n1993]—a far cry from today’s rather drawn-out standardization process. Since then,\nSNMP has evolved from SNMPv1 through SNMPv2 to the most recent version,\nSNMPv3 [RFC 3410], released in April 1999 and updated in December 2002.\nWhen describing any framework for network management, certain questions\nmust inevitably be addressed:\n764\nCHAPTER 9\n•\nNETWORK MANAGEMENT\n\n• What (from a semantic viewpoint) is being monitored? And what form of control\ncan be exercised by the network administrator?\n• What is the specific form of the information that will be reported and/or exchanged?\n• What is the communication protocol for exchanging this information?\nRecall our human organizational analogy from the previous section. The boss\nand the branch managers will need to agree on the measures of activity, productiv-\nity, and budget used to report the branch office’s status. Similarly, they’ll need to\nagree on the actions the boss can take (for example, cut the budget, order the branch\nmanager to change some aspect of the office’s operation, or fire the staff and shut\ndown the branch office). At a lower level of detail, they’ll need to agree on the form\nin which this data is reported. For example, in what currency (dollars, euros?) will\nthe budget be reported? In what units will productivity be measured? While these\nmay seem like trivial details, they must be agreed upon, nonetheless. Finally, the\nmanner in which information is conveyed between the main office and the branch\noffices (that is, their communication protocol) must be specified.\nThe Internet-Standard Management Framework addresses the questions posed\nabove. The framework consists of four parts:\n• Definitions of network management objects, known as MIB objects. In the Inter-\nnet-Standard Management Framework, management information is represented\nas a collection of managed objects that together form a virtual information store,\nknown as the Management Information Base (MIB). An MIB object might be a\ncounter, such as the number of IP datagrams discarded at a router due to errors in\nan IP datagram header, or the number of carrier sense errors in an Ethernet inter-\nface card; descriptive information such as the version of the software running on\na DNS server; status information such as whether a particular device is function-\ning correctly; or protocol-specific information such as a routing path to a\ndestination. MIB objects thus define the management information maintained by\na managed device. Related MIB objects are gathered into MIB modules. In our\nhuman organizational analogy, the MIB defines the information conveyed\nbetween the branch office and the main office.\n• A data definition language, known as SMI (Structure of Management Informa-\ntion). SMI defines the data types, an object model, and rules for writing and\nrevising management information. MIB objects are specified in this data defini-\ntion language. In our human organizational analogy, the SMI is used to define\nthe details of the format of the information to be exchanged.\n• A protocol, SNMP. SNMP is used for conveying information and commands\nbetween a managing entity and an agent executing on behalf of that entity within\na managed network device.\n• Security and administration capabilities. The addition of these capabilities rep-\nresents the major enhancement in SNMPv3 over SNMPv2.\n9.3\n•\nTHE INTERNET-STANDARD MANAGEMENT FRAMEWORK\n765"
    },
    {
      "chunk_id": "c83b14bf-5bed-461f-bbbc-c3c54b6ccd7a",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "9.3.1 Structure of Management Information: SMI",
      "original_titles": [
        "9.3.1 Structure of Management Information: SMI"
      ],
      "path": "Chapter 9 Network Management > 9.3 The Internet-Standard Management Framework > 9.3.1 Structure of Management Information: SMI",
      "start_page": 793,
      "end_page": 796,
      "token_count": 1713,
      "text": "The Internet network management architecture is thus modular by design, with a\nprotocol-independent data definition language and MIB, and an MIB-independent\nprotocol. Interestingly, this modular architecture was first put in place to ease the tran-\nsition from an SNMP-based network management to a network management frame-\nwork being developed by ISO, the competing network management architecture when\nSNMP was first conceived—a transition that never occurred. Over time, however,\nSNMP’s design modularity has allowed it to evolve through three major revisions,\nwith each of the four major parts of SNMP discussed above evolving independently.\nClearly, the right decision about modularity was made, even if for the wrong reason!\nIn the following subsections, we cover the four major components of the Inter-\nnet-Standard Management Framework in more detail.\n9.3.1 Structure of Management Information: SMI\nThe Structure of Management Information, SMI (a rather oddly named component\nof the network management framework whose name gives no hint of its functional-\nity), is the language used to define the management information residing in a man-\naged-network entity. Such a definition language is needed to ensure that the syntax\nand semantics of the network management data are well defined and unambiguous.\nNote that the SMI does not define a specific instance of the data in a managed-network\nentity, but rather the language in which such information is specified. The documents\ndescribing the SMI for SNMPv3 (which rather confusingly, is called SMIv2) are [RFC\n2578; RFC 2579; RFC 2580]. Let’s examine the SMI in a bottom-up manner, starting\nwith the base data types in the SMI. We’ll then look at how managed objects are\ndescribed in SMI, then how related managed objects are grouped into modules.\nSMI Base Data Types\nRFC 2578 specifies the basic data types in the SMI MIB module-definition language.\nAlthough the SMI is based on the ASN.1 (Abstract Syntax Notation One) [ISO X.680\n2002] object-definition language (see Section 9.4), enough SMI-specific data types\nhave been added that SMI should be considered a data definition language in its own\nright. The 11 basic data types defined in RFC 2578 are shown in Table 9.1. In addition\nto these scalar objects, it is also possible to impose a tabular structure on an ordered\ncollection of MIB objects using the SEQUENCE OF construct; see RFC 2578 for\ndetails. Most of the data types in Table 9.1 will be familiar (or self-explanatory) to\nmost readers. The one data type we will discuss in more detail shortly is the OBJECT\nIDENTIFIER data type, which is used to name an object.\nSMI Higher-Level Constructs\nIn addition to the basic data types, the SMI data definition language also provides\nhigher-level language constructs.\n766\nCHAPTER 9\n•\nNETWORK MANAGEMENT\n\nThe OBJECT-TYPE construct is used to specify the data type, status, and\nsemantics of a managed object. Collectively, these managed objects contain the\nmanagement data that lies at the heart of network management. There are more than\n10,000 defined objects in various Internet RFCs [RFC 3410]. The OBJECT-TYPE\nconstruct has four clauses. The SYNTAX clause of an OBJECT-TYPE definition\nspecifies the basic data type associated with the object. The MAX-ACCESS clause\nspecifies whether the managed object can be read, be written, be created, or have its\nvalue included in a notification. The STATUS clause indicates whether the object\ndefinition is current and valid, obsolete (in which case it should not be implemented,\nas its definition is included for historical purposes only), or deprecated (obsolete,\nbut implementable for interoperability with older implementations). The DESCRIP-\nTION clause contains a human-readable textual definition of the object; this “docu-\nments” the purpose of the managed object and should provide all the semantic\ninformation needed to implement the managed object.\nAs an example of the OBJECT-TYPE construct, consider the ipSystem-\nStatsInDelivers object-type definition from [RFC 4293]. This object defines\na 32-bit counter that keeps track of the number of IP datagrams that were received\nat the managed device and were successfully delivered to an upper-layer protocol.\n9.3\n•\nTHE INTERNET-STANDARD MANAGEMENT FRAMEWORK\n767\nData Type\nDescription\nINTEGER\n32-bit integer, as defined in ASN.1, with a value between \u0002231 and 231 \u0002 1\ninclusive, or a value from a list of possible named constant values.\nInteger32\n32-bit integer with a value between \u0002231 and 231 \u0002 1 inclusive.\nUnsigned32\nUnsigned 32-bit integer in the range 0 to 232 \u0002 1 inclusive.\nOCTET STRING\nASN.1-format byte string representing arbitrary binary or textual data, up to 65,535\nbytes long.\nOBJECT IDENTIFIER\nASN.1-format administratively assigned (structured name); see Section 9.3.2.\nIPaddress\n32-bit Internet address, in network-byte order.\nCounter32\n32-bit counter that increases from 0 to 232 \u0002 1 and then wraps around to 0.\nCounter64\n64-bit counter.\nGauge32\n32-bit integer that will not count above 232 \u0002 1 nor decrease beyond 0 when\nincreased or decreased.\nTimeTicks\nTime, measured in 1/100ths of a second since some event.\nOpaque\nUninterpreted ASN.1 string, needed for backward compatibility.\nTable 9.1 \u0002 Basic data types of the SMI\n\nThe final line of this definition is concerned with the name of this object, a topic\nwe’ll consider in the following subsection.\nipSystemStatsInDelivers OBJECT-TYPE\nSYNTAX     Counter32\nMAX-ACCESS read-only\nSTATUS     current\nDESCRIPTION\n\"The total number of datagrams successfully\ndelivered to IPuser-protocols (including ICMP).\nWhen tracking interface statistics, the counter\nof the interface to which these datagrams were\naddressed is incremented. This interface might\nnot be the same as the input interface for\nsome of the datagrams.\nDiscontinuities in the value of this counter can\noccur at re-initialization of the management\nsystem, and at other times as indicated by the\nvalue of ipSystemStatsDiscontinuityTime.\"\n::= { ipSystemStatsEntry 18 }\nThe MODULE-IDENTITY construct allows related objects to be grouped\ntogether within a “module.” For example, [RFC 4293] specifies the MIB module that\ndefines managed objects (including ipSystemStatsInDelivers) for manag-\ning implementations of the Internet Protocol (IP) and its associated Internet Control\nMessage Protocol (ICMP). [RFC 4022] specifies the MIB module for TCP, and [RFC\n4113] specifies the MIB module for UDP. [RFC 4502] defines the MIB module for\nRMON remote monitoring. In addition to containing the OBJECT-TYPE definitions\nof the managed objects within the module, the MODULE-IDENTITY construct\ncontains clauses to document contact information of the author of the module, the\ndate of the last update, a revision history, and a textual description of the module. As\nan example, consider the module definition for management of the IP protocol:\nipMIB MODULE-IDENTITY\nLAST-UPDATED \"200602020000Z\"\nORGANIZATION \"IETF IPv6 MIB Revision Team\"\nCONTACT-INFO\n\"Editor:\nShawn A. Routhier\nInterworking Labs\n108 Whispering Pines Dr. Suite 235\n768\nCHAPTER 9\n•\nNETWORK MANAGEMENT\n\nScotts Valley, CA 95066\nUSA\nEMail: <sar@iwl.com>\"\nDESCRIPTION\n\"The MIB module for managing IP and ICMP\nimplementations, but excluding their \nmanagement of IP routes.\nCopyright (C) The Internet Society (2006). \nThis version of this MIB module is part of \nRFC 4293; see the RFC itself for full legal\nnotices.\"\nREVISION      \"200602020000Z\"\nDESCRIPTION\n\"The IP version neutral revision with added\nIPv6 objects for ND, default routers, and\nrouter advertisements. As well as being the\nsuccessor to RFC 2011, this MIB is also the\nsuccessor to RFCs 2465 and 2466. Published\nas RFC 4293.\"\nREVISION      \"199411010000Z\"\nDESCRIPTION\n\"A separate MIB module (IP-MIB) for IP and\nICMP management objects. Published as RFC\n2011.\"\nREVISION      \"199103310000Z\"\nDESCRIPTION\n\"The initial revision of this MIB module was\npart of MIB-II, which was published as RFC\n1213.\"\n::= { mib-2 48}\nThe NOTIFICATION-TYPE construct is used to specify information regarding\nSNMPv2-Trap and InformationRequest messages generated by an agent, or a man-\naging entity; see Section 9.3.3. This information includes a textual DESCRIPTION\nof when such messages are to be sent, as well as a list of values to be included in the\nmessage generated; see [RFC 2578] for details. The MODULE-COMPLIANCE\nconstruct defines the set of managed objects within a module that an agent must\nimplement. The AGENT-CAPABILITIES construct specifies the capabilities of\nagents with respect to object- and event-notification definitions.\n9.3\n•\nTHE INTERNET-STANDARD MANAGEMENT FRAMEWORK\n769"
    },
    {
      "chunk_id": "2508927b-c6c8-4221-a6fd-9e6232de617c",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "9.3.2 Management Information Base: MIB",
      "original_titles": [
        "9.3.2 Management Information Base: MIB"
      ],
      "path": "Chapter 9 Network Management > 9.3 The Internet-Standard Management Framework > 9.3.2 Management Information Base: MIB",
      "start_page": 797,
      "end_page": 798,
      "token_count": 1032,
      "text": "770\nCHAPTER 9\n•\nNETWORK MANAGEMENT\n9.3.2 Management Information Base: MIB\nAs noted previously, the Management Information Base, MIB, can be thought of\nas a virtual information store, holding managed objects whose values collectively\nreflect the current “state” of the network. These values may be queried and/or set by\na managing entity by sending SNMP messages to the agent that is executing in a\nmanaged device on behalf of the managing entity. Managed objects are specified\nusing the OBJECT-TYPE SMI construct discussed above and gathered into MIB\nmodules using the MODULE-IDENTITY construct.\nThe IETF has been busy standardizing the MIB modules associated with routers,\nhosts, and other network equipment. This includes basic identification data about a\nparticular piece of hardware, and management information about the device’s net-\nwork interfaces and protocols. As of 2006 there were more than 200 standards-based\nMIB modules and an even larger number of vendor-specific (private) MIB modules.\nWith all of these standards, the IETF needed a way to identify and name the standard-\nized modules as well as the specific managed objects within a module. Rather than\nstart from scratch, the IETF adopted a standardized object identification (naming)\nframework that had already been put in place by the International Organization for\nStandardization (ISO). As is the case with many standards bodies, the ISO had\n“grand plans” for its standardized object identification framework—to identify every\npossible standardized object (for example, data format, protocol, or piece of informa-\ntion) in any network, regardless of the network standards organization (for example,\nInternet IETF, ISO, IEEE, or ANSI), equipment manufacturer, or network owner. A\nlofty goal indeed! The object identification framework adopted by ISO is part of the\nASN.1 (Abstract Syntax Notation One) [ISO X.680 2002] object definition language\nthat we’ll discuss in Section 9.4. Standardized MIB modules have their own cozy\ncorner in this all-encompassing naming framework, as discussed below.\nAs shown in Figure 9.3, objects are named in the ISO naming framework in a\nhierarchical manner. Note that each branch point in the tree has both a name and a\nnumber (shown in parentheses); any point in the tree is thus identifiable by the\nsequence of names or numbers that specify the path from the root to that point in the\nidentifier tree. A fun, but incomplete and unofficial, Web-based utility for traversing\npart of the object identifier tree (using branch information contributed by volun-\nteers) may be found in [OID Repository 2012].\nAt the top of the hierarchy are the ISO and the Telecommunication Standard-\nization Sector of the International Telecommunication Union (ITU-T), the two\nmain standards organizations dealing with ASN.1, as well as a branch for joint\nefforts by these two organizations. Under the ISO branch of the tree, we find\nentries for all ISO standards (1.0) and for standards issued by standards bodies of\nvarious ISO-member countries (1.2). Although not shown in Figure 9.3, under (ISO\nmember body, a.k.a. 1.2) we would find USA (1.2.840), under which we would\nfind a number of IEEE, ANSI, and company-specific standards. These include RSA\n(1.2.840.11359) and Microsoft (1.2.840.113556), under which we find the\nMicrosoft File Formats (1.2.840.113556.4) for various Microsoft products, such as\n\nWord (1.2.840.113556.4.2). But we are interested here in networking (not\nMicrosoft Word files), so let us turn our attention to the branch labeled 1.3, the\nstandards issued by bodies recognized by the ISO. These include the U.S. Depart-\nment of Defense (6) (under which we will find the Internet standards), the Open\nSoftware Foundation (22), the airline association SITA (69), NATO-identified bod-\nies (57), as well as many other organizations.\nUnder the Internet branch of the tree (1.3.6.1), there are seven categories.\nUnder the private (1.3.6.1.4) branch, we find a list [IANA 2009b] of the names\nand private enterprise codes of many thousands of private companies that have reg-\nistered with the Internet Assigned Numbers Authority (IANA) [IANA 2009a]. Under\nthe management (1.3.6.1.2) and MIB-2 branches (1.3.6.1.2.1) of the object iden-\ntifier tree, we find the definitions of the standardized MIB modules. Whew—it’s a\nlong journey down to our corner of the ISO name space!\nStandardized MIB Modules\nThe lowest level of the tree in Figure 9.3 shows some of the important hardware-\noriented MIB modules (system and interface) as well as modules associated\n9.3\n•\nTHE INTERNET-STANDARD MANAGEMENT FRAMEWORK\n771\nITU-T (0)\nJoint ISO/ITU-T (2)\nISO (1)\nStandard (0)\nISO member\nbody (2)\nISO identified\norganization (3)\nNATO\nidentified (57)\nOpen Software\nFoundation (22)\nUS\nDoD (6)\nInternet (1)\ndirectory\n(1)\nexperimental\n(3)\nsecurity\n(5)\nmail\n(7)\nprivate\n(4)\nsnmpv2\n(6)\nmanagement\n(2)\nMIB-2 (1)\nsystem\n(1)\naddress\ntranslation\n(3)\nicmp\n(5)\nudp\n(7)\ncmot\n(9)\ninterface\n(2)\nip\n(4)\ntcp\n(6)\negp\n(8)\ntransmission\n(10)\nrmon\n(16)\nsnmp\n(11)\nFigure 9.3 \u0002 ASN.1 object identifier tree"
    },
    {
      "chunk_id": "560db83d-2c09-4983-94d9-062fbc34bd76",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "9.3.3 SNMP Protocol Operations and Transport Mappings",
      "original_titles": [
        "9.3.3 SNMP Protocol Operations and Transport Mappings"
      ],
      "path": "Chapter 9 Network Management > 9.3 The Internet-Standard Management Framework > 9.3.3 SNMP Protocol Operations and Transport Mappings",
      "start_page": 799,
      "end_page": 801,
      "token_count": 1229,
      "text": "772\nCHAPTER 9\n•\nNETWORK MANAGEMENT\nwith some of the most important Internet protocols. [RFC 5000] lists all of the stan-\ndardized MIB modules as of 2008. While MIB-related RFCs make for rather tedious\nand dry reading, it is instructive (that is, like eating vegetables, it is “good for you”)\nto consider a few MIB module definitions to get a flavor for the type of information\nin a module.\nThe managed objects falling under system contain general information about\nthe device being managed; all managed devices must support the system MIB\nobjects. Table 9.2 defines the objects in the system group, as defined in [RFC 1213].\nTable 9.3 defines the managed objects in the MIB module for the UDP protocol at a\nmanaged entity.\n9.3.3 SNMP Protocol Operations and Transport Mappings\nThe Simple Network Management Protocol version 2 (SNMPv2) [RFC 3416] is used\nto convey MIB information among managing entities and agents executing on behalf\nof managing entities. The most common usage of SNMP is in a request-response\nmode in which an SNMPv2 managing entity sends a request to an SNMPv2 agent,\nwho receives the request, performs some action, and sends a reply to the request. Typ-\nically, a request will be used to query (retrieve) or modify (set) MIB object values\nObject Identifier\nName\nType\nDescription (from RFC 1213)\n1.3.6.1.2.1.1.1\nsysDescr\nOCTET STRING\n“Full name and version identification of the system’s hardware\ntype, software operating-system, and networking software.”\n1.3.6.1.2.1.1.2\nsysObjectID\nOBJECT IDENTIFIER\nVendor-assigned object ID that “provides an easy and \nunambiguous means for determining ‘what kind of box’ is \nbeing managed.”\n1.3.6.1.2.1.1.3\nsysUpTime\nTimeTicks\n“The time (in hundredths of a second) since the network \nmanagement portion of the system was last re-initialized.”\n1.3.6.1.2.1.1.4\nsysContact\nOCTET STRING\n“The contact person for this managed node, together with infor-\nmation on how to contact this person.”\n1.3.6.1.2.1.1.5\nsysName\nOCTET STRING\n“An administratively assigned name for this managed node. By\nconvention, this is the node’s fully qualified domain name.”\n1.3.6.1.2.1.1.6\nsysLocation\nOCTET STRING\n“The physical location of this node.”\n1.3.6.1.2.1.1.7\nsysServices\nInteger32\nA coded value that indicates the set of services available \nat this node: physical (for example, a repeater), data \nlink/subnet (for example, bridge), Internet (for example, \nIP gateway), end-to-end (for example, host), applications.\nTable 9.2 \u0002 Managed objects in the MIB-2 system group\n\nassociated with a managed device. A second common usage of SNMP is for an agent\nto send an unsolicited message, known as a trap message, to a managing entity. Trap\nmessages are used to notify a managing entity of an exceptional situation that has\nresulted in changes to MIB object values. We saw earlier in Section 9.1 that the net-\nwork administrator might want to receive a trap message, for example, when an inter-\nface goes down, congestion reaches a predefined level on a link, or some other\nnoteworthy event occurs. Note that there are a number of important trade-offs between\npolling (request-response interaction) and trapping; see the homework problems.\nSNMPv2 defines seven types of messages, known generically as protocol data\nunits—PDUs—as shown in Table 9.4 and described next. The format of the PDU is\nshown in Figure 9.4.\n• The GetRequest, GetNextRequest, and GetBulkRequest PDUs are\nall sent from a managing entity to an agent to request the value of one or more\nMIB objects at the agent’s managed device. The object identifiers of the MIB\nobjects whose values are being requested are specified in the variable binding\nportion of the PDU. GetRequest, GetNextRequest, and GetBulkRe-\nquest differ in the granularity of their data requests. GetRequest can request\nan arbitrary set of MIB values; multiple GetNextRequests can be used to\nsequence through a list or table of MIB objects; GetBulkRequest allows a\nlarge block of data to be returned, avoiding the overhead incurred if multiple\nGetRequest or GetNextRequest messages were to be sent. In all three\ncases, the agent responds with a Response PDU containing the object identi-\nfiers and their associated values.\n• The SetRequest PDU is used by a managing entity to set the value of one or\nmore MIB objects in a managed device. An agent replies with a Response PDU\nwith the “noError” error status to confirm that the value has indeed been set.\nObject Identifier\nName\nType\nDescription (from RFC 4113)\n1.3.6.1.2.1.7.1\nudpInDatagrams\nCounter32\n“total number of UDP datagrams delivered to UDP users”\n1.3.6.1.2.1.7.2\nudpNoPorts\nCounter32\n“total number of received UDP datagrams for which there\nwas no application at the destination port”\n1.3.6.1.2.1.7.3\nudpInErrors\nCounter32\n“number of received UDP datagrams that could not be\ndelivered for reasons other than the lack of an application\nat the destination port”\n1.3.6.1.2.1.7.4\nudpOutDatagrams\nCounter32\n“total number of UDP datagrams sent from this entity”\nTable 9.3 \u0002 Selected managed objects in the MIB-2 UDP module\n9.3\n•\nTHE INTERNET-STANDARD MANAGEMENT FRAMEWORK\n773\n\n774\nCHAPTER 9\n•\nNETWORK MANAGEMENT\nPDU\ntype\n(0–3)\nRequest\nId\nError\nStatus\n(0–5)\nError\nIndex\nName\nValue\nName\nName\nValue\nPDU\nType\n(4)\nEnterprise\nAgent\nAddr\nTrap\nType\n(0–7)\nSpecific\ncode\nTime\nstamp\nValue\nGet/set header\nTrap header\nTrap information\nSNMP PDU\nVariables to get/set\nFigure 9.4 \u0002 SNMP PDU format\nSNMPv2 PDU Type\nSender-receiver\nDescription\nGetRequest\nmanager-to-agent\nget value of one or more MIB object instances\nGetNextRequest\nmanager-to-agent\nget value of next MIB object instance in list or table\nGetBulkRequest\nmanager-to-agent\nget values in large block of data, for example, values in a\nlarge table\nInformRequest\nmanager-to-manager\ninform remote managing entity of MIB values \nremote to its access\nSetRequest\nmanager-to-agent\nset value of one or more MIB object instances\nResponse\nagent-to-manager or\ngenerated in response to\nmanager-to-manager\nGetRequest,\nGetNextRequest,\nGetBulkRequest,\nSetRequest PDU, or\nInformRequest\nSNMPv2-Trap\nagent-to-manager\ninform manager of an exceptional event\nTable 9.4 \u0002 SNMPv2 PDU types"
    },
    {
      "chunk_id": "421c753c-5f28-461d-aadd-dd78d146e111",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "9.3.4 Security and Administration",
      "original_titles": [
        "9.3.4 Security and Administration"
      ],
      "path": "Chapter 9 Network Management > 9.3 The Internet-Standard Management Framework > 9.3.4 Security and Administration",
      "start_page": 802,
      "end_page": 804,
      "token_count": 1677,
      "text": "• The InformRequest PDU is used by a managing entity to notify another\nmanaging entity of MIB information that is remote to the receiving entity. The\nreceiving entity replies with a Response PDU with the “noError” error status\nto acknowledge receipt of the InformRequest PDU.\n• The final type of SNMPv2 PDU is the trap message. Trap messages are generated\nasynchronously; that is, they are not generated in response to a received request but\nrather in response to an event for which the managing entity requires notification.\nRFC 3418 defines well-known trap types that include a cold or warm start by a\ndevice, a link going up or down, the loss of a neighbor, or an authentication failure\nevent. A received trap request has no required response from a managing entity.\nGiven the request-response nature of SNMPv2, it is worth noting here that\nalthough SNMP PDUs can be carried via many different transport protocols, the\nSNMP PDU is typically carried in the payload of a UDP datagram. Indeed, RFC\n3417 states that UDP is “the preferred transport mapping.” Since UDP is an unreli-\nable transport protocol, there is no guarantee that a request, or its response, will be\nreceived at the intended destination. The request ID field of the PDU is used by the\nmanaging entity to number its requests to an agent; an agent’s response takes its\nrequest ID from that of the received request. Thus, the request ID field can be used\nby the managing entity to detect lost requests or replies. It is up to the managing\nentity to decide whether to retransmit a request if no corresponding response is\nreceived after a given amount of time. In particular, the SNMP standard does not\nmandate any particular procedure for retransmission, or even if retransmission is to\nbe done in the first place. It only requires that the managing entity “needs to act\nresponsibly in respect to the frequency and duration of retransmissions.” This, of\ncourse, leads one to wonder how a “responsible” protocol should act!\n9.3.4 Security and Administration\nThe designers of SNMPv3 have said that “SNMPv3 can be thought of as SNMPv2\nwith additional security and administration capabilities” [RFC 3410]. Certainly,\nthere are changes in SNMPv3 over SNMPv2, but nowhere are those changes more\nevident than in the area of administration and security. The central role of security\nin SNMPv3 was particularly important, since the lack of adequate security resulted\nin SNMP being used primarily for monitoring rather than control (for example,\nSetRequest is rarely used in SNMPv1).\nAs SNMP has matured through three versions, its functionality has grown but\nso too, alas, has the number of SNMP-related standards documents. This is evi-\ndenced by the fact that there is even now an RFC [RFC 3411] that “describes an\narchitecture for describing SNMP Management Frameworks”! While the notion of\nan “architecture” for “describing a framework” might be a bit much to wrap one’s\nmind around, the goal of RFC 3411 is an admirable one—to introduce a common\nlanguage for describing the functionality and actions taken by an SNMPv3 agent or\n9.3\n•\nTHE INTERNET-STANDARD MANAGEMENT FRAMEWORK\n775\n\n776\nCHAPTER 9\n•\nNETWORK MANAGEMENT\nCommand\ngenerator\nSNMP\napplications\nSNMP\nengine\nNotification\nreceiver\nProxy\nforwarder\nDispatching\nMessage-\nprocessing\nsystem\nTimeliness,\nauthentication,\nprivacy\nSecurity\nTransport layer\nAccess\ncontrol\nCommand\ngenerator\nNotification\noriginator\nOther\nPDU\nSecurity/message header\nPDU\nFigure 9.5 \u0002 SNMPv3 engine and applications\nmanaging entity. The architecture of an SNMPv3 entity is straightforward, and a\ntour through the architecture will serve to solidify our understanding of SNMP.\nSo-called SNMP applications consist of a command generator, notification\nreceiver, and proxy forwarder (all of which are typically found in a managing\nentity); a command responder and notification originator (both of which are typi-\ncally found in an agent); and the possibility of other applications. The command\ngenerator generates the GetRequest, GetNextRequest, GetBulkRequest,\nand SetRequest PDUs that we examined in Section 9.3.3 and handles the\nreceived responses to these PDUs. The command responder executes in an agent\nand receives, processes, and replies (using the Response message) to received\nGetRequest, GetNextRequest, GetBulkRequest, and SetRequest\nPDUs. The notification originator application in an agent generates Trap PDUs;\nthese PDUs are eventually received and processed in a notification receiver applica-\ntion at a managing entity. The proxy forwarder application forwards request, notifi-\ncation, and response PDUs.\nA PDU sent by an SNMP application next passes through the SNMP “engine”\nbefore it is sent via the appropriate transport protocol. Figure 9.5 shows how a PDU\ngenerated by the command generator application first enters the dispatch module,\n\nwhere the SNMP version is determined. The PDU is then processed in the message-\nprocessing system, where the PDU is wrapped in a message header containing the\nSNMP version number, a message ID, and message size information. If encryption or\nauthentication is needed, the appropriate header fields for this information are\nincluded as well; see [RFC 3411] for details. Finally, the SNMP message (the applica-\ntion-generated PDU plus the message header information) is passed to the appropriate\ntransport protocol. The preferred transport protocol for carrying SNMP messages is\nUDP (that is, SNMP messages are carried as the payload in a UDP datagram), and the\npreferred port number for the SNMP is port 161. Port 162 is used for trap messages.\nWe have seen above that SNMP messages are used not just to monitor, but also\nto control (for example, through the SetRequest command) network elements.\nClearly, an intruder that could intercept SNMP messages and/or generate its own\nSNMP packets into the management infrastructure could wreak havoc in the net-\nwork. Thus, it is crucial that SNMP messages be transmitted securely. Surprisingly,\nit is only in the most recent version of SNMP that security has received the attention\nthat it deserves. SNMPv3 security is known as user-based security [RFC 3414] in\nthat there is the traditional concept of a user, identified by a username, with which\nsecurity information such as a password, key value, or access privileges are associ-\nated. SNMPv3 provides for encryption, authentication, protection against playback\nattacks (see Section 8.3), and access control.\n• Encryption. SNMP PDUs can be encrypted using the Data Encryption Standard\n(DES) in Cipher Block Chaining (CBC) mode. Note that since DES is a shared-\nkey system, the secret key of the user encrypting data must be known by the\nreceiving entity that must decrypt the data.\n• Authentication. SNMP uses the Message Authentication Code (MAC) technique\nthat we studied in Section 8.3.1 to provide both authentication and protection\nagainst tampering [RFC 4301]. Recall that a MAC requires the sender and\nreceiver both to know a common secret key.\n• Protection against playback. Recall from our discussion in Chapter 8 that nonces\ncan be used to guard against playback attacks. SNMPv3 adopts a related\napproach. In order to ensure that a received message is not a replay of some\nearlier message, the receiver requires that the sender include a value in each mes-\nsage that is based on a counter in the receiver. This counter, which functions as a\nnonce, reflects the amount of time since the last reboot of the receiver’s network\nmanagement software and the total number of reboots since the receiver’s net-\nwork management software was last configured. As long as the counter in a\nreceived message is within some margin of error of the receiver’s actual value,\nthe message is accepted as a nonreplay message, at which point it may be authen-\nticated and/or decrypted. See [RFC 3414] for details.\n• Access control. SNMPv3 provides a view-based access control [RFC 3415] that\ncontrols which network management information can be queried and/or set \nby which users. An SNMP entity retains information about access rights and \n9.3\n•\nTHE INTERNET-STANDARD MANAGEMENT FRAMEWORK\n777"
    },
    {
      "chunk_id": "c3759361-098a-43c4-8192-3874962ab966",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "9.4 ASN.1",
      "original_titles": [
        "9.4 ASN.1"
      ],
      "path": "Chapter 9 Network Management > 9.4 ASN.1",
      "start_page": 805,
      "end_page": 809,
      "token_count": 1992,
      "text": "778\nCHAPTER 9\n•\nNETWORK MANAGEMENT\npolicies in a Local Configuration Datastore (LCD). Portions of the LCD are\nthemselves accessible as managed objects, defined in the View-Based Access\nControl Model Configuration MIB [RFC 3415], and thus can be managed and\nmanipulated remotely via SNMP.\n9.4 ASN.1\nIn this book, we have covered a number of interesting topics in computer networking.\nThis section on ASN.1, however, may not make the top-ten list of interesting topics.\nLike vegetables, knowledge about ASN.1 and the broader issue of presentation serv-\nices is something that is “good for you.” ASN.1 is an ISO-originated standard that is\nused in a number of Internet-related protocols, particularly in the area of network man-\nagement. For example, we saw in Section 9.3 that MIB variables in SNMP were inex-\ntricably tied to ASN.1. So while the material on ASN.1 in this section may be rather\ndry, we hope the reader will take it on faith that the material is important.\nIn order to motivate our discussion here, consider the following thought experi-\nment. Suppose one could reliably copy data from one computer’s memory directly\ninto a remote computer’s memory. If one could do this, would the communication\nproblem be “solved?” The answer to the question depends on one’s definition of\n“the communication problem.” Certainly, a perfect memory-to-memory copy would\nexactly communicate the bits and bytes from one machine to another. But does such\nan exact copy of the bits and bytes mean that when software running on the receiv-\ning computer accesses this data, it will see the same values that were stored into the\nsending computer’s memory? The answer to this question is “not necessarily!” The\ncrux of the problem is that different computer architectures, different operating sys-\ntems, and different compilers have different conventions for storing and represent-\ning data. If data is to be communicated and stored among multiple computers (as it\nis in every communication network), this problem of data representation must\nclearly be solved.\nAs an example of this problem, consider the simple C code fragment below.\nHow might this structure be laid out in memory?\nstruct {\nchar code;\nint x;\n} test;\ntest.x = 259;\ntest.code = ‘a’;\nThe left side of Figure 9.6 shows a possible layout of this data on one hypo-\nthetical architecture: there is a single byte of memory containing the character a,\n\nfollowed by a 16-bit word containing the integer value 259, stored with the most\nsignificant byte first. The layout in memory on another computer is shown in the\nright half of Figure 9.6. The character a is followed by the integer value stored\nwith the least significant byte stored first and with the 16-bit integer aligned to\nstart on a 16-bit word boundary. Certainly, if one were to perform a verbatim\ncopy between these two computers’ memories and use the same structure defini-\ntion to access the stored values, one would see very different results on the two\ncomputers!\nThe fact that different architectures have different internal data formats is a real\nand pervasive problem. The particular problem of integer storage in different for-\nmats is so common that it has a name. “Big-endian” order for storing integers has\nthe most significant bytes of the integer stored first (at the lowest storage address).\n“Little-endian” order stores the least significant bytes first. Sun SPARC and\nMotorola processors are big-endian, while Intel processors are little-endian. As an\naside, the terms “big-endian” and “little-endian” come from the book, Gulliver’s\nTravels, by Jonathan Swift, in which two groups of people dogmatically insist \non doing a simple thing in two different ways (hopefully, the analogy to the com-\nputer architecture community is clear). One group in the land of Lilliput insists on\nbreaking their eggs at the larger end (“the big-endians”), while the other insists \non breaking them at the smaller end. The difference was the cause of great civil\nstrife and rebellion.\nGiven that different computers store and represent data in different ways,\nhow should networking protocols deal with this? For example, if an SNMP agent\nis about to send a Response message containing the integer count of the number\nof received UDP datagrams, how should it represent the integer value to be sent\nto the managing entity—in big-endian or little-endian order? One option would\nbe for the agent to send the bytes of the integer in the same order in which they\nwould be stored in the managing entity. Another option would be for the agent to\nsend in its own storage order and have the receiving entity reorder the bytes, as\nneeded. Either option would require the sender or receiver to learn the other’s\nformat for integer representation.\n9.4\n•\nASN.1\n779\na\n00000001\n00000011\ntest.code\ntest.x\ntest.code\ntest.x\na\n00000011\n00000001\nFigure 9.6 \u0002 Two different data layouts on two different architectures\n\n780\nCHAPTER 9\n•\nNETWORK MANAGEMENT\nAging\n60’s hippie\nGroovy\nGrandma\n2012 Teenager\nGroovy\nFigure 9.7 \u0002 The presentation problem\nA third option is to have a machine-independent, OS-independent, language-\nindependent method for describing integers and other data types (that is, a data-\ndefinition language) and rules that state the manner in which each of the data types\nis to be transmitted over the network. When data of a given type is received, it is\nreceived in a known format and can then be stored in whatever machine-specific\nformat is required. Both the SMI that we studied in Section 9.3 and ASN.1 adopt\nthis third option. In ISO parlance, these two standards describe a presentation\nservice—the service of transmitting and translating information from one machine-\nspecific format to another. Figure 9.7 illustrates a real-world presentation problem;\nneither receiver understands the essential idea being communicated—that the\nspeaker likes something. As shown in Figure 9.8, a presentation service can solve\nthis problem by translating the idea into a commonly understood (by the presenta-\ntion service), person-independent language, sending that information to the receiver,\nand then translating into a language understood by the receiver.\nTable 9.5 shows a few of the ASN.1-defined data types. Recall that we\nencountered the INTEGER, OCTET STRING, and OBJECT IDENTIFIER data\ntypes in our earlier study of the SMI. Since our goal here is (mercifully) not to\nprovide a complete introduction to ASN.1, we refer the reader to the standards or\nto the printed and online book [Larmouth 1996] for a description of ASN.1 types\nand constructors, such as SEQUENCE and SET, that allow for the definition of\nstructured data types.\nIn addition to providing a data definition language, ASN.1 also provides Basic\nEncoding Rules (BER) that specify how instances of objects that have been defined\nusing the ASN.1 data definition language are to be sent over the network. The BER\nadopts a so-called TLV (Type, Length, Value) approach to encoding data for\ntransmission. For each data item to be sent, the data type, the length of the data item,\n\nand then the actual value of the data item are sent, in that order. With this simple\nconvention, the received data is essentially self-identifying.\nFigure 9.9 shows how the two data items in a simple example would be sent.\nIn this example, the sender wants to send the character string “smith” followed by\nthe value 259 decimal (which equals 00000001 00000011 in binary, or a byte value\nof 1 followed by a byte value of 3), assuming big-endian order. The first byte in the\n9.4\n•\nASN.1\n781\nAging\n60’s hippie\nPresentation\nservice\nGrandma\n2012 Teenager\nPresentation\nservice\nIt is pleasing\nCat’s pajamas\nAwesome\nGroovy\nIt is pleasing\nPresentation\nservice\nFigure 9.8 \u0002 The presentation problem solved\nTag\nType\nDescription\n1\nBOOLEAN\nvalue is “true” or “false”\n2\nINTEGER\ncan be arbitrarily large\n3\nBITSTRING\nlist of one or more bits\n4\nOCTET STRING\nlist of one or more bytes\n5\nNULL\nno value\n6\nOBJECT IDENTIFIER\nname, in the ASN.1 standard naming\ntree; see Section 9.2.2\n9\nREAL\nfloating point\nTable 9.5 \u0002 Selected ASN.1 data types\n\n782\nCHAPTER 9\n•\nNETWORK MANAGEMENT\nlastname ::= OCTET STRING\nweight ::= INTEGER\n{weight, 259}\n{lastname, \"smith\"}\nModule of data type\ndeclarations written\nin ASN.1\nInstances of data type\nspecified in module\nBasic Encoding Rules\n(BER)\nTransmitted\nbyte stream\n3\n1\n2\n2\nh\nt\ni\nm\ns\n5\n4\nFigure 9.9 \u0002 BER encoding example\ntransmitted stream has the value 4, indicating that the type of the following data\nitem is an OCTET STRING; this is the “T” in the TLV encoding. The second byte\nin the stream contains the length of the OCTET STRING, in this case 5. The third\nbyte in the transmitted stream begins the OCTET STRING of length 5; it contains\nthe ASCII representation of the letter s. The T, L, and V values of the next data item\nare 2 (the INTEGER type tag value), 2 (that is, an integer of length 2 bytes), and\nthe 2-byte big-endian representation of the value 259 decimal.\nIn our previous discussion, we have only touched on a small and simple subset\nof ASN.1. Resources for learning more about ASN.1 include the ASN.1 standards\ndocument [ISO X.680 2002], the online OSI-related book [Larmouth 2012], and the\nASN.1-related Web sites, [OSS 2012] and [OID Repository 2012]."
    },
    {
      "chunk_id": "9bf7e892-9c2a-4cf1-91db-b8919ab130b3",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "9.5 Conclusion",
      "original_titles": [
        "9.5 Conclusion"
      ],
      "path": "Chapter 9 Network Management > 9.5 Conclusion",
      "start_page": 810,
      "end_page": 812,
      "token_count": 1092,
      "text": "9.5 Conclusion\nOur study of network management, and indeed of all of networking, is now complete!\nIn this final chapter on network management, we began by motivating the need\nfor providing appropriate tools for the network administrator—the person whose job\nit is to keep the network “up and running”—for monitoring, testing, polling,\nconfiguring, analyzing, evaluating, and controlling the operation of the network.\nOur analogies with the management of complex systems such as power plants,\nairplanes, and human organization helped motivate this need. We saw that the\narchitecture of network management systems revolves around five key components:\n(1) a network manager, (2) a set of managed remote (from the network manager)\ndevices, (3) the Management Information Bases (MIBs) at these devices, containing\ndata about the devices’ status and operation, (4) remote agents that report MIB infor-\nmation and take action under the control of the network manager, and (5) a protocol\nfor communication between the network manager and the remote devices.\nWe then delved into the details of the Internet-Standard Management Frame-\nwork, and the SNMP protocol in particular. We saw how SNMP instantiates the five\nkey components of a network management architecture, and we spent considerable\ntime examining MIB objects, the SMI—the data definition language for specifying\nMIBs, and the SNMP protocol itself. Noting that the SMI and ASN.1 are inextrica-\nbly tied together, and that ASN.1 plays a key role in the presentation layer in the\nISO/OSI seven-layer reference model, we then briefly examined ASN.1. Perhaps\nmore important than the details of ASN.1 itself was the noted need to provide for\ntranslation between machine-specific data formats in a network. While some net-\nwork architectures explicitly acknowledge the importance of this service by having\na presentation layer, this layer is absent in the Internet protocol stack.\nIt is also worth noting that there are many topics in network management that\nwe chose not to cover—topics such as fault identification and management, proac-\ntive anomaly detection, alarm correlation, and the larger issues of service manage-\nment (for example, as opposed to network management). While important, these\ntopics would form a text in their own right, and we refer the reader to the references\nnoted in Section 9.1.\nHomework Problems and Questions\nChapter 9 Review Questions\nSECTION 9.1\nR1. Why would a network manager benefit from having network management\ntools? Describe five scenarios.\nR2. What are the five areas of network management defined by the ISO?\nHOMEWORK PROBLEMS AND QUESTIONS\n783\n\n784\nCHAPTER 9\n•\nNETWORK MANAGEMENT\nR3. What is the difference between network management and service \nmanagement?\nSECTION 9.2\nR4. Define the following terms: managing entity, managed device, management\nagent, MIB, network management protocol.\nSECTION 9.3\nR5. What is the role of the SMI in network management?\nR6. What is an important difference between a request-response message and a\ntrap message in SNMP?\nR7. What are the seven message types used in SNMP?\nR8. What is meant by an “SNMP engine”?\nSECTION 9.4\nR9. What is the purpose of the ASN.1 object identifier tree?\nR10. What is the role of ASN.1 in the ISO/OSI reference model’s presentation layer?\nR11. Does the Internet have a presentation layer? If not, how are concerns about\ndifferences in machine architectures—for example, the different representa-\ntion of integers on different machines—addressed?\nR12. What is meant by TLV encoding?\nProblems\nP1. Consider the two ways in which communication occurs between a managing\nentity and a managed device: request-response mode and trapping. What \nare the pros and cons of these two approaches, in terms of (1) overhead, \n(2) notification time when exceptional events occur, and (3) robustness \nwith respect to lost messages between the managing entity and the \ndevice?\nP2. In Section 9.3 we saw that it was preferable to transport SNMP messages in\nunreliable UDP datagrams. Why do you think the designers of SNMP chose\nUDP rather than TCP as the transport protocol of choice for SNMP?\nP3. What is the ASN.1 object identifier for the ICMP protocol (see Figure 9.3)?\nP4. Suppose you worked for a US-based company that wanted to develop its \nown MIB for managing a product line. Where in the object identifier tree\n(Figure 9.3) would it be registered? (Hint: You’ll have to do some digging\nthrough RFCs or other documents to answer this question.)\n\nP5. Recall from Section 9.3.2 that a private company (enterprise) can create its\nown MIB variables under the private branch 1.3.6.4. Suppose that IBM\nwanted to create a MIB for its Web server software. What would be the next\nOID qualifier after 1.3.6.1.4? (In order to answer this question, you will need\nto consult [IANA 2009b]). Search the Web and see if you can find out\nwhether such a MIB exists for an IBM server.\nP6. Why do you think the length precedes the value in a TLV encoding (rather\nthan the length following the value)?\nP7. Consider Figure 9.9. What would be the BER encoding of {weight, 165}\n{lastname, “Michael”}?\nP8. Consider Figure 9.9. What would be the BER encoding of {weight, 145}\n{lastname, “Sridhar”}?\nPROBLEMS\n785"
    },
    {
      "chunk_id": "33a4bd2a-5906-4dda-9354-21b9376d942e",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Homework Problems and Questions",
      "original_titles": [
        "Homework Problems and Questions"
      ],
      "path": "Chapter 9 Network Management > Homework Problems and Questions",
      "start_page": 810,
      "end_page": 812,
      "token_count": 1092,
      "text": "9.5 Conclusion\nOur study of network management, and indeed of all of networking, is now complete!\nIn this final chapter on network management, we began by motivating the need\nfor providing appropriate tools for the network administrator—the person whose job\nit is to keep the network “up and running”—for monitoring, testing, polling,\nconfiguring, analyzing, evaluating, and controlling the operation of the network.\nOur analogies with the management of complex systems such as power plants,\nairplanes, and human organization helped motivate this need. We saw that the\narchitecture of network management systems revolves around five key components:\n(1) a network manager, (2) a set of managed remote (from the network manager)\ndevices, (3) the Management Information Bases (MIBs) at these devices, containing\ndata about the devices’ status and operation, (4) remote agents that report MIB infor-\nmation and take action under the control of the network manager, and (5) a protocol\nfor communication between the network manager and the remote devices.\nWe then delved into the details of the Internet-Standard Management Frame-\nwork, and the SNMP protocol in particular. We saw how SNMP instantiates the five\nkey components of a network management architecture, and we spent considerable\ntime examining MIB objects, the SMI—the data definition language for specifying\nMIBs, and the SNMP protocol itself. Noting that the SMI and ASN.1 are inextrica-\nbly tied together, and that ASN.1 plays a key role in the presentation layer in the\nISO/OSI seven-layer reference model, we then briefly examined ASN.1. Perhaps\nmore important than the details of ASN.1 itself was the noted need to provide for\ntranslation between machine-specific data formats in a network. While some net-\nwork architectures explicitly acknowledge the importance of this service by having\na presentation layer, this layer is absent in the Internet protocol stack.\nIt is also worth noting that there are many topics in network management that\nwe chose not to cover—topics such as fault identification and management, proac-\ntive anomaly detection, alarm correlation, and the larger issues of service manage-\nment (for example, as opposed to network management). While important, these\ntopics would form a text in their own right, and we refer the reader to the references\nnoted in Section 9.1.\nHomework Problems and Questions\nChapter 9 Review Questions\nSECTION 9.1\nR1. Why would a network manager benefit from having network management\ntools? Describe five scenarios.\nR2. What are the five areas of network management defined by the ISO?\nHOMEWORK PROBLEMS AND QUESTIONS\n783\n\n784\nCHAPTER 9\n•\nNETWORK MANAGEMENT\nR3. What is the difference between network management and service \nmanagement?\nSECTION 9.2\nR4. Define the following terms: managing entity, managed device, management\nagent, MIB, network management protocol.\nSECTION 9.3\nR5. What is the role of the SMI in network management?\nR6. What is an important difference between a request-response message and a\ntrap message in SNMP?\nR7. What are the seven message types used in SNMP?\nR8. What is meant by an “SNMP engine”?\nSECTION 9.4\nR9. What is the purpose of the ASN.1 object identifier tree?\nR10. What is the role of ASN.1 in the ISO/OSI reference model’s presentation layer?\nR11. Does the Internet have a presentation layer? If not, how are concerns about\ndifferences in machine architectures—for example, the different representa-\ntion of integers on different machines—addressed?\nR12. What is meant by TLV encoding?\nProblems\nP1. Consider the two ways in which communication occurs between a managing\nentity and a managed device: request-response mode and trapping. What \nare the pros and cons of these two approaches, in terms of (1) overhead, \n(2) notification time when exceptional events occur, and (3) robustness \nwith respect to lost messages between the managing entity and the \ndevice?\nP2. In Section 9.3 we saw that it was preferable to transport SNMP messages in\nunreliable UDP datagrams. Why do you think the designers of SNMP chose\nUDP rather than TCP as the transport protocol of choice for SNMP?\nP3. What is the ASN.1 object identifier for the ICMP protocol (see Figure 9.3)?\nP4. Suppose you worked for a US-based company that wanted to develop its \nown MIB for managing a product line. Where in the object identifier tree\n(Figure 9.3) would it be registered? (Hint: You’ll have to do some digging\nthrough RFCs or other documents to answer this question.)\n\nP5. Recall from Section 9.3.2 that a private company (enterprise) can create its\nown MIB variables under the private branch 1.3.6.4. Suppose that IBM\nwanted to create a MIB for its Web server software. What would be the next\nOID qualifier after 1.3.6.1.4? (In order to answer this question, you will need\nto consult [IANA 2009b]). Search the Web and see if you can find out\nwhether such a MIB exists for an IBM server.\nP6. Why do you think the length precedes the value in a TLV encoding (rather\nthan the length following the value)?\nP7. Consider Figure 9.9. What would be the BER encoding of {weight, 165}\n{lastname, “Michael”}?\nP8. Consider Figure 9.9. What would be the BER encoding of {weight, 145}\n{lastname, “Sridhar”}?\nPROBLEMS\n785"
    },
    {
      "chunk_id": "40cf94b3-703b-465d-bdcc-5eb06cded541",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Interview: Jennifer Rexford",
      "original_titles": [
        "Interview: Jennifer Rexford"
      ],
      "path": "Chapter 9 Network Management > Interview: Jennifer Rexford",
      "start_page": 813,
      "end_page": 815,
      "token_count": 1212,
      "text": "Please describe one or two of the most exciting projects you have worked on during your\ncareer. What were the biggest challenges?\nWhen I was a researcher at AT&T, a group of us designed a new way to manage routing in\nInternet Service Provider backbone networks. Traditionally, network operators configure\neach router individually, and these routers run distributed protocols to compute paths\nthrough the network. We believed that network management would be simpler and more\nflexible if network operators could exercise direct control over how routers forward traffic\nbased on a network-wide view of the topology and traffic. The Routing Control Platform\n(RCP) we designed and built could compute the routes for all of AT&T’s backbone on a \nsingle commodity computer, and could control legacy routers without modification. To me,\nthis project was exciting because we had a provocative idea, a working system, and \nultimately a real deployment in an operational network.\nWhat changes and innovations do you see happening in network management in the\nfuture?\nRather than simply “bolting on” network management on top of existing networks,\nresearchers and practitioners alike are starting to design networks that are fundamentally\neasier to manage. Like our early work on the RCP, the main idea in so-called Software\nDefined Networking (SDN) is to run a controller that can install low-level packet-handling\nrules in the underlying switches using a standard protocol. This controller can run various\n786\nJennifer Rexford\nJennifer Rexford is a Professor in the Computer Science department at\nPrinceton University. Her research has the broad goal of making com-\nputer networks easier to design and manage, with particular emphasis\non routing protocols. From 1996–2004, she was a member of the\nNetwork Management and Performance department at AT&T Labs--\nResearch. While at AT&T, she designed techniques and tools for net-\nwork measurement, traffic engineering, and router configuration that were deployed in AT&T’s\nbackbone network. Jennifer is co-author of the book ”Web Protocols and Practice:\nNetworking Protocols, Caching, and Traffic Measurement,” published by Addison-Wesley in\nMay 2001. She served as the chair of ACM SIGCOMM from 2003 to 2007. She received\nher BSE degree in electrical engineering from Princeton University in 1991, and her MSE\nand PhD degrees in electrical engineering and computer science from the University of\nMichigan in 1993 and 1996, respectively. In 2004, Jennifer was the winner of ACM’s\nGrace Murray Hopper Award for outstanding young computer professional and appeared on\nthe MIT TR-100 list of top innovators under the age of 35.\nAN INTERVIEW WITH...\n\nnetwork-management applications, such as dynamic access control, seamless user mobility,\ntraffic engineering, server load balancing, energy-efficient networking, and so on. I believe\nSDN is a great opportunity to get network management right, by rethinking the relationship\nbetween the network devices and the software that manages them.\nWhere do you see the future of networking and the Internet?\nNetworking is an exciting field because the applications and the underlying technologies\nchange all the time. We are always reinventing ourselves! Who would have predicted even\nfive or ten years ago the dominance of smart phones, allowing mobile users to access \nexisting applications as well as new location-based services? The emergence of cloud com-\nputing is fundamentally changing the relationship between users and the applications they\nrun, and networked sensors are enabling a wealth of new applications. The pace of innova-\ntion is truly inspiring.\nThe underlying network is a crucial component in all of these innovations. Yet, the \nnetwork is notoriously “in the way”—limiting performance, compromising reliability, \nconstraining applications, and complicating the deployment and management of services.\nWe should strive to make the network of the future as invisible as the air we breathe, so it\nnever stands in the way of new ideas and valuable services. To do this, we need to raise the\nlevel of abstraction above individual network devices and protocols (and their attendant\nacronyms!), so we can reason about the network as a whole.\nWhat people inspired you professionally?\nI’ve long been inspired by Sally Floyd at the International Computer Science Institute. \nHer research is always purposeful, focusing on the important challenges facing the Internet.\nShe digs deeply into hard questions until she understands the problem and the space of \nsolutions completely, and she devotes serious energy into “making things happen,” such as\npushing her ideas into protocol standards and network equipment. Also, she gives back to\nthe community, through professional service in numerous standards and research organiza-\ntions and by creating tools (such as the widely used ns-2 and ns-3 simulators) that enable\nother researchers to succeed. She retired in 2009 but her influence on the field will be felt\nfor years to come.\nWhat are your recommendations for students who want careers in computer science and\nnetworking?\nNetworking is an inherently interdisciplinary field. Applying techniques from other disciplines\nto networking problems is a great way to move the field forward. We’ve seen tremendous\n787\n\nbreakthroughs in networking come from such diverse areas as queuing theory, game \ntheory, control theory, distributed systems, network optimization, programming languages,\nmachine learning, algorithms, data structures, and so on. I think that becoming conversant\nin a related field, or collaborating closely with experts in those fields, is a wonderful\nway to put networking on a stronger foundation, so we can learn how to build networks\nthat are worthy of society’s trust. Beyond the theoretical disciplines, networking is exciting\nbecause we create real artifacts that real people use. Mastering how to design and build\nsystems—by gaining experience in operating systems, computer architecture, and so\non—is another fantastic way to amplify your knowledge of networking to help change\nthe world.\n788"
    },
    {
      "chunk_id": "65e929ee-3a8a-4ed4-9318-f8ecae32bd8c",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "References",
      "original_titles": [
        "References"
      ],
      "path": "References",
      "start_page": 816,
      "end_page": 849,
      "token_count": 17997,
      "text": "References\n789\nA note on URLs. In the references below, we have provided URLs for Web pages, Web-only\ndocuments, and other material that has not been published in a conference or journal (when\nwe have been able to locate a URL for such material). We have not provided URLs for\nconference and journal publications, as these documents can usually be located via a search\nengine, from the conference Web site (e.g., papers in all ACM SIGCOMM conferences and\nworkshops can be located via http://www.acm.org/sigcomm), or via a digital library\nsubscription. While all URLs provided below were valid (and tested) in Jan. 2012, URLs \ncan become out of date. Please consult the online version of this book (http://www.awl.com/\nkurose-ross) for an up-to-date bibliography.\nA note on Internet Request for Comments (RFCs): Copies of Internet RFCs are available\nat many sites. The RFC Editor of the Internet Society (the body that oversees the RFCs)\nmaintains the site, http://www.rfc-editor.org. This site allows you to search for a specific\nRFC by title, number, or authors, and will show updates to any RFCs listed. Internet RFCs\ncan be updated or obsoleted by later RFCs. Our favorite site for getting RFCs is the original\nsource—http://www.rfc-editor.org.\n[3Com Addressing 2012] 3Com Corp., “White paper: Understanding IP addressing:\nEverything you ever wanted to know,” http://www.3com.com/other/pdfs/infra/corpinfo/\nen_US/501302.pdf\n[3GPP 2012] Third Generation Partnership Project homepage, http://www.3gpp.org/\n[3GPP Network Architecture 2012] 3GPP, “TS 23.002: Network Architecture: Digital\nCellular Telecommunications System (Phase 2+); Universal Mobile Telecommunications\nSystem (UMTS); LTE,” http://www.3gpp.org/ftp/Specs/html-info/23002.htm\n[Albitz 1993] P. Albitz and C. Liu, DNS and BIND, O’Reilly & Associates, Petaluma, CA, 1993.\n[Abramson 1970] N. Abramson, “The Aloha System—Another Alternative for Computer\nCommunications,” Proc. 1970 Fall Joint Computer Conference, AFIPS Conference, p. 37, 1970.\n[Abramson 1985] N. Abramson, “Development of the Alohanet,” IEEE Transactions on\nInformation Theory, Vol. IT-31, No. 3 (Mar. 1985), pp. 119–123.\n[Abramson 2009] N. Abramson, “The Alohanet – Surfing for Wireless Data,” IEEE\nCommunications Magazine, Vol. 47, No. 12, pp. 21–25.\n[Abu-Libdeh 2010] H. Abu-Libdeh, P. Costa, A. Rowstron, G. O’Shea, A. Donnelly,\n“Symbiotic Routing in Future Data Centers,” Proc. 2010 ACM SIGCOMM.\n[Adhikari 2011a] V. K. Adhikari, S. Jain, Y. Chen, Z. L. Zhang, “Vivisecting YouTube: An\nActive Measurement Study,” Technical Report, University of Minnesota, 2011.\n[Adhikari 2012] V. K. Adhikari, Y. Gao, F. Hao, M. Varvello, V. Hilt, M. Steiner, Z. L.\nZhang, “Unreeling Netflix: Understanding and Improving Multi-CDN Movie Delivery,”\nTechnical Report, University of Minnesota, 2012.\n[Afanasyev 2010] A. Afanasyev, N. Tilley, P. Reiher, L. Kleinrock, “Host-to-Host Congestion\nControl for TCP,” IEEE Communications Surveys & Tutorials, Vol. 12, No. 3, pp. 304–342.\n\n[Agarwal 2009] S. Agarwal, J. Lorch, “Matchmaking for Online Games and Other Latency-\nsensitive P2P Systems,” Proc. 2009 ACM SIGCOMM.\n[Ahn 1995] J. S. Ahn, P. B. Danzig, Z. Liu, and Y. Yan, “Experience with TCP Vegas: Emulation\nand Experiment,” Proc. 1995 ACM SIGCOMM (Boston, MA, Aug. 1995), pp. 185–195.\n[Akamai 2012] Akamai homepage, http://www.akamai.com\n[Akella 2003] A. Akella, S. Seshan, A. Shaikh, “An Empirical Evaluation of Wide-Area\nInternet Bottlenecks,” Proc. 2003 ACM Internet Measurement Conference (Miami, FL,\nNov. 2003).\n[Akhshabi 2011] S. Akhshabi, A. C. Begen, C. Dovrolis, “An Experimental Evaluation of\nRate-Adaptation Algorithms in Adaptive Streaming over HTTP,” Proc. 2011 ACM\nMultimedia Systems Conf.\n[Akyildiz 2010] I. Akyildiz, D. Gutierrex-Estevez, E. Reyes, “The Evolution to 4G Cellular\nSystems, LTE Advanced,” Physical Communication, Elsevier, 3 (2010), 217–244.\n[Alcatel-Lucent 2009] Alcatel-Lucent, “Introduction to Evolved Packet Core,”\nhttp://downloads.lightreading.com/wplib/alcatellucent/ALU_WP_Intro_to_EPC.pdf\n[Al-Fares 2008] M. Al-Fares, A. Loukissas, A. Vahdat, “A Scalable, Commodity Data\nCenter Network Architecture,” Proc. 2008 ACM SIGCOMM.\n[Alizadeh 2010] M. Alizadeh, A. Greenberg, D. Maltz, J. Padhye, P. Patel, B. Prabhakar, S.\nSengupta, M. Sridharan, “Data Center TCP (DCTCP),” Proc. 2010 ACM SIGCOMM.\n[Allman 2011] E. Allman, “The Robustness Principle Reconsidered: Seeking a Middle\nGround,” Communications of the ACM, Vol. 54, No. 8 (Aug. 2011), pp. 40–45.\n[Anderson 1995] J. B. Andersen, T. S. Rappaport, S. Yoshida, “Propagation Measurements\nand Models for Wireless Communications Channels,” IEEE Communications Magazine,\n(Jan. 1995), pp. 42–49.\n[Andrews 2002] M. Andrews, M. Shepherd, A. Srinivasan, P. Winkler, F. Zane, “Clustering\nand Server Election Using Passive Monitoring,” Proc. 2002 IEEE INFOCOM.\n[Androutsellis-Theotokis 2004] S. Androutsellis-Theotokis, D. Spinellis, “A Survey of\nPeer-to-Peer Content Distribution Technologies,” ACM Computing Surveys, Vol. 36, No. 4\n(Dec. 2004), pp. 335–371.\n[Aperjis 2008] C. Aperjis, M.J. Freedman, R. Johari, “Peer-Assisted Content Distribution\nwith Prices,” Proc. ACM CoNEXT’08 (Madrid, Dec. 2008).\n[Appenzeller 2004] G. Appenzeller, I. Keslassy, N. McKeown, “Sizing Router Buffers,”\nProc. 2004 ACM SIGCOMM (Portland, OR, Aug. 2004).\n[Ash 1998] G. R. Ash, Dynamic Routing in Telecommunications Networks, McGraw Hill,\nNew York, NY, 1998.\n[ASO-ICANN 2012] The Address Supporting Organization home page, http://www.aso\n.icann.org\n[AT&T SLA 2012] AT&T, “AT&T High Speed Internet Business Edition Service Level\nAgreements,” http://www.att.com/gen/general?pid=6622\n[Atheros 2012] Atheros Communications Inc. “Atheros AR5006 WLAN Chipset Product\nBulletins,” http://www.atheros.com/pt/AR5006Bulletins.htm\n[Augustin 2009] B. Augustin, B. Krishnamurthy, W. Willinger, “IXPs: Mapped?” Proc.\nInternet Measurement Conference (IMC), November 2009.\n790\nREFERENCES\n\n[Ayanoglu 1995] E. Ayanoglu, S. Paul, T. F. La Porta, K. K. Sabnani, R. D. Gitlin,\n“AIRMAIL: A Link-Layer Protocol for Wireless Networks,” ACM ACM/Baltzer Wireless\nNetworks Journal, 1: 47–60, Feb. 1995.\n[Bakre 1995] A. Bakre, B. R. Badrinath, “I-TCP: Indirect TCP for Mobile Hosts,” Proc.\n1995 Int. Conf. on Distributed Computing Systems (ICDCS) (May 1995), pp. 136–143.\n[Balakrishnan 1997] H. Balakrishnan, V. Padmanabhan, S. Seshan, R. Katz, “A Comparison\nof Mechanisms for Improving TCP Performance Over Wireless Links,” IEEE/ACM\nTransactions on Networking Vol. 5, No. 6 (Dec. 1997).\n[Balakrishnan 2003] H. Balakrishnan, F. Kaashoek, D. Karger, R. Morris, I. Stoica,\n“Looking Up Data in P2P Systems,” Communications of the ACM, Vol. 46, No. 2 (Feb. 2003),\npp. 43–48.\n[Baldauf 2007] M. Baldauf, S. Dustdar, F. Rosenberg, “A Survey on Context-Aware\nSystems,” Int. J. Ad Hoc and Ubiquitous Computing, Vol. 2, No. 4 (2007), pp. 263–277.\n[Ballani 2006] H. Ballani, P. Francis, S. Ratnasamy, “A Measurement-based Deployment\nProposal for IP Anycast,” Proc. 2006 ACM Internet Measurement Conf.\n[Ballani 2011] H. Ballani, P. Costa, T. Karagiannis, Ant Rowstron, “Towards Predictable\nDatacenter Networks,” Proc. 2011 ACM SIGCOMM.\n[Baran 1964] P. Baran, “On Distributed Communication Networks,” IEEE Transactions on\nCommunication Systems, Mar. 1964. Rand Corporation Technical report with the same title\n(Memorandum RM-3420-PR, 1964). http://www.rand.org/publications/RM/RM3420/\n[Bardwell 2004] J. Bardwell, “You Believe You Understand What You Think I Said . . . The\nTruth About 802.11 Signal And Noise Metrics: A Discussion Clarifying Often-Misused\n802.11 WLAN Terminologies,” http://www.connect802.com/download/techpubs/2004/\nyou_believe_D100201.pdf\n[Barford 2009] P. Barford, N. Duffield, A. Ron, J. Sommers, “ Network Performance\nAnomaly Detection and Localization,” Proc. 2009 IEEE INFOCOM (Apr. 2009).\n[Baronti 2007] P. Baronti, P. Pillai, V. Chook, S. Chessa, A. Gotta, Y. Hu, “Wireless Sensor\nNetworks: A Survey on the State of the Art and the 802.15.4 and ZigBee Standards,”\nComputer Communications, Vol. 30, No. 7 (2007), pp. 1655–1695.\n[Baset 2006] S. A. Basset and H. Schulzrinne, “An analysis of the Skype peer-to-peer\nInternet Telephony Protocol,” Proc. 2006 IEEE INFOCOM (Barcelona, Spain, Apr. 2006).\n[BBC 2001] BBC news online “A Small Slice of Design,” Apr. 2001, http://news.bbc.co.uk/\n2/hi/science/nature/1264205.stm\n[BBC 2012] BBC, “Multicast,” http://www.bbc.co.uk/multicast/\n[Beheshti 2008] N. Beheshti, Y. Ganjali, M. Ghobadi, N. McKeown, G. Salmon,\n“Experimental Study of Router Buffer Sizing,” Proc. ACM Internet Measurement\nConference (October 2008, Vouliagmeni, Greece).\n[Bender 2000] P. Bender, P. Black, M. Grob, R. Padovani, N. Sindhushayana, A. Viterbi,\n“CDMA/HDR: A bandwidth-efficient high-speed wireless data service for nomadic users,”\nIEEE Commun. Mag., Vol. 38, No. 7 (July 2000) pp. 70–77.\n[Berners-Lee 1989] T. Berners-Lee, CERN, “Information Management: A Proposal,” \nMar. 1989, May 1990. http://www.w3.org/History/1989/proposal.html\n[Berners-Lee 1994] T. Berners-Lee, R. Cailliau, A. Luotonen, H. Frystyk Nielsen, A. Secret,\n“The World-Wide Web,” Communications of the ACM, Vol. 37, No. 8 (Aug. 1994), pp. 76–82.\nREFERENCES\n791\n\n[Bertsekas 1991] D. Bertsekas, R. Gallagher, Data Networks, 2nd Ed., Prentice Hall,\nEnglewood Cliffs, NJ, 1991.\n[Biddle 2003] P. Biddle, P. England, M. Peinado, B. Willman, “The Darknet and the Future\nof Content Distribution,” 2002 ACM Workshop on Digital Rights Management, (Nov. 2002,\nWashington, D.C.) http://crypto.stanford.edu/DRM2002/darknet5.doc\n[Biersack 1992] E. W. Biersack, “Performance evaluation of forward error correction in\nATM networks,” Proc. 1999 ACM SIGCOMM (Baltimore, MD, Aug. 1992), pp. 248–257.\n[BIND 2012] Internet Software Consortium page on BIND, http://www.isc.org/bind.html\n[Bisdikian 2001] C. Bisdikian, “An Overview of the Bluetooth Wireless Technology,” IEEE\nCommunications Magazine, No. 12 (Dec. 2001), pp. 86–94.\n[Bishop 2003] M. Bishop, Computer Security: Art and Science, Boston: Addison Wesley,\nBoston MA, 2003.\n[Black 1995] U. Black, ATM Volume I: Foundation for Broadband Networks, Prentice Hall,\n1995.\n[Black 1997] U. Black, ATM Volume II: Signaling in Broadband Networks, Prentice Hall,\n1997.\n[Blumenthal 2001] M. Blumenthal, D. Clark, “Rethinking the Design of the Internet: the\nEnd-to-end Arguments vs. the Brave New World,” ACM Transactions on Internet\nTechnology, Vol. 1, No. 1 (Aug. 2001), pp. 70–109.\n[Bochman 1984] G. V. Bochmann, C. A. Sunshine, “Formal methods in communication\nprotocol design,” IEEE Transactions on Communications, Vol. 28, No. 4 (Apr. 1980) \npp. 624–631.\n[Bolot 1994] J-C. Bolot, T. Turletti, “A rate control scheme for packet video in the Internet,”\nProc. 1994 IEEE INFOCOM, pp. 1216–1223.\n[Bolot 1996] J-C. Bolot, A. Vega-Garcia, “Control Mechanisms for Packet Audio in the\nInternet,” Proc. 1996 IEEE INFOCOM, pp. 232–239.\n[Bradner 1996] S. Bradner, A. Mankin, IPng: Internet Protocol Next Generation, Addison-\nWesley, Reading, MA, 1996.\n[Brakmo 1995] L. Brakmo, L. Peterson, “TCP Vegas: End to End Congestion Avoidance on\na Global Internet,” IEEE Journal of Selected Areas in Communications, Vol. 13, No. 8 \n(Oct. 1995), pp. 1465–1480.\n[Breslau 2000] L. Breslau, E. Knightly, S. Shenker, I. Stoica, H. Zhang, “Endpoint\nAdmission Control: Architectural Issues and Performance,” Proc. 2000 ACM SIGCOMM\n(Stockholm, Sweden, Aug. 2000).\n[Bryant 1988] B. Bryant, “Designing an Authentication System: A Dialogue in Four\nScenes,” http://web.mit.edu/kerberos/www/dialogue.html\n[Bush 1945] V. Bush, “As We May Think,” The Atlantic Monthly, July 1945. http://www\n.theatlantic.com/unbound/flashbks/computer/bushf.htm\n[Byers 1998] J. Byers, M. Luby, M. Mitzenmacher, A. Rege, “A digital fountain approach to\nreliable distribution of bulk data,” Proc. 1998 ACM SIGCOMM (Vancouver, Canada, Aug. 1998),\npp. 56–67.\n[Cablelabs 2012] CableLabs homepage, http://www.cablelabs.com\n[CacheLogic 2012] CacheLogic homepage, http://www.cachelogic.com\n792\nREFERENCES\n\n[Caesar 2005a] M. Caesar, D. Caldwell, N. Feamster, J. Rexford, A. Shaikh, J. van der\nMerwe, “Design and implementation of a Routing Control Platform,” Proc. Networked\nSystems Design and Implementation (May 2005). \n[Caesar 2005b] M. Caesar, J. Rexford, “BGP Routing Policies in ISP Networks,” IEEE\nNetwork Magazine, Vol. 19, No. 6 (Nov. 2005).\n[Casado 2009] M. Casado, M. Freedman, J. Pettit, J. Luo, N. Gude, N. McKeown, S.\nShenker, “Rethinking Enterprise Network Control,” IEEE/ACM Transactions on Networking\n(ToN), Vol. 17, No. 4 (Aug. 2009), pp. 1270–1283.\n[Caldwell 2012] C. Caldwell, “The Prime Pages,” http://www.utm.edu/research/primes/prove\n[Cardwell 2000] N. Cardwell, S. Savage, T. Anderson, “Modeling TCP Latency,” Proc.\n2000 IEEE INFOCOM (Tel-Aviv, Israel, Mar. 2000).\n[CASA 2012] Center for Collaborative Adaptive Sensing of the Atmosphere, http://www\n.casa.umass.edu\n[Casado 2007] M. Casado, M. Freedman, J. Pettit, J. Luo, N. McKeown, S. Shenker,\n“Ethane: Taking Control of the Enterprise,” Proc. 2007 ACM SIGCOMM (Kyoto, Japan,\nAug. 2007).\n[Casner 1992] S. Casner, S. Deering, “First IETF Internet Audiocast,” ACM SIGCOMM\nComputer Communications Review, Vol. 22, No. 3 (July 1992), pp. 92–97.\n[Ceiva 2012] Ceiva homepage, http://www.ceiva.com/\n[CENS 2012] Center for Embedded Network Sensing, http://www.cens.ucla.edu/\n[Cerf 1974] V. Cerf, R. Kahn, “A Protocol for Packet Network Interconnection,” IEEE\nTransactions on Communications Technology, Vol. COM-22, No. 5, pp. 627–641.\n[CERT 2001–09] CERT, “Advisory 2001–09: Statistical Weaknesses in TCP/IP Initial\nSequence Numbers,” http://www.cert.org/advisories/CA-2001-09.html\n[CERT 2003–04] CERT, “CERT Advisory CA-2003-04 MS-SQL Server Worm,” http://\nwww.cert.org/advisories/CA-2003-04.html\n[CERT 2012] CERT Coordination Center, http://www.cert.org/advisories\n[CERT Filtering 2012] CERT, “Packet Filtering for Firewall Systems,” http://www.cert.org/\ntech_tips/packet_filtering.html\n[Cert SYN 1996] CERT, “Advisory CA-96.21: TCP SYN Flooding and IP Spoofing\nAttacks,” http://www.cert.org/advisories/CA-1998-01.html\n[Chao 2001] H. J. Chao, C. Lam, E. Oki, Broadband Packet Switching Technologies—\nA Practical Guide to ATM Switches and IP Routers, John Wiley & Sons, 2001.\n[Chao 2011] C. Zhang, P. Dunghel, D. Wu, K. W. Ross, “Unraveling the BitTorrent\nEcosystem,” IEEE Transactions on Parallel and Distributed Systems, Vol. 22, No. 7 (July 2011).\n[Chen 2000] G. Chen, D. Kotz, “A Survey of Context-Aware Mobile Computing Research,”\nTechnical Report TR2000-381, Dept. of Computer Science, Dartmouth College, Nov. 2000.\nhttp://www.cs.dartmouth.edu/reports/TR2000-381.pdf\n[Chen 2006] K.-T. Chen, C.-Y. Huang, P. Huang, C.-L. Lei, “Quantifying Skype User\nSatisfaction,” Proc. 2006 ACM SIGCOMM (Pisa, Italy, Sept. 2006).\n[Chen 2010] K. Chen, C. Guo, H. Wu, J. Yuan, Z. Feng, Y. Chen, S. Lu, W. Wu, “Generic\nand Automatic Address Configuration for Data Center Networks,” Proc. 2010 ACM\nSIGCOMM.\nREFERENCES\n793\n\n[Chen 2011] Y. Chen, S. Jain, V. K. Adhikari, Z. Zhang, “Characterizing Roles of Front-End\nServers in End-to-End Performance of Dynamic Content Distribution,” Proc. 2011 ACM\nInternet Measurement Conference (Berlin, Germany, Nov. 2011).\n[Chenoweth 2010] T. Chenoweth, R. Minch, S. Tabor, “Wireless Insecurity: Examining\nUser Security Behavior on Public Networks,” Communications of the ACM, Vol. 53, No. 2\n(Feb. 2010), pp. 134–138.\n[Cheswick 2000] B. Cheswick, H. Burch, S. Branigan, “Mapping and Visualizing the\nInternet,” Proc. 2000 Usenix Conference (San Diego, CA, June 2000).\n[Chiu 1989] D. Chiu, R. Jain, “Analysis of the Increase and Decrease Algorithms for Congestion\nAvoidance in Computer Networks,” Computer Networks and ISDN Systems, Vol. 17, No. 1,\npp. 1–14. http://www.cs.wustl.edu/~jain/papers/cong_av.htm\n[Christiansen 2001] M. Christiansen, K. Jeffay, D. Ott, F. D. Smith, “Tuning Red for Web\nTraffic,” IEEE/ACM Transactions on Networking, Vol. 9, No. 3 (June 2001), pp. 249–264.\n[Chu 2002] Y. Chu, S. Rao, S. Seshan, H Zhang, “A Case for End System Multicast,” IEEE\nJ. Selected Areas in Communications, Vol 20, No. 8 (Oct. 2002), pp. 1456–1471.\n[Chuang 2005] S. Chuang, S. Iyer, N. McKeown, “Practical Algorithms for Performance\nGuarantees in Buffered Crossbars,” Proc. 2005 IEEE INFOCOM.\n[Cicconetti 2006] C. Cicconetti, L. Lenzini, A. Mingozi, K. Eklund, “Quality of Service\nSupport in 802.16 Networks,” IEEE Network Magazine (Mar./Apr. 2006), pp. 50–55.\n[Cisco 12000 2012] Cisco Systems Inc., “Cisco XR 12000 Series and Cisco 12000 Series\nRouters,” http://www.cisco.com/en/US/products/ps6342/index.html\n[Cisco 8500 2012] Cisco Systems Inc., “Catalyst 8500 Campus Switch Router Architecture,”\nhttp://www.cisco.com/univercd/cc/td/doc/product/l3sw/8540/rel_12_0/w5_6f/softcnfg/\n1cfg8500.pdf\n[Cisco 2011] Cisco Visual Networking Index: Forecast and Methodology, 2010–2015, White\nPaper, 2011.\n[Cisco 2012] Cisco 2012, Data Centers, http://www.cisco.com/go/dce\n[Cisco NAT 2012] Cisco Systems Inc., “How NAT Works,” http://www.cisco.com/en/US/\ntech/tk648/tk361/technologies_tech_note09186a0080094831.shtml\n[Cisco QoS 2012] Cisco Systems Inc., “Advanced QoS Services for the Intelligent Internet,”\nhttp://www.cisco.com/warp/public/cc/pd/iosw/ioft/ioqo/tech/qos_wp.htm\n[Cisco Queue 2012] Cisco Systems Inc., “Congestion Management Overview,” http://www\n.cisco.com/en/US/docs/ios/12_2/qos/configuration/guide/qcfconmg.html\n[Cisco Switches 2012] Cisco Systems Inc, “Multiservice Switches,” http://www.cisco.com/\nwarp/public/cc/pd/si/index.shtml\n[Cisco SYN 2012] Cisco Systems Inc., “Defining Strategies to Protect Against TCP SYN\nDenial of Service Attacks,” http://www.cisco.com/en/US/tech/tk828/technologies_tech_\nnote09186a00800f67d5.shtml\n[Cisco VNI 2011] Cisco, “Visual Networking Index,” http://www.cisco.com/web/solutions/\nsp/vni/vni_forecast_highlights/index.html\n[Clark 1988] D. Clark, “The Design Philosophy of the DARPA Internet Protocols,” Proc.\n1988 ACM SIGCOMM (Stanford, CA, Aug. 1988).\n[Clarke 2002] I. Clarke, T. W. Hong, S. G. Miller, O. Sandberg, B. Wiley, “Protecting\nFree Expression Online with Freenet,” IEEE Internet Computing (Jan.–Feb. 2002), \npp. 40–49.\n794\nREFERENCES\n\n[Cohen 1977] D. Cohen, “Issues in Transnet Packetized Voice Communication,” Proc. Fifth\nData Communications Symposium (Snowbird, UT, Sept. 1977), pp. 6–13.\n[Cohen 2003] B. Cohen, “Incentives to Build Robustness in BitTorrent,” First Workshop on\nthe Economics of Peer-to-Peer Systems (Berkeley, CA, June 2003).\n[Cookie Central 2012] Cookie Central homepage, http://www.cookiecentral.com/\nn_cookie_faq.htm\n[CoolStreaming 2005] X. Zhang, J. Liu, J., B. Li, and T.-S. P. Yum, “CoolStreamingDONet/:\nA Data-driven Overlay Network for Peer-to-Peer Live Media Streaming,” Proc. 2005 IEEE\nINFOCOM (Miami, FL, Mar. 2005).\n[Cormen 2001] T. H. Cormen, Introduction to Algorithms, 2nd Ed., MIT Press, Cambridge,\nMA, 2001.\n[Crow 1997] B. Crow, I. Widjaja, J. Kim, P. Sakai, “IEEE 802.11 Wireless Local Area\nNetworks,” IEEE Communications Magazine (Sept. 1997), pp. 116–126.\n[Crowcroft 1995] J. Crowcroft, Z. Wang, A. Smith, J. Adams, “A Comparison of the IETF\nand ATM Service Models,” IEEE Communications Magazine (Nov./Dec. 1995), pp. 12–16.\n[Crowcroft 1999] J. Crowcroft, M. Handley, I. Wakeman, Internetworking Multimedia,\nMorgan-Kaufman, San Francisco, 1999.\n[Curtis 2011] A. R. Curtis, J. C. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma, S. Banerjee,\n“DevoFlow: Scaling Flow Management for High-Performance Networks,” Proc. 2011 ACM\nSIGCOMM.\n[Cusumano 1998] M. A. Cusumano, D. B. Yoffie, Competing on Internet Time: Lessons\nfrom Netscape and its Battle with Microsoft, Free Press, New York, NY, 1998.\n[Dahlman 1998] E. Dahlman, B. Gudmundson, M. Nilsson, J. Sköld, “UMTS/IMT-2000 Based\non Wideband CDMA,” IEEE Communications Magazine (Sept. 1998), pp. 70–80.\n[Daigle 1991] J. N. Daigle, Queuing Theory for Telecommunications, Addison-Wesley,\nReading, MA, 1991.\n[Dalal 1978] Y. Dalal, R. Metcalfe, “Reverse Path Forwarding of Broadcast Packets,”\nCommunications of the ACM, Vol. 21, No. 12 (Dec. 1978), pp. 1040–1048.\n[Davie 2000] B. Davie and Y. Rekhter, MPLS: Technology and Applications, Morgan\nKaufmann Series in Networking, 2000.\n[Davies 2005] G. Davies, F. Kelly, “Network Dimensioning, Service Costing, and Pricing in\na Packet-Switched Environment,” Telecommunications Policy, Vol. 28, No. 4, pp. 391–412.\n[DEC 1990] Digital Equipment Corporation, “In Memoriam: J. C. R. Licklider 1915–1990,”\nSRC Research Report 61, Aug. 1990. http://www.memex.org/licklider.pdf\n[DeClercq 2002] J. DeClercq, O. Paridaens, “Scalability Implications of Virtual Private\nNetworks,” IEEE Communications Magazine, Vol. 40, No. 5 (May 2002), pp. 151–157.\n[Demers 1990] A. Demers, S. Keshav, S. Shenker, “Analysis and Simulation of a Fair Queuing\nAlgorithm,” Internetworking: Research and Experience, Vol. 1, No. 1 (1990), pp. 3–26.\n[Denning 1997] D. Denning (Editor), P. Denning (Preface), Internet Besieged: Countering\nCyberspace Scofflaws, Addison-Wesley, Reading, MA, 1997.\n[dhc 2012] IETF Dynamic Host Configuration working group homepage, http://www.ietf.\norg/html.charters/dhc-charter.html\n[Dhungel 2012] P. Dhungel, K. W. Ross, M. Steiner., Y. Tian, X. Hei, “Xunlei: Peer-Assisted\nDownload Acceleration on a Massive Scale,” Passive and Active Measurement Conference\n(PAM) 2012, Vienna, 2012.\nREFERENCES\n795\n\n[Diffie 1976] W. Diffie, M. E. Hellman, “New Directions in Cryptography,” IEEE\nTransactions on Information Theory, Vol IT-22 (1976), pp. 644–654.\n[Diggavi 2004] S. N. Diggavi, N. Al-Dhahir, A. Stamoulis, R. Calderbank, “Great\nExpectations: The Value of Spatial Diversity in Wireless Networks,” Proceedings of the\nIEEE, Vol. 92, No. 2 (Feb. 2004).\n[Dilley 2002] J. Dilley, B. Maggs, J. Parikh, H. Prokop, R. Sitaraman, B. Weihl, “Globally\nDistributed Content Delivert,” IEEE Internet Computing (Sept.–Oct. 2002).\n[Ding 2011] Y. Ding, Y. Du, Y. Hu, Z. Liu, L. Wang, K. W. Ross, A. Ghose, “Broadcast\nYourself: Understanding YouTube Uploaders,” Proc. 2011 ACM Internet Measurement\nConference (Berlin).\n[Diot 2000] C. Diot, B. N. Levine, B. Lyles, H. Kassem, D. Balensiefen, “Deployment\nIssues for the IP Multicast Service and Architecture,” IEEE Network, Vol. 14, No. 1\n(Jan./Feb. 2000) pp. 78–88.\n[Dischinger 2007] M. Dischinger, A. Haeberlen, K. Gummadi, S. Saroiu, “Characterizing\nresidential broadband networks,” Proc. 2007 ACM Internet Measurement Conference, pp. 24–26.\n[Dmitiropoulos 2007] X. Dmitiropoulos, D. Krioukov, M. Fomenkov, B. Huffaker, Y. Hyun,\nKC Claffy, G. Riley, “AS Relationships: Inference and Validation,” ACM Computer\nCommunication Review (Jan. 2007).\n[DOCSIS 2004] Data-over-cable service interface specifications: Radio-frequency interface\nspecification. ITU-T J.112, 2004.\n[DOCSIS 2011] Data-Over-Cable Service Interface Specifications, DOCSIS 3.0: MAC and\nUpper Layer Protocols Interface Specification, CM-SP-MULPIv3.0-I16-110623, 2011.\n[Dodge 2012] M. Dodge, “An Atlas of Cyberspaces,” http://www.cybergeography.org/atlas/\nisp_maps.html\n[Donahoo 2001] M. Donahoo, K. Calvert, TCP/IP Sockets in C: Practical Guide for\nProgrammers, Morgan Kaufman, 2001.\n[Doucer 2002] J. R. Douceur, “The Sybil Attack,” First International Workshop on Peer-to-\nPeer Systems (IPTPS ’02) (Cambridge, MA, Mar. 2002).\n[DSL 2012] DSL Forum homepage, http://www.dslforum.org/\n[Dhunghel 2008] P. Dhungel, D. Wu, B. Schonhorst, K.W. Ross, “A Measurement Study of\nAttacks on BitTorrent Leechers,” 7th International Workshop on Peer-to-Peer Systems\n(IPTPS 2008) (Tampa Bay, FL, Feb. 2008).\n[Droms 2002] R. Droms, T. Lemon, The DHCP Handbook (2nd Edition), SAMS Publishing,\n2002.\n[Edney 2003] J. Edney and W. A. Arbaugh, Real 802.11 Security: Wi-Fi Protected Access\nand 802.11i, Addison-Wesley Professional, 2003.\n[Edwards 2011] W. K. Edwards, R. Grinter, R. Mahajan, D. Wetherall, “Advancing the State\nof Home Networking,” Communications of the ACM, Vol. 54, No. 6 (June 2011), pp. 62–71.\n[Eklund 2002] K. Eklund, R. Marks, K. Stanswood, S. Wang, “IEEE Standard 802.16: A\nTechnical Overview of the Wireless MAN Air Interface for Broadband Wireless Access,”\nIEEE Communications Magazine (June 2002), pp. 98–107.\n[Ellis 1987] H. Ellis, “The Story of Non-Secret Encryption,” http://jya.com/ellisdoc.htm\n[Ericsson 2011] Ericsson, “LTE—An Introduction,” www.ericsson.com/res/docs/2011/\nlte_an_introduction.pdf\n796\nREFERENCES\n\n[Ericsson 2012] Ericsson, “The Evolution of Edge,” http://www.ericsson.com/technology/\nwhitepapers/broadband/evolution_of_EDGE.shtml\n[Estrin 1997] D. Estrin, M. Handley, A. Helmy, P. Huang, D. Thaler, “A Dynamic Bootstrap\nMechanism for Rendezvous-Based Multicast Routing,” Proc. 1998 IEEE INFOCOM\n(New York, NY, Apr. 1998).\n[Falkner 2007] J. Falkner, M. Piatek, J.P. John, A. Krishnamurthy, T. Anderson, “Profiling a\nMillion Sser DHT,” Proc. 2007 ACM Internet Measurement Conference.\n[Faloutsos 1999] C. Faloutsos, M. Faloutsos, P. Faloutsos, “What Does the Internet Look\nLike? Empirical Laws of the Internet Topology,” Proc. 1999 ACM SIGCOMM (Boston, MA,\nAug. 1999).\n[Farrington 2010] N. Farrington, G. Porter, S. Radhakrishnan, H. Bazzaz, V. Subramanya,\nY. Fainman, G. Papen, A. Vahdat, “Helios: A Hybrid Electrical/Optical Switch Architecture\nfor Modular Data Centers,” Proc. 2010 ACM SIGCOMM.\n[Feamster 2004] N. Feamster, J. Winick, J. Rexford, “A Model for BGP Routing for\nNetwork Engineering,” Proc. 2004 ACM SIGMETRICS (New York, NY, June 2004).\n[Feamster 2005] N. Feamste, H. Balakrishnan, “Detecting BGP Configuration Faults with\nStatic Analysis,” NSDI (May 2005).\n[Feldman 2005] M. Feldman J. Chuang, “Overcoming Free-Riding Behavior in Peer-to-peer\nSystems,” ACM SIGecom Exchanges (July 2005).\n[Feldmeier 1995] D. Feldmeier, “Fast Software Implementation of Error Detection Codes,”\nIEEE/ACM Transactions on Networking, Vol. 3, No. 6 (Dec. 1995), pp. 640–652.\n[FIPS 1995] Federal Information Processing Standard, “Secure Hash Standard,” FIPS\nPublication 180-1. http://www.itl.nist.gov/fipspubs/fip180-1.htm\n[Floyd 1999] S. Floyd, K. Fall, “Promoting the Use of End-to-End Congestion Control \nin the Internet,” IEEE/ACM Transactions on Networking, Vol. 6, No. 5 (Oct. 1998), \npp. 458–472.\n[Floyd 2000] S. Floyd, M. Handley, J. Padhye, J. Widmer, “Equation-Based Congestion Control\nfor Unicast Applications,” Proc. 2000 ACM SIGCOMM (Stockholm, Sweden, Aug. 2000).\n[Floyd 2001] S. Floyd, “A Report on Some Recent Developments in TCP Congestion\nControl,” IEEE Communications Magazine (Apr. 2001).\n[Floyd 2012] S. Floyd, “References on RED (Random Early Detection) Queue\nManagement,” http://www.icir.org/floyd/red.html\n[Floyd Synchronization 1994] S. Floyd, V. Jacobson, “Synchronization of Periodic\nRouting Messages,” IEEE/ACM Transactions on Networking, Vol. 2, No. 2 (Apr. 1997)\npp. 122–136.\n[Floyd TCP 1994] S. Floyd, “TCP and Explicit Congestion Notification,” ACM SIGCOMM\nComputer Communications Review, Vol. 24, No. 5 (Oct. 1994), pp. 10–23.\n[Fluhrer 2001] S. Fluhrer, I. Mantin, A. Shamir, “Weaknesses in the Key Scheduling\nAlgorithm of RC4,” Eighth Annual Workshop on Selected Areas in Cryptography, (Toronto,\nCanada, Aug. 2002).\n[Fortz 2000] B. Fortz, M. Thorup, “Internet Traffic Engineering by Optimizing OSPF\nWeights,” Proc. 2000 IEEE INFOCOM (Tel Aviv, Israel, Apr. 2000).\n[Fortz 2002] B. Fortz, J. Rexford, M. Thorup, “Traffic Engineering with Traditional IP\nRouting Protocols,” IEEE Communication Magazine (Oct. 2002).\nREFERENCES\n797\n\n[Fraleigh 2003] C. Fraleigh, F. Tobagi, C. Diot, “Provisioning IP Backbone Networks to Support\nLatency Sensitive Traffic,” Proc. 2003 IEEE INFOCOM (San Francisco, CA, Mar. 2003).\n[Freedman 2004] M. J. Freedman, E. Freudenthal, D. Mazires, “Democratizing Content\nPublication with Coral,” USENIX NSDI, 2004.\n[Friedman 1999] T. Friedman, D. Towsley “Multicast Session Membership Size\nEstimation,” Proc. 1999 IEEE INFOCOM (New York, NY, Mar. 1999).\n[Frost 1994] J. Frost, “BSD Sockets: A Quick and Dirty Primer,” http://world.std.com/~jimf/\npapers/sockets/sockets.html\n[FTTH Council 2011a] FTTH Council, “NORTH AMERICAN FTTH STATUS—MARCH\n31, 2011” (March 2011), www.ftthcouncil.org\n[FTTH Council 2011b] FTTH Council, “2011 Broadband Consumer Research” (June 2011), \nwww.ftthcouncil.org\n[Gallagher 1983] R. G. Gallagher, P. A. Humblet, P. M. Spira, “A Distributed Algorithm for\nMinimum Weight-Spanning Trees,” ACM Trans. on Programming Languages and Systems,\nVol. 1, No. 5 (Jan. 1983), pp. 66–77.\n[Gao 2001] L. Gao, J. Rexford, “Stable Internet Routing Without Global Coordination,”\nIEEE/ACM Transactions on Networking, Vol. 9, No. 6 (Dec. 2001), pp. 681–692.\n[Garces-Erce 2003] L. Garces-Erce, K. W. Ross, E. Biersack, P. Felber, G. Urvoy-Keller,\n“TOPLUS: Topology Centric Lookup Service,” Fifth Int. Workshop on Networked Group\nCommunications (NGC 2003) (Munich, Sept. 2003) http://cis.poly.edu/~ross/papers/TOPLUS.pdf\n[Gartner 2003] F. C. Gartner, “A Survey of Self-Stabilizing Spanning-Tree Construction\nAlgorithms,” Technical Report IC/2003/38, Swiss Federal Institute of Technology (EPFL),\nSchool of Computer and Communication Sciences, June 10, 2003. http://ic2.epfl.ch/\npublications/documents/IC_TECH_REPORT_200338.pdf\n[Gauthier 1999] L. Gauthier, C. Diot, and J. Kurose, “End-to-end Transmission Control\nMechanisms for Multiparty Interactive Applications on the Internet,” Proc. 1999 IEEE\nINFOCOM (New York, NY, Apr. 1999).\n[Girard 1990] A. Girard, Routing and Dimensioning in Circuit-Switched Networks,\nAddison-Wesley, Reading, MA, 1990.\n[Glitho 1998] R. Glitho, “Contrasting OSI Systems Management to SNMP and TMN,”\nJournal of Network and Systems Management, Vol. 6, No. 2 (June 1998), pp. 113–131.\n[Gnutella 2009] “The Gnutella Protocol Specification, v0.4” http://www9.limewire.com/\ndeveloper/gnutella_protocol_0.4.pdf\n[Goodman 1997] David J. Goodman, Wireless Personal Communications Systems, Prentice-\nHall, 1997.\n[Google Locations 2012] Google data centers. http://www.google.com/corporate/\ndatacenter/locations.html\n[Goralski 1999] W. Goralski, Frame Relay for High-Speed Networks, John Wiley, New\nYork, 1999.\n[Goralski 2001] W. Goralski, Optical Networking and WDM, Osborne/McGraw-Hill,\nBerkeley, CA, 2001.\n[Greenberg 2009a] A. Greenberg, J. Hamilton, D. Maltz, P. Patel, “The Cost of a Cloud:\nResearch Problems in Data Center Networks,” ACM Computer Communications Review\n(Jan. 2009).\n798\nREFERENCES\n\n[Greenberg 2009b] A. Greenberg, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. Maltz, \nP. Patel, S. Sengupta, “VL2: A Scalable and Flexible Data Center Network,” Proc. 2009\nACM SIGCOMM.\n[Greenberg 2011] A. Greenberg, J. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D.\nMaltz, P. Patel, S. Sengupta, “VL2: A Scalable and Flexible Data Center Network,”\nCommunications of the ACM, Vol. 54, No. 3 (Mar. 2011), pp. 95–104.\n[Griffin 2012] T. Griffin, “Interdomain Routing Links,” http://www.cl.cam.ac.uk/~tgg22/\ninterdomain/\n[Guha 2006] S. Guha, N. Daswani, R. Jain, “An Experimental Study of the Skype Peer-to-\nPeer VoIP System,” Proc. Fifth Int. Workshop on P2P Systems (Santa Barbara, CA, 2006).\n[Guo 2005] L. Guo, S. Chen, Z. Xiao, E. Tan, X. Ding, X. Zhang, “Measurement, Analysis,\nand Modeling of BitTorrent-Like Systems,” Proc. 2005 ACM Internet Measurement\nConference.\n[Guo 2009] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, S. Lu,\n“BCube: A High Performance, Server-centric Network Architecture for Modular Data\nCenters,” Proc. 2009 ACM SIGCOMM.\n[Gupta 2001] P. Gupta, N. McKeown, “Algorithms for Packet Classification,” IEEE\nNetwork Magazine, Vol. 15, No. 2 (Mar./Apr. 2001), pp. 24–32.\n[Ha 2008] Ha, S., Rhee, I., L. Xu, “CUBIC: A New TCP-Friendly High-Speed TCP Variant,”\nACM SIGOPS Operating System Review, 2008.\n[Halabi 2000] S. Halabi, Internet Routing Architectures, 2nd Ed., Cisco Press, 2000.\n[Halperin 2008] D. Halperin, T. Heydt-Benjamin, B. Ransford, S. Clark, B. Defend, W.\nMorgan, K. Fu, T. Kohno, W. Maisel, “Pacemakers and implantable cardiac defibrillators:\nSoftware radio attacks and zero-power defenses,” Proc. 29th Annual IEEE Symposium on\nSecurity and Privacy (May 2008).\n[Halperin 2011] D. Halperin, S. Kandula, J. Padhye, P. Bahl, D. Wetherall, “Augmenting\nData Center Networks with Multi-Gigabit Wireless Links,” Proc. 2011 ACM SIGCOMM.\n[Hanabali 2005] A. A. Hanbali, E. Altman, P. Nain, “A Survey of TCP over Ad Hoc\nNetworks,” IEEE Commun. Surveys and Tutorials, Vol. 7, No. 3 (2005), pp. 22–36.\n[Hei 2007] X. Hei, C. Liang, J. Liang, Y. Liu, K. W. Ross, “A Measurement Study of a\nLarge-scale P2P IPTV System,” IEEE Trans. on Multimedia (Dec. 2007).\n[Heidemann 1997] J. Heidemann, K. Obraczka, J. Touch, “Modeling the Performance of\nHTTP over Several Transport Protocols,” IEEE/ACM Transactions on Networking, Vol. 5,\nNo. 5 (Oct. 1997), pp. 616–630.\n[Held 2001] G. Held, Data Over Wireless Networks: Bluetooth, WAP, and Wireless LANs,\nMcGraw-Hill, 2001.\n[Hersent 2000] O. Hersent, D. Gurle, J-P. Petit, IP Telephony: Packet-Based Multimedia\nCommunication Systems, Pearson Education Limited, Edinburgh, 2000.\n[Holland 2001] G. Holland, N. Vaidya, V. Bahl, “A Rate-Adaptive MAC Protocol for Multi-\nHop Wireless Networks,” Proc. 2001 ACM Int. Conference of Mobile Computing and\nNetworking (Mobicom01) (Rome, Italy, July 2001).\n[Hollot 2002] C.V. Hollot, V. Misra, D. Towsley, W. Gong, “Analysis and design of\ncontrollers for AQM routers supporting TCP flows,” IEEE Transactions on Automatic\nControl, Vol. 47, No. 6 (June 2002), pp. 945–959.\nREFERENCES\n799\n\n[Huang 2002] C. Haung, V. Sharma, K. Owens, V. Makam, “Building Reliable MPLS\nNetworks Using a Path Protection Mechanism,” IEEE Communications Magazine, Vol. 40,\nNo. 3 (Mar. 2002), pp. 156–162.\n[Huang 2005] Y. Huang, R. Guerin, “Does Over-Provisioning Become More or Less\nEfficient as Networks Grow Larger?,” Proc. IEEE Int. Conf. Network Protocols (ICNP)\n(Boston MA, November 2005).\n[Huang 2007] C. Huang, Jin Li, K.W. Ross, “Can Internet VoD Be Profitable?,” Proc 2007\nACM SIGCOMM (Kyoto, Aug. 2007).\n[Huang 2008] C. Huang, J. Li, A. Wang, K. W. Ross, “Understanding Hybrid CDN-P2P:\nWhy Limelight Needs its Own Red Swoosh,” Proc. 2008 NOSSDAV, Braunschweig,\nGermany.\n[Huang 2010] C. Huang, N. Holt, Y. A. Wang, A. Greenberg, J. Li, K. W. Ross, “A DNS\nReflection Method for Global Traffic Management,” Proc. 2010 USENIX, Boston.\n[Huitema 1998] C. Huitema, IPv6: The New Internet Protocol, 2nd Ed., Prentice Hall,\nEnglewood Cliffs, NJ, 1998.\n[Huston 1999a] G. Huston, “Interconnection, Peering, and Settlements—Part I,” The\nInternet Protocol Journal, Vol. 2, No. 1 (Mar. 1999).\n[Huston 2004] G. Huston, “NAT Anatomy: A Look Inside Network Address Translators,”\nThe Internet Protocol Journal, Vol. 7, No. 3 (Sept. 2004).\n[Huston 2008a] G. Huston, “Confronting IPv4 Address Exhaustion,” http://www.potaroo\n.net/ispcol/2008-10/v4depletion.html\n[Huston 2008b] G. Huston, G. Michaelson, “IPv6 Deployment: Just where are we?” http://\nwww.potaroo.net/ispcol/2008-04/ipv6.html\n[Huston 2011a] G. Huston, “A Rough Guide to Address Exhaustion,” The Internet Protocol\nJournal, Vol. 14, No. 1 (Mar. 2011).\n[Huston 2011b] G. Huston, “Transitioning Protocols,” The Internet Protocol Journal, Vol.\n14, No. 1 (Mar. 2011).\n[IAB 2012] Internet Architecture Board homepage, http://www.iab.org/\n[IANA 2012a] Internet Assigned Number Authority homepage, http://www.iana.org/\n[IANA 2012b] Internet Assigned Number Authority, “Private Enterprise Numbers” http://\nwww.iana.org/assignments/enterprise-numbers\n[IANA Protocol Numbers 2012] Internet Assigned Numbers Authority, Protocol Numbers,\nhttp://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml\n[IANA TLD 2012] IANA Root Zone Database, http://www.iana.org/domains/root/db/\n[ICANN 2012] The Internet Corporation for Assigned Names and Numbers homepage,\nhttp://www.icann.org\n[IEC Optical 2012] IEC Online Education, “Optical Access,” http://www.iec.org/online/\ntutorials/opt_acc/\n[IEEE 802 2012] IEEE 802 LAN/MAN Standards Committee homepage, http://www\n.ieee802.org/\n[IEEE 802.11 1999] IEEE 802.11, “1999 Edition (ISO/IEC 8802-11: 1999) IEEE Standards\nfor Information Technology—Telecommunications and Information Exchange Between\nSystems—Local and Metropolitan Area Network—Specific Requirements—Part 11:\n800\nREFERENCES\n\nWireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specification,”\nhttp://standards.ieee.org/getieee802/download/802.11-1999.pdf\n[IEEE 802.11n 2012] IEEE, “IEEE P802.11—Task Group N—Meeting Update: Status of\n802.11n,” http://grouper.ieee.org/groups/802/11/Reports/tgn_update.htm\n[IEEE 802.15 2012] IEEE 802.15 Working Group for WPAN homepage, http://grouper.ieee.\norg/groups/802/15/.\n[IEEE 802.15.4 2012] IEEE 802.15 WPAN Task Group 4, http://www.ieee802.org/15/\npub/TG4.html\n[IEEE 802.16d 2004] IEEE, “IEEE Standard for Local and Metropolitan Area Networks,\nPart 16: Air Interface for Fixed Broadband Wireless Access Systems,” http://standards.ieee.\norg/getieee802/download/802.16-2004.pdf\n[IEEE 802.16e 2005] IEEE, “IEEE Standard for Local and Metropolitan Area Networks, Part 16:\nAir Interface for Fixed and Mobile Broadband Wireless Access Systems, Amendment 2: Physical\nand Medium Access Control Layers for Combined Fixed and Mobile Operation in Licensed\nBands and Corrigendum 1,” http://standards.ieee.org/getieee802/download/802 .16e-2005.pdf\n[IEEE 802.1q 2005] IEEE, “IEEE Standard for Local and Metropolitan Area Networks:\nVirtual Bridged Local Area Networks,” http://standards.ieee.org/getieee802/download/802\n.1Q-2005.pdf\n[IEEE 802.1X] IEEE Std 802.1X-2001 Port-Based Network Access Control, http://\nstandards.ieee.org/reading/ieee/std_public/description/lanman/802.1x-2001_desc.html\n[IEEE 802.3 2012] IEEE, “IEEE 802.3 CSMA/CD (Ethernet),” http://grouper.ieee.org/\ngroups/802/3/\n[IEEE 802.5 2012] IEEE, IEEE 802.5 homepage, http://www.ieee802.org/5/www8025org/\n[IETF 2012] Internet Engineering Task Force homepage, http://www.ietf.org\n[Ihm 2011] S. Ihm, V. S. Pai, “Towards Understanding Modern Web Traffic,” Proc. 2011\nACM Internet Measurement Conference (Berlin).\n[IMAP 2012] The IMAP Connection, http://www.imap.org/\n[Intel 2012] Intel Corp, “Intel® 82544 Gigabit Ethernet Controller,” http://www.intel.com/\ndesign/network/products/lan/docs/82544_docs.htm\n[Intel WiMax 2012] Intel Corp., “WiMax Technology,” http://www.intel.com/technology/\nwimax/index.htm\n[Internet2 Multicast 2012] Internet2 Multicast Working Group homepage, http://www\n.internet2.edu/multicast/\n[IPv6 2012] IPv6.com homepage, http://www.ipv6.com/\n[ISC 2012] Internet Systems Consortium homepage, http://www.isc.org\n[ISI 1979] Information Sciences Institute, “DoD Standard Internet Protocol,” Internet\nEngineering Note 123 (Dec. 1979), http://www.isi.edu/in-notes/ien/ien123.txt\n[ISO 2012] International Organization for Standardization homepage, International\nOrganization for Standardization, http://www.iso.org/\n[ISO X.680 2002] International Organization for Standardization, “X.680: ITU-T\nRecommendation X.680 (2002) Information Technology—Abstract Syntax Notation One\n(ASN.1): Specification of Basic Notation.” http://www.itu.int/ITU-T/studygroups/com17/\nlanguages/X.680-0207.pdf\nREFERENCES\n801\n\n[ITU 1999] Asymmetric Digital Subscriber Line (ADSL) Transceivers. ITU-T G.992.1, 1999.\n[ITU 2003] Asymmetric Digital Subscriber Line (ADSL) Transceivers—Extended\nBandwidth ADSL2 (ADSL2Plus). ITU-T G.992.5, 2003.\n[ITU 2005a] International Telecommunication Union, “ITU-T X.509, The Directory: Public-\nkey and attribute certificate frameworks” (August 2005).\n[ITU 2005b] International Telecommunication Union, The Internet of Things, 2005, http://\nwww.itu.int/osg/spu/publications/internetofthings/InternetofThings_summary.pdf\n[ITU 2012] The ITU homepage, http://www.itu.int/\n[ITU Statistics 2012] International Telecommunications Union, “ICT Statistics,” http://\nwww.itu.int/ITU-D/icteye/Reports.aspx\n[ITU 2011] ITU, “Measuring the Information Society, 2011,” http://www.itu.int/ITU-\nD/ict/publications/idi/2011/index.html\n[ITU 2011] ITU, “The World in 2010: ICT Facts and Figures,” http://www.itu.int/ITU-\nD/ict/material/Telecom09_flyer.pdf\n[ITU-T Q.2931 1995] International Telecommunication Union, “Recommendation Q.2931\n(02/95) - Broadband Integrated Services Digital Network (B-ISDN)—Digital subscriber\nsignalling system no. 2 (DSS 2)—User-network interface (UNI)—Layer 3 specification for\nbasic call/connection control.”\n[Iyer 2002] S. Iyer, R. Zhang, N. McKeown, “Routers with a Single Stage of Buffering,”\nProc. 2002 ACM SIGCOMM (Pittsburgh, PA, Aug. 2002).\n[Iyer 2008] S. Iyer, R. R. Kompella, N. McKeown, “Designing Packet Buffers for Router\nLine Cards,” IEEE Transactions on Networking, Vol. 16, No. 3 (June 2008), pp. 705–717.\n[Jacobson 1988] V. Jacobson, “Congestion Avoidance and Control,” Proc. 1988 ACM\nSIGCOMM (Stanford, CA, Aug. 1988), pp. 314–329.\n[Jain 1986] R. Jain, “A timeout-based congestion control scheme for window flow-controlled\nnetworks,” IEEE Journal on Selected Areas in Communications SAC-4, 7 (Oct. 1986).\n[Jain 1989] R. Jain, “A Delay-Based Approach for Congestion Avoidance in Interconnected\nHeterogeneous Computer Networks,” ACM SIGCOMM Computer Communications Review,\nVol. 19, No. 5 (1989), pp. 56–71.\n[Jain 1994] R. Jain, FDDI Handbook: High-Speed Networking Using Fiber and Other\nMedia, Addison-Wesley, Reading, MA, 1994.\n[Jain 1996] R. Jain. S. Kalyanaraman, S. Fahmy, R. Goyal, S. Kim, “Tutorial Paper on ABR\nSource Behavior,” ATM Forum/96-1270, Oct. 1996. http://www.cse.wustl.edu/~jain/atmf/ftp/\natm96-1270.pdf\n[Jaiswal 2003] S. Jaiswal, G. Iannaccone, C. Diot, J. Kurose, D. Towsley, “Measurement and\nClassification of Out-of-Sequence Packets in a Tier-1 IP backbone,” Proc. 2003 IEEE INFOCOM.\n[Ji 2003] P. Ji, Z. Ge, J. Kurose, D. Towsley, “A Comparison of Hard-State and Soft-State\nSignaling Protocols,” Proc. 2003 ACM SIGCOMM (Karlsruhe, Germany, Aug. 2003).\n[Jiang 2001] W. Jiang, J. Lennox, H. Schulzrinne, K. Singh, “Towards Junking the PBX:\nDeploying IP Telephony,” NOSSDAV’01 (Port Jefferson, NY, June 2001).\n[Jimenez 1997] D. Jimenez, “Outside Hackers Infiltrate MIT Network, Compromise Security,”\nThe Tech, Vol. 117, No 49 (Oct. 1997), p. 1, http://www-tech.mit.edu/V117/N49/hackers.49n.html\n[Jin 2004] C. Jin, D. X. We, S. Low, “FAST TCP: Motivation, architecture, algorithms,\nperformance,” Proc. 2004 IEEE INFOCOM (Hong Kong, March 2004).\n802\nREFERENCES\n\n[Kaaranen 2001] H. Kaaranen, S. Naghian, L. Laitinen, A. Ahtiainen, V. Niemi, Networks:\nArchitecture, Mobility and Services, New York: John Wiley & Sons, 2001.\n[Kahn 1967] D. Kahn, The Codebreakers: The Story of Secret Writing, The Macmillan\nCompany, 1967.\n[Kahn 1978] R. E. Kahn, S. Gronemeyer, J. Burchfiel, R. Kunzelman, “Advances in Packet\nRadio Technology,” Proc. 1978 IEEE INFOCOM, 66, 11 (Nov. 1978).\n[Kamerman 1997] A. Kamerman, L. Monteban, “WaveLAN-II: A High–Performance Wireless\nLAN for the Unlicensed Band,” Bell Labs Technical Journal (Summer 1997), pp. 118–133.\n[Kangasharju 2000] J. Kangasharju, K. W. Ross, J. W. Roberts, “Performance Evaluation of\nRedirection Schemes in Content Distribution Networks,” Proc. 5th Web Caching and\nContent Distribution Workshop (Lisbon, Portugal, May 2000).\n[Kar 2000] K. Kar, M. Kodialam, T. V. Lakshman, “Minimum Interference Routing of\nBandwidth Guaranteed Tunnels with MPLS Traffic Engineering Applications,” IEEE J.\nSelected Areas in Communications (Dec. 2000).\n[Karn 1987] P. Karn, C. Partridge, “Improving Round-Trip Time Estimates in Reliable\nTransport Protocols,” Proc. 1987 ACM SIGCOMM.\n[Karol 1987] M. Karol, M. Hluchyj, A. Morgan, “Input Versus Output Queuing on a Space-\nDivision Packet Switch,” IEEE Transactions on Communications, Vol. 35, No. 12 (Dec.\n1987), pp. 1347–1356.\n[Katabi 2002] D. Katabi, M. Handley, C. Rohrs, “Internet Congestion Control for Future\nHigh Bandwidth-Delay Product Environments,” Proc. 2002 ACM SIGCOMM (Pittsburgh,\nPA, Aug. 2002).\n[Katzela 1995] I. Katzela, M. Schwartz. “Schemes for Fault Identification in\nCommunication Networks,” IEEE/ACM Transactions on Networking, Vol. 3, No. 6 \n(Dec. 1995), pp. 753–764.\n[Kaufman 1995] C. Kaufman, R. Perlman, M. Speciner, Network Security, Private\nCommunication in a Public World, Prentice Hall, Englewood Cliffs, NJ, 1995.\n[Kelly 1998] F. P. Kelly, A. Maulloo, D. Tan, “Rate control for communication networks:\nShadow prices, proportional fairness and stability,” J. Operations Res. Soc., Vol. 49, No. 3\n(Mar. 1998), pp. 237–252.\n[Kelly 2003] T. Kelly, “Scalable TCP: improving performance in high speed wide area\nnetworks,” ACM SIGCOMM Computer Communications Review, Volume 33, No. 2 \n(Apr. 2003), pp 83–91.\n[Kilkki 1999] K. Kilkki, Differentiated Services for the Internet, Macmillan Technical\nPublishing, Indianapolis, IN, 1999.\n[Kim 2005] H. Kim, S. Rixner, V. Pai, “Network Interface Data Caching,” IEEE\nTransactions on Computers, Vol. 54, No. 11 (Nov. 2005), pp. 1394–1408.\n[Kim 2008] C. Kim, M. Caesar, J. Rexford, “Floodless in SEATTLE: A Scalable Ethernet\nArchitecture for Large Enterprises,” Proc. 2008 ACM SIGCOMM (Seattle, WA, Aug. 2008).\n[Kleinrock 1961] L. Kleinrock, “Information Flow in Large Communication Networks,”\nRLE Quarterly Progress Report, July 1961.\n[Kleinrock 1964] L. Kleinrock, 1964 Communication Nets: Stochastic Message Flow and\nDelay, McGraw-Hill, New York, NY, 1964.\n[Kleinrock 1975] L. Kleinrock, Queuing Systems, Vol. 1, John Wiley, New York, 1975.\nREFERENCES\n803\n\n[Kleinrock 1975b] L. Kleinrock, F. A. Tobagi, “Packet Switching in Radio Channels: Part\nI—Carrier Sense Multiple-Access Modes and Their Throughput-Delay Characteristics,”\nIEEE Transactions on Communications, Vol. 23, No. 12 (Dec. 1975), pp. 1400–1416.\n[Kleinrock 1976] L. Kleinrock, Queuing Systems, Vol. 2, John Wiley, New York, 1976.\n[Kleinrock 2004] L. Kleinrock, “The Birth of the Internet,” http://www.lk.cs.ucla.edu/LK/\nInet/birth.html\n[Kohler 2006] E. Kohler, M. Handley, S. Floyd, “DDCP: Designing DCCP: Congestion\nControl Without Reliability,” Proc. 2006 ACM SIGCOMM (Pisa, Italy, Sept. 2006).\n[Kolding 2003] T. Kolding, K. Pedersen, J. Wigard, F. Frederiksen, P.Mogensen, “High\nSpeed Downlink Packet Access: WCDMA Evolution,” IEEE Vehicular Technology Society\nNews (Feb. 2003), pp. 4–10.\n[Koponen 2011] T. Koponen, S. Shenker, H. Balakrishnan, N. Feamster, I. Ganichev, A.\nGhodsi, P. B. Godfrey, N. McKeown, G. Parulkar, B. Raghavan, J. Rexford, S. Arianfar, D.\nKuptsov, “Architecting for Innovation,” ACM Computer Communications Review, 2011.\n[Korhonen 2003] J. Korhonen, Introduction to 3G Mobile Communications, 2nd ed., Artech\nHouse, 2003.\n[Koziol 2003] J. Koziol, Intrusion Detection with Snort, Sams Publishing, 2003.\n[Krishnamurthy 2001] B. Krishnamurthy, and J. Rexford, Web Protocols and Practice: HTTP/\n1.1, Networking Protocols, and Traffic Measurement, Addison-Wesley, Boston, MA, 2001.\n[Krishnamurthy 2001b] B. Krishnamurthy, C. Wills, Y. Zhang, “On the Use and Performance\nof Content Distribution Networks,” Proc. 2001 ACM Internet Measurement Conference.\n[Krishnan 2009] R. Krishnan, H. Madhyastha, S. Srinivasan, S. Jain, A. Krishnamurthy, T.\nAnderson, J. Gao, “Moving Beyond End-to-end Path Information to Optimize CDN\nPerformance,” Proc. 2009 ACM Internet Measurement Conference.\n[Kulkarni 2005] S. Kulkarni, C. Rosenberg, “Opportunistic Scheduling: Generalizations to\nInclude Multiple Constraints, Multiple Interfaces, and Short Term Fairness,” Wireless\nNetworks, 11 (2005), 557–569.\n[Kumar 2006] R. Kumar, K.W. Ross, “Optimal Peer-Assisted File Distribution: Single and\nMulti-Class Problems,” IEEE Workshop on Hot Topics in Web Systems and Technologies\n(Boston, MA, 2006).\n[Labovitz 1997] C. Labovitz, G. R. Malan, F. Jahanian, “Internet Routing Instability,” Proc.\n1997 ACM SIGCOMM (Cannes, France, Sept. 1997), pp. 115–126.\n[Labovitz 2010] C. Labovitz, S. Iekel-Johnson, D. McPherson, J. Oberheide, F. Jahanian,\n“Internet Inter-Domain Traffic,” Proc. 2010 ACM SIGCOMM.\n[Labrador 1999] M. Labrador, S. Banerjee, “Packet Dropping Policies for ATM and IP\nNetworks,” IEEE Communications Surveys, Vol. 2, No. 3 (Third Quarter 1999), pp. 2–14.\n[Lacage 2004] M. Lacage, M.H. Manshaei, T. Turletti, “IEEE 802.11 Rate Adaptation: A\nPractical Approach,” ACM Int. Symposium on Modeling, Analysis, and Simulation of\nWireless and Mobile Systems (MSWiM) (Venice, Italy, Oct. 2004).\n[Lakhina 2004] A. Lakhina, M. Crovella, C. Diot, “Diagnosing Network-Wide Traffic\nAnomalies,” Proc. 2004 ACM SIGCOMM.\n[Lakhina 2005] A. Lakhina, M. Crovella, C. Diot, “Mining Anomalies Using Traffic Feature\nDistributions,” Proc. 2005 ACM SIGCOMM.\n804\nREFERENCES\n\n[Lakshman 1997] T. V. Lakshman, U. Madhow, “The Performance of TCP/IP for Networks\nwith High Bandwidth-Delay Products and Random Loss,” IEEE/ACM Transactions on\nNetworking, Vol. 5, No. 3 (1997), pp. 336–350.\n[Lam 1980] S. Lam, “A Carrier Sense Multiple Access Protocol for Local Networks,”\nComputer Networks, Vol. 4 (1980), pp. 21–32.\n[Larmouth 1996] J. Larmouth, Understanding OSI, International Thomson Computer Press\n1996. Chapter 8 of this book deals with ASN.1 and is available online at http://www.salford\n.ac.uk/iti/books/osi/all.html#head8.\n[Larmouth 2012] J. Larmouth, Understanding OSI, http://www.business.salford.ac.uk/\nlegacy/isi/books/osi/osi.html\n[Lawton 2001] G. Lawton, “Is IPv6 Finally Gaining Ground?” IEEE Computer Magazine\n(Aug. 2001), pp. 11–15.\n[LeBlond 2011] S. LeBlond, C. Zhang, A. Legout, K. W. Ross, W. Dabbous, “Exploring the\nPrivacy Limits of Real-Time Communication Applications,” Proc. 2011 ACM Internet\nMeasurement Conference (Berlin, 2011).\n[LeBlond 2011] S. LeBlond, C. Zhang, A. Legout, K. W. Ross, W. Dabbous, “I Know Where\nYou and What You Are Sharing: Exploiting P2P Communications to Invade Users Privacy,”\nProc. 2011 ACM Internet Measurement Conference (Berlin).\n[Leighton 2009] T. Leighton, “Improving Performance on the Internet,” Communications of\nthe ACM, Vol. 52, No. 2 (Feb. 2009), pp. 44–51.\n[Leiner 1998] B. Leiner, V. Cerf, D. Clark, R. Kahn, L. Kleinrock, D. Lynch, J. Postel, L.\nRoberts, S. Woolf, “A Brief History of the Internet,” http://www.isoc.org/internet/history/\nbrief.html\n[Leung 2006] K. Leung, V. O.K. Li, “TCP in Wireless Networks: Issues, Approaches, and\nChallenges,” IEEE Commun. Surveys and Tutorials, Vol. 8, No. 4 (2006), pp. 64–79.\n[Li 2004] L. Li, D. Alderson, W. Willinger, J. Doyle, “A First-Principles Approach to\nUnderstanding the Internet’s Router-Level Topology,” Proc. 2004 ACM SIGCOMM\n(Portland, OR, Aug. 2004).\n[Li 2007] J. Li, M. Guidero, Z. Wu, E. Purpus, T. Ehrenkranz, “BGP Routing Dynamics\nRevisited.” ACM Computer Communication Review (April 2007).\n[Liang 2006] J. Liang, N. Naoumov, K.W. Ross, “The Index Poisoning Attack in P2P File-\nSharing Systems,” Proc. 2006 IEEE INFOCOM (Barcelona, Spain, April 2006).\n[Lin 2001] Y. Lin, I. Chlamtac, Wireless and Mobile Network Architectures, John Wiley and\nSons, New York, NY, 2001.\n[Liogkas 2006] N. Liogkas, R. Nelson, E. Kohler, L. Zhang, “Exploiting BitTorrent For Fun\n(But Not Profit),” 6th International Workshop on Peer-to-Peer Systems (IPTPS 2006).\n[Liu 2002] B. Liu, D. Goeckel, D. Towsley, “TCP-Cognizant Adaptive Forward Error\nCorrection in Wireless Networks,” Proc. 2002 Global Internet.\n[Liu 2003] J. Liu, I. Matta, M. Crovella, “End-to-End Inference of Loss Nature in a Hybrid\nWired/Wireless Environment,” Proc. WiOpt’03: Modeling and Optimization in Mobile, Ad\nHoc and Wireless Networks.\n[Liu 2010] Z. Liu, P. Dhungel, Di Wu, C. Zhang, K. W. Ross, “Understanding and\nImproving Incentives in Private P2P Communities,” ICDCS (Genoa, Italy, 2010).\nREFERENCES\n805\n\n[Locher 2006] T. Locher, P. Moor, S. Schmid, R. Wattenhofer, “Free Riding in BitTorrent is\nCheap,” Proc. ACM HotNets 2006 (Irvine CA, Nov. 2006).\n[Lui 2004] J. Lui, V. Misra, D. Rubenstein, “On the Robustness of Soft State Protocols,”\nProc. IEEE Int. Conference on Network Protocols (ICNP ’04), pp. 50–60.\n[Luotonen 1998] A. Luotonen, Web Proxy Servers, Prentice Hall, Englewood Cliffs, NJ, 1998.\n[Lynch 1993] D. Lynch, M. Rose, Internet System Handbook, Addison-Wesley, Reading,\nMA, 1993.\n[Macedonia 1994] M. Macedonia, D. Brutzman, “MBone Provides Audio and Video Across\nthe Internet,” IEEE Computer Magazine, Vol. 27, No. 4 (Apr. 1994), pp. 30–36.\n[Mahdavi 1997] J. Mahdavi, S. Floyd, “TCP-Friendly Unicast Rate-Based Flow Control,”\nunpublished note (Jan. 1997).\n[Malware 2006] Computer Economics, “2005 Malware Report: The Impact of Malicious\nCode Attacks,” http://www.computereconomics.com\n[manet 2012] IETF Mobile Ad-hoc Networks (manet) Working Group, http://www.ietf.org/\nhtml.charters/manet-charter.html\n[Mao 2002] Z. M. Mao, C. Cranor, F. Boudlis, M. Rabinovich, O. Spatscheck, J. Wang, “A\nPrecise and Efficient Evaluation of the Proximity Between Web Clients and Their Local\nDNS Servers,” Proc. 2002 USENIX ATC.\n[MaxMind 2012] http://www.maxmind.com/app/ip-location\n[Maymounkov 2002] P. Maymounkov, D. Mazières. “Kademlia: A Peer-to-Peer Information\nSystem Based on the XOR Metric.” Proceedings of the 1st International Workshop on\nPeerto-Peer Systems (IPTPS ’02) (Mar. 2002), pp. 53–65.\n[McKeown 1997a] N. McKeown, M. Izzard, A. Mekkittikul, W. Ellersick, M. Horowitz,\n“The Tiny Tera: A Packet Switch Core,” IEEE Micro Magazine (Jan.–Feb. 1997).\n[McKeown 1997b] N. McKeown, “A Fast Switched Backplane for a Gigabit Switched\nRouter,” Business Communications Review, Vol. 27, No. 12. http://tiny-tera.stanford.edu/\n~nickm/papers/cisco_fasts_wp.pdf\n[McKeown 2008] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J.\nRexford, S. Shenker, J. Turner, “OpenFlow: Enabling Innovation in Campus Networks,”\nACM SIGCOMM Computer Communication Review, Vol. 38, No. 2 (Apr. 2008).\n[McQuillan 1980] J. McQuillan, I. Richer, E. Rosen, “The New Routing Algorithm for the\nArpanet,” IEEE Transactions on Communications, Vol. 28, No. 5 (May 1980), pp. 711–719.\n[Medhi 1997] D. Medhi, D. Tipper (eds.), Special Issue: Fault Management in Communication\nNetworks, Journal of Network and Systems Management, Vol. 5. No. 2 (June 1997).\n[Metcalfe 1976] R. M. Metcalfe, D. R. Boggs. “Ethernet: Distributed Packet Switching for\nLocal Computer Networks,” Communications of the Association for Computing Machinery,\nVol. 19, No. 7 (July 1976), pp. 395–404.\n[Meyers 2004] A. Myers, T. Ng, H. Zhang, “Rethinking the Service Model: Scaling Ethernet\nto a Million Nodes,” ACM Hotnets Conference, 2004.\n[MFA Forum 2012] IP/MPLS Forum homepage, http://www.ipmplsforum.org/\n[Mirkovic 2005] J. Mirkovic, S. Dietrich, D. Dittrich. P. Reiher, Internet Denial of Service:\nAttack and Defense Mechanisms, Prentice Hall, 2005.\n[Mockapetris 1988] P. V. Mockapetris, K. J. Dunlap, “Development of the Domain Name\nSystem,” Proc. 1988 ACM SIGCOMM (Stanford, CA, Aug. 1988).\n806\nREFERENCES\n\n[Mockapetris 2005] P. Mockapetris, Sigcomm Award Lecture, video available at http://\nwww.postel.org/sigcomm\n[Mogul 2003] J. Mogul, “TCP offload is a dumb idea whose time has come,” Proc. HotOS\nIX: The 9th Workshop on Hot Topics in Operating Systems (2003), USENIX Association.\n[Molinero-Fernandez 2002] P. Molinaro-Fernandez, N. McKeown, H. Zhang, “Is IP Going\nto Take Over the World (of Communications)?,” Proc. 2002 ACM Hotnets.\n[Molle 1987] M. L. Molle, K. Sohraby, A. N. Venetsanopoulos, “Space-Time Models of\nAsynchronous CSMA Protocols for Local Area Networks,” IEEE Journal on Selected Areas\nin Communications, Vol. 5, No. 6 (1987), pp. 956–968.\n[Moore 2001] D. Moore, G. Voelker, S. Savage, “Inferring Internet Denial of Service\nActivity,” Proc. 2001 USENIX Security Symposium (Washington, DC, Aug. 2001).\n[Moore 2003] D. Moore, V. Paxson, S. Savage, C. Shannon, S. Staniford, N. Weaver, “Inside\nthe Slammer Worm,” 2003 IEEE Security and Privacy Conference.\n[Moshchuck 2006] A. Moshchuk, T. Bragin, S. Gribble, H. Levy, “A Crawler-based Study\nof Spyware on the Web,” Proc. 13th Annual Network and Distributed Systems Security\nSymposium (NDSS 2006) (San Diego, CA, Feb. 2006).\n[Motorola 2007] Motorola, “Long Term Evolution (LTE): A Technical Overview,”\nhttp://www.motorola.com/staticfiles/Business/Solutions/Industry%20Solutions/Service%20P\nroviders/Wireless%20Operators/LTE/_Document/Static%20Files/6834_MotDoc_New.pdf\n[Mouly 1992] M. Mouly, M. Pautet, The GSM System for Mobile Communications, Cell and\nSys, Palaiseau, France, 1992.\n[Moy 1998] J. Moy, OSPF: Anatomy of An Internet Routing Protocol, Addison-Wesley,\nReading, MA, 1998.\n[Mudigonda 2011] J. Mudigonda, P. Yalagandula, J. C. Mogul, B. Stiekes, Y. Pouffary,\n“NetLord: A Scalable Multi-Tenant Network Architecture for Virtualized Datacenters,” Proc.\n2011 ACM SIGCOMM.\n[Mukherjee 1997] B. Mukherjee, Optical Communication Networks, McGraw-Hill, 1997.\n[Mukherjee 2006] B. Mukherjee, Optical WDM Networks, Springer, 2006.\n[Mydotr 2009] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri, S.\nRadhakrishnan, V. Subramanya, A. Vahdat, “PortLand: A Scalable Fault-Tolerant Layer 2\nData Center Network Fabric,” Proc. 2009 ACM SIGCOMM.\n[Nadel 2011] B. Nadel, “4G shootout: Verizon LTE vs. Sprint WiMax,” Computerworld,\nFebruary 3, 2011.\n[Nahum 2002] E. Nahum, T. Barzilai, D. Kandlur, “Performance Issues in WWW Servers,”\nIEEE/ACM Transactions on Networking, Vol 10, No. 1 (Feb. 2002).\n[Naoumov 2006] N. Naoumov, K.W. Ross, “Exploiting P2P Systems for DDoS Attacks,”\nIntl Workshop on Peer-to-Peer Information Management (Hong Kong, May 2006),\n[Neglia 2007] G. Neglia, G. Reina, H. Zhang, D. Towsley, A. Venkataramani, J. Danaher,\n“Availability in BitTorrent Systems,” Proc. 2007 IEEE INFOCOM.\n[Neumann 1997] R. Neumann, “Internet Routing Black Hole,” The Risks Digest: Forum on\nRisks to the Public in Computers and Related Systems, Vol. 19, No. 12 (May 1997). http://\ncatless.ncl.ac.uk/Risks/19.12.html#subj1.1\n[Neville-Neil 2009] G. Neville-Neil, “Whither Sockets?” Communications of the ACM,\nVol. 52, No. 6 (June 2009), pp. 51–55.\nREFERENCES\n807\n\n[Nicholson 2006] A Nicholson, Y. Chawathe, M. Chen, B. Noble, D. Wetherall, “Improved\nAccess Point Selection,” Proc. 2006 ACM Mobisys Conference (Uppsala Sweden, 2006).\n[Nielsen 1997] H. F. Nielsen, J. Gettys, A. Baird-Smith, E. Prud’hommeaux, H. W. Lie, C.\nLilley, “Network Performance Effects of HTTP/1.1, CSS1, and PNG,” W3C Document,\n1997 (also appears in Proc. 1997 ACM SIGCOM (Cannes, France, Sept 1997), \npp. 155–166.\n[NIST 2001] National Institute of Standards and Technology, “Advanced Encryption\nStandard (AES),” Federal Information Processing Standards 197, Nov. 2001, http://csrc.nist\n.gov/publications/fips/fips197/fips-197.pdf\n[NIST IPv6 2012] National Institute of Standards, “Estimating IPv6 & DNSSEC\nDeployment SnapShots,” http://usgv6-deploymon.antd.nist.gov/snap-all.html\n[Nmap 2012] Nmap homepage, http://www.insecure.com/nmap\n[Nonnenmacher 1998] J. Nonnenmacher, E. Biersak, D. Towsley, “Parity-Based Loss\nRecovery for Reliable Multicast Transmission,” IEEE/ACM Transactions on Networking,\nVol. 6, No. 4 (Aug. 1998), pp. 349–361.\n[NTIA 1998] National Telecommunications and Information Administration (NTIA), US\nDepartment of Commerce, “Management of Internet names and addresses,” Docket Number:\n980212036-8146-02. http://www.ntia.doc.gov/ntiahome/domainname/6_5_98dns.htm\n[O’Dell 2009] M. O’Dell, “Network Front-End Processors, Yet Again,” Communications of\nthe ACM, Vol. 52, No. 6 (June 2009), pp. 46–50.\n[OID Repository 2012] OID Repository, http://www.oid-info.com/\n[OSI 2012] International Organization for Standardization homepage, http://www.iso.org/\niso/en/ISOOnline.frontpage\n[OSS 2012] OSS Nokalva, “ASN.1 Resources,” http://www.oss.com/asn1/\n[Padhye 2000] J. Padhye, V. Firoiu, D. Towsley, J. Kurose, “Modeling TCP Reno\nPerformance: A Simple Model and its Empirical Validation,” IEEE/ACM Transactions on\nNetworking, Vol. 8 No. 2 (Apr. 2000), pp. 133–145.\n[Padhye 2001] J. Padhye, S. Floyd, “On Inferring TCP Behavior,” Proc. 2001 ACM\nSIGCOMM (San Diego, CA, Aug. 2001).\n[Pan 1997] P. Pan, H. Schulzrinne, “Staged Refresh Timers for RSVP,” Proc. 2nd Global\nInternet Conference (Phoenix, AZ, Dec. 1997).\n[Parekh 1993] A. Parekh, R. Gallagher, “A generalized processor sharing approach to flow\ncontrol in integrated services networks: the single-node case,” IEEE/ACM Transactions on\nNetworking, Vol. 1, No. 3 (June 1993), pp. 344–357.\n[Partridge 1992] C. Partridge, S. Pink, “An Implementation of the Revised Internet Stream\nProtocol (ST-2),” Journal of Internetworking: Research and Experience, Vol. 3, No. 1 \n(Mar. 1992).\n[Partridge 1998] C. Partridge, et al. “A Fifty Gigabit per second IP Router,” IEEE/ACM\nTransactions on Networking, Vol. 6, No. 3 (Jun. 1998), pp. 237–248.\n[Pathak 2010] A. Pathak, Y. A. Wang, C. Huang, A. Greenberg, Y. C. Hu, J. Li, K. W. Ross,\n“Measuring and Evaluating TCP Splitting for Cloud Services,” Passive and Active\nMeasurement (PAM) Conference (Zurich, 2010).\n[Paxson 1997] V. Paxson, “End-to-End Internet Packet Dynamics,” Proc. 1997 ACM\nSIGCOMM (Cannes, France, Sept. 1997).\n808\nREFERENCES\n\n[Perkins 1994] A. Perkins, “Networking with Bob Metcalfe,” The Red Herring Magazine\n(Nov. 1994).\n[Perkins 1998] C. Perkins, O. Hodson, V. Hardman, “A Survey of Packet Loss Recovery\nTechniques for Streaming Audio,” IEEE Network Magazine (Sept./Oct. 1998), pp. 40–47.\n[Perkins 1998b] C. Perkins, Mobile IP: Design Principles and Practice, Addison-Wesley,\nReading, MA, 1998.\n[Perkins 2000] C. Perkins, Ad Hoc Networking, Addison-Wesley, Reading, MA, 2000.\n[Perlman 1999] R. Perlman, Interconnections: Bridges, Routers, Switches, and\nInternetworking Protocols, 2nd ed., Addison-Wesley Professional Computing Series,\nReading, MA, 1999.\n[PGPI 2012] The International PGP Home Page, http://www.pgpi.org\n[Phifer 2000] L. Phifer, “The Trouble with NAT,” The Internet Protocol Journal, Vol. 3, No. 4\n(Dec. 2000), http://www.cisco.com/warp/public/759/ipj_3-4/ipj_3-4_nat.html\n[Piatek 2007] M. Piatek, T. Isdal, T. Anderson, A. Krishnamurthy, A. Venkataramani, “Do\nIncentives Build Robustness in Bittorrent?,” Proc. NSDI (2007).\n[Piatek 2008] M. Piatek, T. Isdal, A. Krishnamurthy, T. Anderson, “One hop Reputations for\nPeer-to-peer File Sharing Workloads,” Proc. NSDI (2008).\n[Pickholtz 1982] R. Pickholtz, D. Schilling, L. Milstein, “Theory of Spread Spectrum\nCommunication—a Tutorial,” IEEE Transactions on Communications, Vol. 30, No. 5 (May\n1982), pp. 855–884.\n[PingPlotter 2012] PingPlotter homepage, http://www.pingplotter.com\n[Piscatello 1993] D. Piscatello, A. Lyman Chapin, Open Systems Networking, Addison-\nWesley, Reading, MA, 1993.\n[Point Topic 2006] Point Topic Ltd., World Broadband Statistics Q1 2006, http://www\n.pointtopic.com\n[Potaroo 2012] “Growth of the BGP Table–1994 to Present,” http://bgp.potaroo.net/\n[PPLive 2012] PPLive homepage, http://www.pplive.com\n[Quagga 2012] Quagga, “Quagga Routing Suite,” http://www.quagga.net/\n[Quittner 1998] J. Quittner, M. Slatalla, Speeding the Net: The Inside Story of Netscape and\nHow it Challenged Microsoft, Atlantic Monthly Press, 1998.\n[Quova 2012] www.quova.com\n[Raiciu 2011] C. Raiciu , S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, M. Handley,\n“Improving Datacenter Performance and Robustness with Multipath TCP,” Proc. 2011 ACM\nSIGCOMM.\n[Ramakrishnan 1990] K. K. Ramakrishnan, R. Jain, “A Binary Feedback Scheme for\nCongestion Avoidance in Computer Networks,” ACM Transactions on Computer Systems,\nVol. 8, No. 2 (May 1990), pp. 158–181.\n[Raman 1999] S. Raman, S. McCanne, “A Model, Analysis, and Protocol Framework \nfor Soft State-based Communication,” Proc. 1999 ACM SIGCOMM (Boston, MA, Aug.\n1999).\n[Raman 2007] B. Raman, K. Chebrolu, “Experiences in using WiFi for Rural Internet in\nIndia,” IEEE Communications Magazine, Special Issue on New Directions in Networking\nTechnologies in Emerging Economies (Jan. 2007).\nREFERENCES\n809\n\n[Ramaswami 2010] R. Ramaswami, K. Sivarajan, G. Sasaki, Optical Networks: A Practical\nPerspective, Morgan Kaufman Publishers, 2010.\n[Ramjee 1994] R. Ramjee, J. Kurose, D. Towsley, H. Schulzrinne, “Adaptive Playout\nMechanisms for Packetized Audio Applications in Wide-Area Networks,” Proc. 1994 IEEE\nINFOCOM.\n[Rao 1996] K. R. Rao and J. J. Hwang, Techniques and Standards for Image, Video and\nAudio Coding, Prentice Hall, Englewood Cliffs, NJ, 1996.\n[Rao 2011] A. S. Rao, Y. S. Lim, C. Barakat, A. Legout, D. Towsley, W. Dabbous, “Network\nCharacteristics of Video Streaming Traffic,” Proc. 2011 ACM CoNEXT (Tokyo).\n[RAT 2012] Robust Audio Tool, http://www-mice.cs.ucl.ac.uk/multimedia/software/rat/\n[Ratnasamy 2001] S. Ratnasamy, P. Francis, M. Handley, R. Karp, S. Shenker, “A\nScalable Content-Addressable Network,” Proc. 2001 ACM SIGCOMM (San Diego, CA,\nAug. 2001).\n[Ren 2006] S. Ren, L. Guo, and X. Zhang, “ASAP: an AS-aware peer-relay protocol for high\nquality VoIP,” Proc. 2006 IEEE ICDCS (Lisboa, Portugal, July 2006).\n[Rescorla 2001] E. Rescorla, SSL and TLS: Designing and Building Secure Systems,\nAddison-Wesley, Boston, 2001.\n[RFC 001] S. Crocker, “Host Software,” RFC 001 (the very first RFC!).\n[RFC 768] J. Postel, “User Datagram Protocol,” RFC 768, Aug. 1980.\n[RFC 789] E. Rosen, “Vulnerabilities of Network Control Protocols,” RFC 789.\n[RFC 791] J. Postel, “Internet Protocol: DARPA Internet Program Protocol Specification,”\nRFC 791, Sept. 1981.\n[RFC 792] J. Postel, “Internet Control Message Protocol,” RFC 792, Sept. 1981.\n[RFC 793] J. Postel, “Transmission Control Protocol,” RFC 793, Sept. 1981.\n[RFC 801] J. Postel, “NCP/TCP Transition Plan,” RFC 801, Nov. 1981.\n[RFC 826] D. C. Plummer, “An Ethernet Address Resolution Protocol—or—Converting\nNetwork Protocol Addresses to 48 bit Ethernet Address for Transmission on Ethernet\nHardware,” RFC 826, Nov. 1982.\n[RFC 829] V. Cerf, “Packet Satellite Technology Reference Sources,” RFC 829, Nov.\n1982.\n[RFC 854] J. Postel, J. Reynolds, “TELNET Protocol Specification,” RFC 854, May 1993.\n[RFC 950] J. Mogul, J. Postel, “Internet Standard Subnetting Procedure,” RFC 950, Aug.\n1985.\n[RFC 959] J. Postel and J. Reynolds, “File Transfer Protocol (FTP),” RFC 959, Oct. 1985.\n[RFC 977] B. Kantor, P. Lapsley, “Network News Transfer Protocol,” RFC 977, Feb. 1986.\n[RFC 1028] J. Davin, J.D. Case, M. Fedor, M. Schoffstall, “A Simple Gateway Monitoring\nProtocol,” RFC 1028, Nov. 1987.\n[RFC 1034] P. V. Mockapetris, “Domain Names—Concepts and Facilities,” RFC 1034,\nNov. 1987.\n[RFC 1035] P. Mockapetris, “Domain Names—Implementation and Specification,” RFC\n1035, Nov. 1987.\n[RFC 1058] C. L. Hendrick, “Routing Information Protocol,” RFC 1058, June 1988.\n810\nREFERENCES\n\n[RFC 1071] R. Braden, D. Borman, and C. Partridge, “Computing The Internet Checksum,”\nRFC 1071, Sept. 1988.\n[RFC 1075] D. Waitzman, C. Partridge, S. Deering, “Distance Vector Multicast Routing\nProtocol,” RFC 1075, Nov. 1988.\n[RFC 1112] S. Deering, “Host Extension for IP Multicasting,” RFC 1112, Aug. 1989.\n[RFC 1122] R. Braden, “Requirements for Internet Hosts—Communication Layers,” RFC\n1122, Oct. 1989.\n[RFC 1123] R. Braden, ed., “Requirements for Internet Hosts—Application and Support,”\nRFC-1123, Oct. 1989.\n[RFC 1142] D. Oran, “OSI IS-IS Intra-Domain Routing Protocol,” RFC 1142, Feb. 1990.\n[RFC 1190] C. Topolcic, “Experimental Internet Stream Protocol: Version 2 (ST-II),” RFC\n1190, Oct. 1990.\n[RFC 1191] J. Mogul, S. Deering, “Path MTU Discovery,” RFC 1191, Nov. 1990.\n[RFC 1213] K. McCloghrie, M. T. Rose, “Management Information Base for Network\nManagement of TCP/IP-based internets: MIB-II,” RFC 1213, Mar. 1991.\n[RFC 1256] S. Deering, “ICMP Router Discovery Messages,” RFC 1256, Sept. 1991.\n[RFC 1320] R. Rivest, “The MD4 Message-Digest Algorithm,” RFC 1320, Apr. 1992.\n[RFC 1321] R. Rivest, “The MD5 Message-Digest Algorithm,” RFC 1321, Apr. 1992.\n[RFC 1323] V. Jacobson, S. Braden, D. Borman, “TCP Extensions for High Performance,”\nRFC 1323, May 1992.\n[RFC 1422] S. Kent, “Privacy Enhancement for Internet Electronic Mail: Part II: Certificate-\nBased Key Management,” RFC 1422.\n[RFC 1546] C. Partridge, T. Mendez, W. Milliken, “Host Anycasting Service,” RFC 1546,\n1993.\n[RFC 1547] D. Perkins, “Requirements for an Internet Standard Point-to-Point Protocol,”\nRFC 1547, Dec. 1993.\n[RFC 1584] J. Moy, “Multicast Extensions to OSPF,” RFC 1584, Mar. 1994.\n[RFC 1633] R. Braden, D. Clark, S. Shenker, “Integrated Services in the Internet\nArchitecture: an Overview,” RFC 1633, June 1994.\n[RFC 1636] R. Braden, D. Clark, S. Crocker, C. Huitema, “Report of IAB Workshop on\nSecurity in the Internet Architecture,” RFC 1636, Nov. 1994.\n[RFC 1661] W. Simpson (ed.), “The Point-to-Point Protocol (PPP),” RFC 1661, July 1994.\n[RFC 1662] W. Simpson (ed.), “PPP in HDLC-Like Framing,” RFC 1662, July 1994.\n[RFC 1700] J. Reynolds and J. Postel, “Assigned Numbers,” RFC 1700, Oct. 1994.\n[RFC 1752] S. Bradner, A. Mankin, “The Recommendations for the IP Next Generation\nProtocol,” RFC 1752, Jan. 1995.\n[RFC 1918] Y. Rekhter, B. Moskowitz, D. Karrenberg, G. J. de Groot, E. Lear, “Address\nAllocation for Private Internets,” RFC 1918, Feb. 1996.\n[RFC 1930] J. Hawkinson, T. Bates, “Guidelines for Creation, Selection, and Registration of\nan Autonomous System (AS),” RFC 1930, Mar. 1996.\n[RFC 1938] N. Haller, C. Metz, “A One-Time Password System,” RFC 1938, May 1996.\n[RFC 1939] J. Myers and M. Rose, “Post Office Protocol—Version 3,” RFC 1939, May 1996.\nREFERENCES\n811\n\n[RFC 1945] T. Berners-Lee, R. Fielding, H. Frystyk, “Hypertext Transfer Protocol—\nHTTP/1.0,” RFC 1945, May 1996.\n[RFC 2003] C. Perkins, “IP Encapsulation within IP,” RFC 2003, Oct. 1996.\n[RFC 2004] C. Perkins, “Minimal Encapsulation within IP,” RFC 2004, Oct. 1996.\n[RFC 2018] M. Mathis, J. Mahdavi, S. Floyd, A. Romanow, “TCP Selective\nAcknowledgment Options,” RFC 2018, Oct. 1996.\n[RFC 2050] K. Hubbard, M. Kosters, D. Conrad, D. Karrenberg, J. Postel, “Internet Registry\nIP Allocation Guidelines,” RFC 2050, Nov. 1996.\n[RFC 2104] H. Krawczyk, M. Bellare, R. Canetti, “HMAC: Keyed-Hashing for Message\nAuthentication,” RFC 2104, Feb. 1997.\n[RFC 2131] R. Droms, “Dynamic Host Configuration Protocol,” RFC 2131, Mar. 1997.\n[RFC 2136] P. Vixie, S. Thomson, Y. Rekhter, J. Bound, “Dynamic Updates in the Domain\nName System,” RFC 2136, Apr. 1997.\n[RFC 2153] W. Simpson, “PPP Vendor Extensions,” RFC 2153, May 1997.\n[RFC 2205] R. Braden, Ed., L. Zhang, S. Berson, S. Herzog, S. Jamin, “Resource\nReSerVation Protocol (RSVP)—Version 1 Functional Specification,” RFC 2205, Sept. 1997.\n[RFC 2210] J. Wroclawski, “The Use of RSVP with IETF Integrated Services,” RFC 2210,\nSept. 1997.\n[RFC 2211] J. Wroclawski, “Specification of the Controlled-Load Network Element\nService,” RFC 2211, Sept. 1997.\n[RFC 2215] S. Shenker, J. Wroclawski, “General Characterization Parameters for Integrated\nService Network Elements,” RFC 2215, Sept. 1997.\n[RFC 2326] H. Schulzrinne, A. Rao, R. Lanphier, “Real Time Streaming Protocol (RTSP),”\nRFC 2326, Apr. 1998.\n[RFC 2328] J. Moy, “OSPF Version 2,” RFC 2328, Apr. 1998.\n[RFC 2420] H. Kummert, “The PPP Triple-DES Encryption Protocol (3DESE),” RFC 2420,\nSept. 1998.\n[RFC 2453] G. Malkin, “RIP Version 2,” RFC 2453, Nov. 1998.\n[RFC 2460] S. Deering, R. Hinden, “Internet Protocol, Version 6 (IPv6) Specification,” RFC\n2460, Dec. 1998.\n[RFC 2475] S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang, W. Weiss, “An\nArchitecture for Differentiated Services,” RFC 2475, Dec. 1998.\n[RFC 2578] K. McCloghrie, D. Perkins, J. Schoenwaelder, “Structure of Management\nInformation Version 2 (SMIv2),” RFC 2578, Apr. 1999.\n[RFC 2579] K. McCloghrie, D. Perkins, J. Schoenwaelder, “Textual Conventions for\nSMIv2,” RFC 2579, Apr. 1999.\n[RFC 2580] K. McCloghrie, D. Perkins, J. Schoenwaelder, “Conformance Statements for\nSMIv2,” RFC 2580, Apr. 1999.\n[RFC 2597] J. Heinanen, F. Baker, W. Weiss, J. Wroclawski, “Assured Forwarding PHB\nGroup,” RFC 2597, June 1999.\n[RFC 2616] R. Fielding, J. Gettys, J. Mogul, H. Frystyk, L. Masinter, P. Leach, T. Berners-\nLee, R. Fielding, “Hypertext Transfer Protocol—HTTP/1.1,” RFC 2616, June 1999.\n812\nREFERENCES\n\n[RFC 2663] P. Srisuresh, M. Holdrege, “IP Network Address Translator (NAT) Terminology\nand Considerations,” RFC 2663.\n[RFC 2702] D. Awduche, J. Malcolm, J. Agogbua, M. O’Dell, J. McManus, “Requirements\nfor Traffic Engineering Over MPLS,” RFC 2702, Sept. 1999.\n[RFC 2827] P. Ferguson, D. Senie, “Network Ingress Filtering: Defeating Denial of Service\nAttacks which Employ IP Source Address Spoofing,” RFC 2827, May 2000.\n[RFC 2865] C. Rigney, S. Willens, A. Rubens, W. Simpson, “Remote Authentication Dial In\nUser Service (RADIUS),” RFC 2865, June 2000.\n[RFC 2961] L. Berger, D. Gan, G. Swallow, P. Pan, F. Tommasi, S. Molendini, “RSVP\nRefresh Overhead Reduction Extensions,” RFC 2961, Apr. 2001.\n[RFC 3007] B. Wellington, “Secure Domain Name System (DNS) Dynamic Update,” RFC\n3007, Nov. 2000.\n[RFC 3022] P. Srisuresh, K. Egevang, “Traditional IP Network Address Translator\n(Traditional NAT),” RFC 3022, Jan. 2001.\n[RFC 3022] P. Srisuresh, K. Egevang, “Traditional IP Network Address Translator\n(Traditional NAT),” RFC 3022, Jan. 2001.\n[RFC 3031] E. Rosen, A. Viswanathan, R. Callon, “Multiprotocol Label Switching\nArchitecture,” RFC 3031, Jan. 2001.\n[RFC 3032] E. Rosen, D. Tappan, G. Fedorkow, Y. Rekhter, D. Farinacci, T. Li, A. Conta,\n“MPLS Label Stack Encoding,” RFC 3032, Jan. 2001.\n[RFC 3052] M. Eder, S. Nag, “Service Management Architectures Issues and Review,” RFC\n3052, Jan. 2001.\n[RFC 3139] L. Sanchez, K. McCloghrie, J. Saperia, “Requirements for Configuration\nManagement of IP-Based Networks,” RFC 3139, June 2001.\n[RFC 3168] K. Ramakrishnan, S. Floyd, D. Black, “The Addition of Explicit Congestion\nNotification (ECN) to IP,” RFC 3168, Sept. 2001.\n[RFC 3209] D. Awduche, L. Berger, D. Gan, T. Li, V. Srinivasan, G. Swallow, “RSVP-TE:\nExtensions to RSVP for LSP Tunnels,” RFC 3209, Dec. 2001.\n[RFC 3221] G. Huston, “Commentary on Inter-Domain Routing in the Internet,” RFC 3221,\nDec. 2001.\n[RFC 3232] J. Reynolds, “Assigned Numbers: RFC 1700 is Replaced by an On-line\nDatabase,” RFC 3232, Jan. 2002.\n[RFC 3246] B. Davie, A. Charny, J.C.R. Bennet, K. Benson, J.Y. Le Boudec, W. Courtney,\nS. Davari, V. Firoiu, D. Stiliadis, “An Expedited Forwarding PHB (Per-Hop Behavior),”\nRFC 3246, Mar. 2002.\n[RFC 3260] D. Grossman, “New Terminology and Clarifications for Diffserv,” RFC 3260,\nApr. 2002.\n[RFC 3261] J. Rosenberg, H. Schulzrinne, G. Carmarillo, A. Johnston, J. Peterson, R.\nSparks, M. Handley, E. Schooler, “SIP: Session Initiation Protocol,” RFC 3261, July 2002.\n[RFC 3272] J. Boyle, V. Gill, A. Hannan, D. Cooper, D. Awduche, B. Christian, W.S. Lai,\n“Overview and Principles of Internet Traffic Engineering,” RFC 3272, May 2002.\n[RFC 3286] L. Ong, J. Yoakum, “An Introduction to the Stream Control Transmission\nProtocol (SCTP),” RFC 3286, May 2002.\nREFERENCES\n813\n\n[RFC 3346] J. Boyle, V. Gill, A. Hannan, D. Cooper, D. Awduche, B. Christian, W. S. Lai,\n“Applicability Statement for Traffic Engineering with MPLS,” RFC 3346, Aug. 2002.\n[RFC 3376] B. Cain, S. Deering, I. Kouvelas, B. Fenner, A. Thyagarajan, “Internet Group\nManagement Protocol, Version 3,” RFC 3376, Oct. 2002.\n[RFC 3390] M. Allman, S. Floyd, C. Partridge, “Increasing TCP’s Initial Window,” RFC\n3390, Oct. 2002.\n[RFC 3410] J. Case, R. Mundy, D. Partain, “Introduction and Applicability Statements for\nInternet Standard Management Framework,” RFC 3410, Dec. 2002.\n[RFC 3411] D. Harrington, R. Presuhn, B. Wijnen, “An Architecture for Describing\nSimple Network Management Protocol (SNMP) Management Frameworks,” RFC 3411,\nDec. 2002.\n[RFC 3414] U. Blumenthal and B. Wijnen, “User-based Security Model (USM) for\nVersion 3 of the Simple Network Management Protocol (SNMPv3),” RFC 3414,\nDecember 2002.\n[RFC 3415] B. Wijnen, R. Presuhn, K. McCloghrie, “View-based Access Control Model\n(VACM) for the Simple Network Management Protocol (SNMP),” RFC 3415, Dec. 2002.\n[RFC 3416] R. Presuhn, J. Case, K. McCloghrie, M. Rose, S. Waldbusser, “Version 2 of the\nProtocol Operations for the Simple Network Management Protocol (SNMP),” Dec. 2002.\n[RFC 3439] R. Bush and D. Meyer, “Some internet architectural guidelines and philosophy,”\nRFC 3439, Dec. 2003.\n[RFC 3447] J. Jonsson, B. Kaliski, “Public-Key Cryptography Standards (PKCS) #1: RSA\nCryptography Specifications Version 2.1,” RFC 3447, Feb. 2003.\n[RFC 3468] L. Andersson, G. Swallow, “The Multiprotocol Label Switching (MPLS)\nWorking Group Decision on MPLS Signaling Protocols,” RFC 3468, Feb. 2003.\n[RFC 3469] V. Sharma, Ed., F. Hellstrand, Ed, “Framework for Multi-Protocol Label\nSwitching (MPLS)-based Recovery,” RFC 3469, Feb. 2003. ftp://ftp.rfc-editor.org/in-\nnotes/rfc3469.txt\n[RFC 3501] M. Crispin, “Internet Message Access Protocol—Version 4rev1,” RFC 3501,\nMar. 2003.\n[RFC 3550] H. Schulzrinne, S. Casner, R. Frederick, V. Jacobson, “RTP: A Transport\nProtocol for Real-Time Applications,” RFC 3550, July 2003.\n[RFC 3569] S. Bhattacharyya (ed.), “An Overview of Source-Specific Multicast (SSM),”\nRFC 3569, July 2003.\n[RFC 3588] P. Calhoun, J. Loughney, E. Guttman, G. Zorn, J. Arkko, “Diameter Base\nProtocol,” RFC 3588, Sept. 2003.\n[RFC 3618] B. Fenner, D. Meyer, Ed., “Multicast Source Discovery Protocol (MSDP),”\nRFC 3618, Oct. 2003.\n[RFC 3649] S. Floyd, “High Speed TCP for Large Congestion Windows,” RFC 3649, Dec. 2003.\n[RFC 3748] B. Aboba, L. Blunk, J. Vollbrecht, J. Carlson, H. Levkowetz, Ed., “Extensible\nAuthentication Protocol (EAP),” RFC 3748, June 2004.\n[RFC 3782] S. Floyd, T. Henderson, A. Gurtov, “The NewReno Modification to TCP’s Fast\nRecovery Algorithm,” RFC 3782, Apr. 2004.\n814\nREFERENCES\n\n[RFC 3973] A. Adams, J. Nicholas, W. Siadak, “Protocol Independent Multicast—Dense\nMode (PIM-DM): Protocol Specification (Revised),” RFC 3973, Jan. 2005.\n[RFC 4022] R. Raghunarayan, Ed., “Management Information Base for the Transmission\nControl Protocol (TCP),” RFC 4022, Mar. 2005.\n[RFC 4113] B. Fenner, J. Flick, “Management Information Base for the User Datagram\nProtocol (UDP),” RFC 4113, June 2005.\n[RFC 4213] E. Nordmark, R. Gilligan, “Basic Transition Mechanisms for IPv6 Hosts and\nRouters,” RFC 4213, Oct. 2005.\n[RFC 4271] Y. Rekhter, T. Li, S. Hares, Ed., “A Border Gateway Protocol 4 (BGP-4),” RFC\n4271, Jan. 2006.\n[RFC 4272] S. Murphy, “BGP Security Vulnerabilities Analysis,” RFC 4274, Jan. 2006.\n[RFC 4274] Meyer, D. and K. Patel, “BGP-4 Protocol Analysis”, RFC 4274, January\n2006.\n[RFC 4291] R. Hinden, S. Deering, “IP Version 6 Addressing Architecture,” RFC 4291,\nFebruary 2006.\n[RFC 4293] S. Routhier, Ed. “Management Information Base for the Internet Protocol (IP),”\nRFC 4293, Apr. 2006.\n[RFC 4301] S. Kent, K. Seo, “Security Architecture for the Internet Protocol,” RFC 4301,\nDec. 2005.\n[RFC 4302] S. Kent, “IP Authentication Header,” RFC 4302, Dec. 2005.\n[RFC 4303] S. Kent, “IP Encapsulating Security Payload (ESP),” RFC 4303, Dec. 2005.\n[RFC 4305] D. Eastlake, “Cryptographic Algorithm Implementation Requirements for\nEncapsulating Security Payload (ESP) and Authentication Header (AH),” RFC 4305, \nDec. 2005.\n[RFC 4340] E. Kohler, M. Handley, S. Floyd, “Datagram Congestion Control Protocol\n(DCCP),” RFC 4340, Mar. 2006.\n[RFC 4443] A. Conta, S. Deering, M. Gupta, Ed., “Internet Control Message Protocol\n(ICMPv6) for the Internet Protocol Version 6 (IPv6) Specification,” RFC 4443, Mar.\n2006.\n[RFC 4346] T. Dierks, E. Rescorla, “The Transport Layer Security (TLS) Protocol Version\n1.1,” RFC 4346, Apr. 2006.\n[RFC 4502] S. Waldbusser, “Remote Network Monitoring Management Information Base\nVersion 2,” RFC 4502, May 2006.\n[RFC 4514] K. Zeilenga, Ed., “Lightweight Directory Access Protocol (LDAP): String\nRepresentation of Distinguished Names,” RFC 4514, June 2006.\n[RFC 4601] B. Fenner, M. Handley, H. Holbrook, I. Kouvelas, “Protocol Independent\nMulticast—Sparse Mode (PIM-SM): Protocol Specification (Revised),” RFC 4601, Aug. 2006.\n[RFC 4607] H. Holbrook, B. Cain, “Source-Specific Multicast for IP,” RFC 4607, Aug. 2006.\n[RFC 4611] M. McBride, J. Meylor, D. Meyer, “Multicast Source Discovery Protocol\n(MSDP) Deployment Scenarios,” RFC 4611, Aug. 2006.\n[RFC 4632] V. Fuller, T. Li, “Classless Inter-domain Routing (CIDR): The Internet Address\nAssignment and Aggregation Plan,” RFC 4632, Aug. 2006.\nREFERENCES\n815\n\n[RFC 4960] R. Stewart, ed., “Stream Control Transmission Protocol,” RFC 4960, Sept. 2007.\n[RFC 4987] W. Eddy, “TCP SYN Flooding Attacks and Common Mitigations,” RFC 4987,\nAug. 2007.\n[RFC 5000] RFC editor, “Internet Official Protocol Standards,” RFC 5000, May 2008.\n[RFC 5109] A. Li (ed.), “RTP Payload Format for Generic Forward Error Correction,” RFC\n5109, Dec. 2007. \n[RFC 5110] P. Savola, “Overview of the Internet Multicast Routing Architecture,” RFC\n5110, Jan. 2008.\n[RFC 5216] D. Simon, B. Aboba, R. Hurst, “The EAP-TLS Authentication Protocol,” RFC\n5216, Mar. 2008.\n[RFC 5218] D. Thaler, B. Aboba, “What Makes for a Successful Protocol?,” RFC 5218, July\n2008.\n[RFC 5321] J. Klensin, “Simple Mail Transfer Protocol,” RFC 5321, Oct. 2008.\n[RFC 5322] P. Resnick, Ed., “Internet Message Format,” RFC 5322, Oct. 2008.\n[RFC 5348] S. Floyd, M. Handley, J. Padhye, J.Widmer, “TCP Friendly Rate Control\n(TFRC): Protocol Specification,” RFC 5348, Sept. 2008.\n[RFC 5411] J Rosenberg, “A Hitchhiker’s Guide to the Session Initiation Protocol (SIP),”\nRFC 5411, Feb. 2009.\n[RFC 5681] M. Allman, V. Paxson, E. Blanton, “TCP Congestion Control,” RFC 5681, \nSept. 2009.\n[RFC 5944] C. Perkins, Ed., “IP Mobility Support for IPv4, Revised,” RFC 5944, November\n2010.\n[RFC 5996] C. Kaufman, P. Hoffman, Y. Nir, P. Eronen, “Internet Key Exchange Protocol\nVersion 2 (IKEv2),” RFC 5996, Sept. 2010.\n[RFC 6071] S. Frankel, S. Krishnan, “IP Security (IPsec) and Internet Key Exchange (IKE)\nDocument Roadmap,” RFC 6071, Feb. 2011.\n[RFC 6265] A Barth, “HTTP State Management Mechanism,” RFC 6265, Apr. 2011.\n[RFC 6298] V. Paxson, M. Allman, J. Chu, M. Sargent, “Computing TCP’s Retransmission\nTimer,” RFC 6298, June 2011.\n[Rhee 1998] I. Rhee, “Error Control Techniques for Interactive Low-Bit Rate Video\nTransmission over the Internet,” Proc. 1998 ACM SIGCOMM (Vancouver BC, Aug. 1998).\n[Roberts 1967] L. Roberts, T. Merril, “Toward a Cooperative Network of Time-Shared\nComputers,” AFIPS Fall Conference (Oct. 1966).\n[Roberts 2004] J. Roberts, “Internet Traffic, QoS and Pricing,” Proc. 2004 IEEE\nINFOCOM, Vol. 92, No. 9 (Sept. 2004), pp. 1389–1399.\n[Rodriguez 2010] R. Rodrigues, P. Druschel, “Peer-to-Peer Systems,” Communications of\nthe ACM, Vol. 53, No. 10 (Oct. 2010), pp. 72–82.\n[Rohde 2008] Rohde and Schwarz, “UMTS Long Term Evolution (LTE) Technology\nIntroduction,” Application Note 1MA111.\n[Rom 1990] R. Rom, M. Sidi, Multiple Access Protocols: Performance and Analysis,\nSpringer-Verlag, New York, 1990.\n[Root Servers 2012] Root Servers homepage, http://www.root-servers.org/\n816\nREFERENCES\n\n[Rose 1996] M. Rose, The Simple Book: An Introduction to Internet Management, Revised\nSecond Edition, Prentice Hall, Englewood Cliffs, NJ, 1996.\n[Ross 1995] K. W. Ross, Multiservice Loss Models for Broadband Telecommunication\nNetworks, Springer, Berlin, 1995.\n[Rowston 2001] A. Rowston, P. Druschel, “Pastry: Scalable, Distributed Object Location\nand Routing for Large-Scale Peer-to-Peer Systems,” Proc. 2001 IFIP/ACM Middleware\n(Heidelberg, Germany, 2001).\n[RSA 1978] R. Rivest, A. Shamir, L. Adelman, “A Method for Obtaining Digital Signatures\nand Public-key Cryptosystems,” Communications of the ACM, Vol. 21, No. 2 (Feb. 1978),\npp. 120–126.\n[RSA Fast 2012] RSA Laboratories, “How Fast is RSA?” http://www.rsa.com/rsalabs/node\n.asp?id=2215\n[RSA Key 2012] RSA Laboratories, “How large a key should be used in the RSA Crypto\nsystem?” http://www.rsa.com/rsalabs/node.asp?id=2218\n[Rubenstein 1998] D. Rubenstein, J. Kurose, D. Towsley, “Real-Time Reliable Multicast\nUsing Proactive Forward Error Correction,” Proceedings of NOSSDAV ’98 (Cambridge, UK,\nJuly 1998).\n[Rubin 2001] A. Rubin, White-Hat Security Arsenal: Tackling the Threats, Addison-Wesley,\n2001.\n[Ruiz-Sanchez 2001] M. Ruiz-Sánchez, E. Biersack, W. Dabbous, “Survey and Taxonomy\nof IP Address Lookup Algorithms,” IEEE Network Magazine, Vol. 15, No. 2 (Mar./Apr.\n2001), pp. 8–23.\n[Saltzer 1984] J. Saltzer, D. Reed, D. Clark, “End-to-End Arguments in System Design,”\nACM Transactions on Computer Systems (TOCS), Vol. 2, No. 4 (Nov. 1984).\n[Sandvine 2011] “Global Internet Phenomena Report, Spring 2011,” http://www.sandvine.\ncom/news/global broadband trends.asp, 2011.\n[Sardar 2006] B. Sardar, D. Saha, “A Survey of TCP Enhancements for Last-Hop Wireless\nNetworks,” IEEE Commun. Surveys and Tutorials, Vol. 8, No. 3 (2006), pp. 20–34.\n[Saroiu 2002] S. Saroiu, P.K. Gummadi, S.D. Gribble, “A Measurement Study of Peer-to-\nPeer File Sharing Systems,” Proc. of Multimedia Computing and Networking (MMCN)\n(2002).\n[Saroiu 2002b] S. Saroiu, K. P. Gummadi, R. J. Dunn, S. D. Gribble, and H. M. Levy, “An\nAnalysis of Internet Content Delivery Systems,” USENIX OSDI (2002).\n[Saydam 1996] T. Saydam, T. Magedanz, “From Networks and Network Management into\nService and Service Management,” Journal of Networks and System Management, Vol. 4,\nNo. 4 (Dec. 1996), pp. 345–348.\n[Schiller 2003] J. Schiller, Mobile Communications 2nd edition, Addison Wesley, 2003.\n[Schneier 1995] B. Schneier, Applied Cryptography: Protocols, Algorithms, and Source\nCode in C, John Wiley and Sons, 1995.\n[Schulzrinne 1997] H. Schulzrinne, “A Comprehensive Multimedia Control Architecture for\nthe Internet,” NOSSDAV’97 (Network and Operating System Support for Digital Audio and\nVideo) (St. Louis, MO, May 1997).\n[Schulzrinne-RTP 2012] Henning Schulzrinne’s RTP site, http://www.cs.columbia.edu/~hgs/rtp\nREFERENCES\n817\n\n[Schulzrinne-RTSP 2012] Henning Schulzrinne’s RTSP site, http://www.cs.columbia.edu/\n~hgs/rtsp\n[Schulzrinne-SIP 2012] Henning Schulzrinne’s SIP site, http://www.cs.columbia.edu/~hgs/sip\n[Schwartz 1977] M. Schwartz, Computer-Communication Network Design and Analysis,\nPrentice-Hall, Englewood Cliffs, N.J., 1997.\n[Schwartz 1980] M. Schwartz, Information, Transmission, Modulation, and Noise, McGraw\nHill, New York, NY 1980.\n[Schwartz 1982] M. Schwartz, “Performance Analysis of the SNA Virtual Route Pacing\nControl,” IEEE Transactions on Communications, Vol. 30, No. 1 (Jan. 1982), pp. 172–184.\n[Scourias 2012] J. Scourias, “Overview of the Global System for Mobile Communications:\nGSM.” http://www.privateline.com/PCS/GSM0.html\n[Segaller 1998] S. Segaller, Nerds 2.0.1, A Brief History of the Internet, TV Books, New\nYork, 1998.\n[Shacham 1990] N. Shacham, P. McKenney, “Packet Recovery in High-Speed Networks\nUsing Coding and Buffer Management,” Proc. 1990 IEEE INFOCOM (San Francisco, CA,\nApr. 1990), pp. 124–131.\n[Shaikh 2001] A. Shaikh, R. Tewari, M. Agrawal, “On the Effectiveness of DNS-based\nServer Selection,” Proc. 2001 IEEE INFOCOM.\n[Sharma 2003] P. Sharma, E, Perry, R. Malpani, “IP Multicast Operational Network\nmanagement: Design, Challenges, and Experiences,” IEEE Network Magazine (Mar. 2003),\npp. 49–55.\n[Singh 1999] S. Singh, The Code Book: The Evolution of Secrecy from Mary, Queen of\nScotsto Quantum Cryptography, Doubleday Press, 1999.\n[SIP Software 2012] H. Schulzrinne Software Package site, http://www.cs.columbia.edu/\nIRT/software\n[Skoudis 2004] E. Skoudis, L. Zeltser, Malware: Fighting Malicious Code, Prentice Hall, 2004.\n[Skoudis 2006] E. Skoudis, T. Liston, Counter Hack Reloaded: A Step-by-Step Guide to\nComputer Attacks and Effective Defenses (2nd Edition), Prentice Hall, 2006.\n[Skype 2012] Skype homepage, www.skype.com\n[SMIL 2012] W3C Synchronized Multimedia homepage, http://www.w3.org/AudioVideo\n[Smith 2009] J. Smith, “Fighting Physics: A Tough Battle,” Communications of the ACM,\nVol. 52, No. 7 (July 2009), pp. 60–65.\n[Snort 2012] Sourcefire Inc., Snort homepage, http://http://www.snort.org/\n[Solari 1997] S. J. Solari, Digital Video and Audio Compression, McGraw Hill, \nNew York, NY, 1997.\n[Solensky 1996] F. Solensky, “IPv4 Address Lifetime Expectations,” in IPng: Internet\nProtocol Next Generation (S. Bradner, A. Mankin, ed.), Addison-Wesley, Reading, MA, 1996.\n[Spragins 1991] J. D. Spragins, Telecommunications Protocols and Design, Addison-\nWesley, Reading, MA, 1991.\n[Srikant 2004] R. Srikant, The Mathematics of Internet Congestion Control, Birkhauser, 2004 \n[Sripanidkulchai 2004] K. Sripanidkulchai, B. Maggs, and H. Zhang, “An analysis of live\nstreaming workloads on the Internet,” Proc. 2004 ACM Internet Measurement Conference\n(Taormina, Sicily, Italy), pp. 41–54.\n818\nREFERENCES\n\n[Stallings 1993] W. Stallings, SNMP, SNMP v2, and CMIP The Practical Guide to Network\nManagement Standards, Addison-Wesley, Reading, MA, 1993.\n[Stallings 1999] W. Stallings, SNMP, SNMPv2, SNMPv3, and RMON 1 and 2, Addison-\nWesley, Reading, MA, 1999.\n[Steinder 2002] M. Steinder, A. Sethi, “Increasing robustness of fault localization through\nanalysis of lost, spurious, and positive symptoms,” Proc. 2002 IEEE INFOCOM.\n[Stevens 1990] W. R. Stevens, Unix Network Programming, Prentice-Hall, Englewood\nCliffs, NJ.\n[Stevens 1994] W. R. Stevens, TCP/IP Illustrated, Vol. 1: The Protocols, Addison-Wesley,\nReading, MA, 1994.\n[Stevens 1997] W.R. Stevens, Unix Network Programming, Volume 1: Networking APIs-\nSockets and XTI, 2nd edition, Prentice-Hall, Englewood Cliffs, NJ, 1997.\n[Stewart 1999] J. Stewart, BGP4: Interdomain Routing in the Internet, Addison-Wesley,\n1999.\n[Stoica 2001] I. Stoica, R. Morris, D. Karger, M.F. Kaashoek, H. Balakrishnan, “Chord: A\nScalable Peer-to-Peer Lookup Service for Internet Applications,” Proc. 2001 ACM\nSIGCOMM (San Diego, CA, Aug. 2001).\n[Stone 1998] J. Stone, M. Greenwald, C. Partridge, J. Hughes, “Performance of Checksums\nand CRC’s Over Real Data,” IEEE/ACM Transactions on Networking, Vol. 6, No. 5 (Oct. 1998),\npp. 529–543.\n[Stone 2000] J. Stone, C. Partridge, “When Reality and the Checksum Disagree,” Proc. 2000\nACM SIGCOMM (Stockholm, Sweden, Aug. 2000).\n[Strayer 1992] W. T. Strayer, B. Dempsey, A. Weaver, XTP: The Xpress Transfer Protocol,\nAddison-Wesley, Reading, MA, 1992.\n[Stubblefield 2002] A. Stubblefield, J. Ioannidis, A. Rubin, “Using the Fluhrer, Mantin, and\nShamir Attack to Break WEP,” Proceedings of 2002 Network and Distributed Systems\nSecurity Symposium (2002), pp. 17–22.\n[Subramanian 2000] M. Subramanian, Network Management: Principles and Practice,\nAddison-Wesley, Reading, MA, 2000.\n[Subramanian 2002] L. Subramanian, S. Agarwal, J. Rexford, R. Katz, “Characterizing the\nInternet Hierarchy from Multiple Vantage Points,” Proc. 2002 IEEE INFOCOM.\n[Sundaresan 2006] K.Sundaresan, K. Papagiannaki, “The Need for Cross-layer Information\nin Access Point Selection,” Proc. 2006 ACM Internet Measurement Conference (Rio De\nJaneiro, Oct. 2006).\n[Su 2006] A.-J. Su, D. Choffnes, A. Kuzmanovic, and F. Bustamante, “Drafting Behind\nAkamai” Proc. 2006 ACM SIGCOMM.\n[Suh 2006] K. Suh, D. R. Figueiredo, J. Kurose and D. Towsley, “Characterizing and\ndetecting relayed traffic: A case study using Skype,” Proc. 2006 IEEE INFOCOM\n(Barcelona, Spain, Apr. 2006).\n[Sunshine 1978] C. Sunshine, Y. Dalal, “Connection Management in Transport Protocols,”\nComputer Networks, North-Holland, Amsterdam, 1978.\n[Tariq 2008] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, M. Ammar, “Answering\nWhat-If Deployment and Configuration Questions with WISE,” Proc. 2008 ACM\nSIGCOMM (Aug. 2008).\nREFERENCES\n819\n\n[TechnOnLine 2012] TechOnLine, “Protected Wireless Networks,” online webcast tutorial,\nhttp://www.techonline.com/community/tech_topic/internet/21752\n[Teixeira 2006] R. Teixeira and J. Rexford, “Managing Routing Disruptions in Internet\nService Provider Networks,” IEEE Communications Magazine (Mar. 2006).\n[Thaler 1997] D. Thaler and C. Ravishankar, “Distributed Center-Location Algorithms,”\nIEEE Journal on Selected Areas in Communications, Vol. 15, No. 3 (Apr. 1997), \npp. 291–303.\n[Think 2012] Technical History of Network Protocols, “Cyclades,” http://www.cs.utexas\n.edu/users/chris/think/Cyclades/index.shtml\n[Tian 2012] Y. Tian, R. Dey, Y. Liu, K. W. Ross, “China’s Internet: Topology Mapping and\nGeolocating,” IEEE INFOCOM Mini-Conference 2012 (Orlando, FL, 2012).\n[Tobagi 1990] F. Tobagi, “Fast Packet Switch Architectures for Broadband Integrated\nNetworks,” Proc. 1990 IEEE INFOCOM, Vol. 78, No. 1 (Jan. 1990), pp. 133–167.\n[TOR 2012] Tor: Anonymity Online, http://www.torproject.org\n[Torres 2011] R. Torres, A. Finamore, J. R. Kim, M. M. Munafo, S. Rao, “Dissecting Video\nServer Selection Strategies in the YouTube CDN,” Proc. 2011 Int. Conf. on Distributed\nComputing Systems.\n[Turner 1988] J. S. Turner “Design of a Broadcast packet switching network,” IEEE\nTransactions on Communications, Vol. 36, No. 6 (June 1988), pp. 734–743.\n[Turner 2012] B. Turner, “2G, 3G, 4G Wireless Tutorial,” http://blogs.nmscommunications\n.com/communications/2008/10/2g-3g-4g-wireless-tutorial.html\n[UPnP Forum 2012] UPnP Forum homepage, http://www.upnp.org/\n[van der Berg 2008] R. van der Berg, “How the ‘Net works: an introduction to peering and\ntransit,” http://arstechnica.com/guides/other/peering-and-transit.ars\n[Varghese 1997] G. Varghese, A. Lauck, “Hashed and Hierarchical Timing Wheels: Efficient\nData Structures for Implementing a Timer Facility,” IEEE/ACM Transactions on Networking,\nVol. 5, No. 6 (Dec. 1997), pp. 824–834.\n[Vasudevan 2012] S. Vasudevan, C. Diot, J. Kurose, D. Towsley, “Facilitating Access Point\nSelection in IEEE 802.11 Wireless Networks,” Proc. 2005 ACM Internet Measurement\nConference, (San Francisco CA, Oct. 2005).\n[Verizon FIOS 2012] Verizon, “Verizon FiOS Internet: FAQ,” http://www22.verizon.com/\nresidential/fiosinternet/faq/faq.htm\n[Verizon SLA 2012] Verizon, “Global Latency and Packet Delivery SLA,” http://www.\nverizonbusiness.com/terms/global_latency_sla.xml\n[Verma 2001] D. C. Verma, Content Distribution Networks: An Engineering Approach, John\nWiley, 2001.\n[Villamizar 1994] C. Villamizar, C. Song. “High performance tcp in ansnet,” ACM\nSIGCOMM Computer Communications Review, Vol. 24, No. 5 (1994), pp. 45–60.\n[Viterbi 1995] A. Viterbi, CDMA: Principles of Spread Spectrum Communication, Addison-\nWesley, Reading, MA, 1995.\n[Vixie 2009] P. Vixie, “What DNS Is Not,” Communications of the ACM, Vol. 52, No. 12\n(Dec. 2009), pp. 43–47.\n[W3C 1995] The World Wide Web Consortium, “A Little History of the World Wide Web”\n(1995), http://www.w3.org/History.html\n820\nREFERENCES\n\n[Wakeman 1992] I. Wakeman, J. Crowcroft, Z. Wang, D. Sirovica, “Layering Considered\nHarmful,” IEEE Network (Jan. 1992), pp. 20–24.\n[Waldrop 2007] M. Waldrop, “Data Center in a Box,” Scientific American (July 2007).\n[Walker 2000] J. Walker, “IEEE P802.11 Wireless LANs, Unsafe at Any Key Size; An Analysis\nof the WEP Encapsulation,” Oct. 2000, http://www.drizzle.com/~aboba/IEEE/0-362.zip\n[Wall 1980] D. Wall, Mechanisms for Broadcast and Selective Broadcast, Ph.D. thesis,\nStanford University, June 1980.\n[Wang 2004] B. Wang, J. Kurose, P. Shenoy, D. Towsley, “Multimedia Streaming via TCP:\nAn Analytic Performance Study,” Proc. 2004 ACM Multimedia Conference (New York, NY,\nOct. 2004).\n[Wang 2008] B. Wang, J. Kurose, P. Shenoy, D. Towsley, “Multimedia Streaming via TCP:\nAn Analytic Performance Study,” ACM Transactions on Multimedia Computing\nCommunications and Applications (TOMCCAP), Vol. 4, No. 2 (Apr. 2008), pp. 16:1–22.\n[Wang 2010] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. S. E. Ng, M.\nKozuch, M. Ryan, “c-Through: Part-time Optics in Data Centers,” Proc. 2010 ACM\nSIGCOMM.\n[Weatherspoon 2000] S. Weatherspoon, “Overview of IEEE 802.11b Security,” Intel Technology\nJournal (2nd Quarter 2000), http://download.intel.com/technology/itj/q22000/pdf/art_5.pdf\n[Wei 2005] W. Wei, B. Wang, C. Zhang, J. Kurose, D. Towsley, “Classification of Access\nNetwork Types: Ethernet, Wireless LAN, ADSL, Cable Modem or Dialup?,” Proc. 2005\nIEEE INFOCOM (Apr. 2005).\n[Wei 2006] W. Wei, C. Zhang, H. Zang, J. Kurose, D. Towsley, “Inference and Evaluation of\nSplit-Connection Approaches in Cellular Data Networks,” Proc. Active and Passive\nMeasurement Workshop (Adelaide, Australia, Mar. 2006).\n[Wei 2007] D. X. Wei, C. Jin, S. H. Low, S. Hegde, “FAST TCP: Motivation, Architecture,\nAlgorithms, Performance,” IEEE/ACM Transactions on Networking (2007).\n[Weiser 1991] M. Weiser, “The Computer for the Twenty-First Century,” Scientific\nAmerican (Sept. 1991): 94–10. http://www.ubiq.com/hypertext/weiser/SciAmDraft3.html\n[White 2011] A. White, K. Snow, A. Matthews, F. Monrose, “Hookt on fon-iks: Phonotactic\nReconstruction of Encrypted VoIP Conversations,” IEEE Symposium on Security and\nPrivacy, Oakland, CA, 2011.\n[Wigle.net 2012] Wireless Geographic Logging Engine, http://www.wigle.net\n[Williams 1993] R. Williams, “A Painless Guide to CRC Error Detection Algorithms,” \nhttp://www.ross.net/crc/crcpaper.html\n[Wilson 2011] C. Wilson, H. Ballani, T. Karagiannis, A. Rowstron, “Better Never than Late:\nMeeting Deadlines in Datacenter Networks,” Proc. 2011 ACM SIGCOMM.\n[WiMax Forum 2012] WiMax Forum, http://www.wimaxforum.org\n[Wireshark 2012] Wireshark homepage, http://www.wireshark.org\n[Wischik 2005] D. Wischik, N. McKeown, “Part I: Buffer Sizes for Core Routers,” ACM\nSIGCOMM Computer Communications Review, Vol. 35, No. 3 (July 2005).\n[Woo 1994] T. Woo, R. Bindignavle, S. Su, S. Lam, “SNP: an interface for secure network\nprogramming,” Proc. 1994 Summer USENIX (Boston, MA, June 1994), pp. 45–58.\n[Wood 2012] L. Wood, “Lloyds Satellites Constellations,” http://www.ee.surrey.ac.uk/\nPersonal/L.Wood/constellations/iridium.html\nREFERENCES\n821\n\n[Wu 2005] J. Wu, Z. M. Mao, J. Rexford, J. Wang, “Finding a Needle in a Haystack:\nPinpointing Significant BGP Routing Changes in an IP Network,” Proc. USENIX NSDI (2005).\n[Xanadu 2012] Xanadu Project homepage, http://www.xanadu.com/\n[Xiao 2000] X. Xiao, A. Hannan, B. Bailey, L. Ni, “Traffic Engineering with MPLS in the\nInternet,” IEEE Network (Mar./Apr. 2000).\n[Xie 2008] H. Xie, Y.R. Yang, A. Krishnamurthy, Y. Liu, A. Silberschatz, “P4P: Provider\nPortal for Applications,” Proc. 2008 ACM SIGCOMM (Seattle, WA, Aug. 2008).\n[Yannuzzi 2005] M. Yannuzzi, X. Masip-Bruin, O. Bonaventure, “Open Issues in\nInterdomain Routing: A Survey,” IEEE Network Magazine (Nov./Dec. 2005).\n[Yavatkar 1994] R. Yavatkar, N. Bhagwat, “Improving End-to-End Performance of TCP\nover Mobile Internetworks,” Proc. Mobile 94 Workshop on Mobile Computing Systems and\nApplications (Dec. 1994).\n[YouTube 2009] YouTube 2009, Google container data center tour, 2009.\n[Yu 2006] H. Yu, M. Kaminsky, P. B. Gibbons, and A. Flaxman, “SybilGuard: Defending\nAgainst Sybil Attacks via Social Networks,” Proc. 2006 ACM SIGCOMM (Pisa, Italy, \nSept. 2006).\n[Zegura 1997] E. Zegura, K. Calvert, M. Donahoo, “A Quantitative Comparison of Graph-\nbased Models for Internet Topology,” IEEE/ACM Transactions on Networking, Vol. 5, No. 6,\n(Dec. 1997). See also http://www.cc.gatech.edu/projects/gtim for a software package that\ngenerates networks with a transit-stub structure.\n[Zhang 1993] L. Zhang, S. Deering, D. Estrin, S. Shenker, D. Zappala, “RSVP: A New\nResource Reservation Protocol,” IEEE Network Magazine, Vol. 7, No. 9 (Sept. 1993), \npp. 8–18.\n[Zhang 2007] L. Zhang, “A Retrospective View of NAT,” The IETF Journal, Vol. 3, Issue 2\n(Oct. 2007).\n[Zhang M 2010] M. Zhang, W. John, C. Chen, “Architecture and Download Behavior of\nXunlei: A Measurement-Based Study,” Proc. 2010 Int. Conf. on Educational Technology and\nComputers (ICETC).\n[Zhang X 2102] X. Zhang, Y. Xu, Y. Liu, Z. Guo, Y. Wang, “Profiling Skype Video Calls:\nRate Control and Video Quality,” IEEE INFOCOM (Mar. 2012).\n[Zhao 2004] B. Y. Zhao, L. Huang, J. Stribling, S. C. Rhea, A. D. Joseph, J. Kubiatowicz,\n“Tapestry: A Resilient Global-scale Overlay for Service Deployment,” IEEE Journal on\nSelected Areas in Communications, Vol. 22, No. 1 (Jan. 2004).\n[Zimmerman 1980] H. Zimmerman, “OS1 Reference Model-The ISO Model of\nArchitecture for Open Systems Interconnection,” IEEE Transactions on Communications,\nVol. 28, No. 4 (Apr. 1980), pp. 425–432.\n[Zimmermann 2012] P. Zimmermann, “Why do you need PGP?” http://www.pgpi.org/doc/\nwhypgp/en/\n[Zink 2009] M. Zink, K. Suh, Y. Gu, J. Kurose, “Characteristics of YouTube Network\nTraffic at a Campus Network - Measurements, Models, and Implications,” Computer\nNetworks, Vol. 53, No. 4 (2009), pp. 501–514.\n822\nREFERENCES"
    },
    {
      "chunk_id": "137d9430-24f6-462c-92b0-0e411485735d",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Index",
      "original_titles": [
        "Index"
      ],
      "path": "Index",
      "start_page": 850,
      "end_page": 851,
      "token_count": 739,
      "text": "Index\nA\nAAC (Advanced Audio Coding), 590\nAbramson, Norman, 62, 454, 473\nABR ATM network service, 313\nABR (available bit-rate), 259\nspare available bandwidth advantage,\n267\nAbstract Syntax Notation One. See ASN.1\naccess control and SNMPv3, 777–778\naccess control lists, 734–736\naccess delay, 113\naccess ISP, 32–33\naccess networks, 12–18\ncable Internet access, 14–15, 460–461\ndial-up access, 16\nDSL (digital subscriber line), 13\nenterprises, 16–17\nEthernet, 16–17\nFTTH (fiber to the home), 15\nhome access, 13\nlink-layer switches, 4\nLTE (Long-Term Evolution), 18, 533\noptical distribution network, 15\nsatellite links, 16\n2G, 3G, 4G (generation) wide-area\nwireless networks, 18, 547–554\nwide-area wireless access, 18\nwireless LANs, 17\naccess points. See AP\naccess routers, 492\nAccounting Management, 759, 764\nacknowledgments, 210, 234–237\npiggybacked, 237\nTCP (Transmission Control Protocol),\n235–236, 244\nTelnet, 237–238\nACK (positive acknowledgments), 207,\n240, 208, 212, 217, 235, 257\nACK receipt, 242\nactive optical networks. See AONs\nactive queue management algorithms. See\nAQM algorithms\nadapters, 463–465\nMAC addresses, 471\nrouters, 468\nadaptive congestion control, 201\nadaptive playout delay, 616–618\nadaptive streaming, 600–601\nadaptive HTTP streaming, 593\nadditive-increase, multiplicative-decrease.\nSee AIMD\naddress aggregation, 342\naddress indirection, 406\naddressing\nInternet, 331–363\nprocesses, 90\nAddress Resolution Protocol. See ARP\nAddress Supporting Organization of\nICANN, 345\nad hoc networks, 528\n802.15.1 networks, 544\nwireless hosts, 517\nAdleman, Leonard, 684\nAdvanced Audio Coding. See AAC\nAdvanced Encryption Standard. See AES\nAdvanced Research Projects Agency. See\nARPA\nAES (Advanced Encryption Standard),\n680\nagent advertisement, 566–567\nagent discovery, 565–567\nagent solicitation, 567\naging time, 478\nAH (Authentication Header) protocol, 720\nAIMD (additive-increase, multiplicative-\ndecrease) algorithm, 277–278, 280\nAkamai, 114, 133, 609, 603–604, \n35, 273\n823\n\nalias hostname, 140\nALOHAnet, 62–63, 454, 473\nALOHA protocol, 62–63, 452, 473, 511,\n453, 63, 453–455\nalternating-bit protocols, 214\nAmazon cloud, 608–610\nanalog audio, 590\nanchor foreign agent, 563–564\nanchor MSC, 574\nAndreessen, Marc, 64\nanomaly-based IDSs (intrusion detection\nsystems), 742\nanonymity, 738\nanycast, 356\nAONs (active optical networks), 15\nAP (access points), 517, 528–530,\n538–540\nApache Web server, 99, 156\nAPI (Application Programming Interface),\n6, 89\napplication architecture, 86–88\napplication gateways, 732, 736–738\ndeep packet inspection, 739–740\napplication-layer messages, 51, 54–55,\n186\napplication-layer protocols, 49–50\nDNS (domain name system), 131, 132\nelectronic mail, 98\nFTP (File Transfer Protocol), 51\nHTTP (HyperText Transfer Protocol),\n51, 97\nInternet e-mail application, 97\nproprietary, 97\npublic domain, 97\nsecurity, 705\nSMTP (Simple Mail Transfer\nProtocol), 51, 97, 121\nTelnet, 237\nApplication Programming Interface. See\nAPI\nAQM (active queue management) algo-\nrithms, 329\narea border routers, 389\nARPA (Advanced Research Projects\nAgency), 61, 511\nARP (Address Resolution Protocol),\n465–468, 498\nARPAnet, 454, 473, 511\nARP messages 467, 498\nARP tables, 466–467, 481\nARQ (Automatic Repeat reQuest) proto-\ncols, 207–208, 576–577\nASN.1 (Abstract Syntax Notation One),\n766, 770, 778–782\nASN (autonomous system number),\n394\nAS-PATH attribute, 394\nASs (autonomous systems), 380–383\nassociation, 529–531\nassured forwarding PHB, 651\nasynchronous transfer mode. See ATM\nATM ABR (available bit-rate) congestion\ncontrol, 266–269, 313\nATM (asynchronous transfer mode), 259,\n512\ncomplexity and cost, 470\nservices, 312–313\nmultiple service models, 312\nQ2931b protocol, 654\naudio\nAAC (Advanced Audio Coding), 590\nglitches, 591\nhuman speech, 590\nMP3 (MPEG 1 layer 3), 590\nPCM (pulse code modulation), 590\nproperties, 590–591\nquantization, 590\nremoving jitter at receiver, 614–618\nauthentication\ncryptographic techniques, 675\nend-point, 700–705\n802.11i, 729–730\nMD5, 389\nnetworks, 700–705\nsecret password, 701–703\nSNMPv3, 777\nWEP (Wired Equivalent Privacy), 726\n802.11 wireless LANs, 530\nwireless station, 530\nauthentication key, 692–693\n824\nINDEX"
    },
    {
      "chunk_id": "38ae824a-7e1a-4287-ad20-24fd0323af45",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "A",
      "original_titles": [
        "A"
      ],
      "path": "Index > A",
      "start_page": 850,
      "end_page": 851,
      "token_count": 739,
      "text": "Index\nA\nAAC (Advanced Audio Coding), 590\nAbramson, Norman, 62, 454, 473\nABR ATM network service, 313\nABR (available bit-rate), 259\nspare available bandwidth advantage,\n267\nAbstract Syntax Notation One. See ASN.1\naccess control and SNMPv3, 777–778\naccess control lists, 734–736\naccess delay, 113\naccess ISP, 32–33\naccess networks, 12–18\ncable Internet access, 14–15, 460–461\ndial-up access, 16\nDSL (digital subscriber line), 13\nenterprises, 16–17\nEthernet, 16–17\nFTTH (fiber to the home), 15\nhome access, 13\nlink-layer switches, 4\nLTE (Long-Term Evolution), 18, 533\noptical distribution network, 15\nsatellite links, 16\n2G, 3G, 4G (generation) wide-area\nwireless networks, 18, 547–554\nwide-area wireless access, 18\nwireless LANs, 17\naccess points. See AP\naccess routers, 492\nAccounting Management, 759, 764\nacknowledgments, 210, 234–237\npiggybacked, 237\nTCP (Transmission Control Protocol),\n235–236, 244\nTelnet, 237–238\nACK (positive acknowledgments), 207,\n240, 208, 212, 217, 235, 257\nACK receipt, 242\nactive optical networks. See AONs\nactive queue management algorithms. See\nAQM algorithms\nadapters, 463–465\nMAC addresses, 471\nrouters, 468\nadaptive congestion control, 201\nadaptive playout delay, 616–618\nadaptive streaming, 600–601\nadaptive HTTP streaming, 593\nadditive-increase, multiplicative-decrease.\nSee AIMD\naddress aggregation, 342\naddress indirection, 406\naddressing\nInternet, 331–363\nprocesses, 90\nAddress Resolution Protocol. See ARP\nAddress Supporting Organization of\nICANN, 345\nad hoc networks, 528\n802.15.1 networks, 544\nwireless hosts, 517\nAdleman, Leonard, 684\nAdvanced Audio Coding. See AAC\nAdvanced Encryption Standard. See AES\nAdvanced Research Projects Agency. See\nARPA\nAES (Advanced Encryption Standard),\n680\nagent advertisement, 566–567\nagent discovery, 565–567\nagent solicitation, 567\naging time, 478\nAH (Authentication Header) protocol, 720\nAIMD (additive-increase, multiplicative-\ndecrease) algorithm, 277–278, 280\nAkamai, 114, 133, 609, 603–604, \n35, 273\n823\n\nalias hostname, 140\nALOHAnet, 62–63, 454, 473\nALOHA protocol, 62–63, 452, 473, 511,\n453, 63, 453–455\nalternating-bit protocols, 214\nAmazon cloud, 608–610\nanalog audio, 590\nanchor foreign agent, 563–564\nanchor MSC, 574\nAndreessen, Marc, 64\nanomaly-based IDSs (intrusion detection\nsystems), 742\nanonymity, 738\nanycast, 356\nAONs (active optical networks), 15\nAP (access points), 517, 528–530,\n538–540\nApache Web server, 99, 156\nAPI (Application Programming Interface),\n6, 89\napplication architecture, 86–88\napplication gateways, 732, 736–738\ndeep packet inspection, 739–740\napplication-layer messages, 51, 54–55,\n186\napplication-layer protocols, 49–50\nDNS (domain name system), 131, 132\nelectronic mail, 98\nFTP (File Transfer Protocol), 51\nHTTP (HyperText Transfer Protocol),\n51, 97\nInternet e-mail application, 97\nproprietary, 97\npublic domain, 97\nsecurity, 705\nSMTP (Simple Mail Transfer\nProtocol), 51, 97, 121\nTelnet, 237\nApplication Programming Interface. See\nAPI\nAQM (active queue management) algo-\nrithms, 329\narea border routers, 389\nARPA (Advanced Research Projects\nAgency), 61, 511\nARP (Address Resolution Protocol),\n465–468, 498\nARPAnet, 454, 473, 511\nARP messages 467, 498\nARP tables, 466–467, 481\nARQ (Automatic Repeat reQuest) proto-\ncols, 207–208, 576–577\nASN.1 (Abstract Syntax Notation One),\n766, 770, 778–782\nASN (autonomous system number),\n394\nAS-PATH attribute, 394\nASs (autonomous systems), 380–383\nassociation, 529–531\nassured forwarding PHB, 651\nasynchronous transfer mode. See ATM\nATM ABR (available bit-rate) congestion\ncontrol, 266–269, 313\nATM (asynchronous transfer mode), 259,\n512\ncomplexity and cost, 470\nservices, 312–313\nmultiple service models, 312\nQ2931b protocol, 654\naudio\nAAC (Advanced Audio Coding), 590\nglitches, 591\nhuman speech, 590\nMP3 (MPEG 1 layer 3), 590\nPCM (pulse code modulation), 590\nproperties, 590–591\nquantization, 590\nremoving jitter at receiver, 614–618\nauthentication\ncryptographic techniques, 675\nend-point, 700–705\n802.11i, 729–730\nMD5, 389\nnetworks, 700–705\nsecret password, 701–703\nSNMPv3, 777\nWEP (Wired Equivalent Privacy), 726\n802.11 wireless LANs, 530\nwireless station, 530\nauthentication key, 692–693\n824\nINDEX"
    },
    {
      "chunk_id": "4ba229e5-aa79-4a0f-87b4-c189fd857065",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "B",
      "original_titles": [
        "B"
      ],
      "path": "Index > B",
      "start_page": 852,
      "end_page": 852,
      "token_count": 391,
      "text": "authentication protocol examples, 700–705\nauthoritative DNS servers, 134–137, 499\nhostnames, 139\nIP addresses, 142\nnames, 142\nAutomatic Repeat reQuest protocols. See\nARQ\nautonomous system number. See ASN\nautonomous systems. See ASs\navailable bit-rate. See ABR\naverage throughput, 44\nB\nbackbone area, 389\nbandwidth, 29, 281\nguaranteed minimal, 311\nlink-level allocation, 639–640\nuse-it-or-lose-it resource, 640\nvideo, 588–589, 594\nbandwidth flooding, 57\nbandwidth provisioning, 635\nbandwidth-sensitive applications, 92\nBaran, Paul, 60\nbase HTML file, 99\nbase station controller. See BSC\nbase stations, 516–518, 528\nhandoff between, 572–574\nbase station system. See BSS\nbase transceiver station. See BTS\nBasic Encoding Rules. See BER\nbasic service set. See BSS\nbeacon frames, 529–530\nBellman-Ford equation, 371–372\nBellovin, Steven M., 753–754\nBER (Basic Encoding Rules), 780\nBER (bit error rate), 520–521\nBerners-Lee, Tim, 64\nbest-effort delivery service, 190\nbest-effort networks, 634–636\nbest-effort service, 190, 311–312,\n612–614, 633–634\nBGP (Border Gateway Protocol), \n390–399, 498–499\nASN (autonomous system number), 394\nattributes, 394, 395\nBGP peers, 391\nBGP sessions, 393\ncomplexity, 391, 393\nDV (distance-vector) algorithm, 374\neBGP (external BGP) session, 393–395\nelimination rules for routes, 396\niBGP (internal BGP) session, 393–395\ninter-AS routing protocols, 390–391,\n393–399\npeers, 391\nprefix bits, 342, 344, 393\nroute advertisement, 391, 396–399\nroutes, 394–395\nroute selection, 395–396\nrouting policy, 397–399\nrouting tables, 399\nsession, 393\nTCP connections, 391, 393\nbidirectional data transfer, 205–206\nbinary exponential backoff algorithm,\n457–458\nBIND (Berkeley Internet Name Domain),\n131\nbit error rate. See BER\nbit-level error detection and correction,\n438–445\nBITNET, 63\nbits, 19\npropagation delay, 37–38\nBitTorrent, 86, 149–151\naccepting connections from other\nhosts, 352\nchunks, 149\ndeveloping, 182\nKademlia DHT, 156\nP2P (peer-to-peer) protocol, 145, 182\nswarming data principles, 183\ntrading algorithm, 150\nblades, 490\nblock ciphers, 678–681\nBluetooth, 518, 544–545\nBoggs, David, 470, 473\nBorder Gateway Protocol. See BGP\nborder routers, 491\nbotnet, 56\nINDEX\n825"
    },
    {
      "chunk_id": "067bcb26-2f9e-4fff-8255-79b57ca29859",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "C",
      "original_titles": [
        "C"
      ],
      "path": "Index > C",
      "start_page": 853,
      "end_page": 855,
      "token_count": 1167,
      "text": "bottleneck link, 45, 279–280\nbroadcasting (link layer), 433, 464\nchannels, 433, 446\nchannel propagation delay, 456\nframe, 467\nlinks, 445, 475\nbroadcasting (network layer)\nbroadcast routing algorithms, 405\nbroadcast storm, 401\nflooding, 401–403\nN-way-unicast, 400–401\nsequence number, 401\nspanning-tree broadcast, 403–405\nbrowsers, 101–102\nclient processes, 88\nHTTP requests directed to Web cache,\n110–111\nBSC (base station controller), 550\nBSS (base station system), 550\nBSS (basic service set), 527–528\nMAC address, 539\nmobility among, 541–542\nBTS (base transceiver station), 548–549\nbuffered distributors, 475\nbuffering\npackets, 218\nstreaming video, 592\nbuffers\npacket loss, 25\nswitch output interfaces, 476\nTCP connection, 233\nburst size, 646\nbursty traffic, 60\nbus, switching via, 326\nC\ncable Internet access, 14–15\nDOCSIS protocol, 52, 460\nlink-layer protocols, 460–461\ncable modems, 15, 20, 460–461\ncable modem termination system. See\nCMTS\nCA (Certification Authority), 697–699\ncertifying public keys, 708\ncall admission, 653–654\ncall setup protocol, 654\ncanonical hostnames, 131–132, 140\ncare-of-address. See COA\ncarrier sense multiple access protocol. See\nCSMA protocol\ncarrier sense multiple access with colli-\nsion detection. See CSMA/CD\ncarrier sensing, 454–456\nCBC (cipher-block chaining), 681–682\nCBR (constant bit rate) ATM network\nservice, 312\nCDMA (code division multiple access),\n522–526\nCDNs (Content Distribution Networks),\n588, 603–608\naccess networks of ISPs (Internet\nService Providers), 603–604\ncluster selection strategy, 606–608\ndata centers, 604\ndelay and loss performance\nmeasurements, 606\nDNS intercept/redirect, 604–605\nIP anycast, 607–608\nNetflix, 608\noperation, 604–605\nreplicating content across clusters, \n604\ncell phones\ngrowth of, 513\nInternet access, 65, 546–554\ncellular networks\narchitecture, 547–550\ncells, 548–549\nhandoffs in GSM, 572–574\nmanaging mobility, 570–575\nrouting calls to cellular user, 571–572\n2G, 3G, 4G networks, 547–554\nCerf, Vinton G., 62, 231, 431–432, 511\nCERT Coordination Center, 674\ncertificates, 698–699, 713–714\nCertification Authority. See CA\nchannel numbers, 529\nchannel partitioning protocols, 447\nCDMA (code division multiple access)\nprotocol, 449, 522–526\n826\nINDEX\n\nFDM (frequency-division multiplex-\ning), 448\nTDM (time-division multiplexing), \n448\nchannels\nwith bit errors and reliable data trans-\nfer, 207–212\n802.11 wireless LANs, 529–531\nlosing packets, 212–215\npropagation delay, 456\nchecksums, 203, 442\nACK/NAK packets, 210\ncalculation, 203\nInternet, 203\npoor cryptographic hash function, 690\nTCP (Transmission Control Protocol),\n334\nUDP (User Datagram Protocol),\n202–204, 334\nchipping rate, 522\nchoke packet, 266\nchosen-plaintext attack, 678\nchunks, 149\ndelaying playout, 615–618\nCI (congestion indication) bit, 268–269\nCIDR (Classless Interdomain Routing),\n342, 344, 496\ncipher-block chaining. See CBC\nciphertext, 675, 676\nciphertext-only attack, 677\ncircuit-switched networks\nend-to-end connection, 28\nmultiplexing, 28–30\nversus packet switching, 30–31\nreserving time slots, 30–31\nsending packets, 28\ntelephone networks, 27\ncircuit-switched routing algorithm, 379\nCisco Systems, 65, 323, 732, 740\nrouters and switches, 325, 326\ndominating network core, 323\nClark, Dave, 303, 585\nclassful addressing, 344\nClassless Interdomain Routing. See CIDR\ncleartext, 675\nClear to Send control frame. See CTS con-\ntrol frame\nclient application buffer, 597–598\nclient buffering, 594–595\nclients, 10, 86, 88, 89, 156\ninitiating contact with server, 163\nHTTP, 198\nIP addresses, 162\nmatching received replies with sent\nqueries, 140\nport number, 162\nprefetching video, 597\nreceiving and processing packets from,\n162\nTCP socket creation, 163\nWeb caches as, 111\nclient-server application\ndeveloping, 157–168\nTCP, 157\nUDP, 157\nwell-known port number, 157\nclient side socket interface, 99\nclient socket, 160, 163, 165\ncloud applications and data center \nnetworking, 490–495\ncluster selection strategy, 606–608\nCMTS (cable modem termination \nsystem), 15, 460\nCNAME record, 140\nCOA (care-of-address), 559, 565\ncoaxial cable, 20, 474\ncode division multiple access. See CDMA\ncollision detection, 454–455\ncollisions, switch elimination, 479\nComcast, 601, 763–764\ncommercial ISPs (Internet Service\nProviders), 64\ncommunication satellite, 21\ncomputer networking\nhistory of, 60–66\ncomputer networks, 2\nconditional GET, 114–116\nconfidentiality, 672–673, 706–707, 718,\n720, 724\nConfiguration Management, 759, 764\nINDEX\n827\n\ncongestion\ncauses and costs, 259–265\ndropping packets, 265\nlarge queuing delays, 261\ntransmission capacity wasted, 265\nunneeded retransmissions by sender,\n263\ncongestion avoidance, 274–276\ncongestion control, 190, 240, 250, 596\nABR (available bit-rate) service, 259\nAIMD (additive-increase, \nmultiplicative-decrease), 277–278\napproaches, 265–266\nATM (asynchronous transfer mode)\nnetworks, 259\nend-to-end, 266, 269\nnetwork-assisted, 266, 269\nprinciples, 259–269\nsource quench message, 353\nTCP (Transmission Control Protocol),\n269–272, 274–283\nUDP (User Datagram Protocol), 282\ncongestion indication bit. See CI bit\ncongestion window, 269–270, 272–277,\n282\nconnection-establishment request, 195\nconnectionless service, 313\nconnectionless transport and UDP (User\nDatagram Protocol), 198–204\nconnection-oriented service, 94, 313–314\nconnection-oriented transport and TCP\n(Transmission Control Protocol),\n230–258\nconnection replay attack, 717\nconnections, persistent and non-\npersistent, 100–103\nconnection setup, 310\nconnection sockets and processes, 198\nconnection state, 231, 315\nconstant bit rate ATM network service.\nSee CBR ATM network service\ncontainer-based MDCs (modular data cen-\nters), 494\nContent Distribution Networks. See CDNs\ncontent provider networks, 34–35\ncontinuous playout, 591–592\ncontrol connection, 117\ncontrolled flooding, 401–403\ncontrol plane software, 331\nconversational applications, 592–593\ncookies, 108–110\ncorrespondent, 557, 561, 563\ncorrupted ACKs or NAKs, 209–210\ncountdown timer, 214\ncoverage area, 515\nCRC (cyclic redundancy check) codes,\n443–445\ncrossbar switch, 326\ncryptographic hash functions, 689–691\ncryptographic techniques, 675\ncryptography, 675–688\nciphertext, 675–676\ncleartext, 675\nconfidentiality, 675\ncryptographic hash functions,\n689–691\ndecryption algorithm, 676\nencryption algorithm, 675–676\nkeys, 676\nplaintext, 675–676\npublic key encryption, 683–688\nsymmetric key cryptography, \n676–682\nCSMA (carrier sense multiple access),\n453–456\ncarrier sensing, 455–456\ncollisions, 455–456\nCSMA/CA (carrier sense multiple access\nwith collision avoidance), 526,\n531–537\nCSMA/CD (carrier sense multiple access\nwith collision detection), 456–459\ncollision detection, 456–459\nefficiency, 458–459\nEthernet, 475\nCSNET (computer science network), 63\nCTS (Clear to Send) control frame,\n535–537\ncumulative acknowledgments, 222, 236,\n243\n828\nINDEX"
    },
    {
      "chunk_id": "47ced8ae-d09d-4656-8c19-ad5a3b23fdb9",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "D",
      "original_titles": [
        "D"
      ],
      "path": "Index > D",
      "start_page": 856,
      "end_page": 857,
      "token_count": 772,
      "text": "customer ISPs, 34\ncustomer-provider relationship, 33\ncyclic redundancy check codes. See CRC\ncodes\nD\nDARPA (US Department of Defense\nAdvanced Research Projects\nAgency), 62, 431\nDASH (Dynamic Adaptive Streaming over\nHTTP), 600–601\ndatabases, implementing in P2P network,\n151–156\ndata center networks, 490–495\ndata centers, 86\nborder routers, 491\nCDNs (Content Distribution\nNetworks), 604\ncontainer-based MDCs (modular data\ncenters), 494\ncosts, 490\ndata center networks, 490–495\nhierarchy of routers and switches,\n492–493\nhosts, 490\ninterconnection architectures and net-\nwork protocols, 493\nInternet services, 86\nload balancing, 491–492\nservers, 11\ndata definition language, 765\ndata encryption algorithm and WEP (Wired\nEquivalent Privacy), 726–728\nData Encryption Standard. See DES\nDATA frame, 535\ndatagram networks, 313, 317–320\ndatagrams, 55, 189\ndetecting bit errors, 334\nextending IP header, 335\nfragmentation, 335–338\nInternet Protocol (IP) v4 format, 332–338\nlabeling, 487\nlength, 334\nmoving between hosts, 51–52\npassed to transport layer, 337\nreassembling in end systems, 336\nsending off subnet, 468–469\nsource and destination IP addresses,\n334–336\nTOS (type of service) bits, 333\ntransport-layer segments, 242\nTTL (time-to-live) field, 334\ndatagram service and network layer, 364\ndata integrity, 363, 712\ndata link layer, 50, 53\nData-Over-Cable Service Interface\nSpecifications. See DOCSIS\ndata plane and hardware, 331\nDATA SMTP command, 123, 124\ndata transfer, 713\nbidirectional, 205–206\nreliable, 91, 204–230\nSSL (Secure Sockets Layer), 714–715\nunidirectional, 205\nunreliable, 206\nVC (virtual-circuit) networks, 316\nDDoS (distributed denial-of-service)\nattacks, 56–58, 143–144\nDECnet architecture, 266\ndecentralized routing algorithm, 366\ndecryption algorithm, 676, 283\ndeep packet inspection, 739–741\nDeering, Steve, 584\ndefault name server, 136\ndefault router, 364\ndelaying playout, 615–618\ndelays\ncomparing transmission and propaga-\ntion delays, 38–39\nend systems, 43–44\nend-to-end, 42–44\nnodal processing, 36\npacketization, 44\npackets, 36\npacket-switched networks, 35–39\nprocessing, 36–37\npropagation, 36–37\nqueuing, 36–37, 39–42\ntotal nodal, 36\ntransmission, 36–37\nINDEX\n829\n\ndelay-sensitive, 592\ndemilitarized zone. See DMZ\ndemultiplexing, 191–198\nconnectionless, 193–194\nconnection-oriented, 194–197\ndenial-of-service attacks. See DoS\nattacks\nDES (Data Encryption Standard), 680\ndestination port numbers, 192, 234\ndestination router, 364\nDHCP (Dynamic Host Configuration\nProtocol), 345–349\nIP addresses dynamically assigned, \n630\nDHCP request message, 348, 495–496\nDHCP servers, 346–347, 350, 495, 497\nDHTs (distributed hash tables), 145,\n151–156\ndial-up access, 16\nDIAMETER protocol, 530, 730\ndifferentiated service, 634\nDiffie-Hellman algorithm, 683, 687, 725\nDiffserv, 648–652\nDIFS (Distributed Inter-frame Space), 533\ndigital audio, 590\ndigital signatures, 707\ncompared with MAC (message authen-\ntication code), 696–697\nhash functions, 695–696\nmessage integrity, 695\noverheads of encryption and decryp-\ntion, 695\nPGP (Pretty Good Privacy), 710\npublic key certification, 697–699\npublic-key cryptography, 693–694\ndigital subscriber line. See DSL\ndigital subscriber line access multiplexer.\nSee DSLAM\nDijkstra’s least-cost path algorithm,\n367–368, 388, 390\nOSPF (Open-Shortest Path First), \n388\ndimensioning best-effort networks,\n634–636\ndirectory service, 97\ndirect routing, 563–564\nDirect Sequence Wideband CDMA. See\nDS-WCDMA\ndistance vector, 372\ndistance-vector algorithm. See DV\nalgorithm\nDistance-Vector Multicast Routing\nProtocol. See DVMRP\ndistributed applications, 5–6\ndistributed attacks. See DDoS attacks\nDistributed Inter-frame Space. See DIFS\nDMZ (demilitarized zone), 741\nDNS\nDNS database, 142, 144\nDNS (domain name system), 51, 63, 98,\n130–144, 497\nadditional delay to Internet applica-\ntions, 131\napplication-layer protocols, 131–132\nattacks and vulnerabilities, 143–144\ncaching, 138–139\nCDN requests, 604–605\nclient-server paradigm, 132\nDDoS attack against targeted host,\n143–144\ndistributed and hierarchical database,\n131, 134–138\nhost aliasing, 131–132\nhostname-to-IP-address translation\nservice, 133–139\nimproving delay performance, 138\niterative queries, 137–138\nload distribution, 132–133\nobtaining presence in, 392\noperational overview, 133–139\nquerying and replaying messages, 133\nquery messages, 136, 140–142, 497\nrecursive queries, 137–138\nreply messages, 136, 140–142, 499\nrotation, 132–133\nRRs (resource records), 139–140,\n498–499\nsecure communication, 674\nservers, 134\nservices provided by, 131–133\n830\nINDEX"
    },
    {
      "chunk_id": "1fa68a7d-6763-4b6f-a5de-54c20da5fd47",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "E",
      "original_titles": [
        "E"
      ],
      "path": "Index > E",
      "start_page": 858,
      "end_page": 859,
      "token_count": 734,
      "text": "translating hostnames to IP addresses,\n133–139\nunderlying end-to-end transport \nprotocol, 132\nUPDATE option, 144\nDNS servers, 131–135, 495\nauthoritative DNS servers, 134,\n135–136, 140\nBIND (Berkeley Internet Name\nDomain), 131\nDDoS bandwidth-flooding attack, 143\ndiscarding cached information, 139\ndistant centralized database, 134\nhierarchy, 136\nlocal DNS server, 136–137\nrecursion, 140\nroot DNS servers, 134, 135\nsingle point of failure, 133\nTLD (top-level domain) DNS servers,\n134\nDOCSIS (Data-Over-Cable Service\nInterface Specifications), 15, 52,\n460\ndomain name system. See DNS\nDoS (denial-of-service) attacks, 57, 735,\n740\nfragmentation and, 338\nSYN flood attack, 253, 257\ndotted-decimal notation, 339\ndropping packets, 265\ndrop-tail, 329\nDSLAM (digital subscriber line access\nmultiplexer), 13–14\nDSL (digital subscriber line), 13–14, 20\nDS-WCDMA (Direct Sequence Wideband\nCDMA), 552\ndual-stack approach, 359\nduplicate ACKs, 211, 247–248\nduplicate data packets, 210, 213–214\nDV (distance-vector) algorithm, 366,\n371–380\nDVMRP (Distance-Vector Multicast\nRouting Protocol), 411–412\nDynamic Adaptive Streaming over HTTP.\nSee DASH\nDynamic Host Configuration Protocol. See\nDHCP\ndynamic routing algorithm, 366\nE\nEAP (Extensible Authentication Protocol),\n729–730\neavesdropping, 673\neBGP (external BGP), 393–395, 397\nEC2, 66\nedge router, 12\nEFCI (explicit forward congestion\nindication) bit, 268\n802.15.1, 544–545\n802.11i, 728–731\n802.11n wireless network, 527\n802.1Q frames, 484–486\n802.11 wireless LANs, 17, 515, 518,\n526–546, 548\naddress fields, 538–540\nAPs (access points), 517, 528\narchitecture, 527–531\nassociation, 529–531\nauthentication, 530, 728–731\nbase station, 528\nBSS (basic service set), 527–528\nchannels, 529–531\ncollision detection, 532\nCSMA/CA (CSMA with collision\navoidance), 526, 531–537\ndata rates, 526–527\nfrequency band, 526\nlink-layer frames, 526\nMAC addresses, 528\nMAC protocols, 531–537\nreducing transmission rate, 526\ntransmitting frames, 532\nelastic applications, 92\nelectromagnetic noise, 519\ne-mail, 64, 86, 97, 118–130\naccess protocols, 125–126\napplication-layer protocols, 98\nsecurity, 705–711\nWeb-based e-mail system, 120\nencapsulation, 53–55, 565–567\nINDEX\n831\n\nencapsulation/decapsulation and mobile\nIP, 565\nEncapsulation Security Payload \nprotocol. See ESP\nencryption, 712\ncryptography, 675–688\nSNMPv3, 777\nend-point authentication, 59, 673\nend systems, 2, 4–6, 10\nAPI (Application Programming\nInterface), 6\napplications running on, 6\ncommunication links, 4\nconnecting, 5\ndatagram reassembly, 336\ndelays, 43–44\nhosts, 10\nIP addresses, 26\nprocesses, 84, 88\nTCP protocol, 231\ntransport-layer protocols, 188\nend-to-end congestion control, 266\nend-to-end delay, 42–43, 613\nDiffserv, 652\nTraceroute program, 42–43\nend-to-end principle, 203\nend-to-end throughput, 44–47\nenterprises and access networks, 16–17\nEPC (Evolved Packet Core), 553\nER (explicit rate) field, 269\nerror checking\nlink-layer protocols, 203\nparity checks, 440–442\ntransport layer, 203\nerror concealment, 620–621\nerror detection\nARQ (Automatic Repeat reQuest) pro-\ntocols, 208\nchecksumming methods, 203, \n442–443\nCRC (cyclic redundancy check) codes,\n443–445\nparity bits, 440–442\ntwo-dimensional parity scheme,\n441–442\nESP (Encapsulation Security Payload),\n720, 723\nESTABLISHED state, 254\nEstrin, Deborah, 303, 584–585\nEthernet, 16–17, 52, 63, 437, 445,\n469–476\nbinary exponential backoff algorithm,\n457–458\nbroadcast link, 475\ncarrier-sensing random access,\n531–532\ncollision-detection algorithm, 532\nCSMA/CD protocol, 474–475\nframe format, 474\nhome networks, 17\nlink-layer and physical-layer specifica-\ntion, 474\nlocal area networking, 470\nMSS (maximum segment size), 233\nMTU (maximum transmission unit),\n471\nmultiplexing network-layer protocols,\n471–472\nphysical-layer protocols, 52, 473–474\nstandardized, 474\nstore-and-forward packet switching, 475\nswitch-based star topology, 475\nswitches, 17, 470, 475, 496\n10BASE-2, 473–474\n10BASE-T, 473–474\n10GBASE-T, 474–475\nEWMA (exponential weighted moving\naverage), 240\nexpedited forwarding PHB, 651\nexplicit forward congestion indication bit.\nSee EFCI\nexplicit rate field. See ER (explicit rate)\nfield\nexponential weighted moving average. See\nEWMA\nextended FSM (finite-state machine), 220,\n223\nExtensible Authentication Protocol. See\nEAP\nexternal BGP session. See eBGP session\n832\nINDEX"
    },
    {
      "chunk_id": "a77bac4d-b91c-4256-9d8d-bccd961cba45",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "F",
      "original_titles": [
        "F"
      ],
      "path": "Index > F",
      "start_page": 860,
      "end_page": 860,
      "token_count": 388,
      "text": "F\nfairness\nAIMD algorithm, 280\ncongestion-control mechanism, 280\nparallel TCP connections, 282\nTCP (Transmission Control Protocol),\n279–282\nTDM (time-division multiplexing), 448\nUDP (User Datagram Protocol), 282\nFault Management, 758–759, 764\nFCFS (first-come-first-served) scheduling,\n329, 641\nFDDI (fiber distributed data interface),\n460, 470\nFDM (frequency-division multiplexing),\n28–30, 448, 549\nFDM/TDM systems, 549–550\nFEC (forward error correction), 442, 613,\n618–619\nFHSS (frequency-hopping spread \nspectrum), 544\nfiber distributed data interface protocol. See\nFDDI\nfiber optics, 20–21\n100 Mbps Ethernet, 475\nfiber to the home. See FTTH\nFIFO (first-in-first-out), 637–638, 641–642\nfile sharing, 86\nfile transfer, 97, 116–118\nfiltering, 476–477\nFIN bit, 235, 254\nfinite-state machine. See FSM\nFIOS service, 15–16\nfirewalls, 673, 730–738\napplication gateways, 732, 736–738\nauthorized traffic, 731\nblocking packets, 355, 596\nconnection table, 732–736\nfiltering, 733–736\nmalicious packet attacks, 355\nfirst-come-first-served scheduling.\nSee FCFS\nfirst-in-first-out order. See FIFO\nfixed-length labels, 487\nfixed playout delay, 615\nflag field, 235, 334–336\nflooding, 401–405\nflow control, 240\ndifferent from congestion control, 220\nTCP, 250\nflow (of packets), 311, 357\nforeign address, 559\nforeign agent, 557, 566\nCOA (care-of-address), 567\nmobile IP, 565\nreceiving and decapsulating datagram,\n561, 562\nregistration with home agent, 562,\n568–569\nforeign networks, 557–559\nforwarding, 305, 308–310, 321–322, 322,\n476–477, 649\nforwarding function, 320–321\nforwarding plane, 321\nforwarding tables, 26–27, 308–309,\n317–318, 322–323\nadding entries, 396–397\nconfiguring, 308–309, 364\ndatagram networks, 319\ndirecting datagrams to foreign \nnetwork, 558\nrouting algorithms, 364\nVC networks, 319\n4G systems, 18, 553–554\nfourth-generation of wide-area wireless\nnetworks. See 4G\nfragmentation of datagrams, 334–338\nframes, 52, 433–434, 436\nfrequency-division multiplexing. See FDM\nfrequency-hopping spread spectrum. See\nFHSS\nFSM (finite-state machine), 206–207\nextended, 220, 223\ninitial state, 207\nFTP (File Transfer Protocol), 51, 86,\n97–98, 116–118\nFTTH (fiber to the home), 15–16\nfull-duplex service, 232\nfull-table block ciphers, 679–680\nfully connected topology, 493–494\nINDEX\n833"
    },
    {
      "chunk_id": "31c80c5c-c8ac-4290-8ec9-be60658e5621",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "G",
      "original_titles": [
        "G"
      ],
      "path": "Index > G",
      "start_page": 861,
      "end_page": 861,
      "token_count": 388,
      "text": "G\nGateway GPRS Support Nodes. See\nGGSNs\ngateway routers, 380–381\nimport policy, 395\npacket filtering, 732\nprefixes, 393\nGBN (Go-Back-N) protocol, 218–224\nGeneralized Packet Radio Service. See\nGPRS\ngenerator, 443\ngeographically closest, 606\ngeostationary satellites, 21\nGET method, 104–105, 115\nGGSNs (Gateway GPRS Support Nodes),\n552\nGigabit Ethernet, 475\nglobal routing algorithm, 365–366\nGlobal System for Mobile\nCommunications standards. See\nGSM\nglobal transit ISPs (Internet Service\nProviders), 32–33\nGMSC (Gateway Mobile services\nSwitching Center), 570\nGo-Back-N protocol. See GBN protocol\nGoogle, 34–25, 64–66, 86, 114, 431, \n603, 610\nGPRS (Generalized Packet Radio\nService), 552\ngraphs, 364–365\nground stations, 21\nGSM (Global System for Mobile\nCommunications), 547, 549–550,\n570\nanchor MSC, 574\nBTS (base transceiver station),\n548–549\ncells, 548–549\nencoding audio, 628\nhandoffs, 572–574\nhome network, 570\nhome PLMN (home public land mobile\nnetwork), 570\nindirect routing, 570\nmobility, 576\nrouting cells to mobile user, 571–572\nvisited network, 570\nguaranteed delivery, 311\nguided media, 19\nH\nhandoff, 517–518\nGSM, 572–574\nhandshake, 94, 231–232, 252–253, 713,\n716\nhard guarantee, 634\nhardware data plane, 331\nhardware-implemented protocols, 9\nhash functions, 152, 707, 714\ndigital signatures, 695–696\nHDLC (high-level data link control), 445\nheader checksum, 334\nheader fields, 55\nhead-of-line blocking. See HOL blocking\nHFC (hybrid fiber coax), 14\nhidden terminals, 521, 534–537\nhierarchical routing, 379–383\nhigher-layer protocols and wireless net-\nworks and mobility, 575–577\nhigh-level data link control. See HDLC\nHLR (home location register), 570, 572\nHMAC standard, 692–693\nHOL (head-of-the-line) blocking, 330–331\nhome agents, 557–562, 565–566, 568–569\nhome location register. See HLR\nhome MSC, 574\nhome networks, 13, 557\nEthernet, 17\nGMSC (Gateway Mobile services\nSwitching Center), 570\nGSM (Global System for Mobile\nCommunications), 570\nHLR (home location register), 570\nhome agent, 558–559\nIP addresses, 349–352\nmobile nodes, 559\nPLMN (home public land mobile net-\nwork), 570\nWiFi, 17\n834\nINDEX"
    },
    {
      "chunk_id": "cb0e474f-b946-4fcb-ba89-33ad397c01d0",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "H",
      "original_titles": [
        "H"
      ],
      "path": "Index > H",
      "start_page": 861,
      "end_page": 861,
      "token_count": 388,
      "text": "G\nGateway GPRS Support Nodes. See\nGGSNs\ngateway routers, 380–381\nimport policy, 395\npacket filtering, 732\nprefixes, 393\nGBN (Go-Back-N) protocol, 218–224\nGeneralized Packet Radio Service. See\nGPRS\ngenerator, 443\ngeographically closest, 606\ngeostationary satellites, 21\nGET method, 104–105, 115\nGGSNs (Gateway GPRS Support Nodes),\n552\nGigabit Ethernet, 475\nglobal routing algorithm, 365–366\nGlobal System for Mobile\nCommunications standards. See\nGSM\nglobal transit ISPs (Internet Service\nProviders), 32–33\nGMSC (Gateway Mobile services\nSwitching Center), 570\nGo-Back-N protocol. See GBN protocol\nGoogle, 34–25, 64–66, 86, 114, 431, \n603, 610\nGPRS (Generalized Packet Radio\nService), 552\ngraphs, 364–365\nground stations, 21\nGSM (Global System for Mobile\nCommunications), 547, 549–550,\n570\nanchor MSC, 574\nBTS (base transceiver station),\n548–549\ncells, 548–549\nencoding audio, 628\nhandoffs, 572–574\nhome network, 570\nhome PLMN (home public land mobile\nnetwork), 570\nindirect routing, 570\nmobility, 576\nrouting cells to mobile user, 571–572\nvisited network, 570\nguaranteed delivery, 311\nguided media, 19\nH\nhandoff, 517–518\nGSM, 572–574\nhandshake, 94, 231–232, 252–253, 713,\n716\nhard guarantee, 634\nhardware data plane, 331\nhardware-implemented protocols, 9\nhash functions, 152, 707, 714\ndigital signatures, 695–696\nHDLC (high-level data link control), 445\nheader checksum, 334\nheader fields, 55\nhead-of-line blocking. See HOL blocking\nHFC (hybrid fiber coax), 14\nhidden terminals, 521, 534–537\nhierarchical routing, 379–383\nhigher-layer protocols and wireless net-\nworks and mobility, 575–577\nhigh-level data link control. See HDLC\nHLR (home location register), 570, 572\nHMAC standard, 692–693\nHOL (head-of-the-line) blocking, 330–331\nhome agents, 557–562, 565–566, 568–569\nhome location register. See HLR\nhome MSC, 574\nhome networks, 13, 557\nEthernet, 17\nGMSC (Gateway Mobile services\nSwitching Center), 570\nGSM (Global System for Mobile\nCommunications), 570\nHLR (home location register), 570\nhome agent, 558–559\nIP addresses, 349–352\nmobile nodes, 559\nPLMN (home public land mobile net-\nwork), 570\nWiFi, 17\n834\nINDEX"
    },
    {
      "chunk_id": "834a1682-2984-482a-9488-1ae900c50643",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "I",
      "original_titles": [
        "I"
      ],
      "path": "Index > I",
      "start_page": 862,
      "end_page": 864,
      "token_count": 1196,
      "text": "hostnames, 130\nalias, 132\ncanonical, 131\ntranslating to IP addresses, 131, 132,\n133–139\nhosts, 2, 4, 10–11\naliasing, 131–132, 140\nARP table, 466\nassigning IP addresses to, 345–349\ncanonical hostname, 140\nchanging base station, 517–518\nconnected into network, 338\ndata centers, 490\ndefault router, 364\nDHCP discovery message, 530\ndial-up modem connection, 486\nhostnames, 130–131\ninterfaces, 338\nIP addresses, 90, 130–131, 190, \n465\nlink-layer addresses, 462–463\nload balancer, 491–492\nlocal DNS server, 136\nMAC addresses, 465\nmonitoring, 756\nmoving datagrams between, 51–52\nnetwork-layer addresses, 462, 465\nnetwork-layer protocols, 471–472\nprocess-to-process delivery service,\n191–198\nstoring routing information, 380\nwireless, 514\nhot-potato routing, 382\nHSP (High Speed Packet Access), \n552\nHTML, 64, 100–101\nHTTP (HyperText Transfer Protocol), 51,\n97–100, 127\nclient program, 98\nconditional GET, 114–116\ncookies, 108–110\nGET request, 103–105, 499–500, 596,\n601, 610\nIf-Modified-Since: header line, 115\nmessage format, 98, 103–108\nnon-persistent connections, \n100–103\npersistent connections, 103, 124\npull protocol, 124\nPOST method, 104–105\nPUT method, 105\nresponse message, 105–108, 124, 500,\n596\nserver program, 98, 100\nSMTP comparison with, 124\nstateless, 108\nSSL security, 712\nstateless protocols, 100, 117\nTCP and, 99–100, 116, 200\nWeb caching, 110–114\nHTTP streaming, 593, 597–600, 611\nprefetching video, 596–597\nhubs, 470\nHulu, streaming stored video, 591\nhuman protocols, 7–8\nhuman speech, 590\nhybrid fiber coax. See HFC\nhypertext, 64\nHyperText Transfer Protocol. See HTTP\nI\niBGP (internal BGP), 393–395, 397\nIBM SNA architecture, 62, 266\nICANN (Internet Corporation for\nAssigned Names and Numbers),\n142, 345\nICMP (Internet Control Message\nProtocol), 306, 353–355, 359\ndatagram, 258\nmessages, 353–354\nIPv6, 359\nIDEA, 710\nidentification field (IP), 336\nIDSs (intrusion detection systems), 355,\n673, 739–742\nIEEE 802.3 CSMA/CD (Ethernet) work-\ning group, 474\nIEEE 802 LAN/MAN Standards\nCommittee, 5\nIEEE managing MAC address space, 464\nINDEX\n835\n\nIETF (Internet Engineering Task Force), 5,\n668\nAddress Lifetime Expectations work-\ning group, 356\nstandards for CAs, 699\nIf-Modified-Since: header line, 115\nIGMP (Internet Group Management\nProtocol), 359, 407–409\nIKE (Internet Key Exchange), 725\nIMAP (Internet Mail Access Protocol),\n127, 129\nIMAP server, 129\nimport policy, 395\nin-band, 117\nindirect routing,\nGSM (Global System for Mobile\nCommunications), 570\nmobile IP standard, 562\nmobile node, 559–562\ntriangle routing problem, 563\ninfrastructure wireless networks, 517, 528\nin-order packet delivery, 311\ninput ports, 320\npacket queues, 327–331\nprocessing, 322–324\nswitching to output ports, 324–326\ninput processing, 322–324\ninstantaneous throughput, 44\ninstant messaging, 65, 83, 632\nIntel 8254x controller, 437\ninteractivity, 591\ninter-AS routing, 390–391, 393–399\ninter-AS routing protocols, 382, 398\nBGP (Border Gateway Protocol),\n390–391, 393–399\ninterconnection network, 326\ninterfaces\nassigning IP addresses to, 345–349\nIP addresses, 338–339\nrouters, 468\ninterior gateway protocols, 384\ninterleaving, 618–620\nIntermediate-System-to-Intermediate-\nSystem routing algorithm. See IS-IS\nrouting algorithm\ninternal BGP session. See iBGP session\nInternational Organization for\nStandardization. See ISO\nInternet\naccess networks, 12–18\naddress assignment strategy, 342\naddressing, 331–363\nbest-effort service model, 190,\n311–313, 612–614, 636\ncellular access, 546–554\ncommercialization, 64\nconnecting end systems, 5\nconnectivity, 392\ndelivering data, 6\ndifficulty making changes to, 303\ndistributed applications, 5–6\ne-mail, 64, 118–130, 705–711\nend systems, 2, 4, 10\nflag day, 359\nforwarding, 331–363\nglobal traffic, 4\nhistory of, 60–66\nhosts, 2, 4\ninter-AS routing, 390–391, 393–399\ninter-domain protocol, 498\nintra-AS routing, 384–390\nISPs (Internet Service Providers), 4,\n32–35\nIXPs (Internet exchange points), 34\nlayered architecture, 47–53\nlink-layer overview, 434–438\nlink-layer switches, 4, 476–482\nmobile devices, 550–552\nmulticast routing, 411–412\nnetwork layer, 332\nnetwork of networks, 3–5, 32–35\nobtaining presence on, 392–393\npacket forwarding, 26–27\nprotocols, 5, 9\nprotocol stack, 50\nroot DNS servers, 135\nrouters, 4\nrouting protocols, 27, 383–399\nservice classes, 636–640\nservice provider private networks, 66\nservices description, 5–7\nstandards, 5\n836\nINDEX\n\n3G and 4G cellular networks, 65,\n547–554\nTLD (top-level domain) servers, 135\ntransport layer overview, 189–191\ntransport services, 93–96\nWeb, 64, 98–100, 111\nwireless access, 17–18\nInternet API, 6, 156–158\nInternet applications, 83, 100\napplication-layer protocols, 96–97\nInternet checksum, 203, 334, 442\nInternet commerce, 64, 86\ncloud, 66\nInternet Control Message Protocol. See\nICMP\nInternet Corporation for Assigned Names\nand Numbers. See ICANN\nInternet Engineering Task Force. See IETF\nInternet exchange points. See IXPs\nInternet Group Management Protocol. See\nIGMP\nInternet hosts. See hosts\nInternet Key Exchange. See IKE\nInternet Mail Access Protocol. See IMAP\nInternet phone application, 612–614,\n618–621\nInternet Protocol. See IP\nInternet registrar, 392\nInternet routing protocols, 353\nInternet Service Providers. See ISPs\nInternet-Standard Management\nFramework, 764–778\nInternet telephony, 87, 612–623\nSkype, 621–623\ninternetting, 62\nInternet transport protocols, 95\nintra-AS (autonomous system) routing\nprotocols, 380, 381, 397–398\nintra-AS routing, 388–390\nintra-domain routing and DNS servers,\n498–499\nintrusion detection, 758\nintrusion prevention system. See IPS\nintrusion detection systems. See IDSs\nIP addresses, 26, 90, 130–131\nadapters, 465\naddress aggregation, 342\nallocating, 345\nauthoritative DNS server, 142\nblock, 345\nCIDR (Classless Interdomain Routing),\n342, 344\nClassful addressing, 344\ndestination, 334–335\ndistinguishing among devices, 344\nDNS, 130–144\ndotted-decimal notation, 339\nglobally unique, 339\nhierarchical structure, 130, 464\nhome networks, 349–352\nincreasing size of, 356\ninterfaces, 338–339\nIP broadcast, 344\nlease time, 347\nlocal DNS server, 136\nmobility, 556–559\nobtaining, 495\nprefix, 342, 344\nprivate addresses, 349–350\nrange, 392\nrouters, 465, 468\nsetting up call to known, 627–629\nsource, 334–335\nsubnets, 340, 342, 344\ntranslating hostnames to, 131–139\nIP anycast, 607–608\nIP broadcast address 344, 347\nIP datagrams, 233, 496\nattacks and, 355\nencryption of payloads, 363\nEthernet frames, 471\nfragmentation, 335–338\nMTU (maximum transmission unit), 335\nsecurity, 718\ntransport-layer protocols, 334\nIP header, 202, 335\nIP-in-IP encapsulation, 567\nIP (Internet Protocol), 5, 51–52, 190, 306,\n331–363, 353, 387\nbest-effort delivery service, 190,\n311–312, 612–614, 633–634\nconfining Internet’s development, 512\nINDEX\n837"
    },
    {
      "chunk_id": "f6a5c5b7-1503-4984-9c19-386fa08b08d2",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "J",
      "original_titles": [
        "J"
      ],
      "path": "Index > J",
      "start_page": 865,
      "end_page": 865,
      "token_count": 392,
      "text": "IP (Internet Protocol) (continued)\ndatagrams, 242, 332–338\nde facto standard for internetworking,\n512\nInternet addressing and forwarding,\n331–363\nmulticasting, 593\nnetwork layer, 305–307, 332\npervasiveness, 431\nrouters, 53\nsecurity, 362–363\nseparation from TCP, 62\nIP multicast, 412\nIPng (Next Generation IP), 356\nIP protocol version 4. See IPv4\nIP protocol version 6. See IPv6\nIPsec, 362, 72–724\nIPS (intrusion prevention system), 355, 740\nIP spoofing, 59–60, 701\nIP subnet, 541–542\nIP tunnels, 303\nIPTV, 87\nIPv4 addresses, 334, 356\nIPv6 addresses, 356, 512\nIPv4 addressing, 338–352\nIPv4 datagrams, 332–335\nformat, 306\nIPv6 datagrams, 356–359\nIPv4 header, 636–637\nIPv6 header, 359\nIPv4 (IP protocol version 4), 160, 165,\n331, 336\nIPsec, 362\ntransitioning to IPv6, 359–362\nIPv6 (IP protocol version 6), 306, 332,\n356–362\ndual-stack approach, 360\nflow labeling and priority, 357\nIP addresses, 351, 356\ntransitioning from IPv4, 359–362\ntunneling, 360–361\nIPv6/IPv4 nodes, 359–360\nIS-IS (Intermediate-System-to-\nIntermediate-System), 405, 498\nIS-IS protocol, 384\nISO (International Organization for\nStandardization), 52, 758, 770\nISPs (Internet Service Providers), 32–35\naccess networks, 12–18\nASs (autonomous systems), 383\ncustomer-provider relationship, 33\nend-to-end service, 651–652\nglobal transit, 32\nlow-tier and upper-tier, 5\nmulti-homing, 33–34\nnaming and addressing conventions, 5\nobtaining set of addresses from, 345\npeer, 34\npeering agreements, 399\niterative queries, 137–138\nITU-T (International Telecommunication\nUnion), 699, 770\nIXPs (Internet exchange points), 33, 34\nJ\nJacobson, Van, 302, 303, 584\nJava and client-server programming, 157\njitter, 614–618\nK\nKademlia DHT, 156\nKahn, Robert, 62, 231, 431–432, 511\nKanKan, 87, 588, 611–612\nkey derivation, 713–714\nkey management, 726\nKleinrock, Leonard, 60–62, 80–82, 431, 511\nknown-plaintext attack, 677\nL\nlabel-switched routers, 488\nLam, Simon S., 511–512\nLANs (local area networks), 16\nbroadcast address, 464\nconfigured hierarchically, 482–483\nEthernet, 470\nhub-based star topology, 470\nMAC address, 463\nswitches, 17, 470, 475, 483, 496\nVLANs (virtual local area networks),\n482–486\n838\nINDEX"
    },
    {
      "chunk_id": "8b6fcba9-3649-49dd-afe3-f62adc8f90ce",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "K",
      "original_titles": [
        "K"
      ],
      "path": "Index > K",
      "start_page": 865,
      "end_page": 865,
      "token_count": 392,
      "text": "IP (Internet Protocol) (continued)\ndatagrams, 242, 332–338\nde facto standard for internetworking,\n512\nInternet addressing and forwarding,\n331–363\nmulticasting, 593\nnetwork layer, 305–307, 332\npervasiveness, 431\nrouters, 53\nsecurity, 362–363\nseparation from TCP, 62\nIP multicast, 412\nIPng (Next Generation IP), 356\nIP protocol version 4. See IPv4\nIP protocol version 6. See IPv6\nIPsec, 362, 72–724\nIPS (intrusion prevention system), 355, 740\nIP spoofing, 59–60, 701\nIP subnet, 541–542\nIP tunnels, 303\nIPTV, 87\nIPv4 addresses, 334, 356\nIPv6 addresses, 356, 512\nIPv4 addressing, 338–352\nIPv4 datagrams, 332–335\nformat, 306\nIPv6 datagrams, 356–359\nIPv4 header, 636–637\nIPv6 header, 359\nIPv4 (IP protocol version 4), 160, 165,\n331, 336\nIPsec, 362\ntransitioning to IPv6, 359–362\nIPv6 (IP protocol version 6), 306, 332,\n356–362\ndual-stack approach, 360\nflow labeling and priority, 357\nIP addresses, 351, 356\ntransitioning from IPv4, 359–362\ntunneling, 360–361\nIPv6/IPv4 nodes, 359–360\nIS-IS (Intermediate-System-to-\nIntermediate-System), 405, 498\nIS-IS protocol, 384\nISO (International Organization for\nStandardization), 52, 758, 770\nISPs (Internet Service Providers), 32–35\naccess networks, 12–18\nASs (autonomous systems), 383\ncustomer-provider relationship, 33\nend-to-end service, 651–652\nglobal transit, 32\nlow-tier and upper-tier, 5\nmulti-homing, 33–34\nnaming and addressing conventions, 5\nobtaining set of addresses from, 345\npeer, 34\npeering agreements, 399\niterative queries, 137–138\nITU-T (International Telecommunication\nUnion), 699, 770\nIXPs (Internet exchange points), 33, 34\nJ\nJacobson, Van, 302, 303, 584\nJava and client-server programming, 157\njitter, 614–618\nK\nKademlia DHT, 156\nKahn, Robert, 62, 231, 431–432, 511\nKanKan, 87, 588, 611–612\nkey derivation, 713–714\nkey management, 726\nKleinrock, Leonard, 60–62, 80–82, 431, 511\nknown-plaintext attack, 677\nL\nlabel-switched routers, 488\nLam, Simon S., 511–512\nLANs (local area networks), 16\nbroadcast address, 464\nconfigured hierarchically, 482–483\nEthernet, 470\nhub-based star topology, 470\nMAC address, 463\nswitches, 17, 470, 475, 483, 496\nVLANs (virtual local area networks),\n482–486\n838\nINDEX"
    },
    {
      "chunk_id": "d1f18e02-2f53-4826-ac51-e064278ffddd",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "L",
      "original_titles": [
        "L"
      ],
      "path": "Index > L",
      "start_page": 865,
      "end_page": 865,
      "token_count": 392,
      "text": "IP (Internet Protocol) (continued)\ndatagrams, 242, 332–338\nde facto standard for internetworking,\n512\nInternet addressing and forwarding,\n331–363\nmulticasting, 593\nnetwork layer, 305–307, 332\npervasiveness, 431\nrouters, 53\nsecurity, 362–363\nseparation from TCP, 62\nIP multicast, 412\nIPng (Next Generation IP), 356\nIP protocol version 4. See IPv4\nIP protocol version 6. See IPv6\nIPsec, 362, 72–724\nIPS (intrusion prevention system), 355, 740\nIP spoofing, 59–60, 701\nIP subnet, 541–542\nIP tunnels, 303\nIPTV, 87\nIPv4 addresses, 334, 356\nIPv6 addresses, 356, 512\nIPv4 addressing, 338–352\nIPv4 datagrams, 332–335\nformat, 306\nIPv6 datagrams, 356–359\nIPv4 header, 636–637\nIPv6 header, 359\nIPv4 (IP protocol version 4), 160, 165,\n331, 336\nIPsec, 362\ntransitioning to IPv6, 359–362\nIPv6 (IP protocol version 6), 306, 332,\n356–362\ndual-stack approach, 360\nflow labeling and priority, 357\nIP addresses, 351, 356\ntransitioning from IPv4, 359–362\ntunneling, 360–361\nIPv6/IPv4 nodes, 359–360\nIS-IS (Intermediate-System-to-\nIntermediate-System), 405, 498\nIS-IS protocol, 384\nISO (International Organization for\nStandardization), 52, 758, 770\nISPs (Internet Service Providers), 32–35\naccess networks, 12–18\nASs (autonomous systems), 383\ncustomer-provider relationship, 33\nend-to-end service, 651–652\nglobal transit, 32\nlow-tier and upper-tier, 5\nmulti-homing, 33–34\nnaming and addressing conventions, 5\nobtaining set of addresses from, 345\npeer, 34\npeering agreements, 399\niterative queries, 137–138\nITU-T (International Telecommunication\nUnion), 699, 770\nIXPs (Internet exchange points), 33, 34\nJ\nJacobson, Van, 302, 303, 584\nJava and client-server programming, 157\njitter, 614–618\nK\nKademlia DHT, 156\nKahn, Robert, 62, 231, 431–432, 511\nKanKan, 87, 588, 611–612\nkey derivation, 713–714\nkey management, 726\nKleinrock, Leonard, 60–62, 80–82, 431, 511\nknown-plaintext attack, 677\nL\nlabel-switched routers, 488\nLam, Simon S., 511–512\nLANs (local area networks), 16\nbroadcast address, 464\nconfigured hierarchically, 482–483\nEthernet, 470\nhub-based star topology, 470\nMAC address, 463\nswitches, 17, 470, 475, 483, 496\nVLANs (virtual local area networks),\n482–486\n838\nINDEX"
    },
    {
      "chunk_id": "51230dc2-5e57-43fe-ae33-9fffa2a283bc",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "M",
      "original_titles": [
        "M"
      ],
      "path": "Index > M",
      "start_page": 866,
      "end_page": 868,
      "token_count": 1179,
      "text": "Last-Modified: header line, 106, 115\nlayered architecture, 47–48, 51–53\nlayer-2 packet switch, 480\nlayer-4 switch, 492\nLDNS (Local DNS Server), 136–137,\n605–606\nleaky bucket mechanism, 646–648\nleast-cost path, 365, 367–368\nBellman-Ford equation, 371–372\nLEO (low-earth orbiting) satellites, 21–22\nLimelight, 35, 114, 604, 609\nlimited-scope flooding, 405\nlink-cost changes and DV (distance-\nvector) algorithm, 376–377\nlink-layer acknowledgments, 532\nlink-layer addressing, 462–469\nlink-layer frames, 55, 434, 436, 438–445,\n461–462\nlink-layer protocols, 52\ncable Internet access, 460–461\nerror detection and correction, 437\nservices 436–437\nlink layer, 52, 433–436\nbit-level error detection and correction,\n438–445\nbroadcast channels, 433\nCRC (cyclic redundancy check) codes,\n443–445\nerror-detection and-correction tech-\nniques, 438–445\nIEEE protocols, 444\nimplementation, 437–438\nlink-layer frame, 434\nmultiple access protocols, 445–461\nnetworks as, 486–490\npoint-to-point communication link, 434\nservices, 436–437\nswitches, 461\nwireless links, 17–18, 519–522\nlink-layer switches, 4, 22, 53–54, 310,\n476–482\nlink rates, 515\nlinks, 434\nbroadcast, 445\nheterogeneous, 479\nMPLS header format, 488\npoint-to-point, 445\ntransmission rates, 4\nlink-scheduling disciplines\nFIFO (first-in-first-out), 641–642\npriority queuing, 642–643\nround robin queuing discipline, 643–644\nWFQ (weighted fair queuing), 644–645\nwork-conserving round robin disci-\npline, 644\nlink-state advertisements. See LSAs\nlink-state broadcast, 366–367\nlink-state messages, 689\nlink-state protocols, 388, 400\nlink-state routing algorithms, 366–371\nlink virtualization, 486–490\nlink weights, 390\nload balancer, 491–492\nload distribution, 132–133\nload-insensitive routing algorithm, 366\nlocal area networks. See LANs\nLocal DNS Server. See LDNS\nlocal ISP, 392\nlogical communication between processes,\n186\nlongest prefix matching rule, 318–319\nLong-Term Evolution. See LTE\nloss recovery schemes, 618\nloss-tolerant, 91, 592–593\nlost packets, 262–263\nlow-earth orbiting satellites. See LEO\nsatellites\nLSAs (link-state advertisements), 405\nLTE (Long-Term Evolution), 18, 553–554\nM\nMAC addresses, 463–465, 497\nadapters, 464–465, 465, 471\nAP (access point), 528\nBSS, 539\n802.11 wireless LAN, 528\nflat structure, 464\nno two adapters have same, 464\npermanent, 463–464\nswitches, 477, 480\nINDEX\n839\n\nMAC-based VLANs (virtual local area\nnetworks), 486\nMAC (message authentication code),\n691–693, 777\ncompared with digital signatures,\n696–697\nMAC (multiple access protocols), 436,\n464, 531\n802.11 wireless LANs, 531–537\nmail access protocols, 125–130\nmailbox, 120–121\nmail clients, 97, 125\nmail servers, 97, 119–121, 125–126\nmain-in-the-middle attack and DNS\n(domain name system), 143\nmalicious packet attacks, 355\nmalware, 56–57\nmanaged device, 761\nmanaged objects, 761\nManagement Information Base. See MIB\nmanaging entity, 761\nMANETs (mobile ad hoc networks), 518\nmanifest file, 601, 610\nMAP message, 460\nMaster Key. See MK\nMaster Secret. See MS\nmaximum segment size. See MSS\nmaximum transmission unit. See MTU\nMBone multicast network, 411\nMCR (minimum cell transmission rate), 313\nMD5, 710\nauthentication, 389\nMDCs (modular data centers), 494\nMD5 hash algorithm, 690\nmessage authentication code. See MAC\nmessage digests, 707\nmessage integrity, 688–693, 706\ncryptographic techniques, 675\ndigital signatures, 695\nsecure e-mail system, 707–708\nmessage queue, 121\nmessages, 51\napplication-layer, 51\nauthenticating, 689\nbreaking into shorter segments, 51\nconfidentiality, 672–673\neavesdropping, 673\nencrypted, 672\nHTTP format, 103–108\nintegrity, 673\nsegmentation, 77\nsemantics of fields, 97\nsyntax, 96\ntransmitting and receiving, 7–8\nMetcalfe, Bob, 454, 470, 473\nmetering function, 650\nmethod field, 104\nMIB (Management Information Base),\n761–762, 765, 770\nMIB modules, 765, 770–772\nMIB objects, 765\nMicrosoft, 64–66, 66, 114\nmiddleboxes, 303\nMIMO (multiple-input, multiple output)\nantennas, 553\nminimum cell transmission rate. See MCR\nminimum spanning tree, 403\nMinitel project, 63–64\nMK (Master Key), 730\nmobile ad hoc networks. See MANETs\nmobile devices, 81\nInternet, 550–552\npower management, 543–544\nmobile IP, 349, 563–569, 576, 568–569\nmobile IP standard, 562\nmobile nodes, 554–564\nCOA (care-of-address), 559\ndirect routing, 563–564\nforeign address, 559\nforeign network, 559\nhome network, 559\nindirect routing, 559–562\nlocation protocol, 563\npermanent address, 559\npermanent home, 557\nregistering with foreign agent, 566–567\nrouting to, 559–564\nsending datagrams to correspondent, 561\nmobile-node-to-foreign-agent protocol,\n562\n840\nINDEX\n\nmobile phones and wireless LAN base sta-\ntions, 548\nmobile station roaming number. See\nMSRN\nmobile switching center. See MSC\nmobility, 513–514\naddressing, 556–559\ncellular network management, 570–577\nGSM (Global System for Mobile\nCommunications), 576\nin IP subnet, 541–542\nmanagement, 555–564\nmobile IP, 576\nnetwork-layer functionality required,\n561–562\nrouting to mobile node, 559–564\nscalability, 558\nwireless networks, 575–577\nmobility-related services, 513\nmodems, 15\nmodular arithmetic, 685\nmodular data centers. See MDCs\nmodulo arithmetic, 687–688\nMPEG, 624\nMPLS (Multiprotocol Label Switching),\n487–490\nMPLS paths, 303\nMP3 (MPEG 1 layer 3), 590\nMSC (mobile switching center), 550\nMSDP (Multicast Source Discovery\nProtocol), 412\nMS (Master Secret), 714, 716\nMSRN (mobile station roaming \nnumber), 571–572\nMSS (maximum segment size), 232–234\nMTU (maximum transmission unit),\n232–233\nEthernet, 471\nIP datagrams, 335\nmulticast routing, 405–412\nmulticast addresses, 356\nmulticast group, 406\nmulticast packets, 405–412\nmulticast protocols, 362\nmulticast routing, 399, 407–412\nMulticast Source Discovery Protocol. See\nMSDP\nmulticast trees and RTP packets, 624–625\nmulti-homed stub network, 397\nmulti-homing, 33\nmulti-hop wireless networks, 518\nmultihop paths, 263–265\nmultimedia\nnetwork supported for, 632–655\nstreaming stored video, 593–612\nsystem-level approach for delivering,\n633\nVoIP (Voice-over-IP), 612–623\nmultimedia applications, 588\naudio properties, 590–591\nbandwidth sensitive, 92\nconversational voice, 587, 592–593\nimproving quality, 635\nInternet telephony, 592–593\nstreaming live audio and video, 587,\n593\nstreaming stored audio and video, 587,\n591–592\nTCP (Transmission Control Protocol),\n200\ntypes, 591–593\nUDP (User Datagram Protocol),\n200–201, 282\nvideo over IP, 592–593\nvideo properties, 588–589\nmultipath propagation, 519\nmulti-player online games, 83\nmultiple access links, 445–461\nmultiple access problem, 445\nmultiple access protocols, 445–461\nALOHA protocol, 63\nCDMA (code division multiple access),\n449\nchannel partitioning protocols, 447–448\ncharacteristics, 447–448\nFDM (frequency-division multiplex-\ning), 448\nrandom access protocols, 447\ntaking-turns protocols, 447, 459–460\nTDM (time-division multiplexing), 448\nINDEX\n841"
    },
    {
      "chunk_id": "c289a55a-ca9a-443b-aa1d-d9f7aec3cf39",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "N",
      "original_titles": [
        "N"
      ],
      "path": "Index > N",
      "start_page": 869,
      "end_page": 870,
      "token_count": 733,
      "text": "multiple classes of service, 636–640\nmultiple-input, multiple output antennas.\nSee MIMO antennas\nmultiplexing, 191–198\ncircuit-switched networks, 28–30\nconnectionless, 193–194\nconnection-oriented, 194–197\nmultiplexing/demultiplexing service and\ntransport layer, 198–199\nMultiprotocol Label Switching. See MPLS\nmulti-tier hierarchy, 33\nMX record, 140, 142\nN\nNAKs (negative acknowledgments), 207,\n208\nnamed data, 585\nname translation and SIP (Session\nInitiation Protocol), 630–632\nNapster, 65\nNAT (network address translation), 306,\n322–324, 349–352\nSkype, 622\nNCP (network-control protocol), 62\nnegative acknowledgements. See NAKs\nneighbors, 364\nneighbor-to-neighbor communication,\n372–375\nNetflix, 65, 83, 588, 608–610\nNetscape Communications Corporation,\n64\nnetwork adapter, 437–438\nnetwork address translation. See NAT\nnetwork applications, 83\napplication-layer protocols, 97\narchitectures, 86–88\nclient program, 156\ncommunication between client and\nserver, 156\ncreation, 156–168\nprinciples of, 84–98\nprocesses communicating, 88\nproprietary, 156–157\nserver program, 156\nWeb, 98–116\nnetwork-assisted congestion control,\n266–269\nnetwork attacks, 55–60\nnetwork cache servers, 106\nnetwork-control protocol. See NCP\nnetwork core\ncircuit switching, 27–32\nnetwork of networks, 32–35\npacket switching, 22–27\nnetwork dimensioning, 634–635\nnetwork interface card. See NIC\nnetwork-layer addresses, 462, 465\nnetwork-layer components, 266\nnetwork-layer datagram, 55\nnetwork-layer multicast routing\nalgorithms, 408\nnetwork-layer packets, 51–52\nnetwork-layer protocols\nconstrained by service model, 189\ndifficulty changing, 362\nhosts supporting multiple, 471–472\nIP (Internet Protocol), 190\nlogical communication between hosts,\n186, 188–189\nnetwork layer, 50–53, 189–190\nbest-effort service, 311–312\nbroadcast protocols, 405\ncomplexity, 305\nconfidentiality, 718\nconnectionless service, 313\nconnection service, 313\nconnection setup, 310\ndatagram networks, 313\ndatagram service, 364\nencapsulating transport-layer segment\ninto IP datagram, 199\nextracting transport-layer segment from\ndatagram, 186\nforwarding, 305, 308–310, 315\nguaranteed delivery, 311\nhost-to-host communication, 305\nhost-to-host services, 313\nICMP (Internet Control Message\nProtocol), 353\nin-order packet delivery, 311\n842\nINDEX\n\nInternet, 332\nInternet routing protocols, 353\nIP protocol, 51–52, 332, 353\nmobility, 561–562\npassing segments to, 191–198\npath between sender and receiver, 315\nprocess-to-process delivery service,\n191–198\nreporting errors in datagrams, 332\nreserving resources, 316\nrouting, 51–52, 305–306, 308–310\nsecurity, 705, 718–725\nservices offered by, 310–313\ntransport layer relationship, 186–189\nVC (virtual-circuit) networks, 313–315\nnetwork-layer service models, 319\nnetwork management\naccounting management, 759\nComcast case study, 763–764\nconfiguration management, 759\ndefinition, 759\nfault management, 758–759\nhost monitoring, 756\ninfrastructure, 760–762\nintrusion detection, 758\nmanaged device, 761\nmanaged objects, 761\nmanaging entity, 761\nMIB (Management Information Base),\n761–762\nmonitoring traffic, 756–758\nnetwork management protocol, 762\nperformance management, 758\nsecurity management, 759\nSLAs (Service Level Agreements), 758\nstandards, 762\nnetwork management agent, 762\nnetwork management protocol, 762\nnetwork mapping, 740\nnetwork of networks, 32–35, 62\nnetwork prefix, 342\nnetwork protocols, 8–9\nnetworks, 340\naccess networks, 12–18\nattacks, 57–58\ncircuit-switched, 27–30\ncellular, 546–554\ncomponents, 9–21\ncrossbar, 326\ndatagram, 317–319\ndifferentiated service, 634, 648–652\ndimensioning best-effort, 634–636\nDMZ (demilitarized zone), 741\nforeign, 557\ngrowth of, 62\ninterconnecting, 62\nlinking universities, 63\nas link layer, 486–490\nlinks, 635\nmobility, 513–514\nMPLS (Multiprotocol Label Switching)\nnetworks, 487–490\nmultiple classes of service, \n636–640\npacket delay and loss, 35–44, 635\npacket-switched, 4, 22–27\nper-connection QoS (Quality-of-\nService) guarantees, 652–655\nphysical media, 18–22\nprivate, 718\nprograms communicating on, 84\nroutes, 4\nscheduling mechanisms, 640–645\nsecurity, 55–56, 671–674\nsockets, 89–90\nswitched local area networks, \n461–486\nVC (virtual-circuit), 314–317\nvisited, 557\nwireless LANS, 515, 518, 526–546\nnetwork service models, 310–313\nNEXT-HOP attribute, 394–395, 397\nNIC (network Interface card), link layer\nimplementation, 437–438\nNI (no increase) bit, 268–269\nnmap port scanning tool, 196, 258\nNOC (network operations center), 755\nnodal processing, 36\nno increase bit. See NI bit\nnomadic computing, 81\nINDEX\n843"
    },
    {
      "chunk_id": "13de21b1-6912-40c7-955b-36e111cf0670",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "O",
      "original_titles": [
        "O"
      ],
      "path": "Index > O",
      "start_page": 871,
      "end_page": 874,
      "token_count": 1479,
      "text": "nonces, 704–705, 716–717\nnon-persistent connections, 100–103, 198\nnonpreemptive priority queuing \ndiscipline, 643\nnon-real-time applications, 92–93\nnonrepudiation and cryptographic \ntechniques, 675\nnslookup program, 141–142\nN-way-unicast, 400–401\nO\nOBJECT IDENTIFIER data type, 766\nobjects, 99, 103\nOC (Optical Carrier) standard link, 21\nodd parity schemes, 440\noffered load, 261\nOLT (optical line terminator), 16\nONT (optical network terminator), 16\nOpen-Shortest Path First. See OSPF\nOpen Systems Interconnection model. See\nOSI model\noperational security, 673, 731–742\nOptical Carrier standard link. See OC\nstandard link\noptical distribution network, 15–16\noptical line terminator. See OLT\noptical network terminator. See ONT\noptions field, 235\norigin authentication, 363\northogonal frequency division multiplex-\ning. See OFDM\nOSI (Open Systems Interconnection)\nmodel, 52–53\nOSPF (Open-Shortest Path First),\n366–367, 384, 388–390, 498\nLSAs (link-state advertisements), 405\nrouter authentication, 388–389\nOS vulnerability attacks, 740\nout-of-band, 117\nout-of-order segments, 236\noutput buffers, 25\noutput ports, 320–326\npacket queues, 327–331\npacket scheduler, 329\nprocessing, 326\noutput queue, 25\noverlapping fragments, 338\noverlay network, 154, 486\nP\npacket classification, 648–649\npacket delay, 35–44, 102, 635\npacket-discarding policy, 641\npacket filtering, 732, 737\npacket forwarding, 26–27\npacket loss, 25, 41–42, 91, 259, 613, 635\nerror concealment, 620–621\nFEC (forward error correction),\n618–619\ninterleaving, 618–620\npredicting imminent, 278\nrecovering from, 213, 618–621\npacket marking, 638\npacket-radio networks, 62, 511\npacket repetition, 621\npackets, 4, 22\naverage rate, 645\nbit errors, 207\nbuffering, 218\nburst size, 646\ncontrolled flooding, 401–403\ncumulative acknowledgment, 222\ndelays, 36–37\ndelivering, 204–205\ndestination, 35\ndestination IP address, 158\ndetecting loss, 212–215\ndropping, 41, 329\nduplicate, 210, 213–214\nduplicate ACKs, 211\nend-to-end delay, 612\nFIFO (first-in-first-out), 641–642\nformat of, 5\nforwarding, 4, 308–310, 321–322, 649\nheader fields, 55\nIP spoofing, 59–60\njitter, 614\nmoving between nodes, 52\npath, 35\npayload fields, 55\n844\nINDEX\n\npeak rate, 646\nprefix destination address, 318\npriority queuing, 642–643\nprocessing delays, 36–37\nqueuing delays, 25, 37, 39–42\nround-robin queuing, 643–644\nround-trip delays, 42–43\nrouting, 308–310\nRTT (round-trip time), 102–103\nsame priority class, 642\nsender’s source address, 158\nsending, 24\nsending multiple, 218\nsequence numbers, 210, 218, 220\nsockets, 158\nsource, 35\nsource address, 162\nswitching, 324–326\ntracing, 42–43\ntransmitting, 213–214\nuncontrolled flooding, 401\nVC number, 315\nWFQ (weighted fair queuing),\n644–645\nwhat to do when loss occurs, 212–215\nwhere queuing occurs, 327–331\nPacket Satellite, 511\npacket scheduler, 329\npacket sniffers, 58–59, 78\npacket-switched networks, 4, 25\nARPAnet, 61\ncomparing transmission and propaga-\ntion delay, 38–39\ndelays, 35–39\nend-to-end delays, 42–44\npacket loss, 41–42\nprocessing delays, 36–37\npropagation delays, 37–38\nqueuing delays, 37, 39–42\nsending packets, 28\ntransmission delays, 37\npacket switches, 4, 310\nfacilitating exchange of data, 6\nlink-layer switches, 22, 53, 310\noutput buffers, 25\nrouters, 22, 53, 310\nstore-and-forward transmission, \n22, 24\npacket switching, 30–31\nalternative to circuit switching, 60\nforwarding tables, 26–27\npacket loss, 25\nqueuing delays, 25\nqueuing theory, 60\nrouting protocols, 26–27\nsecure voice over military networks, 60\nstore-and-forward transmission, \n22, 24\nVC (virtual-circuit) approach, 267\npacket-switching networks, 62\npaging, 550\nparity checks, 440–442\npassive optical networks. See PONs\npassive spanning, 530\npasswords, 703, 710\npaths, 4, 365\nleast-cost, 365\nmultihop, 263–265\npayload fields, 55\nPBXs (private branch exchanges), 627\nPCM μ-law, 628–629\nPCM (pulse code modulation), 590\nPDU and SNMP applications, 776–777\npeak rate, 646\npeer, 34\npeer churn and DHTs (distributed hash\ntables), 155–156\npeering, 33\npeers, 86, 144–145\nDHT, 155–156\nfile sharing, 145–151\ntorrent, 149\npeer-to-peer applications. See P2P applica-\ntions\nper-connection QoS (Quality-of-Service)\nguarantees, 634, 652–655\nper-connection throughput, 260\nperfectly reliable channel, 206–207\nPerformance Management, 758, 763–764\nper-hop behavior. See PHB\nINDEX\n845\n\npermanent address, 559\npersistent connections, 100–103\npersistent HTTP, 198\npersonal area networks\nBluetooth, 544–545\nZigbee, 545–546\nPGP (Pretty Good Privacy), 678, 706,\n709–711\nPHB (Per-hop behavior), 649–651\nphysical address, 463\nphysical layer, 50, 52–53\nphysical media, 4\ncoaxial cable, 20\ncosts, 19\nfiber optics, 20–21\nguided media, 19\nradio channels, 21\nsatellite radio channels, 21–22\ntwisted-pair copper wire, 19–20\nunguided media, 19\npiconet, 544\npiggybacked acknowledgment, 237\nPIM (Protocol-Independent Multicast),\n411–412, 584\nping program, 353\npipelined reliable data transfer protocols,\n215–218\npipelining, 218\npersistent connections, 103\nTCP (Transmission Control Protocol),\n240\nplaintext, 675\nplayback attacks, 703, 777\nplayout delay, 614\nplug-and-play protocol, 346\nplug-and-play switches, 479–480\nPMS (Pre-Master Secret), 716\npoints of presence. See PoPs\npoint-to-point, 232\npoint-to-point communication link, 434\npoint-to-point links, 436, 445\nPoint-to-Point Protocol. See PPP\npoisoned reverse, 377–378\npoisoning attack, 143\npolicing disciplines, 645–648\npolicing mechanisms, 640\npolling protocols, 459\npolls, 459\npolyalphabetic encryption, 678\npolynomial codes, 443\nPONs (passive optical networks), 15–16\nPOP3 (Post Office Protocol-Version 3),\n127–129\nPOP3 server, 127, 129\nPoPs (points of presence), 33\nPOP3 user agent, 128\nport-based VLAN, 483–484\nport numbers, 90, 158\naddressing processes, 351\ndestination, 234\nprotocols, 90\nsource, 234\nWeb servers, 197–198\nwell-known, 192, 196\nport-based VLAN, 483–484\nport scanners, 196\nport scans, 196, 740\npostive acknowledgment. See ACK\nPOST method, 104–105\nPost Office Protocol-Version 3. See POP3\npower management, 543–544\nP2P (peer to peer)\napplications, 97–98\narchitecture, 86–88, 144–148\nBitTorrent, 149–151\nconnection reversal, 352\nDHTs (distributed hash tables), 145,\n151–156\nfile distribution, 83, 88, 45–151\nNAT, 351–352\nSkype, 621–622\nvideo streaming applications, 592\nPPP (point-to-point protocol), 434, 445\nPPstream, 87\nPPTV and P2P delivery, 611\nprefetching, 592\nvideo, 596–597\nprefixes, 342, 344, 393\nawareness of, 396–397\nBGP attributes, 394\n846\nINDEX\n\nforwarding table, 396–397\ngateway routers, 393\nPre-Master Secret. See PMS\nprerecorded video, 591\npresentation layer, 53\npresentation service, 780\nPretty Good Privacy. See PGP\npriority queuing, 642–643\nprivacy\ncookies, 108\nproxy servers, 738\nQQ, 623\nSkype, 623\nSSL (Secure Sockets Layer), 738\nWeb sites, 738\nprivate branch exchanges. See PBXs\nprivate CDNs (Content Distribution\nNetworks), 603\nprivate key, 684–685, 693, 708\npasswords, 710\nprivate networks, 66, 718\nprocesses, 88–90\ncommunicating by sending messages to\nsockets, 157\ncommunicating using UDP sockets,\n158\nconnection sockets, 198\nhandshaking, 231\nlogical communication between, 186\nsockets, 191\nprocessing delays, 36–37\nprogramming, event-based, 223\npropagation delays, 24, 36–39, 456\nproprietary network applications, \n156–157\nProtocol-Independent Multicast. See PIM\nprotocols, 5, 68\nalternating-bit, 214\napplication-layer, 49–50\ncongestion-control, 9\ndefining, 7–9\nhardware-implemented, 9\nhuman analogy, 7–8\ninterior gateway, 384\nInternet, 9\nIP (Internet Protocol), 5\nlayering, 49–50\nnonce, 704–705\npacket sizes, 335\nplug-and-play, 346\nport numbers, 90\nreal-time interactive applications,\n623–632\nrouting, 51–52\nRTP, 623–626\nSIP, 626–632\nsoft state, 408–409\nSR (selective repeat), 223–230\nstateless, 100\nstop-and-wait, 209, 215, 217\nTCP, 5\nUDP, 5\ntransmission and receipt of messages,\n7–8\ntransport-layer, 50\nprotocol stack, 50\nprovider, 32\nproxy servers, 106, 110, 738\npublic key algorithm, 716\npublic key certification, 697–699\npublic-key cryptography, 708\ndigital signatures, 693–694\nprivate key, 693\npublic key, 693, 697\nsecure e-mail system, 706–707\npublic key encryption, 683–688\npublic-key encryption algorithm, 687\npublic keys, 684–685, 693, 706–708,\n713–714\nbinding to particular entity, 697–698\ncertifying, 708\nencryption/decryption algorithms, \n684\npublic key systems, 676\npull protocol, 124\npulse code modulation. See PCM (pulse\ncode modulation)\npush protocol, 124\nPUT method, 105\nPython, 157, 160, 193\nINDEX\n847"
    },
    {
      "chunk_id": "ccafa7ec-d97c-4de4-81b9-af0e4db13369",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "P",
      "original_titles": [
        "P"
      ],
      "path": "Index > P",
      "start_page": 871,
      "end_page": 874,
      "token_count": 1479,
      "text": "nonces, 704–705, 716–717\nnon-persistent connections, 100–103, 198\nnonpreemptive priority queuing \ndiscipline, 643\nnon-real-time applications, 92–93\nnonrepudiation and cryptographic \ntechniques, 675\nnslookup program, 141–142\nN-way-unicast, 400–401\nO\nOBJECT IDENTIFIER data type, 766\nobjects, 99, 103\nOC (Optical Carrier) standard link, 21\nodd parity schemes, 440\noffered load, 261\nOLT (optical line terminator), 16\nONT (optical network terminator), 16\nOpen-Shortest Path First. See OSPF\nOpen Systems Interconnection model. See\nOSI model\noperational security, 673, 731–742\nOptical Carrier standard link. See OC\nstandard link\noptical distribution network, 15–16\noptical line terminator. See OLT\noptical network terminator. See ONT\noptions field, 235\norigin authentication, 363\northogonal frequency division multiplex-\ning. See OFDM\nOSI (Open Systems Interconnection)\nmodel, 52–53\nOSPF (Open-Shortest Path First),\n366–367, 384, 388–390, 498\nLSAs (link-state advertisements), 405\nrouter authentication, 388–389\nOS vulnerability attacks, 740\nout-of-band, 117\nout-of-order segments, 236\noutput buffers, 25\noutput ports, 320–326\npacket queues, 327–331\npacket scheduler, 329\nprocessing, 326\noutput queue, 25\noverlapping fragments, 338\noverlay network, 154, 486\nP\npacket classification, 648–649\npacket delay, 35–44, 102, 635\npacket-discarding policy, 641\npacket filtering, 732, 737\npacket forwarding, 26–27\npacket loss, 25, 41–42, 91, 259, 613, 635\nerror concealment, 620–621\nFEC (forward error correction),\n618–619\ninterleaving, 618–620\npredicting imminent, 278\nrecovering from, 213, 618–621\npacket marking, 638\npacket-radio networks, 62, 511\npacket repetition, 621\npackets, 4, 22\naverage rate, 645\nbit errors, 207\nbuffering, 218\nburst size, 646\ncontrolled flooding, 401–403\ncumulative acknowledgment, 222\ndelays, 36–37\ndelivering, 204–205\ndestination, 35\ndestination IP address, 158\ndetecting loss, 212–215\ndropping, 41, 329\nduplicate, 210, 213–214\nduplicate ACKs, 211\nend-to-end delay, 612\nFIFO (first-in-first-out), 641–642\nformat of, 5\nforwarding, 4, 308–310, 321–322, 649\nheader fields, 55\nIP spoofing, 59–60\njitter, 614\nmoving between nodes, 52\npath, 35\npayload fields, 55\n844\nINDEX\n\npeak rate, 646\nprefix destination address, 318\npriority queuing, 642–643\nprocessing delays, 36–37\nqueuing delays, 25, 37, 39–42\nround-robin queuing, 643–644\nround-trip delays, 42–43\nrouting, 308–310\nRTT (round-trip time), 102–103\nsame priority class, 642\nsender’s source address, 158\nsending, 24\nsending multiple, 218\nsequence numbers, 210, 218, 220\nsockets, 158\nsource, 35\nsource address, 162\nswitching, 324–326\ntracing, 42–43\ntransmitting, 213–214\nuncontrolled flooding, 401\nVC number, 315\nWFQ (weighted fair queuing),\n644–645\nwhat to do when loss occurs, 212–215\nwhere queuing occurs, 327–331\nPacket Satellite, 511\npacket scheduler, 329\npacket sniffers, 58–59, 78\npacket-switched networks, 4, 25\nARPAnet, 61\ncomparing transmission and propaga-\ntion delay, 38–39\ndelays, 35–39\nend-to-end delays, 42–44\npacket loss, 41–42\nprocessing delays, 36–37\npropagation delays, 37–38\nqueuing delays, 37, 39–42\nsending packets, 28\ntransmission delays, 37\npacket switches, 4, 310\nfacilitating exchange of data, 6\nlink-layer switches, 22, 53, 310\noutput buffers, 25\nrouters, 22, 53, 310\nstore-and-forward transmission, \n22, 24\npacket switching, 30–31\nalternative to circuit switching, 60\nforwarding tables, 26–27\npacket loss, 25\nqueuing delays, 25\nqueuing theory, 60\nrouting protocols, 26–27\nsecure voice over military networks, 60\nstore-and-forward transmission, \n22, 24\nVC (virtual-circuit) approach, 267\npacket-switching networks, 62\npaging, 550\nparity checks, 440–442\npassive optical networks. See PONs\npassive spanning, 530\npasswords, 703, 710\npaths, 4, 365\nleast-cost, 365\nmultihop, 263–265\npayload fields, 55\nPBXs (private branch exchanges), 627\nPCM μ-law, 628–629\nPCM (pulse code modulation), 590\nPDU and SNMP applications, 776–777\npeak rate, 646\npeer, 34\npeer churn and DHTs (distributed hash\ntables), 155–156\npeering, 33\npeers, 86, 144–145\nDHT, 155–156\nfile sharing, 145–151\ntorrent, 149\npeer-to-peer applications. See P2P applica-\ntions\nper-connection QoS (Quality-of-Service)\nguarantees, 634, 652–655\nper-connection throughput, 260\nperfectly reliable channel, 206–207\nPerformance Management, 758, 763–764\nper-hop behavior. See PHB\nINDEX\n845\n\npermanent address, 559\npersistent connections, 100–103\npersistent HTTP, 198\npersonal area networks\nBluetooth, 544–545\nZigbee, 545–546\nPGP (Pretty Good Privacy), 678, 706,\n709–711\nPHB (Per-hop behavior), 649–651\nphysical address, 463\nphysical layer, 50, 52–53\nphysical media, 4\ncoaxial cable, 20\ncosts, 19\nfiber optics, 20–21\nguided media, 19\nradio channels, 21\nsatellite radio channels, 21–22\ntwisted-pair copper wire, 19–20\nunguided media, 19\npiconet, 544\npiggybacked acknowledgment, 237\nPIM (Protocol-Independent Multicast),\n411–412, 584\nping program, 353\npipelined reliable data transfer protocols,\n215–218\npipelining, 218\npersistent connections, 103\nTCP (Transmission Control Protocol),\n240\nplaintext, 675\nplayback attacks, 703, 777\nplayout delay, 614\nplug-and-play protocol, 346\nplug-and-play switches, 479–480\nPMS (Pre-Master Secret), 716\npoints of presence. See PoPs\npoint-to-point, 232\npoint-to-point communication link, 434\npoint-to-point links, 436, 445\nPoint-to-Point Protocol. See PPP\npoisoned reverse, 377–378\npoisoning attack, 143\npolicing disciplines, 645–648\npolicing mechanisms, 640\npolling protocols, 459\npolls, 459\npolyalphabetic encryption, 678\npolynomial codes, 443\nPONs (passive optical networks), 15–16\nPOP3 (Post Office Protocol-Version 3),\n127–129\nPOP3 server, 127, 129\nPoPs (points of presence), 33\nPOP3 user agent, 128\nport-based VLAN, 483–484\nport numbers, 90, 158\naddressing processes, 351\ndestination, 234\nprotocols, 90\nsource, 234\nWeb servers, 197–198\nwell-known, 192, 196\nport-based VLAN, 483–484\nport scanners, 196\nport scans, 196, 740\npostive acknowledgment. See ACK\nPOST method, 104–105\nPost Office Protocol-Version 3. See POP3\npower management, 543–544\nP2P (peer to peer)\napplications, 97–98\narchitecture, 86–88, 144–148\nBitTorrent, 149–151\nconnection reversal, 352\nDHTs (distributed hash tables), 145,\n151–156\nfile distribution, 83, 88, 45–151\nNAT, 351–352\nSkype, 621–622\nvideo streaming applications, 592\nPPP (point-to-point protocol), 434, 445\nPPstream, 87\nPPTV and P2P delivery, 611\nprefetching, 592\nvideo, 596–597\nprefixes, 342, 344, 393\nawareness of, 396–397\nBGP attributes, 394\n846\nINDEX\n\nforwarding table, 396–397\ngateway routers, 393\nPre-Master Secret. See PMS\nprerecorded video, 591\npresentation layer, 53\npresentation service, 780\nPretty Good Privacy. See PGP\npriority queuing, 642–643\nprivacy\ncookies, 108\nproxy servers, 738\nQQ, 623\nSkype, 623\nSSL (Secure Sockets Layer), 738\nWeb sites, 738\nprivate branch exchanges. See PBXs\nprivate CDNs (Content Distribution\nNetworks), 603\nprivate key, 684–685, 693, 708\npasswords, 710\nprivate networks, 66, 718\nprocesses, 88–90\ncommunicating by sending messages to\nsockets, 157\ncommunicating using UDP sockets,\n158\nconnection sockets, 198\nhandshaking, 231\nlogical communication between, 186\nsockets, 191\nprocessing delays, 36–37\nprogramming, event-based, 223\npropagation delays, 24, 36–39, 456\nproprietary network applications, \n156–157\nProtocol-Independent Multicast. See PIM\nprotocols, 5, 68\nalternating-bit, 214\napplication-layer, 49–50\ncongestion-control, 9\ndefining, 7–9\nhardware-implemented, 9\nhuman analogy, 7–8\ninterior gateway, 384\nInternet, 9\nIP (Internet Protocol), 5\nlayering, 49–50\nnonce, 704–705\npacket sizes, 335\nplug-and-play, 346\nport numbers, 90\nreal-time interactive applications,\n623–632\nrouting, 51–52\nRTP, 623–626\nSIP, 626–632\nsoft state, 408–409\nSR (selective repeat), 223–230\nstateless, 100\nstop-and-wait, 209, 215, 217\nTCP, 5\nUDP, 5\ntransmission and receipt of messages,\n7–8\ntransport-layer, 50\nprotocol stack, 50\nprovider, 32\nproxy servers, 106, 110, 738\npublic key algorithm, 716\npublic key certification, 697–699\npublic-key cryptography, 708\ndigital signatures, 693–694\nprivate key, 693\npublic key, 693, 697\nsecure e-mail system, 706–707\npublic key encryption, 683–688\npublic-key encryption algorithm, 687\npublic keys, 684–685, 693, 706–708,\n713–714\nbinding to particular entity, 697–698\ncertifying, 708\nencryption/decryption algorithms, \n684\npublic key systems, 676\npull protocol, 124\npulse code modulation. See PCM (pulse\ncode modulation)\npush protocol, 124\nPUT method, 105\nPython, 157, 160, 193\nINDEX\n847"
    },
    {
      "chunk_id": "c6d1ef79-d385-40cf-be87-55c5e067a8b6",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Q",
      "original_titles": [
        "Q"
      ],
      "path": "Index > Q",
      "start_page": 875,
      "end_page": 877,
      "token_count": 1164,
      "text": "Q\nQAM16 modulation, 521\nQ2931b protocol, 654\nQoS (Quality-of-Service), 329, 653–654\nQQ, 592, 623\nQuality-of-Service. See QoS\nquantization, 590\nquery\nARP message, 467\ninformation about, 141\nquery messages, 140–142\nqueues\nFIFO (first-in-first-out), 641–642\npacket-discarding policy, 641\npriority queuing, 642–643\nprovable maximum delay, 647–648\nround robin queuing discipline,\n643–644\nWFQ (weighted fair queuing),\n644–645\nwork-conserving round robin \ndiscipline, 644\nqueuing, 327–331\nqueuing delays, 25, 36–37, 39–42, 60\nR\nradio channels, 21\nRadio Network Controller. See RNC\nRADIUS protocol, 530, 730\nrandom access protocols, 447, 473\nAloha protocol, 452–453\nCSMA (carrier sense multiple access)\nprotocol, 453–456\nCSMA/CD (carrier sense multiple\naccess with collision detection),\n455–459\nslotted ALOHA protocol, 450–452\nRandom Early Detection algorithm. See\nRED algorithm\nrarest first, 149\nrate adaptation, 542–543\nRC4 algorithm, 727–728\nRCP (Routing Control Platform), 786\nrdt (reliable data transfer protocol), 204\nbuilding, 206–215\npacket reordering, 229–230\npipelined, 215–218\nTCP (Transmission Control Protocol),\n204\nunreliable layer below, 204\nreal-time applications\ntiming, 92\nUDP (User Datagram Protocol), 200\nreal-time interactive applications\nprotocols, 623–632\nRTP (Real-Time Transport Protocol),\n623–626\nSIP, 626–632\nreal-time measurements of delay and loss\nperformance, 606\nReal-Time Streaming Protocol. See RTSP\nReal-Time Transport Protocol. See RTP\nreceive buffer, 233\nreceiver authentication, 706\nreceiver-based recovery, 621\nreceiver feedback, 208\nreceivers\nACK generation policy, 247\ndefining operation, 206\nsequence number of packet acknowl-\nedged by ACK message, 212\nreceiver-side transport layer, 54\nreceive window, 250–252\nreceive window field, 234\nreceiving adapter, 472\nreceiving processes addresses, 90\nrecords, inserting in DNS database, 142,\n144\nrecursive queries and DNS servers,\n137–138, 140\nRED (Random Early Detection)\nalgorithm, 329\nregional ISPs, 33\nregistrars, 142\nregistration with home agent, 568–569\nrelays, 622–623\nreliable channel, 204\nreliable data transfer, 91, 190\napplication layer, 204\nchannel with bit errors, 207–212\n848\nINDEX\n\nlink layer, 204\nlossy channel with bit errors, 212–215\nperfectly reliable channel, 206–207\nprinciples, 204–230\nreliable channel, 204\nTCP (Transmission Control Protocol),\n230–231, 240, 242–250\ntransport layer, 204\ntransport-layer protocols, 91\nreliable data transfer protocol. See rdt\nreliable data transfer service, 235\nreliable delivery, 436\nreliable transport service, 269\nremote host, transferring files, 116–118\nrendezvous point, 404\nrepeater, 474\nreplicated servers, 132\nreply messages and DNS (domain name\nsystem), 140–142\nrepositioning video, 600\nrequest messages and HTTP, 103–105\nrequest-response mode, 772\nrequests for comments. See RFCs\nRequest to Send control frame. See RTS\ncontrol frame\nresidential ISPs, 87\nresource-management cells. See RM cells\nresource records. See RRs\nresource reservation protocols, 362\nresources\nadmitting or blocking flows, 653\nefficient use of, 640\nreservations, 653–654\nresponse ARP, 467\nresponse messages and HTTP, 105–108\nretransmission, 208, 212\nretransmitting data, 241, 262\nretransmitting packets, 259, 261–263\nreverse path broadcast. See RPB\nreverse path forwarding. See RPF\nRexford, Jennifer, 786–787\nRFCs (requests for comments), 5\nRIP advertisements, 384–385\nRIP request message, 387\nRIP response message, 384\nRIP routers, 386–387\nRIP (Routing Information Protocol), 384,\n498\nhops, 384\nimplementation aspects, 386–388\nIP network-layer protocol, 387\nlower-tier ISPs, 388\nmodifying local routing table and prop-\nagating information, 387\nRIP messages, 384–385\nRIP table, 385–386\nrouting updates, 384\nUDP transport-layer protocol, 387\nUNIX implementation, 387–388\nRivest, Ron, 684, 690\nRM (resource-management cells),\n267–269\nRNC (Radio Network Controller), 552\nroaming number, 572\nRoberts, Larry, 61, 511\nroot DNS servers, 134–136\nround robin queuing discipline, 643–644\nround-trip delays, 43\nround-trip time. See RTT\nroute aggregation, 342\nroute attributes, 395\nrouter control plane functions, 322\nrouter discovery message, 566–567\nrouter forwarding plane, 321\nrouters, 4, 12, 22, 53, 303, 310\naccess control lists, 734\nadapters, 468\naddress of, 43\nadministrative autonomy, 380\narea border, 389\nARP modules, 468\nAS-PATH attribute, 394\nASs (autonomous systems), 380\nauthenticated and encrypted channel\nbetween, 725\nbuffering packet bits, 24\nbuffer sizing, 328–329\nconnected into network, 338\nconnection state information, 315\ncontrol functions, 321–322\nINDEX\n849\n\nrouters (continued)\ncontrol plane implemented in, 331\ndata center hierarchy, 492–493\ndefault, 364\ndestination, 364\nfinite buffers, 261–265\nfirewalls, 355, 481\nfirst-hop, 364\nfixed-length labels, 487\nforwarding function, 320–322\nforwarding table, 26, 308–309,\n317–318, 322–323, 394, 396–397,\n469\ngateway, 380–381\nimplementing layers 1 through 3, 53\nincident links, 22\ninput ports, 320\ninput processing, 322–324\ninterfaces, 338, 468\nintra-AS routing protocols, 397\nIP addresses, 394, 465, 468\nIP protocol, 53\nlabel-switched, 488\nlayer-2 packet switch, 480\nlink-layer and MAC addresses,\n462–463, 465\nlongest prefix matching rule, 318–319\nlookup, 323–324\nlooping advertisements, 394\nmemory access times, 324\nnetwork core, 4\nnetwork-layer addresses, 462, 465\noutput ports, 320–321\noutput processing, 326\npacket-forwarding decisions, 364\npacket loss, 327\npackets not cycling through, 481\nphysical links between, 364\nplug-and-play, 481\nprimary role, 306\nprocessing datagrams, 480\nprocessing packets, 351\nprotocols, 9\nqueuing, 327–331\nrouting control plane, 331\nrouting packets, 380–382\nrouting processor, 321\nrouting tables, 385–386\nscale, 379–380\nself-synchronizing, 371\nsource, 364\nspanning tree, 481\nstore-and-forward, 22, 24\nstore-and-forward packet switches, 480\nversus switches, 480–482\nswitching, 320, 324–326\nterminating incoming physical link, 320\nVC setup, 316\nroutes, 4, 394–396\nroute summarization, 342\nrouting, 305–306, 308–310\nadvertising information, 382–383\nbroadcast, 399–405\ncalls to mobile user, 571–572\ndistance vector, 384\nhierarchical, 379–383\nhot-potato, 382\nto mobile node, 559–564\nmulticast, 399, 405–412\nstoring information, 379–380\nrouting algorithms, 309, 363–383\nARPAnet, 366\ncircuit-switched, 379\ndecentralized, 366\nDV (distance-vector) algorithm, 366,\n371–379\ndynamic, 366\nforwarding tables, 364\nglobal, 365–366\nhierarchical routing, 379–383\nleast costly paths, 365\nload-sensitive, 366\nLS (link-state) algorithms, 366–371\npath from source to destination router,\n364\nscale of routers, 379–380\nstatic, 366\nswitches, 494–495\nviewing packet traffic flows, 379\nrouting control plane, 331\n850\nINDEX"
    },
    {
      "chunk_id": "5cf21039-9fc0-4e3d-b1ed-6a3cec259dd1",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "R",
      "original_titles": [
        "R"
      ],
      "path": "Index > R",
      "start_page": 875,
      "end_page": 877,
      "token_count": 1164,
      "text": "Q\nQAM16 modulation, 521\nQ2931b protocol, 654\nQoS (Quality-of-Service), 329, 653–654\nQQ, 592, 623\nQuality-of-Service. See QoS\nquantization, 590\nquery\nARP message, 467\ninformation about, 141\nquery messages, 140–142\nqueues\nFIFO (first-in-first-out), 641–642\npacket-discarding policy, 641\npriority queuing, 642–643\nprovable maximum delay, 647–648\nround robin queuing discipline,\n643–644\nWFQ (weighted fair queuing),\n644–645\nwork-conserving round robin \ndiscipline, 644\nqueuing, 327–331\nqueuing delays, 25, 36–37, 39–42, 60\nR\nradio channels, 21\nRadio Network Controller. See RNC\nRADIUS protocol, 530, 730\nrandom access protocols, 447, 473\nAloha protocol, 452–453\nCSMA (carrier sense multiple access)\nprotocol, 453–456\nCSMA/CD (carrier sense multiple\naccess with collision detection),\n455–459\nslotted ALOHA protocol, 450–452\nRandom Early Detection algorithm. See\nRED algorithm\nrarest first, 149\nrate adaptation, 542–543\nRC4 algorithm, 727–728\nRCP (Routing Control Platform), 786\nrdt (reliable data transfer protocol), 204\nbuilding, 206–215\npacket reordering, 229–230\npipelined, 215–218\nTCP (Transmission Control Protocol),\n204\nunreliable layer below, 204\nreal-time applications\ntiming, 92\nUDP (User Datagram Protocol), 200\nreal-time interactive applications\nprotocols, 623–632\nRTP (Real-Time Transport Protocol),\n623–626\nSIP, 626–632\nreal-time measurements of delay and loss\nperformance, 606\nReal-Time Streaming Protocol. See RTSP\nReal-Time Transport Protocol. See RTP\nreceive buffer, 233\nreceiver authentication, 706\nreceiver-based recovery, 621\nreceiver feedback, 208\nreceivers\nACK generation policy, 247\ndefining operation, 206\nsequence number of packet acknowl-\nedged by ACK message, 212\nreceiver-side transport layer, 54\nreceive window, 250–252\nreceive window field, 234\nreceiving adapter, 472\nreceiving processes addresses, 90\nrecords, inserting in DNS database, 142,\n144\nrecursive queries and DNS servers,\n137–138, 140\nRED (Random Early Detection)\nalgorithm, 329\nregional ISPs, 33\nregistrars, 142\nregistration with home agent, 568–569\nrelays, 622–623\nreliable channel, 204\nreliable data transfer, 91, 190\napplication layer, 204\nchannel with bit errors, 207–212\n848\nINDEX\n\nlink layer, 204\nlossy channel with bit errors, 212–215\nperfectly reliable channel, 206–207\nprinciples, 204–230\nreliable channel, 204\nTCP (Transmission Control Protocol),\n230–231, 240, 242–250\ntransport layer, 204\ntransport-layer protocols, 91\nreliable data transfer protocol. See rdt\nreliable data transfer service, 235\nreliable delivery, 436\nreliable transport service, 269\nremote host, transferring files, 116–118\nrendezvous point, 404\nrepeater, 474\nreplicated servers, 132\nreply messages and DNS (domain name\nsystem), 140–142\nrepositioning video, 600\nrequest messages and HTTP, 103–105\nrequest-response mode, 772\nrequests for comments. See RFCs\nRequest to Send control frame. See RTS\ncontrol frame\nresidential ISPs, 87\nresource-management cells. See RM cells\nresource records. See RRs\nresource reservation protocols, 362\nresources\nadmitting or blocking flows, 653\nefficient use of, 640\nreservations, 653–654\nresponse ARP, 467\nresponse messages and HTTP, 105–108\nretransmission, 208, 212\nretransmitting data, 241, 262\nretransmitting packets, 259, 261–263\nreverse path broadcast. See RPB\nreverse path forwarding. See RPF\nRexford, Jennifer, 786–787\nRFCs (requests for comments), 5\nRIP advertisements, 384–385\nRIP request message, 387\nRIP response message, 384\nRIP routers, 386–387\nRIP (Routing Information Protocol), 384,\n498\nhops, 384\nimplementation aspects, 386–388\nIP network-layer protocol, 387\nlower-tier ISPs, 388\nmodifying local routing table and prop-\nagating information, 387\nRIP messages, 384–385\nRIP table, 385–386\nrouting updates, 384\nUDP transport-layer protocol, 387\nUNIX implementation, 387–388\nRivest, Ron, 684, 690\nRM (resource-management cells),\n267–269\nRNC (Radio Network Controller), 552\nroaming number, 572\nRoberts, Larry, 61, 511\nroot DNS servers, 134–136\nround robin queuing discipline, 643–644\nround-trip delays, 43\nround-trip time. See RTT\nroute aggregation, 342\nroute attributes, 395\nrouter control plane functions, 322\nrouter discovery message, 566–567\nrouter forwarding plane, 321\nrouters, 4, 12, 22, 53, 303, 310\naccess control lists, 734\nadapters, 468\naddress of, 43\nadministrative autonomy, 380\narea border, 389\nARP modules, 468\nAS-PATH attribute, 394\nASs (autonomous systems), 380\nauthenticated and encrypted channel\nbetween, 725\nbuffering packet bits, 24\nbuffer sizing, 328–329\nconnected into network, 338\nconnection state information, 315\ncontrol functions, 321–322\nINDEX\n849\n\nrouters (continued)\ncontrol plane implemented in, 331\ndata center hierarchy, 492–493\ndefault, 364\ndestination, 364\nfinite buffers, 261–265\nfirewalls, 355, 481\nfirst-hop, 364\nfixed-length labels, 487\nforwarding function, 320–322\nforwarding table, 26, 308–309,\n317–318, 322–323, 394, 396–397,\n469\ngateway, 380–381\nimplementing layers 1 through 3, 53\nincident links, 22\ninput ports, 320\ninput processing, 322–324\ninterfaces, 338, 468\nintra-AS routing protocols, 397\nIP addresses, 394, 465, 468\nIP protocol, 53\nlabel-switched, 488\nlayer-2 packet switch, 480\nlink-layer and MAC addresses,\n462–463, 465\nlongest prefix matching rule, 318–319\nlookup, 323–324\nlooping advertisements, 394\nmemory access times, 324\nnetwork core, 4\nnetwork-layer addresses, 462, 465\noutput ports, 320–321\noutput processing, 326\npacket-forwarding decisions, 364\npacket loss, 327\npackets not cycling through, 481\nphysical links between, 364\nplug-and-play, 481\nprimary role, 306\nprocessing datagrams, 480\nprocessing packets, 351\nprotocols, 9\nqueuing, 327–331\nrouting control plane, 331\nrouting packets, 380–382\nrouting processor, 321\nrouting tables, 385–386\nscale, 379–380\nself-synchronizing, 371\nsource, 364\nspanning tree, 481\nstore-and-forward, 22, 24\nstore-and-forward packet switches, 480\nversus switches, 480–482\nswitching, 320, 324–326\nterminating incoming physical link, 320\nVC setup, 316\nroutes, 4, 394–396\nroute summarization, 342\nrouting, 305–306, 308–310\nadvertising information, 382–383\nbroadcast, 399–405\ncalls to mobile user, 571–572\ndistance vector, 384\nhierarchical, 379–383\nhot-potato, 382\nto mobile node, 559–564\nmulticast, 399, 405–412\nstoring information, 379–380\nrouting algorithms, 309, 363–383\nARPAnet, 366\ncircuit-switched, 379\ndecentralized, 366\nDV (distance-vector) algorithm, 366,\n371–379\ndynamic, 366\nforwarding tables, 364\nglobal, 365–366\nhierarchical routing, 379–383\nleast costly paths, 365\nload-sensitive, 366\nLS (link-state) algorithms, 366–371\npath from source to destination router,\n364\nscale of routers, 379–380\nstatic, 366\nswitches, 494–495\nviewing packet traffic flows, 379\nrouting control plane, 331\n850\nINDEX"
    },
    {
      "chunk_id": "7653be17-33be-4cf6-8dfc-ee412ffe0419",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "S",
      "original_titles": [
        "S"
      ],
      "path": "Index > S",
      "start_page": 878,
      "end_page": 881,
      "token_count": 1560,
      "text": "Routing Control Platform. See RCP\nrouting daemons, 674\nRouting Information Protocol. See RIP\nrouting loop, 377\nrouting protocols, 26–27, 51–52\nBGP (Border Gateway Protocol), \n390–399, 498–499\nDV (distance vector) algorithms,\n374–375\nexecuting, 321\ninter-AS, 382\nInternet, 383–399\nintra-AS, 380–381\nIS-IS, 384\nmessages, 309\nOSPF (Open-Shortest Path First), 384\nRIP (Routing Information Protocol),\n384\nRPB (reverse path broadcast), 402\nRPF (reverse path forwarding), 402–403,\n411\nRRs (resource records), 139–141\nRSA algorithm, 684–688, 710\nRST flag bit and segment 235, 258\nRSVP, RSVP-TE protocol, 489, 654\nRTP packets, 624–625\nRTP (Real-Time Transport Protocol), 588,\n623–626, 668\nUDP streaming, 595\nRTS/CTS exchange, 537\nRTS frame, 536–537\nRTSP (Real-Time Streaming Protocol),\n117, 595, 668\nRTS (Request to Send) control frame,\n535–537\nRTT (round-trip time), 102–103\nEWMA (exponential weighted \nmoving average), 240\nTCP (Transmission Control Protocol),\n238–241\nS\nSAD (Security Association Database), 721\nSA (security association), 720–721\nsatellite links, 16, 21–22\nscalability and P2P architecture, 145–148\nscheduling mechanisms, 640–645\nSchulzrinne, Henning, 623, 632, 668–670\nSDN (Software Defined Networking), 786\nsecure communication, 672–674\nsecure e-mail system, 706–708\nSecure Hash Algorithm. See SHA-1\nsecure networking protocols and \nmessage integrity, 689\nSecure Network Programming, 511\nSecure Sockets Layer. See SSL\nsecurity, 55–56\napplication-layer protocol, 705\nattacks, 674\ncryptography, 675–688\ndata link layer, 705\ndigital signatures, 688–699\ne-mail, 705–711\nend-point authentication, 700–705\nIEEE 802.11i, 728–731\nIP datagrams, 718\nIP (Internet Protocol), 362–363\nIPsec, 362\nmessage integrity, 688–693\nmobile IP, 566\nnetwork layer, 705, 718–725\nnetworks, 671–674\noperational, 673, 731–742\nOSPF (Open-Shortest Path First),\n388–389\nP2P architecture, 88\npublic key encryption, 683–688\nRSA, 687\nSNMPv3, 775–778\nswitches, 479\nTCP connection, 711–717\ntransport-layer protocols, 93, 705\ntransport services, 93\nuser-based, 777\nWEP (Wired Equivalent Privacy),\n726–728\nwireless LANs, 726–731\nsecurity and administration capabilities,\n765\nsecurity association. See SA\nINDEX\n851\n\nSecurity Association Database. See SAD\nSecurity Management, 759, 764\nSecurity Policy Database. See SPD\nsegments, 51, 186, 189\nacknowledgment number, 236\ndestination port number field, 192\nfast retransmit, 248\nfields, 191–192\nout-of-order, 236\npiggybacked acknowledgment, 237\nsequence numbers, 235–238\nsource port number field, 192\nTCP (Transmission Control Protocol),\n233\nunique identifiers, 192\nselective acknowledgment, 250\nselective repeat protocols. See SR\nprotocols\nself-learning, 478–479, 497, 542\nself-replicating, 56\nself-scalability, 87\nsend buffer, 232\nsender\ncountdown timer, 214\ndefining operation, 206\ndetecting and recovering from lost\npackets, 212–215\nleftmost state, 208\nreceive window, 250\nrightmost state, 208\nsending multiple packets without\nacknowledgments, 218\nsequence number of packet, 212\nutilization, 217\nsender authentication, 706–708\nsender-to-receiver channel, 213–214\nsending rates, 260\nsend side states rdt2.0 protocols, 208\nsequence-number-controlled flooding,\n401–403, 405\nsequence numbers, 210, 212, 218–220,\n234, 614–615, 618, 717\nIPsec, 724\nRTP packets, 625\nSSL (Secure Sockets Layer), 715\nSYN segment, 252–253\nTCP segments, 235–236\nTCP (Transmission Control Protocol),\n244, 249\nTelnet, 237–238\nserver authentication, 712\nserver processes, 88, 164, 232\nserver program, 156, 163\nservers, 2, 10–11, 88–89\nalways on, 86\ndedicated socket, 167\nhostname of, 160\nIP addresses, 86, 160, 161, 163\nnetwork attacks, 57–58\nnon-persistent connections, 198\npersistent HTTP, 198\nport number, 161, 167\nTCP socket creation, 167\nWeb caches as, 111\nserver SMTP, 122\nserver socket TCP connection, 163\nserver-to-client throughput, 44–45\nService Level Agreements. See SLAs\nservice model, 49\nservice providers and private \nnetworks, 66\nservices, 49\ndescription of Internet, 5–7\nDNS (domain name system), \n131–133\nflow of packets, 311\ntransport layer, 186\ntransport protocols, 189\nService Set Identifier. See SSID\nServing GPRS Support Nodes. See SGSNs\nsession encryption key, 714\nSession Initiation Protocol. See SIP\nsession keys, 687, 707, 714\nsession layer, 53\nSGMP (Simple Gateway Monitoring\nProtocol), 764\nSGSNs (Serving GPRS Support Nodes),\n552\nSHA, 710\nShamir, Adi, 684\n852\nINDEX\n\nShannon, Claude, 80, 82\nshared medium, 20\nSHA-1 (Secure Hash Algorithm), 691\nshortest paths, 365\nSIFS (Shorter Inter-frame Spacing), 532\nsignaling messages, 316\nsignaling protocols, 317\nsignal-to-noise ratio. See SNR\nsignature-based IDSs (intrusion detection\nsystems), 741–742\nsilent periods, 29–30\nsimple authentication, 389\nSimple Gateway Monitoring Protocol. See\nSGMP\nSimple Mail Transfer Protocol. See\nSMTP\nSimple Network Management Protocol.\nSee SNMP\nsingle-hop, wireless networks, 518\nSIP (Session Initiation Protocol), 588,\n626–632, 668–669\nSkype, 65, 83, 87, 588, 621–623\nconversational voice and voice, 592\nproprietary application-layer protocols,\n97\nUDP (User Datagram Protocol), 613\nSLAs (Service Level Agreements), 758\nsliding-window protocol, 220\nslotted ALOHA protocol, 450–452\nnode’s decision to transmit, 453–455\nsmall office, home office subnets. See\nSOHO subnets\nSMI (Structure of Management\nInformation), 765, 766–769\nSMTP clients, 122–123\nSMTP servers, 123\nSMTP (Simple Mail Transfer Protocol),\n51, 97, 117, 120–127\nSNMP applications, 776–777\nSNMP messages, 777\nSNMP (Simple Network Management\nProtocol), 758–759, 762, 764–778\nSNMPv3, 765, 775–778\nSNMPv2 (Simple Network Management\nProtocol version 2), 772, 773–775\nSnort IDS system, 740–742\nSNR (signal-to-noise ratio), 520–521\nsocial networking, 83, 86\nsocial networks, 64–65, 100\nsocket interface, 100\nsocket module, 160\nsocket programming\nTCP (Transmission Control Protocol),\n158, 163\nUDP, 157–158\nsockets, 89–91, 91, 191\nassigning port number, 162\nport number, 158\nsoft guarantee, 634\nsoft state protocols, 408–409\nsoftware control plane, 331\nSoftware Defined Networking. See SDN\nSOHO (small office, home office) subnets\nand IP addresses, 349–352\nsource\nhost and source router, 364\ntotal delay to destination, 42–44\nsource port numbers, 192, 194, 196, 234\nsource quench message, 353\nsource router, 364\nsource-specific congestion-control actions,\n267\nsource-specific multicast. See SSM\nspam, 56\nspanning-tree broadcast, 403–405\nspanning trees, 403–405, 481\nspatial redundancy, 589\nSPD (Security Policy Database), 724\nspecial socket server program, 163\nspeed-matching service, 250\nSPI (Security Parameter Index), 721\nsplit-connection approaches, 577\nSprint, 5, 33, 758\nspyware, 56\nSRAM, 324\nSR (selective repeat) protocols, \n223–230\nSSH protocol, 237\nSSID (Service Set Identifier), 529\nSSL record, 715–716\nINDEX\n853\n\nSSL (Secure Sockets Layer), 711\nanonymity, 738\nAPI (Application Programmer\nInterface) with sockets, 712\nblock ciphers, 678\nbreaking data stream into records, 714\nconnection closure, 717\ncryptographic algorithms, 716\ndata transfer, 713–715\ndesigned by Netscape, 711\nhandshake, 713–714, 716–717\nHTTP transactions security, 712\nkey derivation, 713–714\nnonces, 717\npopularity, 711\nprivacy, 738\npublic key certification, 697\nsequence numbers, 715\nSSL classes/libraries, 712\nSSL record, 715–716\ntransport protocols, 712\nSSM (source-specific multicast), 412\nstate, 117\nstateful packet filters, 732, 735–736\nstateless protocols, 100\nstatic routing algorithm, 366\nstations, 531–533\nstatus line in HTTP response messages,\n106\nsteaming prerecorded videos, 591\nstop-and-wait protocols, 209–210, 215,\n217\nstore-and-forward packet switches, 22, 24,\n480\nstream ciphers, 678\nstreaming, 591\nlive audio and video, 587, 593\nstored audio and video, 587, 591–592\nvideo, 589\nstreaming stored video, 593–612\nadaptive HTTP streaming, 593\nadaptive streaming, 600–601\nbandwidth, 594\nCDNs (content distribution \nnetworks), 602–608\nclient buffering, 594–595\ncontinuous playout, 591–592\nDASH (Dynamic Adaptive Streaming\nover HTTP), 600–601\nend-to-end delays, 594\nHTTP streaming, 593, 596–600\ninteractivity, 591\nKanKan, 611–612\nNetflix, 608–610\nstreaming, 591\nUDP streaming, 593, 595–596\nYouTube, 610–611\nstreaming video, 592\nTCP (Transmission Control Protocol),\n596\nStructure of Management Information. See\nSMI\nstub network, 397–398\nmulti-homed, 397\nsubnet mask, 340\nsubnets, 340\nadvertising existence to Internet, \n391\nclass A, B and C networks, 344\ndefining, 341\nDHCP offer message, 347\nDHCP servers, 346\nIP addresses, 340, 342, 345\nIP definition of, 340–341\nprefixes, 393\nsending datagrams off, 468–469\nshortest-path tree, 388\nsuccessful slots, 451\nswitched Ethernet, 470\nswitched-LANs\nARP (Address Resolution Protocol),\n465–468\nEthernet, 469–476\nlink-layer addressing, 462–469\nlink-layer switches, 476–482\nMAC addresses, 463–465\nswitch poisoning, 480\nVLANs (virtual local area networks),\n482–486\nswitched networks, 481\n854\nINDEX"
    },
    {
      "chunk_id": "20321d46-2194-4fc6-8adc-c2ce7270bfb2",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "T",
      "original_titles": [
        "T"
      ],
      "path": "Index > T",
      "start_page": 882,
      "end_page": 885,
      "token_count": 1515,
      "text": "switches, 80\naging time, 478\nbroadcasting frames, 464\nbroadcast storms, 481\ncollisions elimination, 479\ncongestion-related information, 268\ndata center hierarchy, 492–493\nenhanced security, 479\nEthernet, 470, 475\nfiltering, 476–477\nfiltering frame, 477\nforwarding, 476–477\ngathering statistics, 479\nheterogeneous links, 479\nhigh filtering and forwarding rates, 480\nlink-layer, 461, 476–482\nlink-layer addresses, 462\nlink-layer frames, 476\nMAC addresses, 480\nmanagement, 479\nplug-and-play devices, 479–480\nprocessing frames, 480\nversus routers, 480–482\nrouting algorithms, 494–495\nself-learning, 478–479, 497, 542\nsmall networks, 482\nstore-and-forward packet switches, \n480\nswitch table, 476\ntracking behavior of senders, 267\ntransparent, 476\ntrunk port to interconnect, 484\nVLANs (virtual local area \nnetworks), 483–484\nswitch fabric, 320, 322, 327, 329–330\nswitching and routers, 324–326\nswitch output interfaces buffers, 476\nswitch poisoning, 480\nswitch table, 476–477\nsymmetric algorithm, 716\nsymmetric key, 706–707, 707\nsymmetric key algorithm\nblock ciphers, 678–681\nCaesar cipher, 676\nmonoalphabetic cipher, 676–677\npolyalphabetic encryption, 678\nstream ciphers, 678\nsymmetric key encryption and CBC\n(cipher-block chaining), 681–682\nSYNACK segment, 257–258\nSYN bit, 235, 253\nSYN cookies, 257\nSYN flood attack, 252, 253, 257\nSYN packet, 258\nSYN segments, 252–254, 257–258\nSYN_SENT state, 254\nT\ntaking-turns protocols, 447, 459–460\nTCAMs (Ternary Content Address\nMemories), 324\nTCP buffers, 597–598\nTCPClient.py client program, 164–166\nTCP clients, 195, 253–255\nTCP congestion-control algorithm,\n272–277, 279\nTCP connections, 57, 94\nallocating buffers and variables, 253\nbandwidth, 281\nbottleneck link, 279–281\nbuffers, 233\nbetween client and server, 166\nclient process, 232\nclient-side TCP sending TCP\nsegment to server-side TCP,\n252–253\nclient socket, 163\nconnection-granted segment, 253\nending, 253–254\nestablishing, 232, 252–253, 713\nfull-duplex service, 232\nHTTP server, 596\nmanagement, 252–256, 258\nout-of-order segments, 236\npacket loss, 281\nparallel and fairness, 282\npoint-to-point, 232\nprocesses sending data, 232–233\nreceive buffer, 233, 250\nregulating rate of traffic, 190\nINDEX\n855\n\nTCP connections (continued)\nsecurity, 711–717\nsend buffer, 232\nserver process, 232\nserver socket, 163\nsocket connection to process, 233\nsplit-connection approaches, 577\nthree-way handshake, 102–103, 166,\n232\nthroughput, 280\ntransporting request message and\nresponse message, 101\nvariables, 233\nTCP header, 234–235\nTCP/IP (Transmission Control\nProtocol/Internet Protocol), 5, 63,\n93, 231, 431\nTCP ports, 258\nTCP Reno, 276, 278\nTCP segments, 233–236, 253\nwith different source IP addresses,\n194–195\nheader overhead, 200\nloss, 266\nreordering, 715\nstructure, 233–238\nTCP sender, 242–243, 269, 270\nawareness of wireless links, 577\ncongestion control, 250\nTCP server, 163, 195\nTCPServer.py server program, 166–168\nTCP sockets, 165–166, 497, 499\nserver-side connection socket, 163\nwelcoming socket, 163\nTCP splitting, 273\nTCP streaming and prefetching video, 597\nTCP SYNACK segment, 499\nTCP SYN segment, 499\nTCP Tahoe, 276\nTCP (Transmission Control Protocol), 5,\n51, 93, 189, 313, 338\nacknowledgment numbers, 244\nblock ciphers, 678\nbuffer and out-of-order segments, 249\nbuffer overflow, 251\nbyte stream, 242\nchecksum, 334\nclient-server application, 157\ncongestion avoidance, 272–276\ncongestion control, 95, 190, 199–200,\n240, 247, 269–272, 274–283,\n576–577, 596, 613\ncongestion window, 269–270,\n276–277, 576\nconnection-establishment delays, 200\nconnection-oriented, 94, 163, 230–238\nconnection state, 200, 231\ncontinued evolution of, 279\ncumulative acknowledgments, 236,\n243, 248–249\nduplicate ACK, 247–248\nearly versions, 62\nend-to-end congestion control, 266,\n269\nextending IP’s delivery service, 190\nfairness, 279–282\nfast retransmit, 247–248\nflow control, 240, 250–252\nfull-duplex, 235\nGBN (Go-Back-N) protocol, 248–250\nhigh-bandwidth paths, 279\nhost-based congestion control, 63\nHTTP and, 116, 200\nimplicit NAK mechanism, 240\nintegrity checking, 190\nInternet checksum, 442\nlost acknowledgment, 244\nlost segments, 238\nMSS (maximum segment size),\n232–234\nMTU (maximum transmission unit),\n232–233\nmultimedia applications, 200\nnegative acknowledgments, 248\npacket loss, 247–248, 613\npipelining, 240\npositive acknowledgments, 240\nreceive buffer, 270\nreceiver-so-sender ACK, 576\nreceive window, 251\n856\nINDEX\n\nreliable data transfer, 96, 190,\n230–231, 240\nreliable data transfer service, 95, 100,\n123, 163, 199–200, 235, 242–250\nresending segment until acknowledged,\n199\nretransmission timeout interval, 241\nretransmission timer, 242\nretransmitting data, 473\nretransmitting segments, 239–240, 246,\n249, 575–576\nRST segment, 258\nRTT (round-trip time) estimation,\n238–241\nsecurity services, 95\nsegments, 189\nselective acknowledgment, 250\nseparation of IP, 62\nsequence numbers, 244, 249\nserver-to-client transmission rate, 596\nservices, 94–95\nsocket programming, 158, 163\nstates, 254\nstate variable, 243\nsteady-state behavior, 278–279\nstreaming media, 200–201\nstreaming video, 596\nSYNACK segment, 258\nSYN segments, 257–258\nTCP Reno, 276, 278\nTCP segments, 233\nTCP Tahoe, 276\nTCP Vegas, 278\n32-bit sequence number, 220\nthree-way handshake, 163, 200, 253\nthroughput macroscopic description,\n278–279\ntimeout, 238–241, 243\ntimeout, 244–247\ntimeout/retransmit mechanism, 238\ntransmission rate, 278\nWeb servers, 197–198\nwindow size, 266\nwireless networks, 575–577\nTCP Vegas, 278\nTDM (time-division multiplexing), 28–30,\n31, 448, 549\ntelco (telephone company), 13–14\nTelenet, 62\ntelephone company. See telco\ntelephone networks, 27\ncircuit switching, 60\ncomplexity, 319\nfrequency band, 29\npacket switching, 31\nTelnet, 86\nblocked, 737\nsending message to mail server, 125\nSMTP server, 124\nTCP example, 234, 237–238\ntemporary IP address, 346\n10BASE-2, 473–474\n10BASE-T, 473–474\n10GBASE-T, 474–475\nTernary Content Address Memories. See\nTCAMs\n3GPP (3rd Generation Partnership\nProject), 550, 552, 362\nthird-party CDNs (Content Distribution\nNetworks), 603\n3DES, 680\n3G cellular data networks, 550–552\n3G cellular mobile systems versus wire-\nless LANs, 548\n3G core network, 550–552\n3G networks, 669\n3G radio access networks, 552\n3G systems, 547\n3G UMTS and DS-WCDMA (Direct\nSequence Wideband CDMA), 552\nthree-way handshake, 102–103, 232, 253,\n499, 735\nthroughput, 260\naverage, 44\nend-to-end, 44–47\nfluctuations in, 92\ninstantaneous, 44\nmacroscopic description for TCP,\n278–279\nserver-to-client, 44–45\nINDEX\n857\n\nthroughput (continued)\nstreaming video, 592\nTCP connection, 280\ntransmission rates of links, 47\ntransport-layer protocols, 92\nzero in heavy traffic, 265\ntier-1 ISPs, 33–34\ntime-division multiplexing. See TDM\ntimeout\ndoubling interval, 246–247\nevent, 222, 244\nlength of intervals, 238–239\nsetting and managing interval, 241\nTCP (Transmission Control Protocol),\n238–241, 243\ntimer management and overhead, 242\ntime-sensitive applications, 95\ntime-sharing networks, 62\ntime slots, 448\ntimestamps, 614–615, 617, 625\ntime-to-live field. See TTL (time-to-live)\nfield\ntiming guarantees, 92–93\nTLD (top-level domain) DNS servers,\n134–136, 143\nDNS servers, 134\nTLS (Transport Layer Security), 711\nTLV (Type, Length, Value) approach, 780\ntoken-passing protocol, 459–460\ntop-down approach, 50\ntop-level domain DNS servers. See TLD\nDNS servers\ntop-level domains, 135\nTop of Rack switch. See TOR switch\ntop-tier switch, 492\nTOR anonymizing and privacy service,\n738\ntorrents, 149\nTOR (Top of Rack) switch, 490, 492\nTOS (type of service) bits, 333\ntotal nodal delay, 36\nTraceroute program, 27, 353–355\nend-to-end delays, 42–43\ntracker, 149\ntraditional packet filters, 732–734\ntraffic\nbursty, 60\nconditioning, 648–649\nintensity, 40\ntraffic engineering, 489\ntraffic isolation, 638–640\ntraffic policing, 638–639\ntraffic profile, 650\ntransferring files, 116–118\ntransfer time, 45\nTransmission Control Protocol. See TCP\nTransmission Control Protocol/Internet\nProtocol.\ntransmission delays, 36–39\ntransmission rates, 4, 45–46\ntransmitting\nframes, 532\npackets in datagram networks, 317\ntransport layer, 51, 53, 185\napplication-layer message, 54\nautomatically assigning port number,\n193–194\nchecksumming, 442–443\ncongestion control, 266\nconnectionless service, 313\nconnection-oriented service, 313–314\ndatagram passed, 337\ndelivering data to socket, 191\ndemultiplexing, 191–198\ndestination host, 191\nerror checking, 203\nmultiplexing, 191–198\nmultiplexing/demultiplexing\nservice, 198–199\nnetwork layer relationship, 186–189\noverview, 189–191\nprocess-to-process communication,\n305, 313\nreliable data transfer, 204\nresponsibility of delivering data to\nappropriate application, 191\nsegments, 189\nservices, 186\ntransport-layer multiplexing, 192\ntransport-layer packets, 186\n858\nINDEX"
    },
    {
      "chunk_id": "bb73389c-079b-4103-a450-3f8d34f44c90",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "U",
      "original_titles": [
        "U"
      ],
      "path": "Index > U",
      "start_page": 886,
      "end_page": 886,
      "token_count": 374,
      "text": "transport-layer protocols, 50, 91\nend systems implementation, 186\nIP datagrams, 334\nliving in end systems, 188\nlogical communication between\nprocesses, 186, 188–189\nreliable data transfer, 91\nreliable delivery, 436\nsecurity, 93, 705\nTCP (Transmission Control Protocol),\n189\nthroughput, 92\ntiming, 92–93\nUDP (User Datagram Protocol), 189\nTransport Layer Security. See TLS\ntransport-layer segments, 54–55, 186\ndatagrams, 242\ndelivering data to correct socket,\n191–198\nfields, 191\nunreliability, 242\ntransport mode, 721\ntransport protocols\nInternet applications, 96\nservices, 189\nSSL (Secure Sockets Layer), 712\nTCP, 51\nUDP, 51\ntransport services\navailable to applications, 91–93\nconnection-oriented service, 94\nprovided by Internet, 93–96\nreliable data transfer, 91\nsecurity, 93\nTCP services, 94–95\nthroughput, 92\ntiming, 92–93\nUDP, 95\ntrap messages, 773\ntree-join messages, 404–405\ntriangle routing problem, 563\ntriple-DES, 710\ntruncation attack, 717\nTTL (time-to-live) field, 139–140, 334\ntunneling, 360–361, 561\ntunnel mode, 721–722\ntwisted-pair copper wire, 19–20, 475\nTwitter, 65, 83, 86\ntwo-dimensional parity scheme, 441–442\n2G cellular networks architecture,\n548–550\nType, Length, Value approach. See TLV\napproach\ntype of service bits. See TOS bits\nU\nUDP checksum, 202–204\nUDPClient.py client program, 158–161\nUDP header, 202\nUDP packet, 258, 346, 595\nUDP ports, 258\nUDP segments, 202–204, 495–497, 613\nUDPServer.py server program, 158, 161,\n194\nUDP sockets\ncommunicating to processes, 158\ncreation, 161\nidentifying, 194\nport numbers, 193–194\nUDP streaming, 593, 595–596\nUDP (User Datagram Protocol), 51, 93,\n189, 387\nchecksum, 208, 334\nclient-server application, 157\ncongestion control, 201, 282\nconnection establishment, 200\nconnectionless transport, 95, 198–204\nconnection state, 200\ndatagrams, 189\ndelays, 200\ndestination port number, 199\ndevelopment, 62\ndirectly talking with IP, 199\ndiscarding damaged segment, 204\nDNS and, 199–200\nend-to-end principle, 203\nend-to-end throughput, 95\nerror checking, 199\nerror detection, 202–204\nextending IP’s delivery service, 190\nINDEX\n859"
    },
    {
      "chunk_id": "b85c0e6d-2499-4ef4-a655-f149ae480047",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "V",
      "original_titles": [
        "V"
      ],
      "path": "Index > V",
      "start_page": 887,
      "end_page": 887,
      "token_count": 378,
      "text": "UDP (User Datagram Protocol) \n(continued)\nfairness, 282\nfiner application-level control over\ndata, 199\nflow control, 252\ngaps in data, 473\nhandshaking, 199\nheader overhead, 200\nintegrity checking, 190\nInternet checksum, 442\nInternet telephony applications, 96\nmultimedia applications, 200–201, \n282\nmultiplexing/demultiplexing\nfunction, 199\nnetwork management data, 200\nno-frills segment-delivery service, 199\npacket loss, 613\npassing damaged segment to \napplication, 204\nreal-time applications, 200\nreliable data transfer, 201\nRIP routing table updates, 200\nRTP and, 624\nsegments, 189\nsmall packet header overhead, 200\nsocket programming, 157–158\ntransport services, 95\nunreliability, 95, 190\nwireless networks, 575–577, 301\nUMTS (Universal Mobile\nTelecommunications Service) \n3G standards, 550\nunchoked, 150\nuncontrolled flooding, 401\nundetected bit errors, 440\nunguided media, 19\nunicast addresses, 356\nunicast applications and RTP packets, \n624\nunicast communication and IP addresses,\n406\nunidirectional data transfer, 205\nUniversal Plug and Play. See UPnP\nUNIX\nBSD (Berkeley Software Distribution)\nversion, 384\nnslookup program, 141–142\nRIP implemented in, 387–388\nSnort, 742\nunreliable data transfer, 206\nunreliable service, 190\nunshielded twisted pair. See UTP\nUPnP (Universal Plug and Play), 352\nurgent data pointer field, 235\nURL field, 104\nURLs, 99\nUS Department of Defense Advanced\nResearch Projects Agency. See\nDARPA\nuser agents, 119–121, 126–127\nuser-based security, 777\nuser-server interaction and HTTP\n(HyperText Transfer Protocol),\n108–110\nutilization, 217\nUTP (unshielded twisted pair), 19–20\nV\nVANET (vehicular ad hoc network), 518\nvariables and TCP connection, 233\nVC networks, 314–317, 319–320\nVC (virtual-circuit), 267, 314\nroots in telephony world, 319\nterminating, 316\nvehicular ad hoc network. See VANET\nVerizon, 758\nFIOS service and PONs (passive opti-\ncal networks), 15–16\nversion number, 333\nvideo, 588–589\nP2P delivery, 611\nprefetching, 596–597\nprerecorded, 591\nrepositioning, 600\nstreaming stored, 593–612\ntiming considerations and tolerance of\ndata loss, 592\ntraversing firewalls and NATs, 596\n860\nINDEX"
    },
    {
      "chunk_id": "52253824-a358-461a-a7ce-1c8b81f81437",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "W",
      "original_titles": [
        "W"
      ],
      "path": "Index > W",
      "start_page": 888,
      "end_page": 888,
      "token_count": 393,
      "text": "video conferencing, 83\nvideo over IP, 592–593\nvideo stream, 625\nvirtual-circuit. See VC (virtual-circuit)\nvirtual local area networks. See VLANs\nvirtual private networks. See VPNs\nviruses, 56, 740\nvisited MSC, 574\nvisited networks, 557, 570\nvisitor location register. See VLR\nVLANs (virtual local area networks),\n482–486\nVLAN tag, 484–486\nVLAN trunking, 484–485\nVLR (visitor location register), 570\nvoice and video applications, 83\nVoIP (Voice-over-IP), 83\nadaptive playout delay, 615–618\nend-to-end delay, 613–614\nenhancing over best-effort network, 612\nfixed playout delay, 615\njitter and audio, 614–618\nmedia packetization delays, 44\npacket loss, 613\nrecovering from packet loss, 618–621\nsequence numbers, 615\ntimestamps, 615\nwireless systems, 668\nVPNs (virtual private networks), 362\nconfidentiality, 720\nend points, 725\nIPsec, 718–720\nIPv4, 719\nMPLS (Multiprotocol Label\nSwitching), 489–490\nSA (security association), 720\ntunnel mode, 721\nvulnerability attacks, 57\nW\nWeb, 64, 86, 97\nclient-server application architecture,\n100\nHTTP (HyperText Transfer Protocol),\n98–100\nnetwork applications, 98–116\noperating on demand, 98\nplatform for applications emerging\nafter 2003, 98\nterminology, 98–99\nWeb applications, 97\nclient and server processes, 88\nclient-server architecture, 86\nWeb-based e-mail, 86, 129–130\nWeb browsers, 97\nclient side of HTTP, 99\nGUI interfaces, 64\nWeb caches, 59, 110–115\nWeb client-server interaction, 499\nweb of trust, 710\nWeb pages, 99\ndisplaying, 101\nrequests, 495–500\nWeb proxy caches, 104\nWeb servers, 89, 97\ndeleting objects, 105\ninitial versions, 64\nIP addresses, 392\nport numbers, 197–198\nserver processes, 88\nserver side of HTTP, 99\nspawning new process for connections,\n198\nTCP (Transmission Control Protocol),\n197–198\nuploading objects to, 105\nWeb sites, 108\nanonymity, 738\nprivacy, 738\nweighted fair queuing. See WFQ\nwell-known port number, 192\nWEP (Wired Equivalent Privacy), 726–728\nWFQ (weighted fair queuing), 329, 644–645\nleaky bucket, 647–648\nwide-area wireless access, 18\nWiFi, 17, 52, 526–546\nhigh-speed, 65\nhome networks, 17\nhotspots, 515, 546\npublic access, 515\nINDEX\n861"
    },
    {
      "chunk_id": "52329f5e-baac-4e95-9aa4-18f8be209ba4",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "X",
      "original_titles": [
        "X"
      ],
      "path": "Index > X",
      "start_page": 889,
      "end_page": 889,
      "token_count": 365,
      "text": "WiMAX (World Interoperability for\nMicrowave Access), 554, 668\nWindows\nnslookup program, 141–142\nSnort, 742\nWireshark packet sniffer, 78\nwindow size, 220\nwired-access ISPs tiered levels of \nservice, 636\nwired broadcast links, 521\nwired environments and packet sniffer,\n58–59\nWired Equivalent Privacy. See WEP\nwired link differences from wireless links,\n519\nwired networks, 519\nwireless, 513–514\nwireless communication links, 515–516\nwireless devices, 58–59\nwireless hosts, 514, 516–517, 530\nwireless LANs, 445\naccess point, 17\nLAN base stations, 548\nDHCP (Dynamic Host Configuration\nProtocol), 346\nversus 3G cellular mobile systems, 548\nIEEE 802.11 technology, 17\nsecurity, 726–731\nWiFi, 17\nwireless LANs and 802.11 standards, 526\nwireless links\nbit errors, 519\ndecreasing signal strength, 519\ndifferences from wired links, 519\nfading signal’s strength, 521–522\nhidden terminal problem, 521\ninterference from other sources, 519\nmultipath propagation, 519\nTCP sender awareness, 577\nundetectable collisions, 521–522\nwireless mesh networks, 518\nwireless networks, 513\napplication layer, 575\nbase station, 516–518\nCDMA (code division multiple access)\nprotocol, 522–526\ncharacteristics, 519–526\n802.11 wireless LANs, 526–546\nlink layer, 575\nlink rates, 515\nmobility, 575–577\nmulti-hop, infrastructure-based, 518\nmulti-hop, infrastructure-less, 518\nnetwork infrastructure, 518\nnetwork layer, 575\nsingle-hop, infrastructure-based, 518\nsingle-hop, infrastructure-less, 518\nTCP (Transmission Control Protocol),\n575–577\nUDP (User Datagram Protocol),\n575–577\nwireless communication links, 515–516\nwireless hosts, 514\nwireless personal area network. See WPAN\nWireless Philadelphia, 515\nwireless station, 529–530\nWireshark labs, 59, 78\nwork-conserving round robin discipline, 644\nworkload model, 635\nWorld Wide Web. See Web\nworms, 56–57, 740\nWPAN (wireless personal area network),\n544\nX\nX.25, 512\nXNS (Xerox Network Systems) \narchitecture, 384\nY\nYahoo!, 65, 86, 130\nYouTube, 65, 588, 610–611\nHTTP streaming (over TCP), 596\nstreaming stored video, 591\nvideo, 602\nZ\nZigbee, 545–546\n862\nINDEX"
    },
    {
      "chunk_id": "b094e461-2843-486f-81f6-c5f37d33027b",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Y",
      "original_titles": [
        "Y"
      ],
      "path": "Index > Y",
      "start_page": 889,
      "end_page": 889,
      "token_count": 365,
      "text": "WiMAX (World Interoperability for\nMicrowave Access), 554, 668\nWindows\nnslookup program, 141–142\nSnort, 742\nWireshark packet sniffer, 78\nwindow size, 220\nwired-access ISPs tiered levels of \nservice, 636\nwired broadcast links, 521\nwired environments and packet sniffer,\n58–59\nWired Equivalent Privacy. See WEP\nwired link differences from wireless links,\n519\nwired networks, 519\nwireless, 513–514\nwireless communication links, 515–516\nwireless devices, 58–59\nwireless hosts, 514, 516–517, 530\nwireless LANs, 445\naccess point, 17\nLAN base stations, 548\nDHCP (Dynamic Host Configuration\nProtocol), 346\nversus 3G cellular mobile systems, 548\nIEEE 802.11 technology, 17\nsecurity, 726–731\nWiFi, 17\nwireless LANs and 802.11 standards, 526\nwireless links\nbit errors, 519\ndecreasing signal strength, 519\ndifferences from wired links, 519\nfading signal’s strength, 521–522\nhidden terminal problem, 521\ninterference from other sources, 519\nmultipath propagation, 519\nTCP sender awareness, 577\nundetectable collisions, 521–522\nwireless mesh networks, 518\nwireless networks, 513\napplication layer, 575\nbase station, 516–518\nCDMA (code division multiple access)\nprotocol, 522–526\ncharacteristics, 519–526\n802.11 wireless LANs, 526–546\nlink layer, 575\nlink rates, 515\nmobility, 575–577\nmulti-hop, infrastructure-based, 518\nmulti-hop, infrastructure-less, 518\nnetwork infrastructure, 518\nnetwork layer, 575\nsingle-hop, infrastructure-based, 518\nsingle-hop, infrastructure-less, 518\nTCP (Transmission Control Protocol),\n575–577\nUDP (User Datagram Protocol),\n575–577\nwireless communication links, 515–516\nwireless hosts, 514\nwireless personal area network. See WPAN\nWireless Philadelphia, 515\nwireless station, 529–530\nWireshark labs, 59, 78\nwork-conserving round robin discipline, 644\nworkload model, 635\nWorld Wide Web. See Web\nworms, 56–57, 740\nWPAN (wireless personal area network),\n544\nX\nX.25, 512\nXNS (Xerox Network Systems) \narchitecture, 384\nY\nYahoo!, 65, 86, 130\nYouTube, 65, 588, 610–611\nHTTP streaming (over TCP), 596\nstreaming stored video, 591\nvideo, 602\nZ\nZigbee, 545–546\n862\nINDEX"
    },
    {
      "chunk_id": "52180b7e-fccf-4847-b3c3-74e471299ff4",
      "book_id": "47c60631-5da7-420a-9731-509c5c3d9fdd",
      "title": "Z",
      "original_titles": [
        "Z"
      ],
      "path": "Index > Z",
      "start_page": 889,
      "end_page": 889,
      "token_count": 365,
      "text": "WiMAX (World Interoperability for\nMicrowave Access), 554, 668\nWindows\nnslookup program, 141–142\nSnort, 742\nWireshark packet sniffer, 78\nwindow size, 220\nwired-access ISPs tiered levels of \nservice, 636\nwired broadcast links, 521\nwired environments and packet sniffer,\n58–59\nWired Equivalent Privacy. See WEP\nwired link differences from wireless links,\n519\nwired networks, 519\nwireless, 513–514\nwireless communication links, 515–516\nwireless devices, 58–59\nwireless hosts, 514, 516–517, 530\nwireless LANs, 445\naccess point, 17\nLAN base stations, 548\nDHCP (Dynamic Host Configuration\nProtocol), 346\nversus 3G cellular mobile systems, 548\nIEEE 802.11 technology, 17\nsecurity, 726–731\nWiFi, 17\nwireless LANs and 802.11 standards, 526\nwireless links\nbit errors, 519\ndecreasing signal strength, 519\ndifferences from wired links, 519\nfading signal’s strength, 521–522\nhidden terminal problem, 521\ninterference from other sources, 519\nmultipath propagation, 519\nTCP sender awareness, 577\nundetectable collisions, 521–522\nwireless mesh networks, 518\nwireless networks, 513\napplication layer, 575\nbase station, 516–518\nCDMA (code division multiple access)\nprotocol, 522–526\ncharacteristics, 519–526\n802.11 wireless LANs, 526–546\nlink layer, 575\nlink rates, 515\nmobility, 575–577\nmulti-hop, infrastructure-based, 518\nmulti-hop, infrastructure-less, 518\nnetwork infrastructure, 518\nnetwork layer, 575\nsingle-hop, infrastructure-based, 518\nsingle-hop, infrastructure-less, 518\nTCP (Transmission Control Protocol),\n575–577\nUDP (User Datagram Protocol),\n575–577\nwireless communication links, 515–516\nwireless hosts, 514\nwireless personal area network. See WPAN\nWireless Philadelphia, 515\nwireless station, 529–530\nWireshark labs, 59, 78\nwork-conserving round robin discipline, 644\nworkload model, 635\nWorld Wide Web. See Web\nworms, 56–57, 740\nWPAN (wireless personal area network),\n544\nX\nX.25, 512\nXNS (Xerox Network Systems) \narchitecture, 384\nY\nYahoo!, 65, 86, 130\nYouTube, 65, 588, 610–611\nHTTP streaming (over TCP), 596\nstreaming stored video, 591\nvideo, 602\nZ\nZigbee, 545–546\n862\nINDEX"
    }
  ]
}